The euro area crisis, often also referred to as the eurozone crisis, European debt crisis, or European sovereign debt crisis, was a multi-year debt and financial crisis that took place in the European Union (EU) from 2009 until the mid to late 2010s. Several eurozone member states (namely Greece, Italy, Portugal, Ireland, and Cyprus) were unable to repay or refinance their government debt or to bail out fragile banks under their national supervision without the assistance of other eurozone countries, the European Central Bank (ECB), or the International Monetary Fund (IMF).

The euro area crisis was caused by a sudden stop of the flow of foreign capital into countries that had substantial current account deficits and were dependent on foreign lending. The crisis was worsened by the inability of states to resort to devaluation (reductions in the value of the national currency) due to having the euro as a shared currency.[3][4] Debt accumulation in some eurozone members was in part due to macroeconomic differences among eurozone member states prior to the adoption of the euro. It also involved a process of cross-border financial contagion. The European Central Bank (ECB) adopted an interest rate that incentivized investors in Northern eurozone members to lend to the South, whereas the South was incentivized to borrow because interest rates were very low. Over time, this led to the accumulation of deficits in the South, primarily by private economic actors.[3][4] A lack of fiscal policy coordination among eurozone member states contributed to imbalanced capital flows in the eurozone,[3][4] while a lack of financial regulatory centralization or harmonization among eurozone member states, coupled with a lack of credible commitments to provide bailouts to banks, incentivized risky financial transactions by banks.[3][4] The detailed causes of the crisis varied from country to country. In several EU countries, private debts arising from real-estate bubbles were transferred to sovereign debt as a result of banking system bailouts and government responses to slowing economies post-bubble. European banks own a significant amount of sovereign debt, such that concerns regarding the solvency of banking systems or sovereigns are negatively reinforcing.[5]

The onset of crisis was in late 2009 when the Greek government disclosed that its budget deficits were far higher than previously thought.[3] Greece called for external help in early 2010, receiving an EU–IMF bailout package in May 2010.[3] European nations implemented a series of financial support measures such as the European Financial Stability Facility (EFSF) in early 2010 and the European Stability Mechanism (ESM) in late 2010. The ECB also contributed to solve the crisis by lowering interest rates and providing cheap loans of more than one trillion euro in order to maintain money flows between European banks. On 6 September 2012, the ECB calmed financial markets by announcing free unlimited support for all eurozone countries involved in a sovereign state bailout/precautionary programme from EFSF/ESM, through some yield lowering Outright Monetary Transactions (OMT).[6] Ireland and Portugal received EU-IMF bailouts In November 2010 and May 2011, respectively.[3] In March 2012, Greece received its second bailout. Cyprus also received rescue packages in June 2012.[3]

Return to economic growth and improved structural deficits enabled Ireland and Portugal to exit their bailout programmes in July 2014. Greece and Cyprus both managed to partly regain market access in 2014. Spain never officially received a bailout programme. Its rescue package from the ESM was earmarked for a bank recapitalisation fund and did not include financial support for the government itself. The crisis had significant adverse economic effects and labour market effects, with unemployment rates in Greece and Spain reaching 27%,[7] and was blamed for subdued economic growth, not only for the entire eurozone but for the entire European Union. The austerity policies implemented as a result of the crisis also produced a sharp rise in poverty levels and a significant increase in income inequality across Southern Europe.[8] It had a major political impact on the ruling governments in 10 out of 19 eurozone countries, contributing to power shifts in Greece, Ireland, France, Italy, Portugal, Spain, Slovenia, Slovakia, Belgium, and the Netherlands as well as outside of the eurozone in the United Kingdom.[9]

The eurozone crisis resulted from the structural problem of the eurozone and a combination of complex factors. There is a consensus that the root of the eurozone crisis lay in a balance-of-payments crisis (a sudden stop of foreign capital into countries that were dependent on foreign lending), and that this crisis was worsened by the fact that states could not resort to devaluation (reductions in the value of the national currency to make exports more competitive in foreign markets).[3][4] Other important factors include the globalisation of finance; easy credit conditions during the 2002–2008 period that encouraged high-risk lending and borrowing practices;[11] the 2008 financial crisis; international trade imbalances; real estate bubbles that have since burst; the Great Recession of 2008–2012; fiscal policy choices related to government revenues and expenses; and approaches used by states to bail out troubled banking industries and private bondholders, assuming private debt burdens or socializing losses.

Macroeconomic divergence among eurozone member states led to imbalanced capital flows between the member states. Despite different macroeconomic conditions, the European Central Bank could only adopt one interest rate, choosing one that meant that real interest rates in Germany were high (relative to inflation) and low in Southern eurozone member states. This incentivized investors in Germany to lend to the South, whereas the South was incentivized to borrow (because interest rates were very low). Over time, this led to the accumulation of deficits in the South, primarily by private economic actors.[3][4]

Comparative political economy explains the fundamental roots of the European crisis in varieties of national institutional structures of member countries (north vs. south), which conditioned their asymmetric development trends over time and made the union susceptible to external shocks. Imperfections in the Eurozone's governance construction to react effectively exacerbated macroeconomic divergence.[12]

Eurozone member states could have alleviated the imbalances in capital flows and debt accumulation in the South by coordinating national fiscal policies. Germany could have adopted more expansionary fiscal policies (to boost domestic demand and reduce the outflow of capital) and Southern eurozone member states could have adopted more restrictive fiscal policies (to curtail domestic demand and reduce borrowing from the North).[3][4] Per the requirements of the 1992 Maastricht Treaty, governments pledged to limit their deficit spending and debt levels. However, some of the signatories, including Germany and France, failed to stay within the confines of the Maastricht criteria and turned to securitising future government revenues to reduce their debts and/or deficits, sidestepping best practice and ignoring international standards.[13] This allowed the sovereigns to mask their deficit and debt levels through a combination of techniques, including inconsistent accounting, off-balance-sheet transactions, and the use of complex currency and credit derivatives structures.[13] From late 2009 on, after Greece's newly elected, PASOK government stopped masking its true indebtedness and budget deficit, fears of sovereign defaults in certain European states developed in the public, and the government debt of several states was downgraded. The crisis subsequently spread to Ireland and Portugal, while raising concerns about Italy, Spain, and the European banking system, and more fundamental imbalances within the eurozone.[14] The under-reporting was exposed through a revision of the forecast for the 2009 budget deficit from "6–8%" of GDP (no greater than 3% of GDP was a rule of the Maastricht Treaty) to 12.7%, almost immediately after PASOK won the October 2009 Greek national elections. Large upwards revision of budget deficit forecasts were not limited to Greece: for example, in the United States forecast for the 2009 budget deficit was raised from $407 billion projected in the 2009 fiscal year budget, to $1.4 trillion, while in the United Kingdom there was a final forecast more than 4 times higher than the original.[15][16] In Greece, the low ("6–8%") forecast was reported until very late in the year (September 2009), clearly not corresponding to the actual situation.

Fragmented financial regulation contributed to irresponsible lending in the years prior to the crisis. In the eurozone, each country had its own financial regulations, which allowed financial institutions to exploit gaps in monitoring and regulatory responsibility to resort to loans that were high-yield but very risky. Harmonization or centralization in financial regulations could have alleviated the problem of risky loans. Another factor that incentivized risky financial transaction was that national governments could not credibly commit not to bailout financial institutions who had undertaken risky loans, thus causing a moral hazard problem.[3][4] The Eurozone can incentivize overborrowing through a tragedy of the commons.[17]

The European debt crisis erupted in the wake of the Great Recession around late 2009, and was characterized by an environment of overly high government structural deficits and accelerating debt levels. When, as a negative repercussion of the Great Recession, the relatively fragile banking sector had suffered large capital losses, most states in Europe had to bail out several of their most affected banks with some supporting recapitalization loans, because of the strong linkage between their survival and the financial stability of the economy. As of January 2009, a group of 10 central and eastern European banks had already asked for a bailout.[19] At the time, the European Commission released a forecast of a 1.8% decline in EU economic output for 2009, making the outlook for the banks even worse.[19][20] The many public funded bank recapitalizations were one reason behind the sharply deteriorated debt-to-GDP ratios experienced by several European governments in the wake of the Great Recession. The main root causes for the four sovereign debt crises erupting in Europe were reportedly a mix of: weak actual and potential growth; competitive weakness; liquidation of banks and sovereigns; large pre-existing debt-to-GDP ratios; and considerable liability stocks (government, private, and non-private sector).[21]

In the first few weeks of 2010, there was renewed anxiety about excessive national debt, with lenders demanding ever-higher interest rates from several countries with higher debt levels, deficits, and current account deficits. This in turn made it difficult for four out of eighteen eurozone governments to finance further budget deficits and repay or refinance existing government debt, particularly when economic growth rates were low, and when a high percentage of debt was in the hands of foreign creditors, as in the case of Greece and Portugal.

The states that were adversely affected by the crisis faced a strong rise in interest rate spreads for government bonds as a result of investor concerns about their future debt sustainability. Four eurozone states had to be rescued by sovereign bailout programs, which were provided jointly by the International Monetary Fund and the European Commission, with additional support at the technical level from the European Central Bank. Together these three international organisations representing the bailout creditors became nicknamed "the Troika".

To fight the crisis some governments have focused on raising taxes and lowering expenditures, which contributed to social unrest and significant debate among economists, many of whom advocate greater deficits when economies are struggling. Especially in countries where budget deficits and sovereign debts have increased sharply, a crisis of confidence has emerged with the widening of bond yield spreads and risk insurance on CDS between these countries and other EU member states, most importantly Germany.[22] By the end of 2011, Germany was estimated to have made more than €9 billion out of the crisis as investors flocked to safer but near zero interest rate German federal government bonds (bunds).[23] By July 2012 also the Netherlands, Austria, and Finland benefited from zero or negative interest rates. Looking at short-term government bonds with a maturity of less than one year the list of beneficiaries also includes Belgium and France.[24] While Switzerland (and Denmark)[24] equally benefited from lower interest rates, the crisis also harmed its export sector due to a substantial influx of foreign capital and the resulting rise of the Swiss franc. In September 2011 the Swiss National Bank surprised currency traders by pledging that "it will no longer tolerate a euro-franc exchange rate below the minimum rate of 1.20 francs", effectively weakening the Swiss franc. This is the biggest Swiss intervention since 1978.[25]

Despite sovereign debt having risen substantially in only a few eurozone countries, with the three most affected countries Greece, Ireland and Portugal collectively only accounting for 6% of the eurozone's gross domestic product (GDP),[26] it became a perceived problem for the area as a whole,[27] leading to concerns about further contagion of other European countries and a possible break-up of the eurozone. In total, the debt crisis forced five out of 17 eurozone countries to seek help from other nations by the end of 2012.

In mid-2012, due to successful fiscal consolidation and implementation of structural reforms in the countries being most at risk and various policy measures taken by EU leaders and the ECB (see below), financial stability in the eurozone improved significantly and interest rates fell steadily. This also greatly diminished contagion risk for other eurozone countries. As of October 2012[update] only 3 out of 17 eurozone countries, namely Greece, Portugal, and Cyprus still battled with long-term interest rates above 6%.[28] By early January 2013, successful sovereign debt auctions across the eurozone but most importantly in Ireland, Spain, and Portugal, showed investors' confidence in the ECB backstop.[29] As of May 2014 only two countries (Greece and Cyprus) still needed help from third parties.[30]

The Greek economy had fared well for much of the 20th century, with high growth rates and low public debt.[31] By 2007 (i.e., before the 2008 financial crisis), it was still one of the fastest growing in the eurozone, with a public debt-to-GDP that did not exceed 104%,[31] but it was associated with a large structural deficit.[32] As the world economy was affected by the 2008 financial crisis, Greece was hit especially hard because its main industries—shipping and tourism—were especially sensitive to changes in the business cycle. The government spent heavily to keep the economy functioning and the country's debt increased accordingly.

The Greek crisis was triggered by the turmoil of the Great Recession, which led the budget deficits of several Western nations to reach or exceed 10% of GDP.[31] In the case of Greece, the high budget deficit (which, after several corrections, had been allowed to reach 10.2% and 15.1% of GDP in 2008 and 2009, respectively[33]) was coupled with a high public debt to GDP ratio (which, until then, was relatively stable for several years, at just above 100% of GDP, as calculated after all corrections).[31] Thus, the country appeared to lose control of its public debt to GDP ratio, which already reached 127% of GDP in 2009.[34] In contrast, Italy was able (despite the crisis) to keep its 2009 budget deficit at 5.1% of GDP,[33] which was crucial, given that it had a public debt to GDP ratio comparable to Greece's.[34] 
In addition, being a member of the Eurozone, Greece had essentially no autonomous monetary policy flexibility.[citation needed]

Finally, there was an effect of controversies about Greek statistics (due the aforementioned drastic budget deficit revisions which led to an increase in the calculated value of the Greek public debt by about 10%, a public debt-to-GDP ratio of about 100% until 2007), while there have been arguments about a possible effect of media reports. Consequently, Greece was "punished" by the markets which increased borrowing rates, making it impossible for the country to finance its debt since early 2010.

Despite the drastic upwards revision of the forecast for the 2009 budget deficit in October 2009, Greek borrowing rates initially rose rather slowly. By April 2010 it was apparent that the country was becoming unable to borrow from the markets; on 23 April 2010, the Greek government requested an initial loan of €45 billion from the EU and International Monetary Fund (IMF) to cover its financial needs for the remaining part of 2010.[35] A few days later Standard & Poor's slashed Greece's sovereign debt rating to BB+ or "junk" status amid fears of default,[36] in which case investors were liable to lose 30–50% of their money.[36] Stock markets worldwide and the euro currency declined in response to the downgrade.[37]

On 1 May 2010, the Greek government announced a series of austerity measures (the third austerity package within months)[38] to secure a three-year €110 billion loan (First Economic Adjustment Programme).[39] This was met with great anger by some Greeks, leading to massive protests, riots, and social unrest throughout Greece.[40] The Troika, a tripartite committee formed by the European Commission, the European Central Bank and the International Monetary Fund (EC, ECB and IMF), offered Greece a second bailout loan worth €130 billion in October 2011 (Second Economic Adjustment Programme), but with the activation being conditional on implementation of further austerity measures and a debt restructure agreement.[41] Surprisingly, the Greek prime minister George Papandreou first answered that call by announcing a December 2011 referendum on the new bailout plan,[42][43] but had to back down amidst strong pressure from EU partners, who threatened to withhold an overdue €6 billion loan payment that Greece needed by mid-December.[42][44] On 10 November 2011, Papandreou resigned following an agreement with the New Democracy party and the Popular Orthodox Rally to appoint non-MP technocrat Lucas Papademos as new prime minister of an interim national union government, with responsibility for implementing the needed austerity measures to pave the way for the second bailout loan.[45][46]

All the implemented austerity measures have helped Greece bring down its primary deficit—i.e., fiscal deficit before interest payments—from €24.7bn (10.6% of GDP) in 2009 to just €5.2bn (2.4% of GDP) in 2011,[47][48] but as a side-effect they also contributed to a worsening of the Greek recession, which began in October 2008 and only became worse in 2010 and 2011.[49] The Greek GDP had its worst decline in 2011 with −6.9%,[50] a year where the seasonal adjusted industrial output ended 28.4% lower than in 2005,[51][52] and with 111,000 Greek companies going bankrupt (27% higher than in 2010).[53][54] As a result, Greeks have lost about 40% of their purchasing power since the start of the crisis,[55] they spend 40% less on goods and services,[56] and the seasonal adjusted unemployment rate grew from 7.5% in September 2008 to a record high of 27.9% in June 2013,[57] while the youth unemployment rate rose from 22.0% to as high as 62%.[58][59] Youth unemployment ratio hit 16.1 per cent in 2012.[60][61][62]

Overall the share of the population living at "risk of poverty or social exclusion" did not increase notably during the first two years of the crisis. The figure was measured to 27.6% in 2009 and 27.7% in 2010 (only being slightly worse than the EU27-average at 23.4%),[63] but for 2011 the figure was now estimated to have risen sharply above 33%.[64] In February 2012, an IMF official negotiating Greek austerity measures admitted that excessive spending cuts were harming Greece.[47] The IMF predicted the Greek economy to contract by 5.5% by 2014. Harsh austerity measures led to an actual contraction after six years of recession of 17%.[65]

Some economic experts argue that the best option for Greece, and the rest of the EU, would be to engineer an "orderly default", allowing Athens to withdraw simultaneously from the eurozone and reintroduce its national currency the drachma at a debased rate.[66][67] If Greece were to leave the euro, the economic and political consequences would be devastating. According to Japanese financial company Nomura an exit would lead to a 60% devaluation of the new drachma. Analysts at French bank BNP Paribas added that the fallout from a Greek exit would wipe 20% off Greece's GDP, increase Greece's debt-to-GDP ratio to over 200%, and send inflation soaring to 40–50%.[68] Also UBS warned of hyperinflation, a bank run and even "military coups and possible civil war that could afflict a departing country".[69][70] Eurozone National Central Banks (NCBs) may lose up to €100bn in debt claims against the Greek national bank through the ECB's TARGET2 system. The Deutsche Bundesbank alone may have to write off €27bn.[71]

To prevent this from happening, the Troika (EC, IMF and ECB) eventually agreed in February 2012 to provide a second bailout package worth €130 billion,[72] conditional on the implementation of another harsh austerity package that would reduce Greek expenditure by €3.3bn in 2012 and another €10bn in 2013 and 2014.[48]

Then, in March 2012, the Greek government did finally default on parts of its debt - as there was a new law passed by the government so that private holders of Greek government bonds (banks, insurers and investment funds) would "voluntarily" accept a bond swap with a 53.5% nominal write-off, partly in short-term EFSF notes, partly in new Greek bonds with lower interest rates and the maturity prolonged to 11–30 years (independently of the previous maturity).[73] This counted as a "credit event" and holders of credit default swaps were paid accordingly.[74] It was the world's biggest debt restructuring deal ever done, affecting some €206 billion of Greek government bonds.[75] The debt write-off had a size of €107 billion, and caused the Greek debt level to temporarily fall from roughly €350bn to €240bn in March 2012 (it would subsequently rise again, due to the resulting bank recapitalization needs), with improved predictions about the debt burden.[76][77][78][79] In December 2012, the Greek government bought back €21 billion ($27 billion) of their bonds for 33 cents on the euro.[80]

Critics such as the director of LSE's Hellenic Observatory[81] argue that the billions of taxpayer euros are not saving Greece but financial institutions.[82] Of all €252bn in bailouts between 2010 and 2015, just 10% has found its way into financing continued public deficit spending on the Greek government accounts. Much of the rest went straight into refinancing the old stock of Greek government debt (originating mainly from the high general government deficits being run in previous years), which was mainly held by private banks and hedge funds by the end of 2009.[83] According to LSE, "more than 80% of the rescue package" is going to refinance the expensive old maturing Greek government debt towards private creditors (mainly private banks outside Greece), replacing it with new debt to public creditors on more favourable terms, that is to say paying out their private creditors with new debt issued by its new group of public creditors known as the Troika.[84]

The shift in liabilities from European banks to European taxpayers has been staggering. One study found that the public debt of Greece to foreign governments, including debt to the EU/IMF loan facility and debt through the Eurosystem, increased from €47.8bn to €180.5bn (+132,7bn) between January 2010 and September 2011,[85] while the combined exposure of foreign banks to (public and private) Greek entities was reduced from well over €200bn in 2009 to around €80bn (−€120bn) by mid-February 2012.[86] As of 2015[update], 78% of Greek debt is owed to public sector institutions, primarily the EU.[83] According to a study by the European School of Management and Technology only €9.7bn or less than 5% of the first two bailout programs went to the Greek fiscal budget, while most of the money went to French and German banks[87] (In June 2010, France's and Germany's foreign claims vis-a-vis Greece were $57bn and $31bn respectively. German banks owned $60bn of Greek, Portuguese, Irish and Spanish government debt and $151bn of banks' debt of these countries).[88]

According to a leaked document, dated May 2010, the IMF was fully aware of the fact that the Greek bailout program was aimed at rescuing the private European banks – mainly from France and Germany. A number of IMF Executive Board members from India, Brazil, Argentina, Russia, and Switzerland criticized this in an internal memorandum, pointing out that Greek debt would be unsustainable. However their French, German and Dutch colleagues refused to reduce the Greek debt or to make (their) private banks pay.[89][90]

In mid May 2012, the crisis and impossibility to form a new government after elections and the possible victory by the anti-austerity axis led to new speculations Greece would have to leave the eurozone shortly.[91][92][93] This phenomenon became known as "Grexit" and started to govern international market behaviour.[94][95][96] The centre-right's narrow victory in 17 June election gave hope that Greece would honour its obligations and stay in the Euro-zone.

Due to a delayed reform schedule and a worsened economic recession, the new government immediately asked the Troika to be granted an extended deadline from 2015 to 2017 before being required to restore the budget into a self-financed situation; which in effect was equal to a request of a third bailout package for 2015–16 worth €32.6bn of extra loans.[97][98] On 11 November 2012, facing a default by the end of November, the Greek parliament passed a new austerity package worth €18.8bn,[99] including a "labour market reform" and "mid term fiscal plan 2013–16".[100][101] In return, the Eurogroup agreed on the following day to lower interest rates and prolong debt maturities and to provide Greece with additional funds of around €10bn for a debt-buy-back programme. The latter allowed Greece to retire about half of the €62 billion in debt that Athens owes private creditors, thereby shaving roughly €20 billion off that debt. This should bring Greece's debt-to-GDP ratio down to 124% by 2020 and well below 110% two years later.[102] Without agreement the debt-to-GDP ratio would have risen to 188% in 2013.[103]

The Financial Times special report on the future of the European Union argues that the liberalisation of labour markets has allowed Greece to narrow the cost-competitiveness gap with other southern eurozone countries by approximately 50% over the past two years.[104] This has been achieved primary through wage reductions, though businesses have reacted positively.[104] The opening of product and service markets is proving tough because interest groups are slowing reforms.[104] The biggest challenge for Greece is to overhaul the tax administration with a significant part of annually assessed taxes not paid.[104] Poul Thomsen, the IMF official who heads the bailout mission in Greece, stated that "in structural terms, Greece is more than halfway there".[104]

In June 2013, Equity index provider MSCI reclassified Greece as an emerging market, citing failure to qualify on several criteria for market accessibility.[105]

Both of the latest bailout programme audit reports, released independently by the European Commission and IMF in June 2014, revealed that even after transfer of the scheduled bailout funds and full implementation of the agreed adjustment package in 2012, there was a new forecast financing gap of: €5.6bn in 2014, €12.3bn in 2015, and €0bn in 2016. The new forecast financing gaps will need either to be covered by the government's additional lending from private capital markets, or to be countered by additional fiscal improvements through expenditure reductions, revenue hikes or increased amount of privatizations.[106][107] Due to an improved outlook for the Greek economy, with return of a government structural surplus in 2012, return of real GDP growth in 2014, and a decline of the unemployment rate in 2015,[108] it was possible for the Greek government to return to the bond market during the course of 2014, for the purpose of fully funding its new extra financing gaps with additional private capital. A total of €6.1bn was received from the sale of three-year and five-year bonds in 2014, and the Greek government now plans to cover its forecast financing gap for 2015 with additional sales of seven-year and ten-year bonds in 2015.[109]

The latest recalculation of the seasonally adjusted quarterly GDP figures for the Greek economy revealed that it had been hit by three distinct recessions in the turmoil of the 2008 financial crisis:[110]

Greece experienced positive economic growth in each of the three first quarters of 2014.[110] The return of economic growth, along with the now existing underlying structural budget surplus of the general government, build the basis for the debt-to-GDP ratio to start a significant decline in the coming years ahead,[111] which will help ensure that Greece will be labelled "debt sustainable" and fully regain complete access to private lending markets in 2015.[a] While the Greek government-debt crisis hereby is forecast officially to end in 2015, many of its negative repercussions (e.g. a high unemployment rate) are forecast still to be felt during many of the subsequent years.[111]

During the second half of 2014, the Greek government again negotiated with the Troika. The negotiations were this time about how to comply with the programme requirements, to ensure activation of the payment of its last scheduled eurozone bailout tranche in December 2014, and about a potential update of its remaining bailout programme for 2015–16. When calculating the impact of the 2015 fiscal budget presented by the Greek government, there was a disagreement, with the calculations of the Greek government showing it fully complied with the goals of its agreed "Midterm fiscal plan 2013–16", while the Troika calculations were less optimistic and returned a not covered financing gap at €2.5bn (being required to be covered by additional austerity measures). As the Greek government insisted their calculations were more accurate than those presented by the Troika, they submitted an unchanged fiscal budget bill on 21 November, to be voted for by the parliament on 7 December. The Eurogroup was scheduled to meet and discuss the updated review of the Greek bailout programme on 8 December (to be published on the same day), and the potential adjustments to the remaining programme for 2015–16. There were rumours in the press that the Greek government has proposed immediately to end the previously agreed and continuing IMF bailout programme for 2015–16, replacing it with the transfer of €11bn unused bank recapitalization funds currently held as reserve by the Hellenic Financial Stability Fund (HFSF), along with establishment of a new precautionary Enhanced Conditions Credit Line (ECCL) issued by the European Stability Mechanism. The ECCL instrument is often used as a follow-up precautionary measure, when a state has exited its sovereign bailout programme, with transfers only taking place if adverse financial/economic circumstances materialize, but with the positive effect that it help calm down financial markets as the presence of this extra backup guarantee mechanism makes the environment safer for investors.[114]

The positive economic outlook for Greece—based on the return of seasonally adjusted real GDP growth across the first three quarters of 2014—was replaced by a new fourth recession starting in Q4-2014.[115] This new fourth recession was widely assessed as being direct related to the premature snap parliamentary election called by the Greek parliament in December 2014 and the following formation of a Syriza-led government refusing to accept respecting the terms of its current bailout agreement. The rising political uncertainty of what would follow caused the Troika to suspend all scheduled remaining aid to Greece under its second programme, until such time as the Greek government either accepted the previously negotiated conditional payment terms or alternatively could reach a mutually accepted agreement of some new updated terms with its public creditors.[116] This rift caused a renewed increasingly growing liquidity crisis (both for the Greek government and Greek financial system), resulting in plummeting stock prices at the Athens Stock Exchange while interest rates for the Greek government at the private lending market spiked to levels once again making it inaccessible as an alternative funding source.

Faced by the threat of a sovereign default and potential resulting exit of the eurozone, some final attempts were made by the Greek government in May 2015 to settle an agreement with the Troika about some adjusted terms for Greece to comply with in order to activate the transfer of the frozen bailout funds in its second programme. In the process, the Eurogroup granted a six-month technical extension of its second bailout programme to Greece.

On 5 July 2015, the citizens of Greece voted decisively (a 61% to 39% decision with 62.5% voter turnout) to reject a referendum that would have given Greece more bailout help from other EU members in return for increased austerity measures. As a result of this vote, Greece's finance minister Yanis Varoufakis stepped down on 6 July. Greece was the first developed country not to make a payment to the IMF on time, in 2015 (payment was made with a 20-day delay[117][118]). Eventually, Greece agreed on a third bailout package in August 2015.

Between 2009 and 2017 the Greek government debt rose from €300 bn to €318 bn, i.e. by only about 6% (thanks, in part, to the 2012 debt restructuring);[34][119] however, during the same period, the critical debt-to-GDP ratio shot up from 127% to 179%[34] basically due to the severe GDP drop during the handling of the crisis.[31]

Greece's bailouts successfully ended (as declared) on 20 August 2018.[120]

The Irish sovereign debt crisis arose not from government over-spending, but from the state guaranteeing the six main Irish-based banks who had financed a property bubble. On 29 September 2008, Finance Minister Brian Lenihan Jnr issued a two-year guarantee to the banks' depositors and bondholders.[121] The guarantees were subsequently renewed for new deposits and bonds in a slightly different manner. In 2009, a National Asset Management Agency (NAMA) was created to acquire large property-related loans from the six banks at a market-related "long-term economic value".[122]

Irish banks had lost an estimated 100 billion euros, much of it related to defaulted loans to property developers and homeowners made in the midst of the property bubble, which burst around 2007. The economy collapsed during 2008. Unemployment rose from 4% in 2006 to 14% by 2010, while the national budget went from a surplus in 2007 to a deficit of 32% GDP in 2010, the highest in the history of the eurozone, despite austerity measures.[123][124]

With Ireland's credit rating falling rapidly in the face of mounting estimates of the banking losses, guaranteed depositors and bondholders cashed in during 2009–10, and especially after August 2010. (The necessary funds were borrowed from the central bank.) With yields on Irish Government debt rising rapidly, it was clear that the Government would have to seek assistance from the EU and IMF, resulting in a €67.5 billion "bailout" agreement of 29 November 2010.[125] Together with additional €17.5 billion coming from Ireland's own reserves and pensions, the government received €85 billion,[126] of which up to €34 billion was to be used to support the country's failing financial sector (only about half of this was used in that way following stress tests conducted in 2011).[127] In return the government agreed to reduce its budget deficit to below three per cent by 2015.[127] In April 2011, despite all the measures taken, Moody's downgraded the banks' debt to junk status.[128]

In July 2011, European leaders agreed to cut the interest rate that Ireland was paying on its EU/IMF bailout loan from around 6% to between 3.5% and 4% and to double the loan time to 15 years. The move was expected to save the country between 600 and 700 million euros per year.[129] On 14 September 2011, in a move to further ease Ireland's difficult financial situation, the European Commission announced it would cut the interest rate on its €22.5 billion loan coming from the European Financial Stability Mechanism, down to 2.59 per cent—which is the interest rate the EU itself pays to borrow from financial markets.[130]

The Euro Plus Monitor report from November 2011 attests to Ireland's vast progress in dealing with its financial crisis, expecting the country to stand on its own feet again and finance itself without any external support from the second half of 2012 onwards.[131] According to the Centre for Economics and Business Research, Ireland's export-led recovery "will gradually pull its economy out of its trough".
As a result of the improved economic outlook, the cost of 10-year government bonds has fallen from its record high at 12% in mid July 2011 to below 4% in 2013 (see the graph "Long-term Interest Rates").

On 26 July 2012, for the first time since September 2010, Ireland was able to return to the financial markets, selling over €5 billion in long-term government debt, with an interest rate of 5.9% for the 5-year bonds and 6.1% for the 8-year bonds at sale.[132] In December 2013, after three years on financial life support, Ireland finally left the EU/IMF bailout programme, although it retained a debt of €22.5 billion to the IMF; in August 2014, early repayment of €15 billion was being considered, which would save the country €375 million in surcharges.[133] Despite the end of the bailout the country's unemployment rate remains high and public sector wages are still around 20% lower than at the beginning of the crisis.[134] Government debt reached 123.7% of GDP in 2013.[135]

On 13 March 2013, Ireland managed to regain complete lending access on financial markets, when it successfully issued €5bn of 10-year maturity bonds at a yield of 4.3%.[136]
Ireland ended its bailout programme as scheduled in December 2013, without any need for additional financial support.[113]

Unlike other European countries that were also severely hit by the Great Recession in the late 2000s and eventually received bailouts in the early 2010s (such as Greece and Ireland), Portugal had the characteristic that the 2000s were not marked by economic growth, but were already a period of economic crisis, marked by stagnation, two recessions (in 2002–03[137] and 2008–09[138]) and government-sponsored fiscal austerity in order to reduce the budget deficit to the limits allowed by the European Union's Stability and Growth Pact.[139][140][141]

According to a report by the Diário de Notícias,[142] Portugal had allowed considerable slippage in state-managed public works and inflated top management and head officer bonuses and wages in the period between the Carnation Revolution in 1974 and 2010. Persistent and lasting recruitment policies boosted the number of redundant public servants. Risky credit, public debt creation, and European structural and cohesion funds were mismanaged across almost four decades.[143] When the global crisis disrupted the markets and the world economy, together with the US subprime mortgage crisis and the eurozone crisis, Portugal was one of the first economies to succumb, and was affected very deeply.

In the summer of 2010, Moody's Investors Service cut Portugal's sovereign bond rating,[144] which led to an increased pressure on Portuguese government bonds.[145] In the first half of 2011, Portugal requested a €78 billion IMF-EU bailout package in a bid to stabilise its public finances.[146]

Portugal's debt was in September 2012 forecast by the Troika to peak at around 124% of GDP in 2014, followed by a firm downward trajectory after 2014. Previously the Troika had predicted it would peak at 118.5% of GDP in 2013, so the developments proved to be a bit worse than first anticipated, but the situation was described as fully sustainable and progressing well. As a result, from the slightly worse economic circumstances, the country has been given one more year to reduce the budget deficit to a level below 3% of GDP, moving the target year from 2013 to 2014. The budget deficit for 2012 has been forecast to end at 5%. The recession in the economy is now also projected to last until 2013, with GDP declining 3% in 2012 and 1% in 2013; followed by a return to positive real growth in 2014.[147] Unemployment rate increased to over 17% by end of 2012 but it has since decreased gradually to 10,5% as of November 2016.[148]

As part of the bailout programme, Portugal was required to regain complete access to financial markets by September 2013. The first step towards this target was successfully taken on 3 October 2012, when the country managed to regain partial market access by selling a bond series with 3-year maturity. Once Portugal regains complete market access, measured as the moment it successfully manages to sell a bond series with a full 10-year maturity, it is expected to benefit from interventions by the ECB, which announced readiness to implement extended support in the form of some yield-lowering bond purchases (OMTs),[147] aiming to bring governmental interest rates down to sustainable levels. A peak for the Portuguese 10-year governmental interest rates happened on 30 January 2012, where it reached 17.3% after the rating agencies had cut the governments credit rating to "non-investment grade" (also referred to as "junk").[149] As of December 2012, it has been more than halved to only 7%.[citation needed] A successful return to the long-term lending market was made by the issuing of a 5-year maturity bond series in January 2013,[150] and the state regained complete lending access when it successfully issued a 10-year maturity bond series on 7 May 2013.[113][151]

According to the Financial Times special report on the future of the European Union, the Portuguese government has "made progress in reforming labour legislation, cutting previously generous redundancy payments by more than half and freeing smaller employers from collective bargaining obligations, all components of Portugal's €78 billion bailout program".[104] Additionally, unit labour costs have fallen since 2009, working practices are liberalizing, and industrial licensing is being streamlined.[104]

On 18 May 2014, Portugal left the EU bailout mechanism without additional need for support,[30] as it had already regained a complete access to lending markets back in May 2013,[113] and with its latest issuing of a 10-year government bond being successfully completed with a rate as low as 3.59%.[152] Portugal still has many tough years ahead. During the crisis, Portugal's government debt increased from 93 to 139 percent of GDP.[152] On 3 August 2014, Banco de Portugal announced the country's second biggest bank Banco Espírito Santo would be split in two after losing the equivalent of $4.8 billion in the first 6 months of 2014, sending its shares down by 89 percent.

Spain had a comparatively low debt level among advanced economies prior to the crisis.[153] Its public debt relative to GDP in 2010 was only 60%, more than 20 points less than Germany, France or the US, and more than 60 points less than Italy or Greece.[154][155] Debt was largely avoided by the ballooning tax revenue from the housing bubble, which helped accommodate a decade of increased government spending without debt accumulation.[156] When the bubble burst, Spain spent large amounts of money on bank bailouts. In May 2012, Bankia received a 19 billion euro bailout,[157] on top of the previous 4.5 billion euros to prop up Bankia.[158] Questionable accounting methods disguised bank losses.[159] During September 2012, regulators indicated that Spanish banks required €59 billion (US$77 billion) in additional capital to offset losses from real estate investments.[160]

The bank bailouts and the economic downturn increased the country's deficit and debt levels and led to a substantial downgrading of its credit rating. To build up trust in the financial markets, the government began to introduce austerity measures and in 2011 it passed a law in congress to approve an amendment to the Spanish Constitution to require a balanced budget at both the national and regional level by 2020.[161] The amendment states that public debt can not exceed 60% of GDP, though exceptions would be made in case of a natural catastrophe, economic recession or other emergencies.[162][163] As one of the largest eurozone economies (larger than Greece, Portugal and Ireland combined[164]) the condition of Spain's economy is of particular concern to international observers. Under pressure from the United States, the IMF, other European countries and the European Commission[165][166] the Spanish governments eventually succeeded in trimming the deficit from 11.2% of GDP in 2009 to 7.1% in 2013.[167]

Nevertheless, in June 2012, Spain became a prime concern for the Euro-zone[168] when interest on Spain's 10-year bonds reached the 7% level and it faced difficulty in accessing bond markets. This led the Eurogroup on 9 June 2012 to grant Spain a financial support package of up to €100 billion.[169] The funds will not go directly to Spanish banks, but be transferred to a government-owned Spanish fund responsible to conduct the needed bank recapitalisations (FROB), and thus it will be counted for as additional sovereign debt in Spain's national account.[170][171][172] An economic forecast in June 2012 highlighted the need for the arranged bank recapitalisation support package, as the outlook promised a negative growth rate of 1.7%, unemployment rising to 25%, and a continued declining trend for housing prices.[164] In September 2012 the ECB removed some of the pressure from Spain on financial markets, when it announced its "unlimited bond-buying plan", to be initiated if Spain would sign a new sovereign bailout package with EFSF/ESM.[173][174] Strictly speaking, Spain was not hit by a sovereign debt-crisis in 2012, as the financial support package that they received from the ESM was earmarked for a bank recapitalization fund and did not include financial support for the government itself.

According to the latest debt sustainability analysis published by the European Commission in October 2012, the fiscal outlook for Spain, if assuming the country will stick to the fiscal consolidation path and targets outlined by the country's current EDP programme, will result in a debt-to-GDP ratio reaching its maximum at 110% in 2018—followed by a declining trend in subsequent years. In regards of the structural deficit the same outlook has promised, that it will gradually decline to comply with the maximum 0.5% level required by the Fiscal Compact in 2022/2027.[175]

Though Spain was suffering with 27% unemployment and the economy was shrinking 1.4% in 2013, Mariano Rajoy's conservative government has pledged to speed up reforms, according to the Financial Times special report on the future of the European Union.[176] "Madrid is reviewing its labour market and pension reforms and has promised by the end of this year to liberalize its heavily regulated professions".[104] But Spain is benefiting from improved labour cost competitiveness.[104] "They have not lost export market share," says Eric Chaney, chief economist at Axa.[104] "If credit starts flowing again, Spain could surprise us".[104]

On 23 January 2014, as foreign investor confidence in the country has been restored, Spain formally exited the EU/IMF bailout mechanism.[177] By the end of March 2018, unemployment rate of Spain has fallen to 16.1%[178] and the debt is 98,30% of the GDP.[179]

The economy of the small island of Cyprus with 840,000 people was hit by several huge blows in and around 2012 including, amongst other things, the €22 billion exposure of Cypriot banks to the Greek debt haircut, the downgrading of the Cypriot economy into junk status by international rating agencies and the inability of the government to refund its state expenses.[180]

On 25 June 2012, the Cypriot Government requested a bailout from the European Financial Stability Facility or the European Stability Mechanism, citing difficulties in supporting its banking sector from the exposure to the Greek debt haircut.[181]

On 30 November the Troika (the European Commission, the International Monetary Fund, and the European Central Bank) and the Cypriot Government had agreed on the bailout terms with only the amount of money required for the bailout remaining to be agreed upon.[182] Bailout terms include strong austerity measures, including cuts in civil service salaries, social benefits, allowances and pensions and increases in VAT, tobacco, alcohol and fuel taxes, taxes on lottery winnings, property, and higher public health care charges.[183][184][185] At the insistence of the Commission negotiators, at first the proposal also included an unprecedented one-off levy of 6.7% for deposits up to €100,000 and 9.9% for higher deposits on all domestic bank accounts.[186] Following public outcry, the eurozone finance ministers were forced to change the levy, excluding deposits of less than €100,000, and introducing a higher 15.6% levy on deposits of above €100,000 ($129,600)—in line with the EU minimum deposit guarantee.[187] This revised deal was also rejected by the Cypriot parliament on 19 March 2013 with 36 votes against, 19 abstentions and one not present for the vote.[188]

The final agreement was settled on 25 March 2013, with the proposal to close the most troubled Laiki Bank, which helped significantly to reduce the needed loan amount for the overall bailout package, so that €10bn was sufficient without need for imposing a general levy on bank deposits.[189] The final conditions for activation of the bailout package was outlined by the Troika's MoU agreement, which was endorsed in full by the Cypriot House of Representatives on 30 April 2013. It includes:[189][190]

The Cypriot debt-to-GDP ratio is on this background now forecasted only to peak at 126% in 2015 and subsequently decline to 105% in 2020, and thus considered to remain within sustainable territory.[190]

Although the bailout support programme feature sufficient financial transfers until March 2016, Cyprus began slowly to regain its access to the private lending markets already in June 2014. At this point of time, the government sold €0.75bn of bonds with a five-year maturity, to the tune of a 4.85% yield. A continued selling of bonds with a ten-year maturity, which would equal a regain of complete access to the private lending market (and mark the end of the era with need for bailout support), is expected to happen sometime in 2015.[191] The Cypriot minister of finance recently confirmed, that the government plan to issue two new European Medium Term Note (EMTN) bonds in 2015, likely shortly ahead of the expiry of another €1.1bn bond on 1 July and a second expiry of a €0.9bn bond on 1 November.[192] As announced in advance, the Cypriot government issued €1bn of seven-year bonds with a 4.0% yield by the end of April 2015.[193][194]


The table below provides an overview of the financial composition of all bailout programs being initiated for EU member states, since the 2008 financial crisis. EU member states outside the eurozone (marked with yellow in the table) have no access to the funds provided by EFSF/ESM, but can be covered with rescue loans from EU's Balance of Payments programme (BoP), IMF and bilateral loans (with an extra possible assistance from the Worldbank/EIB/EBRD if classified as a development country). Since October 2012, the ESM as a permanent new financial stability fund to cover any future potential bailout packages within the eurozone, has effectively replaced the now defunct GLF + EFSM + EFSF funds. Whenever pledged funds in a scheduled bailout program were not transferred in full, the table has noted this by writing "Y out of X".

On 9 May 2010, the 27 EU member states agreed to create the European Financial Stability Facility, a legal instrument[263] aiming at preserving financial stability in Europe, by providing financial assistance to eurozone states in difficulty. The EFSF can issue bonds or other debt instruments on the market with the support of the German Debt Management Office to raise the funds needed to provide loans to eurozone countries in financial troubles, recapitalise banks or buy sovereign debt.[264]

Emissions of bonds are backed by guarantees given by the euro area member states in proportion to their share in the paid-up capital of the European Central Bank. The €440 billion lending capacity of the facility is jointly and severally guaranteed by the eurozone countries' governments and may be combined with loans up to €60 billion from the European Financial Stabilisation Mechanism (reliant on funds raised by the European Commission using the EU budget as collateral) and up to €250 billion from the International Monetary Fund (IMF) to obtain a financial safety net up to €750 billion.[265]

The EFSF issued €5 billion of five-year bonds in its inaugural benchmark issue 25 January 2011, attracting an order book of €44.5 billion. This amount is a record for any sovereign bond in Europe, and €24.5 billion more than the European Financial Stabilisation Mechanism (EFSM), a separate European Union funding vehicle, with a €5 billion issue in the first week of January 2011.[266]

On 29 November 2011, the member state finance ministers agreed to expand the EFSF by creating certificates that could guarantee up to 30% of new issues from troubled euro-area governments, and to create investment vehicles that would boost the EFSF's firepower to intervene in primary and secondary bond markets.[267]

The transfers of bailout funds were performed in tranches over several years and were conditional on the governments simultaneously implementing a package of fiscal consolidation, structural reforms, privatization of public assets and setting up funds for further bank recapitalization and resolution.

Stocks surged worldwide after the EU announced the EFSF's creation. The facility eased fears that the Greek debt crisis would spread,[268] and this led to some stocks rising to the highest level in a year or more.[269] The euro made its biggest gain in 18 months,[270] before falling to a new four-year low a week later.[271] Shortly after the euro rose again as hedge funds and other short-term traders unwound short positions and carry trades in the currency.[272] Commodity prices also rose following the announcement.[273]

The dollar Libor held at a nine-month high.[274] Default swaps also fell.[275] The VIX closed down a record almost 30%, after a record weekly rise the preceding week that prompted the bailout.[276] The agreement is interpreted as allowing the ECB to start buying government debt from the secondary market, which is expected to reduce bond yields.[277] As a result, Greek bond yields fell sharply from over 10% to just over 5%.[278] Asian bonds yields also fell with the EU bailout.[279]

The EFSF only raises funds after an aid request is made by a country.[280] As of the end of July 2012, it has been activated various times. In November 2010, it financed €17.7 billion of the total €67.5 billion rescue package for Ireland (the rest was loaned from individual European countries, the European Commission and the IMF). In May 2011 it contributed one-third of the €78 billion package for Portugal. As part of the second bailout for Greece, the loan was shifted to the EFSF, amounting to €164 billion (130bn new package plus 34.4bn remaining from Greek Loan Facility) throughout 2014.[281] On 20 July 2012, European finance ministers sanctioned the first tranche of a partial bailout worth up to €100 billion for Spanish banks.[282] This leaves the EFSF with €148 billion[282] or an equivalent of €444 billion in leveraged firepower.[283]

The EFSF is set to expire in 2013, running some months parallel to the permanent €500 billion rescue funding program called the European Stability Mechanism (ESM), which will start operating as soon as member states representing 90% of the capital commitments have ratified it. (see section: ESM)

On 13 January 2012, Standard & Poor's downgraded France and Austria from AAA rating, lowered Spain, Italy (and five other[284]) euro members further. Shortly after, S&P also downgraded the EFSF from AAA to AA+.[284][285]

On 5 January 2011, the European Union created the European Financial Stabilisation Mechanism (EFSM), an emergency funding programme reliant upon funds raised on the financial markets and guaranteed by the European Commission using the budget of the European Union as collateral.[286] It runs under the supervision of the Commission[287] and aims at preserving financial stability in Europe by providing financial assistance to EU member states in economic difficulty.[288] The Commission fund, backed by all 27 European Union members, has the authority to raise up to €60 billion[289] and is rated AAA by Fitch, Moody's and Standard & Poor's.[290]

Under the EFSM, the EU successfully placed in the capital markets an €5 billion issue of bonds as part of the financial support package agreed for Ireland, at a borrowing cost for the EFSM of 2.59%.[291]

Like the EFSF, the EFSM was replaced by the permanent rescue funding programme ESM, which was launched in September 2012.[292]

On 26 October 2011, leaders of the 17 eurozone countries met in Brussels and agreed on a 50% write-off of Greek sovereign debt held by banks, a fourfold increase (to about €1 trillion) in bail-out funds held under the European Financial Stability Facility, an increased mandatory level of 9% for bank capitalisation within the EU and a set of commitments from Italy to take measures to reduce its national debt. Also pledged was €35 billion in "credit enhancement" to mitigate losses likely to be suffered by European banks. European Commission president José Manuel Barroso characterised the package as a set of "exceptional measures for exceptional times".[293][294]

The package's acceptance was put into doubt on 31 October when Greek Prime Minister George Papandreou announced that a referendum would be held so that the Greek people would have the final say on the bailout, upsetting financial markets.[295] On 3 November 2011 the promised Greek referendum on the bailout package was withdrawn by Prime Minister Papandreou.

In late 2011, Landon Thomas in the New York Times noted that some, at least, European banks were maintaining high dividend payout rates and none were getting capital injections from their governments even while being required to improve capital ratios. Thomas quoted Richard Koo, an economist based in Japan, an expert on that country's banking crisis, and specialist in balance sheet recessions, as saying:

I do not think Europeans understand the implications of a systemic banking crisis. ... When all banks are forced to raise capital at the same time, the result is going to be even weaker banks and an even longer recession—if not depression. ... Government intervention should be the first resort, not the last resort.
Beyond equity issuance and debt-to-equity conversion, then, one analyst "said that as banks find it more difficult to raise funds, they will move faster to cut down on loans and unload lagging assets" as they work to improve capital ratios. This latter contraction of balance sheets "could lead to a depression", the analyst said.[296] Reduced lending was a circumstance already at the time being seen in a "deepen[ing] crisis" in commodities trade finance in western Europe.[297]

In a marathon meeting on 20/21 February 2012 the Eurogroup agreed with the IMF and the Institute of International Finance on the final conditions of the second bailout package worth €130 billion. The lenders agreed to increase the nominal haircut from 50% to 53.5%. EU Member States agreed to an additional retroactive lowering of the interest rates of the Greek Loan Facility to a level of just 150 basis points above Euribor. Furthermore, governments of Member States where central banks currently hold Greek government bonds in their investment portfolio commit to pass on to Greece an amount equal to any future income until 2020. Altogether this should bring down Greece's debt to between 117%[76] and 120.5% of GDP by 2020.[78]

The European Central Bank (ECB) has taken a series of measures aimed at reducing volatility in the financial markets and at improving liquidity.[298]

In May 2010 it took the following actions:

The move took some pressure off Greek government bonds, which had just been downgraded to junk status, making it difficult for the government to raise money on capital markets.[305]

On 30 November 2011, the ECB, the US Federal Reserve, the central banks of Canada, Japan, Britain and the Swiss National Bank provided global financial markets with additional liquidity to ward off the debt crisis and to support the real economy. The central banks agreed to lower the cost of dollar currency swaps by 50 basis points to come into effect on 5 December 2011. They also agreed to provide each other with abundant liquidity to make sure that commercial banks stay liquid in other currencies.[306]

With the aim of boosting the recovery in the eurozone economy by lowering interest rates for businesses, the ECB cut its bank rates in multiple steps in 2012–2013, reaching an historic low of 0.25% in November 2013. The lowered borrowing rates have also caused the euro to fall in relation to other currencies, which is hoped will boost exports from the eurozone and further aid the recovery.[307]

With inflation falling to 0.5% in May 2014, the ECB again took measures to stimulate the eurozone economy, which grew at just 0.2% during the first quarter of 2014.[308] (Deflation or very low inflation encourages holding cash, causing a decrease in purchases). On 5 June, the central bank cut the prime interest rate to 0.15%, and set the deposit rate at −0.10%.[309] The latter move in particular was seen as "a bold and unusual move", as a negative interest rate had never been tried on a wide-scale before.[308] Additionally, the ECB announced it would offer long-term four-year loans at the cheap rate (normally the rate is primarily for overnight lending), but only if the borrowing banks met strict conditions designed to ensure the funds ended up in the hands of businesses instead of, for example, being used to buy low risk government bonds.[308] Collectively, the moves are aimed at avoiding deflation, devaluing the euro to make exportation more viable, and at increasing "real world" lending.[308][309]

Stock markets reacted strongly to the ECB rate cuts. The German DAX index, for example, set a record high the day the new rates were announced.[309] Meanwhile, the euro briefly fell to a four-month low against the dollar.[308] However, due to the unprecedented nature of the negative interest rate, the long-term effects of the stimulus measures are hard to predict.[309] Bank president Mario Draghi signalled the central bank was willing to do whatever it takes to turn around the eurozone economies, remarking "Are we finished? The answer is no".[308] He laid the groundwork for large-scale bond repurchasing, a controversial idea known as quantitative easing.[309]

In September 2011, Jürgen Stark became the second German after Axel A. Weber to resign from the ECB Governing Council in 2011. Weber, the former Deutsche Bundesbank president, was once thought to be a likely successor to Jean-Claude Trichet as bank president. He and Stark were both thought to have resigned due to "unhappiness with the ECB's bond purchases, which critics say erode the bank's independence". Stark was "probably the most hawkish" member of the council when he resigned. Weber was replaced by his Bundesbank successor Jens Weidmann, while Belgium's Peter Praet took Stark's original position, heading the ECB's economics department.[310]

On 22 December 2011, the ECB[311] started the biggest infusion of credit into the European banking system in the euro's 13-year history. Under its Long Term Refinancing Operations (LTROs) it loaned €489 billion to 523 banks for an exceptionally long period of three years at a rate of just one per cent.[312] Previous refinancing operations matured after three, six, and twelve months.[313] The by far biggest amount of €325 billion was tapped by banks in Greece, Ireland, Italy and Spain.[314]

This way the ECB tried to make sure that banks have enough cash to pay off €200 billion of their own maturing debts in the first three months of 2012, and at the same time keep operating and loaning to businesses so that a credit crunch does not choke off economic growth. It also hoped that banks would use some of the money to buy government bonds, effectively easing the debt crisis.[315] On 29 February 2012, the ECB held a second auction, LTRO2, providing 800 eurozone banks with further €529.5 billion in cheap loans.[316] Net new borrowing under the €529.5 billion February auction was around €313 billion; out of a total of €256 billion existing ECB lending (MRO + 3m&6m LTROs), €215 billion was rolled into LTRO2.[317]

ECB lending has largely replaced inter-bank lending. Spain has €365 billion and Italy has €281 billion of borrowings from the ECB (June 2012 data). Germany has €275 billion on deposit.[318]

On 16 June 2012 the European Central Bank together with other European leaders hammered out plans for the ECB to become a bank regulator and to form a deposit insurance program to augment national programs. Other economic reforms promoting European growth and employment were also proposed.[319]

On 6 September 2012, the ECB announced to offer additional financial support in the form of some yield-lowering bond purchases (OMT), for all eurozone countries involved in a sovereign state bailout program from EFSF/ESM.[6] A eurozone country can benefit from the program if -and for as long as- it is found to suffer from stressed bond yields at excessive levels; but only at the point of time where the country possesses/regains a complete market access -and only if the country still complies with all terms in the signed Memorandum of Understanding (MoU) agreement.[6][173] Countries receiving a precautionary programme rather than a sovereign bailout will, by definition, have complete market access and thus qualify for OMT support if also suffering from stressed interest rates on its government bonds. In regards of countries receiving a sovereign bailout (Ireland, Portugal and Greece), they will on the other hand not qualify for OMT support before they have regained complete market access, which will normally only happen after having received the last scheduled bailout disbursement.[6][112] Despite none OMT programmes were ready to start in September/October, the financial markets straight away took notice of the additionally planned OMT packages from ECB, and started slowly to price-in a decline of both short-term and long-term interest rates in all European countries previously suffering from stressed and elevated interest levels (as OMTs were regarded as an extra potential back-stop to counter the frozen liquidity and highly stressed rates; and just the knowledge about their potential existence in the very near future helped to calm the markets).

The European Stability Mechanism (ESM) is a permanent rescue funding programme to succeed the temporary European Financial Stability Facility and European Financial Stabilisation Mechanism in July 2012[292] but it had to be postponed until after the Federal Constitutional Court of Germany had confirmed the legality of the measures on 12 September 2012.[320][321] The permanent bailout fund entered into force for 16 signatories on 27 September 2012. It became effective in Estonia on 4 October 2012 after the completion of their ratification process.[322]

On 16 December 2010 the European Council agreed a two line amendment to the EU Lisbon Treaty to allow for a permanent bail-out mechanism to be established[323] including stronger sanctions. In March 2011, the European Parliament approved the treaty amendment after receiving assurances that the European Commission, rather than EU states, would play 'a central role' in running the ESM.[324][325] The ESM is an intergovernmental organisation under public international law. It is located in Luxembourg.[326][327]

Such a mechanism serves as a "financial firewall". Instead of a default by one country rippling through the entire interconnected financial system, the firewall mechanism can ensure that downstream nations and banking systems are protected by guaranteeing some or all of their obligations. Then the single default can be managed while limiting financial contagion.

In March 2011 a new reform of the Stability and Growth Pact was initiated, aiming at straightening the rules by adopting an automatic procedure for imposing of penalties in case of breaches of either the 3% deficit or the 60% debt rules.[328] By the end of the year, Germany, France and some other smaller EU countries went a step further and vowed to create a fiscal union across the eurozone with strict and enforceable fiscal rules and automatic penalties embedded in the EU treaties.[329][330] On 9 December 2011 at the European Council meeting, all 17 members of the eurozone and six countries that aspire to join agreed on a new intergovernmental treaty to put strict caps on government spending and borrowing, with penalties for those countries who violate the limits.[331] All other non-eurozone countries apart from the UK are also prepared to join in, subject to parliamentary vote.[292] The treaty will enter into force on 1 January 2013, if by that time 12 members of the euro area have ratified it.[332]

Originally EU leaders planned to change existing EU treaties but this was blocked by British prime minister David Cameron, who demanded that the City of London be excluded from future financial regulations, including the proposed EU financial transaction tax.[333][334] By the end of the day, 26 countries had agreed to the plan, leaving the United Kingdom as the only country not willing to join.[335] Cameron subsequently conceded that his action had failed to secure any safeguards for the UK.[336] Britain's refusal to be part of the fiscal compact to safeguard the eurozone constituted a de facto refusal (PM David Cameron vetoed the project) to engage in any radical revision of the Lisbon Treaty. John Rentoul of The Independent concluded that "Any Prime Minister would have done as Cameron did".[337]

On 28 June 2012, eurozone leaders agreed to permit loans by the European Stability Mechanism to be made directly to stressed banks rather than through eurozone states, to avoid adding to sovereign debt. The reform was linked to plans for banking regulation by the European Central Bank. The reform was immediately reflected by a reduction in yield of long-term bonds issued by member states such as Italy and Spain and a rise in value of the Euro.[338][339][340]

There has been substantial criticism over the austerity measures implemented by most European nations to counter this debt crisis. US economist and Nobel laureate Paul Krugman argues that an abrupt return to "'non-Keynesian' financial policies" is not a viable solution.[341] Pointing at historical evidence, he predicts that deflationary policies now being imposed on countries such as Greece and Spain will prolong and deepen their recessions.[342] Together with over 9,000 signatories of "A Manifesto for Economic Sense"[343] Krugman also dismissed the belief of austerity focusing policy makers such as EU economic commissioner Olli Rehn and most European finance ministers[344] that "budget consolidation" revives confidence in financial markets over the longer haul.[345][346] In a 2003 study that analysed 133 IMF austerity programmes, the IMF's independent evaluation office found that policy makers consistently underestimated the disastrous effects of rigid spending cuts on economic growth.[347][348] In early 2012 an IMF official, who negotiated Greek austerity measures, admitted that spending cuts were harming Greece.[47] In October 2012, the IMF said that its forecasts for countries which implemented austerity programmes have been consistently overoptimistic, suggesting that tax hikes and spending cuts have been doing more damage than expected, and countries which implemented fiscal stimulus, such as Germany and Austria, did better than expected.[349] Also Portugal did comparably better than Spain. The latter introduced drastic austerity measures but was unable not meet its EU budget deficit targets. On the other hand, Portugal's leftist coalition fought austerity (it increased the minimum wage by 25 percent and took back cuts in the pension system and the public sector) and at the same time reduced its budget deficit to below three percent in 2016.[350] According to historian Florian Schui from University of St. Gallen no austerity program has ever worked. Schui particularly notes Winston Churchill's attempt in 1925 and Heinrich Brüning's attempt in 1930 during the Weimar Republic. Both led to disastrous consequences.[351]

According to Keynesian economists "growth-friendly austerity" relies on the false argument that public cuts would be compensated for by more spending from consumers and businesses, a theoretical claim that has not materialised.[352]
The case of Greece shows that excessive levels of private indebtedness and a collapse of public confidence (over 90% of Greeks fear unemployment, poverty and the closure of businesses)[353] led the private sector to decrease spending in an attempt to save up for rainy days ahead.
This led to even lower demand for both products and labour, which further deepened the recession and made it ever more difficult to generate tax revenues and fight public indebtedness.[354] According to Financial Times chief economics commentator Martin Wolf, "structural tightening does deliver actual tightening. But its impact is much less than one to one. A one percentage point reduction in the structural deficit delivers a 0.67 percentage point improvement in the actual fiscal deficit". This means that Ireland e.g. would require structural fiscal tightening of more than 12% to eliminate its 2012 actual fiscal deficit. A task that is difficult to achieve without an exogenous eurozone-wide economic boom.[355] According to the Europlus Monitor Report 2012, no country should tighten its fiscal reins by more than 2% of GDP in one year, to avoid recession.[356]

Instead of public austerity, a "growth compact" centring on tax increases[354] and deficit spending is proposed. Since struggling European countries lack the funds to engage in deficit spending, German economist and member of the German Council of Economic Experts Peter Bofinger and Sony Kapoor of the global think tank Re-Define suggest providing €40 billion in additional funds to the European Investment Bank (EIB), which could then lend ten times that amount to the employment-intensive smaller business sector.[354] The EU is currently planning a possible €10 billion increase in the EIB's capital base. Furthermore, the two suggest financing additional public investments by growth-friendly taxes on "property, land, wealth, carbon emissions and the under-taxed financial sector". They also called on EU countries to renegotiate the EU savings tax directive and to sign an agreement to help each other crack down on tax evasion and avoidance. Currently authorities capture less than 1% in annual tax revenue on untaxed wealth transferred between EU members.[354] According to the Tax Justice Network, worldwide, a global super-rich elite had between $21 and $32 trillion (up to 26,000bn Euros) hidden in secret tax havens by the end of 2010, resulting in a tax deficit of up to $280bn.[357][358]

Apart from arguments over whether or not austerity, rather than increased or frozen spending, is a macroeconomic solution,[359] union leaders have also argued that the working population is being unjustly held responsible for the economic mismanagement errors of economists, investors, and bankers. Over 23 million EU workers have become unemployed as a consequence of the global economic crisis of 2007–2010, and this has led many to call for additional regulation of the banking sector across not only Europe, but the entire world.[360]

After the 2008 financial crisis and the Great Recession, the focus across all EU member states was to gradually to implement austerity measures, with the purpose of lowering the budget deficits to levels below 3% of GDP, so that the debt level would either stay below -or start decline towards- the 60% limit defined by the Stability and Growth Pact. To further restore the confidence in Europe, 23 out of 27 EU countries also agreed to adopt the Euro Plus Pact, consisting of political reforms to improve fiscal strength and competitiveness; 25 out of 27 EU countries also decided to implement the Fiscal Compact which include the commitment of each participating country to introduce a balanced budget amendment as part of their national law/constitution. The Fiscal Compact is a direct successor of the previous Stability and Growth Pact, but it is more strict, not only because criteria compliance will be secured through its integration into national law/constitution, but also because it starting from 2014 will require all ratifying countries not involved in ongoing bailout programmes, to comply with the new strict criteria of only having a structural deficit of either maximum 0.5% or 1% (depending on the debt level).[329][330] Each of the eurozone countries being involved in a bailout programme (Greece, Portugal, and Ireland) was asked both to follow a programme with fiscal consolidation/austerity, and to restore competitiveness through implementation of structural reforms and internal devaluation, i.e. lowering their relative production costs.[361] The measures implemented to restore competitiveness in the weakest countries are needed, not only to build the foundation for GDP growth, but also in order to decrease the current account imbalances among eurozone member states.[362][363]

Germany has come under pressure due to not having a government budget deficit and funding it by borrowing more. As of late 2014, the government (federal and state) has spent less than it receives in revenue, for the third year in a row, despite low economic growth.[364] The 2015 budget includes a surplus for the first time since 1969. Current projections are that by 2019 the debt will be less than required by the Stability and Growth Pact.

It has been a long known belief that austerity measures will always reduce the GDP growth in the short term. Some economists believing in Keynesian policies criticised the timing and amount of austerity measures being called for in the bailout programmes, as they argued such extensive measures should not be implemented during the crisis years with an ongoing recession, but if possible delayed until the years after some positive real GDP growth had returned. In October 2012, a report published by International Monetary Fund (IMF) also found, that tax hikes and spending cuts during the most recent decade had indeed damaged the GDP growth more severely, compared to what had been expected and forecasted in advance (based on the "GDP damage ratios" previously recorded in earlier decades and under different economic scenarios).[349] Already a half-year earlier, several European countries as a response to the problem with subdued GDP growth in the eurozone, likewise had called for the implementation of a new reinforced growth strategy based on additional public investments, to be financed by growth-friendly taxes on property, land, wealth, and financial institutions. In June 2012, EU leaders agreed as a first step to moderately increase the funds of the European Investment Bank, in order to kick-start infrastructure projects and increase loans to the private sector. A few months later 11 out of 17 eurozone countries also agreed to introduce a new EU financial transaction tax to be collected from 1 January 2014.[365]

In April 2012, Olli Rehn, the European commissioner for economic and monetary affairs in Brussels, "enthusiastically announced to EU parliamentarians in mid-April that 'there was a breakthrough before Easter'. He said the European heads of state had given the green light to pilot projects worth billions, such as building highways in Greece".[366] Other growth initiatives include "project bonds" wherein the EIB would "provide guarantees that safeguard private investors. In the pilot phase until 2013, EU funds amounting to €230 million are expected to mobilise investments of up to €4.6 billion".[366] Der Spiegel also said: "According to sources inside the German government, instead of funding new highways, Berlin is interested in supporting innovation and programs to promote small and medium-sized businesses. To ensure that this is done as professionally as possible, the Germans would like to see the southern European countries receive their own state-owned development banks, modeled after Germany's [Marshall Plan-era-origin] KfW [Kreditanstalt für Wiederaufbau] banking group. It's hoped that this will get the economy moving in Greece and Portugal".[366]

In multiple steps during 2012–2013, the ECB lowered its bank rate to historical lows, reaching 0.25% in November 2013. Soon after the rates were shaved to 0.15%, then on 4 September 2014 the central bank shocked financial markets by cutting the razor-thin rates by a further two thirds from 0.15% to 0.05%, the lowest on record.[367] The moves were designed to make it cheaper for banks to borrow from the ECB, with the aim that lower cost of money would be passed on to businesses taking out loans, boosting investment in the economy. The lowered borrowing rates caused the euro to fall in relation to other currencies, which it was hoped would boost exports from the eurozone.[307]

Crisis countries must significantly increase their international competitiveness to generate economic growth and improve their terms of trade. Indian-American journalist Fareed Zakaria notes in November 2011 that no debt restructuring will work without growth, even more so as European countries "face pressures from three fronts: demography (an aging population), technology (which has allowed companies to do much more with fewer people) and globalisation (which has allowed manufacturing and services to locate across the world)".[368]

In case of economic shocks, policy makers typically try to improve competitiveness by depreciating the currency, as in the case of Iceland, which suffered the largest financial crisis in 2008–2011 in economic history but has since vastly improved its position. Eurozone countries cannot devalue their currency.

As a workaround many policy makers try to restore competitiveness through internal devaluation, a painful economic adjustment process, where a country aims to reduce its unit labour costs.[361][369] German economist Hans-Werner Sinn noted in 2012 that Ireland was the only country that had implemented relative wage moderation in the last five years, which helped decrease its relative price/wage levels by 16%. Greece would need to bring this figure down by 31%, effectively reaching the level of Turkey.[370][371] By 2012, wages in Greece had been cut to a level last seen in the late 1990s. Purchasing power dropped even more to the level of 1986.[372] Similarly, salaries in Italy fell to 1986 levels and consumption fell to the level of 1950.[373]

Other economists argue that no matter how much Greece and Portugal drive down their wages, they could never compete with low-cost developing countries such as China or India. Instead weak European countries must shift their economies to higher quality products and services, though this is a long-term process and may not bring immediate relief.[374][375]

Another option would be to implement fiscal devaluation, based on an idea originally developed by John Maynard Keynes in 1931.[376][377] According to this neo-Keynesian logic, policy makers can increase the competitiveness of an economy by lowering corporate tax burden such as employer's social security contributions, while offsetting the loss of government revenues through higher taxes on consumption (VAT) and pollution, i.e. by pursuing an ecological tax reform.[378][379][380]

Germany has successfully pushed its economic competitiveness by increasing the value added tax (VAT) by three percentage points in 2007, and using part of the additional revenues to lower employer's unemployment insurance contribution. Portugal has taken a similar stance[380] and also France appears to follow this suit. In November 2012 French president François Hollande announced plans to reduce tax burden of the corporate sector by €20 billion within three years, while increasing the standard VAT from 19.6% to 20% and introducing additional eco-taxes in 2016. To minimise negative effects of such policies on purchasing power and economic activity the French government will partly offset the tax hikes by decreasing employees' social security contributions by €10 billion and by reducing the lower VAT for convenience goods (necessities) from 5.5% to 5%.[381]

On 15 November 2011, the Lisbon Council published the Euro Plus Monitor 2011. According to the report most critical eurozone member countries are in the process of rapid reforms. The authors note that "Many of those countries most in need to adjust [...] are now making the greatest progress towards restoring their fiscal balance and external competitiveness".
Greece, Ireland and Spain are among the top five reformers and Portugal is ranked seventh among 17 countries included in the report (see graph).[382]

In its Euro Plus Monitor Report 2012, published in November 2012, the Lisbon Council finds that the eurozone has slightly improved its overall health. With the exception of Greece, all eurozone crisis countries are either close to the point where they have achieved the major adjustment or are likely to get there over the course of 2013. Portugal and Italy are expected to progress to the turnaround stage in spring 2013, possibly followed by Spain in autumn, while the fate of Greece continues to hang in the balance. Overall, the authors suggest that if the eurozone gets through the current acute crisis and stays on the reform path "it could eventually emerge from the crisis as the most dynamic of the major Western economies".[356]

The Euro Plus Monitor update from spring 2013 notes that the eurozone remains on the right track. According to the authors, almost all vulnerable countries in need of adjustment "are slashing their underlying fiscal deficits and improving their external competitiveness at an impressive speed", for which they expected the eurozone crisis to be over by the end of 2013.[383]

Regardless of the corrective measures chosen to solve the current predicament, as long as cross border capital flows remain unregulated in the euro area,[384] current account imbalances are likely to continue. A country that runs a large current account or trade deficit (i.e., importing more than it exports) must ultimately be a net importer of capital; this is a mathematical identity called the balance of payments.
In other words, a country that imports more than it exports must either decrease its savings reserves or borrow to pay for those imports. Conversely, Germany's large trade surplus (net export position) means that it must either increase its savings reserves or be a net exporter of capital, lending money to other countries to allow them to buy German goods.[385]

The 2009 trade deficits for Italy, Spain, Greece, and Portugal were estimated to be $42.96bn, $75.31bn and $35.97bn, and $25.6bn respectively, while Germany's trade surplus was $188.6bn.[386] A similar imbalance exists in the US, which runs a large trade deficit (net import position) and therefore is a net borrower of capital from abroad. Ben Bernanke warned of the risks of such imbalances in 2005, arguing that a "savings glut" in one country with a trade surplus can drive capital into other countries with trade deficits, artificially lowering interest rates and creating asset bubbles.[387][388]

A country with a large trade surplus would generally see the value of its currency appreciate relative to other currencies, which would reduce the imbalance as the relative price of its exports increases. This currency appreciation occurs as the importing country sells its currency to buy the exporting country's currency used to purchase the goods. Alternatively, trade imbalances can be reduced if a country encouraged domestic saving by restricting or penalising the flow of capital across borders, or by raising interest rates, although this benefit is likely offset by slowing down the economy and increasing government interest payments.[389]

Either way, many of the countries involved in the crisis are on the euro, so devaluation, individual interest rates, and capital controls are not available. The only solution left to raise a country's level of saving is to reduce budget deficits and to change consumption and savings habits. For example, if a country's citizens saved more instead of consuming imports, this would reduce its trade deficit.[389] It has therefore been suggested that countries with large trade deficits (e.g., Greece) consume less and improve their exporting industries.
On the other hand, export driven countries with a large trade surplus, such as Germany, Austria and the Netherlands would need to shift their economies more towards domestic services and increase wages to support domestic consumption.[390][391]

Economic evidence indicates the crisis may have more to do with trade deficits (which require private borrowing to fund) than public debt levels. Economist Paul Krugman wrote in March 2013: "... the really strong relationship within the [eurozone countries] is between interest spreads and current account deficits, which is in line with the conclusion many of us have reached, that the euro area crisis is really a balance of payments crisis, not a debt crisis".[392] A February 2013 paper from four economists concluded that, "Countries with debt above 80% of GDP and persistent current-account [trade] deficits are vulnerable to a rapid fiscal deterioration..".[393][394][395]

In its spring 2012 economic forecast, the European Commission finds "some evidence that the current-account rebalancing is underpinned by changes in relative prices and competitiveness positions as well as gains in export market shares and expenditure switching in deficit countries".[396] In May 2012 German finance minister Wolfgang Schäuble has signalled support for a significant increase in German wages to help decrease current account imbalances within the eurozone.[397]

According to the Euro Plus Monitor Report 2013, the collective current account of Greece, Ireland, Italy, Portugal, and Spain is improving rapidly and is expected to balance by mid 2013. Thereafter these countries as a group would no longer need to import capital.[383] In 2014, the current account surplus of the eurozone as a whole almost doubled compared to the previous year, reaching a new record high of 227.9bn Euros.[398]

Several proposals were made in mid-2012 to purchase the debt of distressed European countries such as Spain and Italy. Markus Brunnermeier,[399] the economist Graham Bishop, and Daniel Gros were among those advancing proposals. Finding a formula, which was not simply backed by Germany, is central in crafting an acceptable and effective remedy.[400]

The key policy issue that has to be addressed in the long run is how to harmonise different political-economic institutional set-ups of the north and south European economies to promote economic growth and make the currency union sustainable. The Eurozone member states must adopt structural reforms, aimed at promoting labour market mobility and wage flexibility, restoring the south's economies’ competitiveness by increasing their productivity.[401]

At the same time, it is vital to keep in mind that just putting emphasis on emulating LME's wage-setting system to CMEs and mixed-market economies will not work. Therefore, apart from wage issues, structural reforms should be focused on developing capacities for innovations, technologies, education, R&D, etc., i.e. all institutional subsystems, crucial for firms’ success.[402] In economies of the south special attention should be given to creating less labour-intensive industries to avoid price competition pressure from emerging low-cost countries (such as China) via an exchange rate channel, and providing a smooth transition of workers from old unsustainable industries to new ones based on the so-called Nordic-style ‘flexicurity’ market model.[403][12]

The crisis is pressuring Europe to move beyond a regulatory state and towards a more federal EU with fiscal powers.[404] Increased European integration giving a central body increased control over the budgets of member states was proposed on 14 June 2012 by Jens Weidmann, President of the Deutsche Bundesbank,[405] expanding on ideas first proposed by Jean-Claude Trichet, former president of the European Central Bank. Control, including requirements that taxes be raised or budgets cut, would be exercised only when fiscal imbalances developed.[406] This proposal is similar to contemporary calls by Angela Merkel for increased political and fiscal union which would "allow Europe oversight possibilities".[407]

European banks are estimated to have incurred losses approaching €1 trillion between 2007 and 2010. The European Commission approved some €4.5 billion in state aid for banks between October 2008 and October 2011, a sum which includes the value of taxpayer-funded recapitalisations and public guarantees on banking debts.[408] This has prompted some economists such as Joseph Stiglitz and Paul Krugman to note that Europe is not suffering from a sovereign debt crisis but rather from a banking crisis.[409]

On 6 June 2012, the European Commission adopted a legislative proposal for a harmonised bank recovery and resolution mechanism. The proposed framework sets out the necessary steps and powers to ensure that bank failures across the EU are managed in a way that avoids financial instability.[410] The new legislation would give member states the power to impose losses, resulting from a bank failure, on the bondholders to minimise costs for taxpayers. The proposal is part of a new scheme in which banks will be compelled to "bail-in" their creditors whenever they fail, the basic aim being to prevent taxpayer-funded bailouts in the future.[411] The public authorities would also be given powers to replace the management teams in banks even before the lender fails. Each institution would also be obliged to set aside at least one per cent of the deposits covered by their national guarantees for a special fund to finance the resolution of banking crisis starting in 2018.[408]

A growing number of investors and economists say eurobonds would be the best way of solving a debt crisis,[412] though their introduction matched by tight financial and budgetary co-ordination may well require changes in EU treaties.[412] On 21 November 2011, the European Commission suggested that eurobonds issued jointly by the 17 euro nations would be an effective way to deal with the financial crisis. Using the term "stability bonds", Jose Manuel Barroso insisted that any such plan would have to be matched by tight fiscal surveillance and economic policy coordination as an essential counterpart so as to avoid moral hazard and ensure sustainable public finances.[413][414]

Germany remains largely opposed at least in the short term to a collective takeover of the debt of states that have run excessive budget deficits and borrowed excessively over the past years.[415]

A group of economists from Princeton University suggest a new form of European Safe Bonds (ESBies), i.e. bundled European government bonds (70% senior bonds, 30% junior bonds) in the form of a "union-wide safe asset without joint liability". According to the authors, ESBies "would be at least as safe as German bonds and approximately double the supply of euro safe assets when protected by a 30%-thick junior tranche". ESBies could be issued by public or private-sector entities and would "weaken the diabolic loop and its diffusion across countries". It requires "no significant change in treaties or legislation.“[416][417]

In 2017 the idea was picked up by the European Central Bank. The European Commission has also shown interest and plans to include ESBies in a future white paper dealing with the aftermath of the financial crisis.[418] The European Commission has recently introduced a proposal to introduce what it calls Sovereign Bond Backed Securities (SBBS) which are essentially the same as ESBies and the European Parliament endorsed the changes in regulations necessary to facilitate these securities in April 2019.[419]

On 20 October 2011, the Austrian Institute of Economic Research published an article that suggests transforming the EFSF into a European Monetary Fund (EMF), which could provide governments with fixed interest rate Eurobonds at a rate slightly below medium-term economic growth (in nominal terms). These bonds would not be tradable but could be held by investors with the EMF and liquidated at any time. Given the backing of all eurozone countries and the ECB, "the EMU would achieve a similarly strong position vis-à-vis financial investors as the US where the Fed backs government bonds to an unlimited extent". To ensure fiscal discipline despite lack of market pressure, the EMF would operate according to strict rules, providing funds only to countries that meet fiscal and macroeconomic criteria.
Governments lacking sound financial policies would be forced to rely on traditional (national) governmental bonds with less favourable market rates.[420]

The econometric analysis suggests that "If the short-term and long- term interest rates in the euro area were stabilised at 1.5% and 3%, respectively, aggregate output (GDP) in the euro area would be 5 percentage points above baseline in 2015". At the same time, sovereign debt levels would be significantly lower with, e.g., Greece's debt level falling below 110% of GDP, more than 40 percentage points below the baseline scenario with market-based interest levels. Furthermore, banks would no longer be able to benefit unduly from intermediary profits by borrowing from the ECB at low rates and investing in government bonds at high rates.[420]

According to the Bank for International Settlements, the combined private and public debt of 18 OECD countries nearly quadrupled between 1980 and 2010, and will likely continue to grow, reaching between 250% (for Italy) and about 600% (for Japan) by 2040.[421] A BIS study released in June 2012 warns that budgets of most advanced economies, excluding interest payments, "would need 20 consecutive years of surpluses exceeding 2 per cent of gross domestic product—starting now—just to bring the debt-to-GDP ratio back to its pre-crisis level".[422] The same authors found in a previous study that increased financial burden imposed by ageing populations and lower growth makes it unlikely that indebted economies can grow out of their debt problem if only one of the following three conditions is met:[423]

The first condition, suggested by an influential paper written by Kenneth Rogoff & Carmen Reinhart has been disputed due to major calculation errors. In fact, the average GDP growth at public debt/GDP ratios over 90% is not dramatically different from when debt/GDP ratios are lower.[424]

The Boston Consulting Group (BCG) adds that if the overall debt load continues to grow faster than the economy, then large-scale debt restructuring becomes inevitable. To prevent a vicious upward debt spiral from gaining momentum the authors urge policymakers to "act quickly and decisively" and aim for an overall debt level well below 180% for the private and government sector. This number is based on the assumption that governments, non-financial corporations, and private households can each sustain a debt load of 60% of GDP, at an interest rate of five per cent and a nominal economic growth rate of three per cent per year. Lower interest rates and/or higher growth would help reduce the debt burden further.[425]

To reach sustainable levels the eurozone must reduce its overall debt level by €6.1 trillion. According to BCG, this could be financed by a one-time wealth tax of between 11 and 30% for most countries, apart from the crisis countries (particularly Ireland) where a write-off would have to be substantially higher. The authors admit that such programmes would be "drastic", "unpopular" and "require broad political coordination and leadership" but they maintain that the longer politicians and central bankers wait, the more necessary such a step will be.[425]

Thomas Piketty, French economist and author of the bestselling book Capital in the Twenty-First Century regards taxes on capital as a more favorable option than austerity (inefficient and unjust) and inflation (only affects cash but neither real estates nor business capital). According to his analysis, a flat tax of 15 percent on private wealth would provide the state with nearly a year's worth national income, which would allow for immediate reimbursement of the entire public debt.[426]

Instead of a one-time write-off, German economist Harald Spehl has called for a 30-year debt-reduction plan, similar to the one Germany used after World War II to share the burden of reconstruction and development.[427] Similar calls have been made by political parties in Germany including the Greens and The Left.[428][429][430]

In 2015 Hans-Werner Sinn, president of German Ifo Institute for Economic Research, called for a debt relief for Greece.[431] In addition, economists from London School of Economics suggested a debt relief similar to the London agreement. In 1953, private sector lenders as well as governments agreed to write off about half of West Germany’s outstanding debt; this was followed by the beginning of Germany's "economic miracle" (or Wirtschaftswunder). According to this agreement, West Germany had to make repayments only when it was running a trade surplus, that is "when it had earned the money to pay up, rather than having to borrow more, or dip into its foreign currency reserves. Its repayments were also limited to 3% of export earnings". As LSE researchers note, this had the effect that, Germany's creditors had an incentive to buy the country's goods, so that it would be able to afford to pay them.[83]

The European bailouts are largely about shifting exposure from banks and others, who otherwise are lined up for losses on the sovereign debt they have piled up, onto European taxpayers.[82][85][432][433][434][435]

The EU's Maastricht Treaty contains juridical language that appears to rule out intra-EU bailouts. First, the "no bail-out" clause (Article 125 TFEU) ensures that the responsibility for repaying public debt remains national and prevents risk premiums caused by unsound fiscal policies from spilling over to partner countries. The clause thus encourages prudent fiscal policies at the national level.

The European Central Bank's purchase of distressed country bonds can be viewed as violating the prohibition of monetary financing of budget deficits (Article 123 TFEU). The creation of further leverage in EFSF with access to ECB lending would also appear to violate the terms of this article.

Articles 125 and 123 were meant to create disincentives for EU member states to run excessive deficits and state debt, and prevent the moral hazard of over-spending and lending in good times. They were also meant to protect the taxpayers of the other more prudent member states.
By issuing bail-out aid guaranteed by prudent eurozone taxpayers to rule-breaking eurozone countries such as Greece, the EU and eurozone countries also encourage moral hazard in the future.[436] While the no bail-out clause remains in place, the "no bail-out doctrine" seems to be a thing of the past.[437]

The EU treaties contain so called convergence criteria, specified in the protocols of the Treaties of the European Union. As regards government finance, the states agreed that the annual government budget deficit should not exceed 3% of gross domestic product (GDP) and that the gross government debt to GDP should not exceed 60% of GDP (see protocol 12 and 13). For eurozone members there is the Stability and Growth Pact, which contains the same requirements for budget deficit and debt limitation but with a much stricter regime. In the past, many European countries have substantially exceeded these criteria over a long period of time.[438] Around 2005 most eurozone members violated the pact, resulting in no action taken against violators.

The international US-based credit rating agencies—Moody's, Standard & Poor's and Fitch—which have already been under fire during the housing bubble[439][440] and the Icelandic crisis[441][442]—have also played a central and controversial role[443] in the current European bond market crisis.[444] On one hand, the agencies have been accused of giving overly generous ratings due to conflicts of interest.[445] On the other hand, ratings agencies have a tendency to act conservatively, and to take some time to adjust when a firm or country is in trouble.[446] In the case of Greece, the market responded to the crisis before the downgrades, with Greek bonds trading at junk levels several weeks before the ratings agencies began to describe them as such.[32]

According to a study by economists at St Gallen University credit rating agencies have fuelled rising euro zone indebtedness by issuing more severe downgrades since the sovereign debt crisis unfolded in 2009. The authors concluded that rating agencies were not consistent in their judgments, on average rating Portugal, Ireland, and Greece 2.3 notches lower than under pre-crisis standards, eventually forcing them to seek international aid.[447] On a side note: as of the end of November 2013 only three countries in the eurozone retain AAA ratings from Standard & Poor, (i.e. Germany, Finland and Luxembourg.)[448]

European policy makers have criticised ratings agencies for acting politically, accusing the Big Three of bias towards European assets and fuelling speculation.[449] Particularly Moody's decision to downgrade Portugal's foreign debt to the category Ba2 "junk" has infuriated officials from the EU and Portugal alike.[449]
State-owned utility and infrastructure companies like ANA – Aeroportos de Portugal, Energias de Portugal, Redes Energéticas Nacionais, and Brisa – Auto-estradas de Portugal were also downgraded despite claims to having solid financial profiles and significant foreign revenue.[450][451][452][453]

French central bank chief Christian Noyer criticised the decision of Standard & Poor's to lower the rating of France but not that of the United Kingdom, which "has more deficits, as much debt, more inflation, less growth than us".[454]

Similar comments were made by high-ranking politicians in Germany. Michael Fuchs, deputy leader of the leading Christian Democrats, said: "Standard and Poor's must stop playing politics. Why doesn't it act on the highly indebted United States or highly indebted Britain?", adding that the latter's collective private and public sector debts are the largest in Europe. He further added: "If the agency downgrades France, it should also downgrade Britain in order to be consistent".[454]

Credit rating agencies were also accused of bullying politicians by systematically downgrading eurozone countries just before important European Council meetings. As one EU source put it: "It is interesting to look at the downgradings and the timings of the downgradings... It is strange that we have so many downgrades in the weeks of summits".[455]

Think-tanks such as the World Pensions Council (WPC) [fr] have criticised European powers such as France and Germany for pushing for the adoption of the Basel II recommendations, adopted in 2005 and transposed in European Union law through the Capital Requirements Directive (CRD), effective since 2008. In essence, this forced European banks and more importantly the European Central Bank, e.g. when gauging the solvency of EU-based financial institutions, to rely heavily on the standardised assessments of credit risk marketed by only two private US firms- Moody's and S&P.[456]

Due to the failures of the ratings agencies, European regulators obtained new powers to supervise ratings agencies.[443] With the creation of the European Supervisory Authority in January 2011 the EU set up a whole range of new financial regulatory institutions,[457] including the European Securities and Markets Authority (ESMA),[458] which became the EU's single credit-ratings firm regulator.[459] Credit-ratings companies have to comply with the new standards or will be denied operation on EU territory, says ESMA Chief Steven Maijoor.[460]

Germany's foreign minister Guido Westerwelle called for an "independent" European ratings agency, which could avoid the conflicts of interest that he claimed US-based agencies faced.[461] European leaders are reportedly studying the possibility of setting up a European ratings agency in order that the private US-based ratings agencies have less influence on developments in European financial markets in the future.[462][463] According to German consultant company Roland Berger, setting up a new ratings agency would cost €300 million.
On 30 January 2012, the company said it was already collecting funds from financial institutions and business intelligence agencies to set up an independent non-profit ratings agency by mid-2012, which could provide its first country ratings by the end of the year.[464] In April 2012, in a similar attempt, the Bertelsmann Stiftung presented a blueprint for establishing an international non-profit credit rating agency (INCRA) for sovereign debt, structured in way that management and rating decisions are independent from its financiers.[465]

But attempts to regulate credit rating agencies more strictly in the wake of the eurozone crisis have been rather unsuccessful. World Pensions Council (WPC) [fr] financial law and regulation experts have argued that the hastily drafted, unevenly transposed in national law, and poorly enforced EU rule on ratings agencies (Regulation EC N° 1060/2009) has had little effect on the way financial analysts and economists interpret data or on the potential for conflicts of interests created by the complex contractual arrangements between credit rating agencies and their clients"[466]

Some in the Greek, Spanish, and French press and elsewhere spread conspiracy theories that claimed that the U.S. and Britain were deliberately promoting rumors about the euro in order to cause its collapse or to distract attention from their own economic vulnerabilities. The Economist rebutted these "Anglo-Saxon conspiracy" claims, writing that although American and British traders overestimated the weakness of southern European public finances and the probability of the breakup of the eurozone, these sentiments were an ordinary market panic, rather than some deliberate plot.[467]

Greek Prime Minister Papandreou is quoted as saying that there was no question of Greece leaving the euro and suggested that the crisis was politically as well as financially motivated. "This is an attack on the eurozone by certain other interests, political or financial".[468] The Spanish Prime Minister José Luis Rodríguez Zapatero has also suggested that the recent financial market crisis in Europe is an attempt to undermine the euro.[469][470] He ordered the Centro Nacional de Inteligencia intelligence service (National Intelligence Centre, CNI in Spanish) to investigate the role of the "Anglo-Saxon media" in fomenting the crisis.[471][472][473][474][475][476] So far, no results have been reported from this investigation.

Other commentators believe that the euro is under attack so that countries, such as the UK and the US, can continue to fund their large external deficits and government deficits,[477] and to avoid the collapse of the US$.[478][479][480] The US and UK do not have large domestic savings pools to draw on and therefore are dependent on external savings e.g. from China.[481][482] This is not the case in the eurozone, which is self-funding.[483][484][485]

Both the Spanish and Greek Prime Ministers have accused financial speculators and hedge funds of worsening the crisis by short selling euros.[486][487] German chancellor Merkel has stated that "institutions bailed out with public funds are exploiting the budget crisis in Greece and elsewhere".[488]

Goldman Sachs and other banks faced an inquiry by the Federal Reserve over their derivatives arrangements with Greece. The Guardian reported that "Goldman was reportedly the most heavily involved of a dozen or so Wall Street banks" that assisted the Greek government in the early 2000s "to structure complex derivatives deals early in the decade and 'borrow' billions of dollars in exchange rate swaps, which did not officially count as debt under eurozone rules".[489] Critics of the bank's conduct said that these deals "contributed to unsustainable public finances" which in turn destabilized the eurozone.[489]

In response to accusations that speculators were worsening the problem, some markets banned naked short selling for a few months.[490]

Some economists, mostly from outside Europe and associated with Modern Monetary Theory and other post-Keynesian schools, condemned the design of the euro currency system from the beginning because it ceded national monetary and economic sovereignty but lacked a central fiscal authority. When faced with economic problems, they maintained, "Without such an institution, EMU would prevent effective action by individual countries and put nothing in its place".[491][492] US economist Martin Feldstein went so far to call the euro "an experiment that failed".[493] Some non-Keynesian economists, such as Luca A. Ricci of the IMF, contend that the eurozone does not fulfil the necessary criteria for an optimum currency area, though it is moving in that direction.[382][494]

As the debt crisis expanded beyond Greece, these economists continued to advocate, albeit more forcefully, the disbandment of the eurozone. If this was not immediately feasible, they recommended that Greece and the other debtor nations unilaterally leave the eurozone, default on their debts, regain their fiscal sovereignty, and re-adopt national currencies.[66][67][495][496][497] Bloomberg suggested in June 2011 that, if the Greek and Irish bailouts should fail, an alternative would be for Germany to leave the eurozone to save the currency through depreciation[498] instead of austerity. The likely substantial fall in the euro against a newly reconstituted Deutsche Mark would give a "huge boost" to its members' competitiveness.[499]

Iceland, not part of the EU, is regarded as one of Europe's recovery success stories. It defaulted on its debt and drastically devalued its currency, which has effectively reduced wages by 50% making exports more competitive.[500] Lee Harris argues that floating exchange rates allows wage reductions by currency devaluations, a politically easier option than the economically equivalent but politically impossible method of lowering wages by political enactment.[501] Sweden's floating rate currency gives it a short-term advantage, structural reforms and constraints account for longer-term prosperity. Labour concessions, a minimal reliance on public debt, and tax reform helped to further a pro-growth policy.[502]

British discount retailer Poundland chose the name Dealz and not "Euroland" for its 2011 expansion into Ireland because, CEO Jim McCarthy said, "'Eurozone' ... is usually reported in association with bad news — job losses, debts and increased taxes". His company planned to use Dealz in continental Europe; McCarthy stated that "There is less certainty about the longevity [of the currency union] now".[503] The Wall Street Journal conjectured as well that Germany could return to the Deutsche Mark,[504] or create another currency union[505] with the Netherlands, Austria, Finland, Luxembourg and other European countries such as Denmark, Norway, Sweden, Switzerland, and the Baltics.[506] A monetary union of these countries with current account surpluses would create the world's largest creditor bloc, bigger than China[507] or Japan. The Wall Street Journal added that without the German-led bloc, a residual euro would have the flexibility to keep interest rates low[508] and engage in quantitative easing or fiscal stimulus in support of a job-targeting economic policy[509] instead of inflation targeting in the current configuration.

There is opposition in this view. The national exits are expected to be an expensive proposition. The breakdown of the currency would lead to insolvency of several euro zone countries, a breakdown in intrazone payments. Having instability and the public debt issue still not solved, the contagion effects and instability would spread into the system.[510] Having that the exit of Greece would trigger the breakdown of the eurozone, this is not welcomed by many politicians, economists and journalists. According to Steven Erlanger from The New York Times, a "Greek departure is likely to be seen as the beginning of the end for the whole euro zone project, a major accomplishment, whatever its faults, in the post-War construction of a Europe "whole and at peace".[511] Likewise, the two big leaders of the Euro zone, German Chancellor Angela Merkel and former French president Nicolas Sarkozy have said on numerous occasions that they would not allow the eurozone to disintegrate and have linked the survival of the Euro with that of the entire European Union.[512][513]
In September 2011, EU commissioner Joaquín Almunia shared this view, saying that expelling weaker countries from the euro was not an option: "Those who think that this hypothesis is possible just do not understand our process of integration".[514] The former ECB president Jean-Claude Trichet also denounced the possibility of a return of the Deutsche Mark.[515]

The challenges to the speculation about the break-up or salvage of the eurozone is rooted in its innate nature that the break-up or salvage of eurozone is not only an economic decision but also a critical political decision followed by complicated ramifications that "If Berlin pays the bills and tells the rest of Europe how to behave, it risks fostering destructive nationalist resentment against Germany and ... it would strengthen the camp in Britain arguing for an exit—a problem not just for Britons but for all economically liberal Europeans.[516] Solutions which involve greater integration of European banking and fiscal management and supervision of national decisions by European umbrella institutions can be criticised as Germanic domination of European political and economic life.[517] According to US author Ross Douthat "This would effectively turn the European Union into a kind of postmodern version of the old Austro-Hungarian Empire, with a Germanic elite presiding uneasily over a polyglot imperium and its restive local populations".[517]

The Economist provides a somewhat modified approach to saving the euro in that "a limited version of federalisation could be less miserable solution than break-up of the euro".[516] The recipe to this tricky combination of the limited federalisation, greatly lies on mutualisation for limiting the fiscal integration. In order for overindebted countries to stabilise the dwindling euro and economy, the overindebted countries require "access to money and for banks to have a "safe" euro-wide class of assets that is not tied to the fortunes of one country" which could be obtained by "narrower Eurobond that mutualises a limited amount of debt for a limited amount of time".[516] The proposition made by German Council of Economic Experts provides detailed blue print to mutualise the current debts of all euro-zone economies above 60% of their GDP. Instead of the break-up and issuing new national governments bonds by individual euro-zone governments, "everybody, from Germany (debt: 81% of GDP) to Italy (120%) would issue only these joint bonds until their national debts fell to the 60% threshold. The new mutualised-bond market, worth some €2.3 trillion, would be paid off over the next 25 years. Each country would pledge a specified tax (such as a VAT surcharge) to provide the cash." So far, German Chancellor Angela Merkel has opposed all forms of mutualisation.[516]

The Hungarian-American business magnate George Soros warns in "Does the Euro have a Future?" that there is no escape from the "gloomy scenario" of a prolonged European recession and the consequent threat to the Eurozone's political cohesion so long as "the authorities persist in their current course". He argues that to save the Euro long-term structural changes are essential in addition to the immediate steps needed to arrest the crisis. The changes he recommends include even greater economic integration of the European Union.[518] Soros writes that a treaty is needed to transform the European Financial Stability Fund into a full-fledged European Treasury. Following the formation of the Treasury, the European Council could then authorise the ECB to "step into the breach", with risks to the ECB's solvency being indemnified.
Soros acknowledges that converting the EFSF into a European Treasury will necessitate "a radical change of heart". In particular, he cautions, Germans will be wary of any such move, not least because many continue to believe that they have a choice between saving the Euro and abandoning it. Soros writes that a collapse of the European Union would precipitate an uncontrollable financial meltdown and thus "the only way" to avert "another Great Depression" is the formation of a European Treasury.[518]

Some protesters, commentators such as Libération correspondent Jean Quatremer and the Liège-based NGO Committee for the Abolition of the Third World Debt (CADTM) allege that the debt should be characterised as odious debt.[519] The Greek documentary Debtocracy,[520] and a book of the same title and content examine whether the recent Siemens scandal and uncommercial ECB loans which were conditional on the purchase of military aircraft and submarines are evidence that the loans amount to odious debt and that an audit would result in invalidation of a large amount of the debt.[521]

The revision of Greece's 2009 budget deficit from a forecast of "6–8% of GDP" to 15.4% in 2010 was the key trigger for the Greek debt crisis. However accusations of the use of "creative accounting" by several other governments have been raised[522][523][524][525][526]
the United Kingdom,[527][528][529][530][531] Spain,[532] the United States,[533][534][535] and even Germany.[536][537][538]

On 18 August 2011, as requested by the Finnish parliament as a condition for any further bailouts, it became apparent that Finland would receive collateral from Greece, enabling it to participate in the potential new €109 billion support package for the Greek economy.[539] Austria, the Netherlands, Slovenia, and Slovakia responded with irritation over this special guarantee for Finland and demanded equal treatment across the eurozone, or a similar deal with Greece, so as not to increase the risk level over their participation in the bailout.[540] The main point of contention was that the collateral is aimed to be a cash deposit, a collateral the Greeks can only give by recycling part of the funds loaned by Finland for the bailout, which means Finland and the other eurozone countries guarantee the Finnish loans in the event of a Greek default.[539] Finland's recommendation to the crisis countries is to issue asset-backed securities to cover the immediate need, a tactic successfully used in Finland's early 1990s recession,[541] in addition to spending cuts and bad banking.

After extensive negotiations to implement a collateral structure open to all eurozone countries, on 4 October 2011, a modified escrow collateral agreement was reached. The expectation is that only Finland will utilise it, due, in part, to a requirement to contribute initial capital to European Stability Mechanism in one instalment instead of five instalments over time. Finland, as one of the strongest AAA countries, can raise the required capital with relative ease.[542]

In February 2012, the four largest Greek banks agreed to provide the €880 million in collateral to Finland to secure the second bailout programme.[543]

The handling of the crisis has led to the premature end of several European national governments and influenced the outcome of many elections:A flexible-fuel vehicle (FFV) or dual-fuel vehicle (colloquially called a flex-fuel vehicle) is an alternative fuel vehicle with an internal combustion engine designed to run on more than one fuel, usually gasoline blended with either ethanol or methanol fuel, and both fuels are stored in the same common tank. Modern flex-fuel engines are capable of burning any proportion of the resulting blend in the combustion chamber as fuel injection and spark timing are adjusted automatically according to the actual blend detected by a fuel composition sensor.  Flex-fuel vehicles are distinguished from bi-fuel vehicles, where two fuels are stored in separate tanks and the engine runs on one fuel at a time, for example, compressed natural gas (CNG), liquefied petroleum gas (LPG), or hydrogen.

The most common commercially available FFV in the world market is the ethanol flexible-fuel vehicle,[1] with about 60 million automobiles, motorcycles and light duty trucks manufactured and sold worldwide by March 2018, and concentrated in four markets, Brazil (30.5 million light-duty vehicles and over 6 million motorcycles),[2] the United States (27 million by the end of 2021),[3] Canada (1.6 million by 2014),[4] and Europe, led by Sweden (243,100).[5][6][7] In addition to flex-fuel vehicles running with ethanol, in Europe and the US, mainly in California, there have been successful test programs with methanol flex-fuel vehicles, known as M85 flex-fuel vehicles.[1][8] There have been also successful tests using P-series fuels with E85 flex fuel vehicles, but as of June 2008, this fuel is not yet available to the general public.[9][10] These successful tests with P-series fuels were conducted on Ford Taurus and Dodge Caravan flexible-fuel vehicles.[11]

Though technology exists to allow ethanol FFVs to run on any mixture of gasoline and ethanol, from pure gasoline up to 100% ethanol (E100),[12][13] North American and European flex-fuel vehicles are optimized to run on E85, a blend of 85% anhydrous ethanol fuel with 15% gasoline. This upper limit in the ethanol content is set to reduce ethanol emissions at low temperatures and to avoid cold starting problems during cold weather, at temperatures lower than 11 °C (52 °F).[14] The alcohol content is reduced during the winter in regions where temperatures fall below 0 °C (32 °F)[15] to a winter blend of E70 in the U.S.[16][17] or to E75 in Sweden[18] from November until March.[19] Brazilian flex fuel vehicles are optimized to run on any mix of E20-E25 gasoline and up to 100% hydrous ethanol fuel (E100). The Brazilian flex vehicles were built-in with a small gasoline reservoir for cold starting the engine when temperatures drop below 15 °C (59 °F).[20] An improved flex motor generation was launched in 2009 which eliminated the need for the secondary gas tank.[21][22][23][24]

As ethanol FFVs became commercially available during the late 1990s, the common use of the term "flexible-fuel vehicle" became synonymous with ethanol FFVs.[12][25][26][27][28] In the United States flex-fuel vehicles are also known as "E85 vehicles". In Brazil, the FFVs are popularly known as "total flex" or simply "flex" cars. In Europe, FFVs are also known as "flexifuel" vehicles. Automakers, particularly in Brazil and the European market, use badging in their FFV models with the some variant of the word "flex", such as Volvo Flexifuel, or Volkswagen Total Flex, or Chevrolet FlexPower or Renault Hi-Flex, and Ford sells its Focus model in Europe as Flexifuel and as Flex in Brazil. In the US, only since 2008 FFV models feature a yellow gas cap with the label "E85/Gasoline" written on the top of the cap to differentiate E85s from gasoline only models.[29]

Flexible-fuel vehicles (FFVs) are based on dual-fuel systems that supply both fuels into the combustion chamber at the same time in various calibrated proportions. The most common fuels used by FFVs today are unleaded gasoline and ethanol fuel. Ethanol FFVs can run on pure gasoline, pure ethanol (E100) or any combination of both.[26][27][30] Methanol has also been blended with gasoline in flex-fuel vehicles known as M85 FFVs, but their use has been limited mainly to demonstration projects and small government fleets, particularly in California.[8]

The Ford Model T, produced from 1908 through 1927, was fitted with a carburetor with adjustable jetting, allowing use of ethanol, gasoline or kerosene (each by itself), or a combination of the first two mentioned fuels.[56][57][58][59] Other car manufactures also provided engines for ethanol fuel use.[13] Henry Ford continued to advocate for ethanol as fuel even during Prohibition. However, cheaper oil caused gasoline to prevail, until the 1973 oil crisis resulted in gasoline shortages and awareness on the dangers of oil dependence. This crisis opened a new opportunity for ethanol and other alternative fuels, such as methanol, gaseous fuels such as CNG and LPG, and also hydrogen.[8][13] Ethanol, methanol and natural gas were the three alternative fuels that received more attention for research and development, and government support.

Since 1975, and as a response to the shock caused by the first oil crisis, the Brazilian government implemented the National Alcohol Program -Pró-Álcool- (Portuguese: Programa Nacional do Álcool), a nationwide program financed by the government to phase out automotive fuels derived from fossil fuels in favor of ethanol made from sugar cane.[60][61] It began with a low blend of anhydrous alcohol with regular gasoline in 1976,[62] and since July 2007 the mandatory blend is 25% of alcohol or gasohol E25.[41] In 1979, and as a response to the second oil crisis, the first vehicle capable of running with pure hydrous ethanol (E100) was launched to the market, the Fiat 147,[61][63] after testing with several prototypes developed by Fiat, Volkswagen, GM and Ford.[60] The Brazilian government provided three important initial drivers for the ethanol industry: guaranteed purchases by the state-owned oil company Petrobras, low-interest loans for agro-industrial ethanol firms, and fixed gasoline and ethanol prices. After reaching more than 4 million cars and light trucks running on pure ethanol by the late 1980s,[64] the use of E100-only vehicles sharply declined after increases in sugar prices produced shortages of ethanol fuel.

After extensive research that began in the 90s, a second push took place in March 2003, when the Brazilian subsidiary of Volkswagen launched to the market the first full flexible-fuel car, the Gol 1.6 Total Flex.[65][66][67] Several months later was followed by other Brazilian automakers, and by 2010 General Motors, Fiat, Ford, Peugeot, Renault, Volkswagen, Honda, Mitsubishi, Toyota, Citroën, Nissan and Kia Motors were producing popular models of flex cars and light trucks.[68][69][70][71] The adoption of ethanol flex fuel vehicles was so successful, that production of flex cars went from almost 40 thousand in 2003 to 1.7 million in 2007.[64] This rapid adoption of the flex technology was facilitated by the fuel distribution infrastructure already in place, as around 27,000 filling stations countrywide were available by 1997 with at least one ethanol pump, a heritage of the Pró-Álcool program.[72]

In the United States, initial support to develop alternative fuels by the government was also a response to the first oil crisis, and some time later, as a goal to improve air quality. Also, liquid fuels were preferred over gaseous fuels not only because they have a better volumetric energy density but also because they were the most compatible fuels with existing distribution systems and engines, thus avoiding a big departure from the existing technologies and taking advantage of the vehicle and the refueling infrastructure.[8] California led the search of sustainable alternatives with interest focused in methanol. Ford Motor Company and other automakers responded to California's request for vehicles that run on methanol. In 1981, Ford delivered 40 dedicated methanol fuel (M100) Escorts to Los Angeles County, but only four refueling stations were installed.[8] The biggest challenge in the development of alcohol vehicle technology was getting all of the fuel system materials compatible with the higher chemical reactivity of the fuel. Methanol was even more of a challenge than ethanol but much of the early experience gained with neat ethanol vehicle production in Brazil was transferable to methanol. The success of this small experimental fleet of M100s led California to request more of these vehicles, mainly for government fleets. In 1983, Ford built 582 M100 vehicles; 501 went to California, and the remaining to New Zealand, Sweden, Norway, United Kingdom, and Canada.[8]

As an answer to the lack of refueling infrastructure, Ford began development of a flexible-fuel vehicle in 1982, and between 1985 and 1992, 705 experimental FFVs were built and delivered to California and Canada, including the 1.6L Ford Escort, the 3.0L Taurus, and the 5.0L LTD Crown Victoria. These vehicles could operate on either gasoline or methanol with only one fuel system. Legislation was passed to encourage the US auto industry to begin production, which started in 1993 for the M85 FFVs at Ford. In 1996, a new FFV Ford Taurus was developed, with models fully capable of running on either methanol or ethanol blended with gasoline.[8][73] This ethanol version of the Taurus became the first commercial production of an E85 FFV.[74] The momentum of the FFV production programs at the American car companies continued, although by the end of the 1990s, the emphasis shifted to the FFV E85 version, as it is today.[8] Ethanol was preferred over methanol because there is a large support from the farming community, and thanks to the government's incentive programs and corn-based ethanol subsidies available at the time.[75] Sweden also tested both the M85 and the E85 flexifuel vehicles, but due to agriculture policy, in the end emphasis was given to the ethanol flexifuel vehicles.[76] Support for ethanol also comes from the fact that it is a biomass fuel, which addresses climate change concerns and greenhouse gas emissions, though nowadays these benefits are questioned and depend on the feedstock used for ethanol production and their indirect land use change impacts.[13][77][78][79]

The demand for ethanol fuel produced from field corn in the United States was stimulated by the discovery in the late 90s that methyl tertiary butyl ether (MTBE), an oxygenate additive in gasoline, was contaminating groundwater.[13][80] Due to the risks of widespread and costly litigation, and because MTBE use in gasoline was banned in almost 20 states by 2006, the substitution of MTBE opened a new market for ethanol fuel.[13] This demand shift for ethanol as an oxygenate additive took place at a time when oil prices were already significantly rising. By 2006, about 50 percent of the gasoline used in the U.S. contained ethanol at different proportions, and ethanol production grew so fast that the US became the world's top ethanol producer, overtaking Brazil in 2005.[81][82] This shift also contributed to a sharp increase in the production and sale of E85 flex vehicles since 2002.[83]

Flexible-fuel technology started being developed by Brazilian engineers near the end of the 1990s. The Brazilian flexible fuel car is built with an ethanol-ready engine and one fuel tank for both fuels. The small gasoline reservoir for starting the engine in cold weather, used in earlier neat ethanol vehicles, was kept to avoid start up problems in the central and southern regions, where winter temperatures normally drop below 15 °C (59 °F).[20] An improved flex motor generation was launched in 2009 and allowed to eliminate the need for this secondary gas reservoir tank.[21][22] Another improvement was the reduction of fuel consumption and tailpipe emissions, between 10% and 15% as compared to flex motors sold in 2008.[22] In March 2009 Volkswagen do Brasil launched the Polo E-Flex, the first flex fuel model without an auxiliary tank for cold start.[23][24]

A key innovation in the Brazilian flex technology was avoiding the need for an additional dedicated sensor to monitor the ethanol-gasoline mix, which made the first American M85 flex fuel vehicles too expensive.[89]

Brazilian flex cars are capable of running on just hydrated ethanol (E100), or just on a blend of gasoline with 25 to 27% anhydrous ethanol (the mandatory blend), or on any arbitrary combination of both fuels.[13][41][42]

The flexibility of Brazilian FFVs empowers the consumers to choose the fuel depending on current market prices. As ethanol fuel economy is lower than gasoline because of ethanol's energy content is close to 34% less per unit volume than gasoline,[90][91][92] flex cars running on ethanol get a lower mileage than when running on pure gasoline. However, this effect is partially offset by the usually lower price per liter of ethanol fuel. As a rule of thumb, Brazilian consumers are frequently advised by the media to use more alcohol than gasoline in their mix only when ethanol prices are 30% lower or more than gasoline, as ethanol price fluctuates heavily depending on the result of seasonal sugar cane harvests.[93][94]

In March 2003 Volkswagen do Brasil launched in the market the Gol 1.6 Total Flex, the first commercial flexible fuel vehicle capable of running on any blend of gasoline and ethanol.[65][66][67] GM do Brasil followed three months later with the Chevrolet Corsa 1.8 Flexpower, using an engine developed by a joint-venture with Fiat called PowerTrain.[95] Passenger flex-fuel vehicles became a commercial success in the country, and as of December 2013[update], a total of 15 car manufacturers produce flex-fuel engines for the Brazilian market, dominating all light vehicle segments except sports cars, off-road vehicles and minivans.[96]

The production of flex-fuel cars and light commercial vehicles since 2003 reached the milestone of 10 million vehicles in March 2010.[97][98] At the end of 2012 registrations of flex-fuel cars and light trucks represented 87% of all passenger and light duty vehicles sold in the country in 2012,[99] and climbed to a 94% market share of all new passenger vehicles sales in 2013.[96] Production passed the 20 million-unit mark in June 2013.[100]

By the end of 2014, flex-fuel cars represented 54% of the Brazilian registered stock of light-duty vehicles, while gasoline only vehicles represented 34.3%.[101] As of June 2015[update], flex-fuel light-duty vehicle sales totaled 25.5 million units.[102] The market share of flex vehicles reached 88.6% of all light-duty registrations in 2017. As of March 2018[update], fifteen years after the launch of the first flex fuel car, there were 30.5 million flex cars and light trucks registered in the country, and 6 million flex motorcycles.[2]

The rapid success of flex vehicles was made possible by the existence of 33,000 filling stations with at least one ethanol pump available by 2006, a heritage of the early Pró-Álcool ethanol program.[103][104] These facts, together with the mandatory use of E25 blend of gasoline throughout the country,[41] allowed Brazil in 2008 to achieve more than 50% of fuel consumption in the gasoline market from sugar cane-based ethanol.[105][106][107]

According to two separate research studies conducted in 2009, at the national level 65% of the flex-fuel registered vehicles regularly used ethanol fuel, and the usage climbed to 93% in São Paulo, the main ethanol producer state where local taxes are lower, and prices at the pump are more competitive than gasoline.[108] However, as a result of higher ethanol prices caused by the Brazilian ethanol industry crisis that began in 2009, combined with government subsidies to keep gasoline price lower than the international market value, by November 2013 only 23% flex-fuel car owners were using ethanol, down from 66% in 2009.[109]

One of the latest innovation within the Brazilian flexible-fuel technology is the development of flex-fuel motorcycles. The first flex-fuel motorcycle was launched by Honda in March 2009, the CG 150 Titan Mix.[110][111] In September 2009, Honda launched a second flexible-fuel motorcycle, the on-off-road NXR 150 Bros Mix.[112] By December 2012 the five available models of flexible-fuel motorcycles from Honda and Yamaha reached a cumulative production of 2,291,072 units, representing 31.8% of all motorcycles manufactured in Brazil since 2009, and 48.2% of motorcycle production in 2012.[85][86][87][88] Flexible-fuel motorcycle production passed the 3 million-unit milestone in October 2013.[113] The 4 million mark was reached in March 2015.[114]

Flexible-fuel vehicles were introduced in Sweden as a demonstration test in 1994, when three Ford Taurus were imported to show the technology existed. Because of the existing interest, a project was started in 1995 with 50 Ford Taurus E85 flexifuel in different parts of Sweden: Umeå, Örnsköldsvik, Härnösand, Stockholm, Karlstad, Linköping, and Växjö. From 1997 to 1998 an additional 300 Taurus were imported, and the number of E85 fueling grew to 40.[115] Then in 1998 the city of Stockholm placed an order for 2,000 of FFVs for any car manufacturer willing to produce them. The objective was to jump-start the FFV industry in Sweden. The two domestic car makers Volvo Group and Saab AB refused to participate arguing there were not in place any ethanol filling stations. However, Ford Motor Company took the offer and began importing the flexifuel version of its Focus model, delivering the first cars in 2001, and selling more than 15,000 FFV Focus by 2005, then representing an 80% market share of the flexifuel market.[116]

In 2005 both Volvo and Saab introduced to the Sweden market their flexifuel models. Saab began selling its 9-5 2.0 Biopower, joined in 2006 by its 9-5 2.3 Biopower. Volvo introduced its S40 and V50 with flexible-fuel engines, joined in late 2006 by the new C30. All Volvo models were initially restricted to the Sweden market, until 2007, when these three models were launched in eight new European markets.[117] In 2007, Saab also started selling a BioPower version of its popular Saab 9-3 line. In 2008 the Saab-derived Cadillac BLS was introduced with E85 compatible engines, and Volvo launched the V70 with a 2.5-litre turbocharged Flexifuel engine.[118]

All flexible-fuel vehicles in Sweden use an E75 winter blend instead of E85 to avoid engine starting problems during cold weather.[18] This blend was introduced since the winter 2006–07 and E75 is used from November until March.[19] For temperature below −15 °C (5 °F) E85 flex vehicles require an engine block heater.[19] The use of this device is also recommended for gasoline vehicles when temperatures drop below −23 °C (−9 °F).[13] Another option when extreme cold weather is expected is to add more pure gasoline in the tank, thus reducing the ethanol content below the E75 winter blend, or simply not to use E85 during extreme low temperature spells.[13][19]

Sweden has achieved the largest E85 flexible-fuel vehicle fleet in Europe, with a sharp growth from 717 vehicles in 2001 to 243,136 through December 2014.[5][6][7][116] As of 2008 a total of 70% of all flexifuel vehicles operating in the EU were registered in Sweden.[119] The recent and accelerated growth of the Swedish fleet of E85 flexifuel vehicles is the result of the National Climate Policy in Global Cooperation Bill passed in 2005, which not only ratified the Kyoto Protocol but also sought to meet the 2003 EU Biofuels Directive regarding targets for use of biofuels, and also let to the 2006 government's commitment to eliminate oil imports by 2020.[116][120]

In order to achieve these goals several government incentives were implemented. Ethanol, as the other biofuels, was exempted of both, the CO2 and energy taxes until 2009, resulting in a 30% price reduction at the pump of E85 fuel over gasoline. Furthermore, other demand side incentives for flexifuel vehicle owners include a US$1,800 bonus to buyers of FFVs, exemption from the Stockholm congestion tax, up to 20% discount on auto insurance, free parking spaces in most of the largest cities, owner annual registration taxes, and a 20% tax reduction for flexifuel company cars. Also, a part of the program, the Swedish Government ruled that 25% of their vehicle purchases (excluding police, fire and ambulance vehicles) must be alternative fuel vehicles.[116][120][121] By the first months of 2008, this package of incentives resulted in sales of flexible-fuel cars representing 25% of new car sales.[116]

On the supply side, since 2005 the gasoline fuelling stations selling more than 3 million liters of fuel a year are required to sell at least one type of biofuel, resulting in more than 1,200 gas stations selling E85 by August 2008.[116][120] Despite all the sharp growth of E85 flexifuel cars, by 2007 they represented just two percent of the four million Swedish vehicles.[122] In addition, this law also mandated all new filling stations to offer alternative fuels, and stations with an annual volume of more than one million liters are required to have an alternative fuel pump by December 2009. Therefore, the number of E85 pumps is expected to reach by 2009 nearly 60% of Sweden's 4,000 filling stations.[121]

The Swedish-made Koenigsegg Jesko 300, the low downforce version of the Koenigsegg Jesko, is currently the fastest and most powerful flexible fuel vehicle with its turbocharged V8 producing over 1600 hp when running on biofuel, as compared to 1280 hp on 95 octane unleaded gasoline.[123][124]

Flexifuel vehicles are sold in 18 European countries, including Austria, Belgium, Czech Republic, Denmark, Estonia, Finland, France, Germany, Hungary, Ireland, Italy, the Netherlands, Norway, Poland, Spain, Sweden, Switzerland, and the United Kingdom.[132][133] Ford, Volvo and Saab are the main automakers offering flexifuel autos in the region.[117][132][134][135]

Biofuel cars in general get strong tax incentives in France, including a 0 or 50% reduction on the tax on new vehicles, and a 40% reduction on CO2 tax for new cars. For company cars there is a corporate car tax free for two years and a recovery of 80% of the value added tax (VAT) on E85 vehicles.[130] Also, E85 fuel price is set significantly lower than diesel or gasoline, resulting in E85 at € 0.80, diesel at €1.15, and gasoline at €1.30 per liter, as of April 2007. By May 2008, France had 211 pumps selling E85,[136] even though the government made plans for the installation of up to 500 E85 pumps by year end 2007.[137] French automakers Renault and PSA (Citroen & Peugeot) announced they will start selling FFV cars beginning in the summer 2007.

Biofuel emphasis in Germany is on biodiesel,[120] and no specific incentives have been granted for E85 flex-fuel cars; however, there is complete exemption of taxes on all biofuels while there is a normal tax of €0.65 per liter of petroleum fuels.[120][130] The distribution of E85 began in 2005,[138] and with 219 stations as of September 2008, Germany ranks second after Sweden with the most E85 fueling stations in the EU.[139]
As of July 2012 retail prices of E85 was €1.09 per liter, and gasoline was priced at €1.60 per liter (for gasoline RON 95), then providing enough margin to compensate for ethanol's lower fuel economy.[137]
Ford has offered the Ford Focus since August 2005 in Germany. Ford is about to offer also the Mondeo and other models as FFV versions between 2008 and 2010. The Saab 9-5 and Saab 9-3 Biopower, the Peugeot 308 Bioflex, the Citroën C4 Bioflex, the Audi A5, two models of the Cadillac BLS, and five Volvo models are also available in the German market by 2008.[140] Since 2011, Dacia offers the Logan MCV with a 1.6l 16v flexfuel engine.

Ireland is the third best seller European market of E85 flex-fuel vehicles, after Sweden and France.[141] Bioethanol (E85) in Ireland is made from whey, a waste product of cheese manufacturing.[141] The Irish government established several incentives, including a 50% discount in vehicle registration taxes (VRT), which can account for more than one third of the retail price of a new car in Ireland (around €6,500). The bioethanol element of the E85 fuel is excise-free for fuel companies, allowing retail prices to be low enough to offset the 25 per cent cut in fuel economy that E-85 cars offer, due to ethanol's lower energy content than gasoline. Also, the value added tax (VAT) on the fuel can also be claimed back.[130][141] E-85 fuel is available across the country in more than 20 of Maxol service stations.[142] In October 2005, the 1.8 Ford Focus FFV[143] became the first flexible-fuel vehicle to be commercially sold in Ireland. Later Ford launched the C-max and the Mondeo flexifuel models. Saab and Volvo also have E85 models available.[142]

From 1 January 2011 E85 fuel is no longer excise-free in Ireland. Maxol has announced they will not provide E85 when their current supplies have run out.[144]

The first flexifuel vehicles were introduced in Spain by late 2007, with the acquisition of 80 cars for use in the Spaniard official government fleet. At that time the country had only three gas stations selling E85, making necessary to deploy an official E85 fueling station in Madrid to attend these vehicles.[145] Despite the introduction in the Spaniard market of several flexifuel models, by the end of 2008 still persists the problems of adequate E85 fueling infrastructure, as only 10 gas stations were selling E85 fuel to the public in the entire country.[146]

The UK government established several incentives for E85 flex-fuel vehicles. These include a fuel duty rebate on E85 fuel of 20 p per liter, until 2010; a £10 to 15 reduction in the vehicle excise duty (VED); and a 2% annual company car tax discount for flex-fuel cars.[130] Despite the small number of E85 pump stations available, limited to the Morrisons supermarket chain stations,[129][130] most automakers offer the same models in the UK that are available in the European market. In 2005 the Ford Focus Flexi-Fuel became the first flexible-fuel car sold in the UK, though E85 pumps were not opened until 2006.[147] Volvo now offers its flexifuel models S80, S40, C30, V50 and V70.[148] Other models available in the UK are the Ford C-Max Flexi-Fuel,[149] and the Saab models 9-5 and 9-3 Flex-Fuel Biopower, and the new Saab Aero X BioPower E100 bioethanol.[150] Despite being introduced around a decade ago, E85 is no longer commercially available in the UK[151]

As of 2017[update], there were more than 21 million E85 flex-fuel vehicles in the United States,[3] up from about 11 million flex-fuel cars and light trucks in operation as of early 2013.[153][154] The number of flex-fuel vehicles on U.S. roads increased from 1.4 million in 2001, to 4.1 million in 2005, and rose to 7.3 million in 2008.[13][83]

For the 2011 model year there are about 70 vehicles E85 capable, including sedans, vans, SUVs and pick-up trucks. Many of the models available in the market are trucks and sport-utility vehicles getting less than 20 mpg‑US (12 L/100 km; 24 mpg‑imp) when filled with gasoline.[155] Actual consumption of E85 among flex-fuel vehicle owners is limited. Nevertheless, the U.S. Department of Energy estimated that in 2011 only 862,837 flex-fuel fleet-operated vehicles were regularly fueled with E85.[156] As a result, from all the ethanol fuel consumed in the country in 2009, only 1% was E85 consumed by flex-fuel vehicles.[157]

The E85 blend is used in gasoline engines modified to accept such higher concentrations of ethanol, and the fuel injection is regulated through a dedicated sensor, which automatically detects the amount of ethanol in the fuel, allowing to adjust both fuel injection and spark timing accordingly to the actual blend available in the vehicle's tank. Because ethanol contains close to 34% less energy per unit volume than gasoline, E85 FFVs have a lower mileage per gallon than gasoline.[91][92] Based on EPA tests for all 2006 E85 models, the average fuel economy for E85 vehicles was 25.56% lower than unleaded gasoline.[13]

The American E85 flex-fuel vehicle was developed to run on any mixture of unleaded gasoline and ethanol, anywhere from 0% to 85% ethanol by volume. Both fuels are mixed in the same tank, and E85 is sold already blended. In order to reduce ethanol evaporative emissions and to avoid problems starting the engine during cold weather, the maximum blend of ethanol was set to 85%.[14] There is also a seasonal reduction of the ethanol content to E70 (called winter E85 blend) in very cold regions,[16] where temperatures fall below 0 °C (32 °F) during the winter.[14][15] In Wyoming for example, E70 is sold as E85 from October to May.[16]

E85 flex-fuel vehicles are becoming increasingly common in the Midwest, where corn is a major crop and is the primary feedstock for ethanol fuel production. Regional retail E85 prices vary widely across the US, with more favorable prices in the Midwest region, where most corn is grown and ethanol produced. Depending on the vehicle capabilities, the break-even price of E85 has to be between 25 and 30% lower than gasoline.[13]

A 2005 survey found that 68% of American flex-fuel car owners were not aware they owned an E85 flex.[13][25] This was because the exteriors of flex and non-flex vehicles look exactly the same; there is no sale price difference between them; the lack of consumers' awareness about E85s; and also the initial decision of American automakers of not putting any kind of exterior labeling, so buyers could be unaware they are purchasing an E85 vehicle.[13][104] Since 2008, all new FFV models in the US feature a bright yellow gas cap to remind drivers of the E85 capabilities and proper flex-fuel badging.[29][158][159][160]

Some critics have argued that American automakers have been producing E85 flex models motivated by a loophole in the Corporate Average Fuel Economy (CAFE) requirements, that allows for a fuel economy credit for every flex-fuel vehicle sold, whether or not in practice these vehicles are fueled with E85.[104][161] This loophole might allow the car industry to meet the CAFE targets in fuel economy just by spending between US$100 and US$200 that it cost to turn a conventional vehicle into a flex-fuel, without investing in new technology to improve fuel economy, and saving them the potential fines for not achieving that standard in a given model year.[161][162] The CAFE standards proposed in 2011 for the period 2017–2025 will allow flexible-fuel vehicles to receive extra credit but only when the carmakers present data proving how much E85 such vehicles have actually consumed.[163]

A major restriction hampering sales of E85 flex vehicles, or fueling with E85, is the limited infrastructure available to sell E85 to the public with only 2% of the motor fuel stations offering E85 by March 2014.[164] As of November 2015[update], there were only 3,218 fueling stations selling E85 to the public in the entire U.S.,[165] while about 156,000 retail motor fuel outlets do not offer any ethanol blend.[164] In addition, there has been a great concentration of E85 stations in the Corn Belt states.[166] The main constraint for a more rapid expansion of E85 availability is that it requires dedicated storage tanks at filling stations,[13] at an estimated cost of US$60,000 for each dedicated ethanol tank.[167] The Obama Administration set the goal of installing 10,000 blender pumps nationwide until 2015, and to support this target the US Department of Agriculture (USDA) issued a rule in May 2011 to include flexible fuel pumps in the Rural Energy for America Program (REAP). This ruling will provide financial assistance to fuel station owners to install E85 and blender pumps.[168][169]

A flex fuel conversion kit is a kit that allows a conventional equipment manufactured vehicle to be altered to operate on propane, natural gas, methane gas, ethanol, or electricity are classified as aftermarket AFV conversions. All vehicle conversions, except those that are completed for a vehicle to run on electricity, must meet current applicable U.S. Environmental Protection Agency (EPA) standards.[170]


In 2008, Ford delivered the first flex-fuel plug-in hybrid as part of a demonstration project, a Ford Escape Plug-in Hybrid capable of running on E85 or gasoline.[48] General Motors announced that the new Chevrolet Volt plug-in hybrid, launched in the United States market in December 2010, would be flex-fuel-capable in 2013.[52] General Motors do Brasil announced that it will import from five to ten Volts to Brazil during the first semester of 2011 as part of a demonstration and also to lobby the federal government to enact financial incentives for green cars. If successful, GM would adapt the Volt to operate on ethanol fuel, as most new cars sold in Brazil are flex-fuel.[171][172]
In 2008, Chrysler, General Motors, and Ford pledged to manufacture 50 percent of their entire vehicle line as flexible fuel in model year 2012, if enough fueling infrastructure develops.[29][173][174] The Open Fuel Standard Act (OFS), introduced to Congress in May 2011, is intended to promote a massive adoption of flex-fuel vehicles capable of running on ethanol or methanol. The bill requires that 50 percent of automobiles made in 2014, 80 percent in 2016, and 95 percent in 2017, would be manufactured and warranted to operate on non-petroleum-based fuels, which includes existing technologies such as flex-fuel, natural gas, hydrogen, biodiesel, plug-in electric and fuel cell.[175][176][177]

As of December 2014[update], almost half of new vehicles produced by Chrysler, Ford, and General Motors are flex-fuel, meaning roughly one-quarter of all new vehicles sold by 2015 are capable of using up to E85. However, obstacles to widespread use of E85 fuel remain. A 2014 analysis by the Renewable Fuels Association (RFA) found that oil companies prevent or discourage affiliated retailers from selling E85 through rigid franchise and branding agreements, restrictive supply contracts, and other tactics. The report showed independent retailers are five times more likely to offer E85 than retailers carrying an oil company brand.[178]

In January 2007 GM brought UK-sourced Saab 9-5 Biopower E85 flex-fuel vehicles to Australia as a trial, in order to measure interest in ethanol-powered vehicles in the country. Saab Australia placed the vehicles with the fleets of the Queensland Government, the media, and some ethanol producers. E85 is not available widely in Australia, but the Manildra Group provided the E85 blend fuel for this trial.[179]

Saab Australia became the first car maker to produce an E85 flex-fuel car for the Australian market with the Saab 9-5 BioPower. One month later launched the new 9-3 BioPower, the first vehicle in Australia to give drivers a choice of three fuels, E85, diesel or gasoline, and both automobiles are sold for a small premium. Australia's largest independent fuel retailer, United Petroleum, announced plans to install Australia's first commercial E85 fuel pumps, one in Sydney and one in Melbourne.[180][181]

GM Holden, the Victorian state government, Coskata, Caltex, Veolia Environmental Services and Mitsui have announced a consortium with a co-ordinated plan to build a bio-ethanol plant from household waste for use as E85 fuel.[182] In August 2010 Caltex launched the E85 ethanol fuel called Bio E-Flex, designed for use in the Holden Commodore VE Series II flex-fuel vehicles to be released later in 2010. Caltex Australia plans to begin selling Bio E-Flex in Melbourne from September and expects to have Bio E-Flex available in more than 30 service stations in Melbourne, Sydney, Brisbane, Adelaide and Canberra by the end of October, with plans to increase to 100 metropolitan and regional locations in 2011.[183][obsolete source]

As part of the North American auto market, by 2007 Canada had available 51 models of E85 flex-vehicles, most from Chrysler, Ford and General Motors, including automobiles, pickup trucks, and SUVs.[184] The country had around 1.6 million capable flex fuel E85s on the roads by 2014.[4] However, most users are not aware they own an E85, as vehicles are not clearly labeled as such, and even though the newer models have a yellow cap in the fuel tank informing that the vehicle can handle E85, most users are still not aware because there are very few gas stations offering E85.[185] Another major drawback to greater E85 fuel use is the fact that by June 2008 Canada had only three public E85 pumps, all located in Ontario, in the cities of Guelph, Chatham, and Woodstock.[186][187] E85 fueling is available primarily for fleet vehicles, including 20 government refueling stations not available for the public.[185][186][188] The main feedstocks for E85 production in Canada are corn and wheat,[185] and there were several proposals being discussed to increase the actual use of E85 fuel in FFVs, such as creating an ethanol-friendly highway or ethanol corridor.[186][189]

In March 2009 the Colombian government enacted a mandate to introduce E85 flexible-fuel cars. The executive decree applies to all gasoline-powered vehicles with engines smaller than 2.0 liters manufactured, imported, and commercialized in the country beginning in 2012, mandating that 60% of such vehicles must have flex-fuel engines capable of running with gasoline or E85, or any blend of both. By 2014 the mandatory quota is 80% and it will reach 100 percent by 2016. All vehicles with engines bigger than 2.0 liters must be E85 capable starting in 2013. The decree also mandates that by 2011 all gasoline stations must provide infrastructure to guarantee availability of E85 throughout the country.[190] The mandatory introduction of E85 flex-fuels has caused controversy among carmakers, car dealers, gasoline station owners, and even some ethanol producers complained the industry is not ready to supply enough ethanol for the new E85 fleet.[191][192]

Union Minister for Road Transport and Highways Nitin Gadkari has emphasised the adoption of alternative fuels which will be import substitutes, cost-effective, pollution-free, indigenous, and discourage the use of Petrol or diesel. During an event held on 20 October 2021, while addressing the media and journalists he has said that the government will ask all vehicle manufacturers to make flex-fuel engines under the Euro VI emission norms in the next six-eight months. Flex-fuel, or flexible fuel, is an alternative fuel made of a combination of gasoline and methanol or ethanol.

Mr. Gadkari has predicted that in the next 15 years, Indian automobile industry will be worth Rs 15 lakh crore and the Government is planning to submit an affidavit in the Hon'ble Supreme Court of India to allow manufacturing of flex-fuel engines under the Euro IV emission norms but for now he said that the Indian Government will ask all vehicle manufacturers to make flex-fuel engines (that can run on more than one fuel) under the Euro VI emission norms in the next 6–8 months.

In 2006 New Zealand began a pilot project with two E85 Ford Focus Flexi-Fuel evaluation cars. The main feedstock used in New Zealand for ethanol production is whey, a by-product of milk production.[193]

Government officials and businessmen from Paraguay began negotiations in 2007 with Brazilian automakers in order to import flex cars that run on any blend of gasoline and ethanol. If successful, Paraguay would become the first destination for Brazilian flex-fuel car exports.[194] In May 2008, the Paraguayan government announced a plan to eliminate import taxes of flex-fuel vehicles and an incentive program for ethanol production.[195] The plan also includes the purchase of 20,000 flex cars in 2009 for the government fleet.[196]

In 2006, tax incentives were established in Thailand for the introduction of compressed natural gas (CNG) as an alternative fuel, by eliminating import duties and lowering excise taxes on CNG-compatible cars. Then in 2007, Thai authorities approved incentives for the production of "eco-cars", with the goal of the country to become a regional hub for the production of small, affordable and fuel-efficient cars. Seven automakers joint in the program, Toyota, Suzuki, Nissan, Mitsubishi, Honda, Tata and Volkswagen.[197] In 2008 the government announced priority for E85, expecting these flex-fuel vehicles to become widely available in Thailand in 2009, three years ahead of schedule. The incentives include cuts in excise tax rates for E85-compatible cars and reduction of corporate taxes for ethanol producers to make sure E85 fuel supply will be met.[197][198] This new plan however, brought confusion and protests by the automakers which sign-up for the "eco-cars", as competition with the E85 flex-fuel cars will negatively affect their ongoing plans and investments, and their production lines will have to be upgraded at a high cost for them to produce flex-fuel cars.[197][198] They also complained that flex-fuel vehicles popular in a few countries around the world, limiting their export potential as compared with other engine technologies.[197]

Despite the controversy, the first E85 flexible fuel vehicles were introduced in November 2008. The first two models available in the Thai market were the Volvo S80 and the C30. The S80 is manufactured locally and the C30 is imported. By the time of the introduction of flex vehicles there were already two gas stations with E85 fuel available. During 2009 it was expected that 15 fueling stations in Bangkok will have E85 fuel available.[199][200] In October 2009 the Mitsubishi Lancer Ex was launched becoming the first mass-production E85 flexi-fuel vehicle produced in Thailand.[201]Futurama is an American animated science fiction sitcom created by Matt Groening for the Fox Broadcasting Company and later revived by Comedy Central, and then Hulu. The series follows Philip J. Fry, who is cryogenically preserved for 1,000 years and revived on December 31, 2999. Fry finds work at the interplanetary delivery company Planet Express, working alongside the one-eyed mutant Leela and the robot Bender. The series was envisioned by Groening in the mid-1990s while working on The Simpsons; he brought David X. Cohen aboard to develop storylines and characters to pitch the show to Fox.[1]

Following its initial cancellation by Fox, Futurama began airing reruns on Cartoon Network's Adult Swim programming block, which lasted from 2003 to 2007. It was revived in 2007 as four direct-to-video films, the last of which was released in early 2009. Comedy Central entered into an agreement with 20th Century Fox Television to syndicate the existing episodes and air the films as 16 new, half-hour episodes, constituting a fifth season.[2][3]

In June 2009, Comedy Central picked up the show for 26 new half-hour episodes, which began airing in 2010 and 2011.[4][5] The show was renewed for a seventh season, with the first half airing in 2012 and the second in 2013.[6][7][8] An audio-only episode featuring the original cast members was released in 2017 as an episode of The Nerdist Podcast.[9] On February 9, 2022, Hulu revived the series with a 20-episode order, which premiered on July 24, 2023.[10][11] In November 2023, the show was renewed by Hulu for two more broadcast seasons, which will air through 2026.[12][13]

Futurama received critical acclaim throughout its run and was nominated for 17 Annie Awards, winning nine of them, and 12 Emmy Awards, winning six. It was nominated four times for a Writers Guild of America Award, winning for the episodes "Godfellas" and "The Prisoner of Benda". It was nominated for a Nebula Award and received Environmental Media Awards for the episodes "The Problem with Popplers" and "The Futurama Holiday Spectacular".[14] Merchandise includes a tie-in comic book series, video games, calendars, clothes, and action figures. In 2013, TV Guide ranked Futurama one of the top 60 Greatest TV Cartoons of All Time.[15]

Futurama is essentially a workplace sitcom, the plot of which revolves around the Planet Express interplanetary delivery company and its employees,[16] a small group that largely fails to conform to future society.[17] Episodes usually feature the central trio of Fry, Leela, and Bender, though occasional storylines center on the other main characters.

Futurama is set in New New York at the turn of the 31st century, in a time filled with technological wonders. The city of New New York has been built over the ruins of present-day New York City, which has become a catacomb-like space that acts as New New York's sewer, referred to as "Old New York". Parts of the sewers are inhabited by mutants. Various devices and architecture are similar to the Populuxe style. Global warming, inflexible bureaucracy, and substance abuse are a few of the subjects given a 31st-century exaggeration in a world where the problems have become both more extreme and more common. Just as New York has become a more extreme version of itself in the future, other Earth locations are given the same treatment; Los Angeles, for example, is depicted as a smog-filled apocalyptic wasteland.

Numerous technological advances have been made between the present day and the 31st century. The Head Museum, which keeps a collection of heads alive in jars thanks to technology invented by Ron Popeil (who has a guest cameo in "A Big Piece of Garbage"), has resulted in many historical figures and current celebrities being present, including Groening himself; this became the writers' device to feature and poke fun at contemporary celebrities in the show. Several of the preserved heads shown are those of people who were already dead well before the advent of this technology; one of the most prominent examples of this anomaly is former U.S. president Richard Nixon, who died in 1994 and appears in numerous episodes. The Internet, while being fully immersive and encompassing all senses—even featuring its own digital world (similar to Tron or The Matrix)—is slow and largely consists of pornography, pop-up ads, and "filthy" (or Filthy Filthy) chat rooms. Some of it is edited to include educational material ostensibly for youth. Television is still a primary form of entertainment. Self-aware robots are a common sight, and are the main cause of global warming due to the exhaust from their alcohol-powered systems. The wheel is obsolete (no one but Fry even seems to recognize the design),[22] having been forgotten and replaced by hover cars and a network of large, clear pneumatic transportation tubes.

Environmentally, common animals still remain, alongside mutated, cross-bred (sometimes with humans) and extraterrestrial animals. Ironically, spotted owls are often shown to have replaced rats as common household pests. Although rats still exist, sometimes rats act like pigeons, though pigeons still exist, as well. Anchovies have been extinct for 800 years because of the Decapodians. Earth still suffers the effects of greenhouse gases, although in one episode Leela states that its effects have been counteracted by nuclear winter. In another episode, the effects of global warming have been somewhat mitigated by the dropping of a giant ice cube into the ocean, and later by pushing Earth farther away from the Sun, which also extended the year by one week.

Religion is a prominent part of society, although the dominant religions have evolved. A merging of the major religious groups of the 20th century has resulted in the First Amalgamated Church,[23] while Voodoo is now mainstream. New religions include Oprahism, Robotology, and the banned religion of Star Trek fandom. Religious figures include Father Changstein-El-Gamal, the Robot Devil, Reverend Lionel Preacherbot, and passing references to the Space Pope, who appears to be a large crocodile-like creature. Several major holidays have robots associated with them, including the murderous Robot Santa and Kwanzaa-bot. While very few episodes focus exclusively on religion within the Futurama universe, they do cover a wide variety of subjects including predestination, prayer, the nature of salvation, and religious conversion.[23]

Futurama's setting is a backdrop, and the writers are not above committing continuity errors if they serve to further the gags. For example, while the pilot episode implies that the previous Planet Express crew was killed by a space wasp, the later episode "The Sting" is based on the crew having been killed by space bees instead.[24] The "world of tomorrow" setting is used to highlight and lampoon issues of today and to parody the science-fiction genre.[25]

This list follows the season box sets, which feature the episodes in the original production season order, ignoring the order of broadcast. The Hulu revival has alternatively been titled the eighth and ninth seasons (production),[26] and the eleventh and twelfth seasons (broadcast).[27][28] The tenth and eleventh production seasons will also be considered as Futurama's thirteenth and fourteenth broadcast seasons.[29]

The television network Fox expressed a strong desire in the mid-1990s for Matt Groening to create a new series after the success of his previous series, The Simpsons; Groening began conceiving Futurama during this period. In 1995, he enlisted David X. Cohen, then a writer and producer for The Simpsons, to assist in developing the show. The two spent time researching science fiction books, television shows, and films. When they pitched the series to Fox in April 1998, Groening and Cohen had composed many characters and story lines; Groening claimed they had gone "overboard" in their discussions.[30] Groening described trying to get the show on the air as "by far the worst experience of my grown-up life".[31]

Fox ordered thirteen episodes. Immediately after, however, Fox feared the themes of the show were not suitable for the network and Groening and Fox executives argued over whether the network would have any creative input into the show.[30] With The Simpsons, the network has no input.[32] Fox was particularly disturbed by the concept of suicide booths, Doctor Zoidberg, and Bender's anti-social behavior.[33] Groening explains, "When they tried to give me notes on Futurama, I just said: 'No, we're going to do this just the way we did Simpsons.' And they said, 'Well, we don't do business that way anymore.' And I said, 'Oh, well, that's the only way I do business.'"[34] The episode "I, Roommate" was produced to address Fox's concerns, with the script written to their specifications.[33][35] Fox strongly disliked the episode, but after negotiations, Groening received the same independence with Futurama.[36]

The name Futurama comes from a pavilion at the 1939 New York World's Fair. Designed by Norman Bel Geddes, the Futurama pavilion depicted how he imagined the world would look in 1959.[37] Many other titles were considered for the series, including Aloha, Mars! and Doomsville, which Groening notes were "resoundly rejected, by everyone concerned with it".[38][39] It takes approximately six to nine months to produce an episode of Futurama.[40][41] The long production time results in several episodes being worked on simultaneously.[42]

The planning for each episode began with a table meeting of writers, who discussed the plot ideas as a group. The writers are given index cards with plot points that they are required to use as the center of activity in each episode. A single staff writer wrote an outline and then produced a script. Once the first draft of a script was finished, the writers and executive producers called in the actors for a table read.[43] After this script reading, the writers collaborated to rewrite the script as a group before sending it to the animation team.[44] At this point the voice recording was also started and the script was out of the writers' hands.[41]

The writing staff held three Ph.D.s, seven master's degrees, and cumulatively had more than 50 years at Harvard University. Series writer Patric M. Verrone stated, "we were easily the most overeducated cartoon writers in history".[45]

Futurama had eight main cast members. Billy West performed the voices of Philip J. Fry, Professor Farnsworth, Doctor Zoidberg, Zapp Brannigan, and many other incidental characters. West auditioned for "just about every part", landing the roles of the Professor and Doctor Zoidberg.[46] Although West read for Fry, his friend Charlie Schlatter was initially given the role.[46] Due to a casting change, West was called back to audition again and was given the role. West claims that the voice of Fry is deliberately modeled on his own, so as to make it difficult for another person to replicate the voice.[46] Doctor Zoidberg's voice was based on Lou Jacobi and George Jessel.[47] The character of Zapp Brannigan was originally created for and intended to be performed by Phil Hartman.[46][47] Hartman insisted on auditioning for the role, and "just nailed it" according to Groening. Due to Hartman's death, West was given the role. West states that his version of Zapp Brannigan was an imitation of Hartman and also "modeled after a couple of big dumb announcers I knew".[46][47]

Katey Sagal voiced Leela, and is the only member of the main cast to voice only one character. The role of Leela was originally assigned to Nicole Sullivan.[46] In an interview in June 2010, Sagal remarked that she did not know that another person was to originally voice Leela until many years after the show first began.[48]

John DiMaggio performed the voice of the robot Bender Bending Rodríguez and other, more minor, characters. Bender was the most difficult character to cast, as the show's creators had not decided what a robot should sound like.[25] DiMaggio originally auditioned for the role of Professor Farnsworth, using the voice he uses to perform Bender, and also auditioned for Bender using a different voice.[49] DiMaggio described Bender's voice as a combination of a sloppy drunk, Slim Pickens and a character his college friend created named "Charlie the sausage-lover".[48]

Phil LaMarr voices Hermes Conrad, his son Dwight, Ethan Bubblegum Tate, and Reverend Preacherbot. Lauren Tom voices Amy Wong, and Tress MacNeille voices Mom and various other characters. Maurice LaMarche voices Kif Kroker and several supporting characters. LaMarche won the Emmy Award for Outstanding Voice-Over Performance in 2011 for his performances as Lrrr and Orson Welles in the episode "Lrrreconcilable Ndndifferences".[50] David Herman voiced Scruffy and various supporting characters. During seasons 1–4, LaMarche is billed as supporting cast and Tom, LaMarr and Herman billed as guest stars, despite appearing in most episodes. LaMarche was promoted to main cast and Tom, LaMarr and Herman to supporting cast in season 5, and promoted again to main cast in season 6.

In addition to the main cast, Frank Welker voiced Nibbler and Kath Soucie voiced Cubert and several supporting and minor characters. Like The Simpsons, many episodes of Futurama feature guest voices from a wide range of professions, including actors, entertainers, bands, musicians, and scientists. Many guest-stars voiced supporting characters, although many voiced themselves, usually as their own head preserved in a jar. Recurring guest stars included Tom Kenny, Dan Castellaneta (as the Robot Devil), Dawnn Lewis, Nicole St. John, Al Gore, Phil Hendrie, Coolio and George Takei, among others. Bumper Robinson used to be a cast member of the series (who played Hermes' son Dwight), but left the series after season 4 which caused Phil LaMarr to take over the role afterwards. John Goodman was meant to reprise the role of Robot Santa after "Xmas Story" in future episodes but was unable to reprise the role due to scheduling problems. As a result, John DiMaggio took over that role starting with "A Tale of Two Santas". In that same episode, Dan Castellaneta was unable to reprise the Robot Devil due to his work on The Simpsons. Maurice LaMarche took over that role for that episode only as Castellaneta did reprise the role in future episodes. Following Coolio's death in 2022, David X. Cohen has revealed on TMZ that he recorded new dialogue for Kwanzaabot before his death in an upcoming episode, scheduled to be released in 2023.[51]

Futurama is produced by The Curiosity Company and 20th Television Animation (which was previously credited as 30th Century Fox Television and is now credited as 30th Television Animation) with the animation being done by Rough Draft Studios. The studio would receive the completed script of an episode and create a storyboard consisting of more than 100 drawings. It would then produce a pencil-drawn animatic with 1,000 frames. Rough Draft's sister studio in South Korea would render the 30,000-frame finished episode.[43]

In addition to traditional cartoon drawing, Rough Draft Studios often used CGI for fast or complex shots, such as the movement of spaceships, explosions, nebulae, large crowds, and snow scenes. The opening sequence was entirely rendered in CGI. The CGI was rendered at 24 frames per second (as opposed to hand-drawn often done at 12 frames per second) and the lack of artifacts made the animation appear very smooth and fluid. CGI characters looked slightly different due to spatially "cheating" hand-drawn characters by drawing slightly out of proportion or off-perspective features to emphasize traits of the face or body, improving legibility of an expression. PowerAnimator and Maya were used to draw the comic-like CGI whilst Toonz was used for digital ink and paint and compositing.[52]

The series began high-definition production in season 6, with Bender's Big Score. The opening sequence was re-rendered and scaled to adapt to the show's transition to 16:9 widescreen format.

For the final episode of season 6, Futurama was completely reanimated in three different styles: the first segment of the episode features black-and-white Fleischer- and Walter Lantz-style animation, the second was drawn in the style of a low-resolution video game, and the final segment was in the style of Japanese anime.[53]

Much like the opening sequence in The Simpsons with its chalkboard gags, Lisa’s sax solo, and couch gags, Futurama has a distinctive opening sequence featuring minor gags. As the show begins, blue lights fill the screen and the Planet Express Ship flies across the screen with the title of the show being spelled out in its wake. Underneath the title is a joke caption such as "Painstakingly drawn before a live audience" or "When you see the robot: DRINK!"[54] After flying through downtown New New York and past various recurring characters, the Planet Express ship crashes through a large screen showing a short clip from a classic cartoon. These have included clips from Quasi at the Quackadero, Looney Tunes shorts, cartoons produced by Fleischer Studios and Famous Studios, a short of The Simpsons from a Tracey Ullman episode,[55] the show's own opening sequence in "The Devil's Hands Are Idle Playthings" or a scene from the episode. Most episodes in Season 6 and Season 7 use an abridged opening sequence, omitting the brief clip of a classic cartoon. "Rebirth", "That Darn Katz!", "Benderama", "Yo Leela Leela", "Decision 3012", "Forty Percent Leadbelly", "T.: The Terrestrial", "Leela and the Genestalk", and "Stench and Stenchability" have been the only episodes since "Spanish Fry" to feature a classic cartoon clip. Several episodes begin with a cold opening before the opening sequence, although these scenes do not always correspond with the episode's plot. The opening sequence has been lampooned several times within the show, in episodes including "That's Lobstertainment!", "The Problem with Popplers", as "Future-roma" in "The Duh-Vinci Code" and as "Futurella" in "Lrrreconcilable Ndndifferences". "Decision 3012" and "The Problem with Popplers" are the only episodes that directly tie into the opening, with Bender deliberately crashing the ship after seeing an advertisement for free beer in "Decision 3012", while in "The Problem with Popplers", Leela crashes through it during an ad for Popplers, with Fry saying "That's the second billboard you've crashed through this week!". "Viva Mars Vegas" features a unique handmade variant of the opening, using cardboard, plastic, and model ships guided by strings and rods.

Series director Scott Vanzo has remarked on the difficulty of animating the sequence. It took four to five weeks to fully animate the sequence, and it consists of over 80 levels of 3D animation composited together.[56] It takes approximately one hour to render a single frame, and each second of the sequence consists of around 30 frames.[57]

The Futurama theme was created by Christopher Tyng. The theme is played on the tubular bells but is occasionally remixed for use in specific episodes, including a version by the Beastie Boys used for the episode "Hell Is Other Robots", in which they guest starred as their own heads for both a concert and as part of the Robot Devil's song.[54] The original version of the theme also samples a drum break originating from "Amen, Brother" by American soul group The Winstons. The theme has been noted for its similarities to Pierre Henry's 1967 Psyché Rock.[58]

It was originally intended for the Futurama theme to be remixed in every episode.[59] This was first trialled in the opening sequence for "Mars University", however it was realized upon broadcast that the sound did not transmit well through most television sets and the idea was subsequently abandoned.[60] Despite this, beatbox renditions of the theme performed by Billy West and John DiMaggio are used for the episodes "Bender Should Not Be Allowed on TV" and "Spanish Fry".

As the series began high-definition production in 2007 with the four direct-to-DVD movies and season 6 on Comedy Central, the opening sequence was updated to match, and a slightly slower and revamped remix of the theme song, which replaced the Amen break with a higher-fidelity drum recording and was previously used in the Futurama video game, replaced the original theme. Bender's Big Score has an extended opening sequence, introducing each of the main characters. In The Beast with a Billion Backs and Bender's Game the ship passes through the screen's glass and temporarily becomes part of the environment depicted therein—a pastiche of Disney's Steamboat Willie and Yellow Submarine respectively—before crashing through the screen glass on the way out. In Into the Wild Green Yonder, a completely different opening sequence involves a trip through a futuristic version of Las Vegas located on Mars. The theme tune is sung by Family Guy and American Dad! creator Seth MacFarlane and is different from the standard theme tune. The end of the film incorporates a unique variation of the opening sequence; as the Planet Express Ship enters a wormhole, it converts into a pattern of lights similar to the lights that appear in the opening sequence. All four movies' end credits utilized an edited version from the full-length remix of the theme instead of the original end credits theme, and an even shorter edit was introduced in season 6. Another update of the opening sequence in season 8 for Hulu added more visual gags, and the end credits theme was replaced again with an edit of the 2012 digital download release version, which added an extra layer of beats.

There are three alternative alphabets that appear often in the background of episodes, usually in the forms of graffiti, advertisements, or warning labels. Nearly all messages using alternative scripts transliterate directly into English. The first alphabet consists of abstract characters and is referred to as Alienese,[18] a simple substitution cipher from the Latin alphabet.[61] The second alphabet uses a more complex modular addition code, where the "next letter is given by the summation of all previous letters plus the current letter".[62] The codes often provide additional jokes for fans dedicated enough to decode the messages.[25] The third language sometimes used is Hebrew. Aside from these alphabets, most of the displayed wording on the show uses the Latin alphabet.

The show predicts that several English expressions will have evolved by the year 3000. For example, in the show the word Christmas has been replaced with Xmas (pronounced "ex-mas"), and the word ask with aks (pronounced axe). According to David X. Cohen, it is a running joke that the French language is extinct in the Futurama universe (though the culture remains alive), much like Latin is in the present.[63] In the French dubbing of the show, German is used as the extinct language instead.

A modified version of the 20th Century Fox Television logo is displayed at the end of each episode, reading "30th Century Fox Television"[64] to fit the show's setting in the 31st century. Syndicated episodes use a 30th Television closing logo instead of the 20th Television one, while episodes from Season 8 onward use a 30th Television Animation logo. Initially, Fox did not want this logo to be used on the show, but when creator Matt Groening purchased the rights to the logo, the network had a change of heart and allowed the altered version to be aired.[65][66]

Although the series uses a wide range of styles of humor, including self-deprecation, black comedy, off-color humor, slapstick, and surreal humor, its primary source of comedy is its satirical depiction of everyday life in the future and its parodical comparisons to the present.[16] Groening notes that, from the show's conception, his goal was to make what was, on the surface, a goofy comedy that would have underlying "legitimate literary science fiction concepts".[67] The series contrasted "low culture" and "high culture" comedy; for example, Bender's catchphrase is the insult "Bite my shiny metal ass" while his most terrifying nightmare is a vision of the number 2, a joke referring to the binary numeral system (Fry assures him, "there's no such thing as two").[16]

The series developed a cult following partially due to the large number of in-jokes it contains, most of which are aimed at "nerds".[16] In commentary on the DVD releases, David X. Cohen points out and sometimes explains his "nerdiest joke[s]".[68] These included mathematical jokes –  such as "Loew's 




ℵ

0




{\displaystyle \aleph _{0}}

-plex" (aleph-null-plex) movie theater[68] –  as well as various forms of science humor –  for example, Professor Farnsworth, at a racetrack, complains about the use of a quantum finish to decide the winner, exclaiming "No fair! You changed the outcome by measuring it", a reference to the uncertainty principle of quantum mechanics.[16][69] In the season six episode "Law and Oracle", Fry and the robot peace officer URL track down a traffic violator who turns out to be Erwin Schrödinger, the 20th-century quantum physicist. On the front seat of the car is a box, and when questioned about the contents, Schrödinger replies "A cat, some poison, and a cesium atom". Fry asks if the cat is alive or dead, and Schrödinger answers "It's a superposition of both states until you open the box and collapse the wave function." When Fry opens the box, the cat jumps out and attacks him. The run is a reference to the Schrödinger's cat thought experiment of quantum mechanics. The series makes passing references to quantum chromodynamics (the appearance of Strong Force-brand glue),[70] computer science (two separate books in a closet labeled P and NP respectively, referring to the possibility that P and NP-complete problem classes are distinct),[71] electronics (an X-ray –  or more accurately, an "F-ray" –  of Bender's head reveals a 6502 microprocessor),[72] and genetics (a mention of Bender's "robo- or R-NA").[73]

The show often features subtle references to classic science fiction. These are most often to Star Trek –  many soundbites are used in homage[16] –  but also include the reference to the origin of the word robot made in the name of the robot-dominated planet Chapek 9,[74] and the black rectangular monolith labeled "Out of Order" in orbit around Jupiter (a reference to Arthur C. Clarke's Space Odyssey series).[75] Bender and Fry sometimes watch a television show called The Scary Door, a humorous parody of The Twilight Zone.[76]


Journalist/critic Frank Lovece in Newsday contrasted the humor tradition of Groening's two series, finding that 
The Simpsons echoes the strains of American-Irish vaudeville humor –  the beer-soaked, sneaking-in-late-while-the-wife's-asleep comedy of Harrigan and Hart, McNulty and Murray, the Four Cohans (which, yes, included George M.) and countless others: knockabout yet sentimental, and ultimately about the bonds of blood family. Futurama, conversely, stems from Jewish-American humor, and not just in the obvious archetype of Dr. Zoidberg. From vaudeville to the Catskills to Woody Allen, it's that distinctly rueful humor built to ward away everything from despair to petty annoyance –  the 'You gotta do what you gotta do' philosophy that helps the Futurama characters cope in a mega-corporate world where the little guy is essentially powerless.[77]

Animation maven Jerry Beck concurred: 
I'm Jewish, and I know what you're saying. Fry has that [type of humor], Dr. Zoidberg, all the [vocal artist] Billy West characters. I see it. The bottom line is, the producers are trying to make sure the shows are completely different entities.[77]
In an interview with Diego Molano, creator of Victor and Valentino, in April 2019, he said that he found Futurama "incredibly influential", calling the humor smart but "not alienating".[78] He added that it makes him "feel smart" and adding that Groening's "sense of comedic timing is masterful".

Groening and Cohen wanted Futurama to be shown at 8:30 pm on Sunday, following The Simpsons. The Fox network disagreed, opting instead to show two episodes in the Sunday night lineup before moving the show to a regular time slot on Tuesday.[79] Beginning with its second broadcast season, Futurama was again placed in the 8:30 pm Sunday spot,[80] but by mid-season the show was moved again, this time to 7:00 pm on Sunday, its third position in less than a year.[81] Even by the fourth season, Futurama was being aired erratically.[82] Because the show was regularly pre-empted by sporting events, it became difficult to predict when new episodes would air. The erratic schedule resulted in Fox not airing several episodes that had been produced for seasons three and four, instead holding them for a fifth broadcast season. According to Groening, Fox executives were not supporters of the show.[83] Although Futurama was never officially cancelled, midway through the production of the fourth season, Fox decided to stop buying episodes of Futurama, letting it go out of production before the fall 2003 lineup.[84][85]

In 2003, Cartoon Network acquired syndication rights to Futurama and Family Guy, another animated show Fox had cancelled, for its Adult Swim block. Both shows proved to be successful immediately, leading to sister network TBS picking up the show later in 2003.[86][87] The run on Adult Swim revived interest in both series, and when Family Guy found success in direct-to-DVD productions, Futurama's producers decided to try the same.[88][89] In 2005, Comedy Central entered negotiations to take over the syndication rights, during which they discussed the possibility of producing new episodes. In 2006, it was announced that four straight-to-DVD films would be produced, and later split into 16 episodes comprising a fifth season of the show.[90] Since no new Futurama projects were in production at the time of release, the final movie release Into the Wild Green Yonder was designed to stand as the Futurama series finale. However, Groening had expressed a desire to continue the franchise in some form, including a theatrical film.[91] In an interview with CNN, Groening said that "we have a great relationship with Comedy Central and we would love to do more episodes for them, but I don't know... We're having discussions and there is some enthusiasm but I can't tell if it's just me."[92] Futurama left Adult Swim's lineup on December 31, 2007, following a week-long marathon of the entire series. Comedy Central began airing the show the next day, with season 5 making its broadcast debut on March 23, 2008.

In June 2009, 20th Century Fox Television announced that Comedy Central had picked up the show for 26 new half-hour episodes that began airing on June 24, 2010.[93][94][95] The returning writing crew was smaller than the original crew.[96] It was originally announced that main voice actors West, DiMaggio, and Sagal would return as well, but on July 17, 2009, it was announced that a casting notice was posted to replace the entire cast when 20th Century Fox Television would not meet their salary demands.[97] The situation was later resolved, and the entire original voice-cast returned for the new episodes.

Near the end of a message from Maurice LaMarche sent to members of the "Save the Voices of Futurama" group on Facebook, LaMarche announced that the original cast would be returning for the new episodes.[98] The Toronto Star confirmed, announcing on their website that the original cast of Futurama signed contracts with Fox to return for 26 more episodes.[99] Similarly, an email sent to fans from Cohen and Groening reported that West, Sagal, DiMaggio, LaMarche, MacNeille, Tom, LaMarr, and Herman would all be returning for the revival.[100]

Cohen told Newsday in August 2009 that the reported 26-episode order means "[i]t will be up to 26. I can't guarantee it will be 26. But I think there's a pretty good chance it'll be exactly 26. Fox has been a little bit cagey about it, even internally. But nobody's too concerned. We're plunging ahead".[77] Two episodes were in the process of being voice-recorded at that time, with an additional "six scripts ... in the works, ranging in scale from 'it's a crazy idea that someone's grandmother thought of' to 'it's all on paper'.[77]

When Futurama aired June 24, 2010, on Comedy Central, it helped the network to its highest-rated night in 2010 and its highest-rated Thursday primetime in the network's history.[101] In March 2011, it was announced that Futurama had been renewed for a seventh season, consisting of at least 26 episodes, scheduled to air in 2012 and 2013.[6][7] The first episode of season 7 premiered June 20, 2012, on Comedy Central.[102]

In July 2011, it was reported that the show had been picked up for syndication by both local affiliates and WGN America. Broadcast of old episodes began in September 2011.[103] On September 19, 2011, WGN America began re-running Futurama, carrying it until 2014.[104] Futurama doubled its viewership in syndication in 2012.[105]

Due to the uncertain future of the series, there have been four designated series finales. "The Devil's Hands Are Idle Playthings", Into the Wild Green Yonder, "Overclockwise", and "Meanwhile" have all been written to serve as a final episode for the show.[106][107]

Comedy Central announced in April 2013 that they would be airing the final episode "Meanwhile" on September 4, 2013.[108] The producers said that they are exploring options for the future of the series as "[they] have many more stories to tell", but would gauge fan reaction to the news.[109] Groening and Cohen had previously expressed a desire to produce a theatrical film or another direct-to-video film upon conclusion of the series.[110]

In an August 2013 interview with Milwaukee Journal Sentinel, Katey Sagal said regarding the series finale, "So I don't believe it... I just hold out hope for it because it has such a huge fan base, it's such a smart show, and why wouldn't somebody want to keep making that show; so that's my thought, I'm just in denial that it's over". Sagal also mentioned during the same interview that Groening told her at Comic-Con that "we'll find a place" and "don't worry, it's not going to end".[111]

The Simpsons episode "Simpsorama" is an official crossover with Futurama. It originally aired during the twenty-sixth season of The Simpsons on Fox on November 9, 2014, over a year after the Futurama series finale aired on Comedy Central.[112][113][114]

In July 2011, it was reported that the show had been picked up for syndication by both local affiliates and WGN America. Broadcast of old episodes began in September 2011.[103] On September 19, 2011, WGN America began re-running Futurama, carrying it until 2014.[104] Futurama doubled its viewership in syndication in 2012.[105]

In October 2017, Syfy announced that they had acquired syndication rights to all 140 episodes of Futurama, adding it to its lineup on November 11, 2017, with a weekend long marathon.[115] Futurama was Syfy's first ever American animated series (the network had an anime programming block in the past), and eventually became paired with Syfy's TZGZ block of animated original series on Saturday nights. Comedy Central continued to air the series concurrently with Syfy, usually in the mornings and early afternoon. Syfy aired episodes from the first four seasons cropped to 16:9 instead of airing them in their original 4:3 aspect ratio; Comedy Central (since 2017) and FXX would do the same.

In September 2021, FXX, which already carries The Simpsons and other 20th Television animated programming, announced that it would begin airing Futurama that November.[116] Syfy stopped airing the show on November 10, 2021, and FXX began airing the show on November 15, 2021. Adult Swim then picked the show back up on December 27, 2021.[117] Unlike FXX, Adult Swim airs the first four seasons in their original 4:3 aspect ratio, pillarboxed. However, seasons 5 through 7, which were produced in 16:9, are cropped to 4:3. As of 2024, Comedy Central still holds rights to the series, usually airing it in the early morning.[118]

In February 2022, Hulu revived the series with a 20-episode order to premiere in 2023. At the time of the announcement, the majority of the main voice cast was set to return, while John DiMaggio was still in negotiations.[10] The next day, Disney+ (Hulu's sister streaming service) announced in a press release that the new season would stream internationally as a Star Original.[119] DiMaggio stated that he had not accepted the role in mid-February 2022 because he believed the entire cast of Futurama should be paid more. He stated, "Bender is part of my soul and nothing about this is meant to be disrespectful to the fans or my Futurama family. It's about self-respect. And honestly, [it's about] being tired of an industry that's become far too corporate and takes advantage of artists' time and talent... I wish I could give you every detail so you would understand, but it's not my place."[120]

In March, DiMaggio officially rejoined the series after working out a new deal, calling the prior events "Bendergate".[121] He later revealed that he did not get a raise, "but what I did get was a lot of respect". Had he not returned, Bender would have been voiced by a different guest star in each episode.[122] In August 2022, the titles of the first ten episodes were announced by Hulu,[123] and by November 2022, the production team was aiming to complete the episodes by the end of the year.[124] In February 2023, a new release date was set sometime around the 2023 summer season.[125] On May 18, 2023, a teaser trailer was released, announcing the premiere date as July 24, 2023.[11] This second revival has alternatively been titled the eighth and ninth seasons (production)[126] and the eleventh and twelfth seasons (broadcast).[127][28] With the revival on Hulu, the season carries a new version of the opening title sequence, containing a briefly altered name "Hulurama" before reverting to "Futurama".[128][129]

On November 2, 2023, Hulu renewed the series for tenth and eleventh production seasons;[130] a total of 20 episodes were ordered for the new seasons, which will air from 2025 to 2026.[12][13]

The show received critical acclaim. The first season holds an 82% approval rating at review aggregator site Rotten Tomatoes, based on 32 reviews, an average rating of 8.75/10. The critical consensus reads, "Good news, everyone! Futurama is an inventive, funny, and sometimes affecting look at the world of tomorrow." Season 5 holds a rating of 100%, based on seven reviews, and an average score of 8.67/10. Season 6 has an approval rating of 100%, based on 16 reviews, and the average rating is 8.31/10. The website's critical consensus states, "Good news everyone! Futurama is as funny and endearing as ever in its sixth season." The next season received a rating of 92%, and an 8.24/10 average score based on 12 reviews. The critics consensus reads, "Futurama makes the most of its second life with biting social satire, kooky sci-fi scenarios, and a stirringly sweet series finale." For the eighth season,[c] 86% of 29 reviews are positive, while the consensus states, "Shut up and take our money!".[137]

Futurama's 7:00 p.m. Sunday time slot caused the show to often be pre-empted by sports and usually have a later-than-average season premiere. It also allowed the writers and animators to get ahead of the broadcast schedule so that episodes intended for one season were not aired until the following season. By the beginning of the fourth broadcast season, all the episodes to be aired that season had already been completed and writers were working at least a year in advance.[41]

When Futurama debuted in the Fox Sunday night lineup at 8:30 p.m. between The Simpsons and The X-Files on March 28, 1999, it managed 19 million viewers, tying for 11th overall in that week's Nielsen ratings.[144] The following week, airing at the same time, Futurama drew 14.2 million viewers. The third episode, the first airing on Tuesday, drew 8.85 million viewers.[145] Though its ratings were well below The Simpsons, the first season of Futurama rated higher than competing animated series: King of the Hill, Family Guy, Dilbert, South Park, and The PJs.[146]

When Futurama was effectively canceled in 2003, it had averaged 6.4 million viewers for the first half of its fourth broadcast season.[147]

In late 2002, Cartoon Network acquired exclusive cable syndication rights to Futurama for a reported $10 million (equivalent to $16 million in 2023).[148][149] In January 2003,[149] the network began airing Futurama episodes as the centerpiece to the expansion of their Adult Swim cartoon block. In October 2005, Comedy Central picked up the cable syndication rights to air Futurama's 72-episode run at the start of 2008, following the expiration of Cartoon Network's contract.[150] A Comedy Central teaser trailer announced the return of Futurama March 23, 2008,[151] which was Bender's Big Score divided into four episodes followed by the other three movies.

On June 24, 2010, the season 6 premiere, "Rebirth", drew 2.92 million viewers in the 10:00 p.m. time slot on Comedy Central.[152] The second episode of the sixth season, "In-A-Gadda-Da-Leela", aired at 10:30 p.m., immediately following the season premiere. "In-A-Gadda-Da-Leela" drew 2.78 million viewers.[152] This was the series' premiere on the network, with original episodes—the fifth season had previously aired on the network, but it had originally been released in the form of the four direct-to-video films.

First started in November 2000, Futurama Comics is a comic book series published by Bongo Comics based in the Futurama universe.[170] While originally published only in the US, a UK, German and Australian version of the series is also available.[171] In addition, three issues were published in Norway. Other than a different running order and presentation, the stories are the same in all versions. While the comics focus on the same characters in the Futurama fictional universe, the comics may not be canonical as the events portrayed within them do not necessarily have any effect upon the continuity of the show.

Like the TV series, each comic (except US comic #20) has a caption at the top of the cover. For example: "Made In The USA! (Printed in Canada)". Some of the UK and Australian comics have different captions on the top of their comics (for example, the Australian version of #20 says "A 21st Century Comic Book" across the cover, while the US version does not have a caption on that issue). All series contain a letters page, artwork from readers, and previews of other upcoming Bongo comics.

When Comedy Central began negotiating for the rights to air Futurama reruns, Fox suggested that there was a possibility of also creating new episodes. Negotiations were already underway with the possibility of creating two or three straight-to-DVD films. When Comedy Central committed to sixteen new episodes, it was decided that four films would be produced.[90] On April 26, 2006, Groening noted in an interview that co-creator David X. Cohen and numerous writers from the original series would be returning to work on the movies.[172] All the original voice actors participated. In February 2007, Groening explained the format of the new stories: "[The crew is] writing them as movies and then we're going to chop them up, reconfigure them, write new material and try to make them work as separate episodes."[173]

The first film, Bender's Big Score, was written by Ken Keeler and Cohen, and includes return appearances by the Nibblonians, Seymour, Barbados Slim, Robot Santa, the "God" space entity, Al Gore, and Zapp Brannigan.[174] It was animated in widescreen and was released on standard DVD on November 27, 2007, with a possible Blu-ray Disc release to follow.[175] A release on HD DVD was rumored but later officially denied. Futurama: Bender's Big Score was the first DVD release for which 20th Century Fox implemented measures intended to reduce the total carbon footprint of the production, manufacturing, and distribution processes. Where it was not possible to completely eliminate carbon, output carbon offsets were used, thus making the complete process carbon neutral.[176]

The second movie, The Beast with a Billion Backs, was released on June 24, 2008. The third movie, Bender's Game, was released on DVD and Blu-ray Disc[177] on November 3, 2008, in the UK, November 4, 2008, in the US, and December 10, 2008, in Australia. The fourth movie, Into the Wild Green Yonder, was released on DVD and Blu-ray Disc on February 24, 2009.[178]

Following the series' revival, the show's executive producer Claudia Katz had announced more upcoming movies for Hulu while also hinting a theatrical release.[179][180]

On September 15, 2000, Unique Development Studios acquired the license to develop a Futurama video game for consoles and handheld systems. Fox Interactive signed on to publish the game.[181] Sierra Entertainment later became the game's publisher, and it was released on August 14, 2003.[182] Versions are available for PlayStation 2 and Xbox, both of which use cel-shading technology. However, the game was subsequently canceled on the GameCube and Game Boy Advance in North America and Europe.

In 2012, an app inspired by the head in a jar gag was launched by Matt Groening.[183]

In September 2016, Futurama characters and content would be playable in a crossover free-to-play digital collectable card video game Animation Throwdown: The Quest for Cards, which also contained characters and content from other 20th Television Animation properties; particularly Family Guy, American Dad!, King of the Hill, Bob's Burgers, and (as of 2023) Archer.[184] The game was released on September 27, 2016 and is available for Android, iOS, Steam, Kartridge[185] and on the web via the game's developer Kongregate.[186]

Licensing for mobile games were done in 2016 with Futurama: Game of Drones and in 2017 with Futurama: Worlds of Tomorrow, which was released for Android and iOS in 2017.[187][188]

On July 25, 2023, Fortnite released an update that included theme content centered on the show Futurama, including purchasable outfits. These outfits include Bender Bending Rodríguez, Philip J. Fry, and Turanga Leela, as well as other themed accessories. The weapon "Bender's Shiny Metal Raygun" has also been included in the game.[189]The General Dynamics F-16 Fighting Falcon is an American single-engine supersonic multirole fighter aircraft originally developed by General Dynamics for the United States Air Force (USAF). Designed as an air superiority day fighter, it evolved into a successful all-weather multirole aircraft with over 4,600 built since 1976.[4] Although no longer purchased by the U.S. Air Force, improved versions are being built for export. In 1993, General Dynamics sold its aircraft manufacturing business to the Lockheed Corporation,[5] which became part of Lockheed Martin after a 1995 merger with Martin Marietta.[6]

The F-16's key features include a frameless bubble canopy for enhanced cockpit visibility, a side-mounted control stick to ease control while maneuvering, an ejection seat reclined 30 degrees from vertical to reduce the effect of g-forces on the pilot, and the first use of a relaxed static stability/fly-by-wire flight control system that helps to make it an agile aircraft. The fighter has a single turbofan engine, an internal M61 Vulcan cannon and 11 hardpoints. Although officially named "Fighting Falcon", the aircraft is commonly known by the nickname "Viper" among its crews and pilots.[7]

In addition to active duty in the U.S. Air Force, Air Force Reserve Command, and Air National Guard units, the aircraft is also used by the U.S. Air Force Thunderbirds aerial demonstration team, the US Air Combat Command F-16 Viper Demonstration Team,[8] and as an adversary/aggressor aircraft by the United States Navy. The F-16 has also been procured by the air forces of 25 other nations.[9] As of 2025, it is the world's most common fixed-wing aircraft in military service, with 2,084 F-16s operational.[10]

US Vietnam War experience showed the need for air superiority fighters and better air-to-air training for fighter pilots.[11] Based on his experience in the Korean War and as a fighter tactics instructor in the early 1960s, Colonel John Boyd with mathematician Thomas Christie developed the energy–maneuverability theory to model a fighter aircraft's performance in combat. Boyd's work called for a small, lightweight aircraft that could maneuver with the minimum possible energy loss and which also incorporated an increased thrust-to-weight ratio.[12][13] In the late 1960s, Boyd gathered a group of like-minded innovators who became known as the Fighter Mafia, and in 1969, they secured Department of Defense funding for General Dynamics and Northrop to study design concepts based on the theory.[14][15]

Air Force F-X proponents were opposed to the concept because they perceived it as a threat to the F-15 program, but the USAF's leadership understood that its budget would not allow it to purchase enough F-15 aircraft to satisfy all of its missions.[16] The Advanced Day Fighter concept, renamed F-XX, gained civilian political support under the reform-minded Deputy Secretary of Defense David Packard, who favored the idea of competitive prototyping. As a result, in May 1971, the Air Force Prototype Study Group was established, with Boyd a key member, and two of its six proposals would be funded, one being the Lightweight Fighter (LWF). The request for proposals issued on 6 January 1972 called for a 20,000-pound (9,100 kg) class air-to-air day fighter with a good turn rate, acceleration, and range, and optimized for combat at speeds of Mach 0.6–1.6 and altitudes of 30,000–40,000 feet (9,100–12,000 m). This was the region where USAF studies predicted most future air combat would occur. The anticipated average flyaway cost of a production version was $3 million. This production plan was hypothetical as the USAF had no firm plans to procure the winner.[17][18]

Five companies responded, and in 1972, the Air Staff selected General Dynamics' Model 401 and Northrop's P-600 for the follow-on prototype development and testing phase. GD and Northrop were awarded contracts worth $37.9 million and $39.8 million to produce the YF-16 and YF-17, respectively, with the first flights of both prototypes planned for early 1974. To overcome resistance in the Air Force hierarchy, the Fighter Mafia and other LWF proponents[which?] successfully advocated the idea of complementary fighters in a high-cost/low-cost force mix.[19] The "high/low mix" would allow the USAF to be able to afford sufficient fighters for its overall fighter force structure requirements. The mix gained broad acceptance by the time of the prototypes' flyoff, defining the relationship between the LWF and the F-15.[20][21]

The YF-16 was developed by a team of General Dynamics engineers led by Robert H. Widmer.[22] The first YF-16 was rolled out on 13 December 1973. Its 90-minute maiden flight was made at the Air Force Flight Test Center at Edwards AFB, California, on 2 February 1974. Its actual first flight occurred accidentally during a high-speed taxi test on 20 January 1974. While gathering speed, a roll-control oscillation caused a fin of the port-side wingtip-mounted missile and then the starboard stabilator to scrape the ground, and the aircraft then began to veer off the runway. The test pilot, Phil Oestricher, decided to lift off to avoid a potential crash, safely landing six minutes later. The slight damage was quickly repaired and the official first flight occurred on time.[23] The YF-16's first supersonic flight was accomplished on 5 February 1974, and the second YF-16 prototype first flew on 9 May 1974. This was followed by the first flights of Northrop's YF-17 prototypes on 9 June and 21 August 1974, respectively. During the flyoff, the YF-16s completed 330 sorties for a total of 417 flight hours;[24] the YF-17s flew 288 sorties, covering 345 hours.[25]

Increased interest turned the LWF into a serious acquisition program. NATO allies Belgium, Denmark, the Netherlands, and Norway were seeking to replace their F-104G Starfighter fighter-bombers.[26] In early 1974, they reached an agreement with the U.S. that if the USAF ordered the LWF winner, they would consider ordering it as well. The USAF also needed to replace its F-105 Thunderchief and F-4 Phantom II fighter-bombers. The U.S. Congress sought greater commonality in fighter procurements by the Air Force and Navy, and in August 1974 redirected Navy funds to a new Navy Air Combat Fighter program that would be a naval fighter-bomber variant of the LWF. The four NATO allies had formed the Multinational Fighter Program Group (MFPG) and pressed for a U.S. decision by December 1974; thus, the USAF accelerated testing.[27][28][29]

To reflect this serious intent to procure a new fighter-bomber, the LWF program was rolled into a new Air Combat Fighter (ACF) competition in an announcement by U.S. Secretary of Defense James R. Schlesinger in April 1974. The ACF would not be a pure fighter, but multirole, and Schlesinger made it clear that any ACF order would be in addition to the F-15, which extinguished opposition to the LWF.[28][29][30] ACF also raised the stakes for GD and Northrop because it brought in competitors intent on securing what was touted at the time as "the arms deal of the century".[31] These were Dassault-Breguet's proposed Mirage F1M-53, the Anglo-French SEPECAT Jaguar, and the proposed Saab 37E "Eurofighter". Northrop offered the P-530 Cobra, which was similar to the YF-17. The Jaguar and Cobra were dropped by the MFPG early on, leaving two European and two U.S. candidates. On 11 September 1974, the U.S. Air Force confirmed plans to order the winning ACF design to equip five tactical fighter wings. Though computer modeling predicted a close contest, the YF-16 proved significantly quicker going from one maneuver to the next and was the unanimous choice of those pilots that flew both aircraft.[32]

On 13 January 1975, Secretary of the Air Force John L. McLucas announced the YF-16 as the winner of the ACF competition.[33] The chief reasons given by the secretary were the YF-16's lower operating costs, greater range, and maneuver performance that was "significantly better" than that of the YF-17, especially at supersonic speeds. Another advantage of the YF-16 – unlike the YF-17 – was its use of the Pratt & Whitney F100 turbofan engine, the same powerplant used by the F-15; such commonality would lower the cost of engines for both programs.[34] Secretary McLucas announced that the USAF planned to order at least 650, possibly up to 1,400 production F-16s. In the Navy Air Combat Fighter competition, on 2 May 1975, the Navy selected the YF-17 as the basis for what would become the McDonnell Douglas F/A-18 Hornet.[35][36]

The U.S. Air Force initially ordered 15 full-scale development (FSD) aircraft (11 single-seat and four two-seat models) for its flight test program which was reduced to eight (six F-16A single-seaters and two F-16B two-seaters).[37] The YF-16 design was altered for the production F-16. The fuselage was lengthened by 10.6 in (0.269 m), a larger nose radome was fitted for the AN/APG-66 radar, wing area was increased from 280 to 300 sq ft (26 to 28 m2), the tailfin height was decreased, the ventral fins were enlarged, two more stores stations were added, and a single door replaced the original nosewheel double doors. The F-16's weight was increased by 25% over the YF-16 by these modifications.[38][39]

The FSD F-16s were manufactured by General Dynamics in Fort Worth, Texas, at United States Air Force Plant 4 in late 1975; the first F-16A rolled out on 20 October 1976 and first flew on 8 December. The initial two-seat model achieved its first flight on 8 August 1977. The initial production-standard F-16A flew for the first time on 7 August 1978 and its delivery was accepted by the USAF on 6 January 1979. The aircraft entered USAF operational service with the 34th Tactical Fighter Squadron, 388th Tactical Fighter Wing, at Hill AFB in Utah, on 1 October 1980.[40]

The F-16 was given its name of "Fighting Falcon" on 21 July 1980. Its pilots and crews often use the name "Viper" instead, because of a perceived resemblance to a viper snake as well as to the fictional Colonial Viper starfighter from the television program Battlestar Galactica, which aired at the time the F-16 entered service.[41][7]

On 7 June 1975, the four European partners, now known as the European Participation Group, signed up for 348 aircraft at the Paris Air Show. This was split among the European Participation Air Forces (EPAF) as 116 for Belgium, 58 for Denmark, 102 for the Netherlands, and 72 for Norway. Two European production lines, one in the Netherlands at Fokker's Schiphol-Oost facility and the other at SABCA's Gosselies plant in Belgium, would produce 184 and 164 units respectively. Norway's Kongsberg Vaapenfabrikk and Denmark's Terma A/S also manufactured parts and subassemblies for EPAF aircraft. European co-production was officially launched on 1 July 1977 at the Fokker factory. Beginning in November 1977, Fokker-produced components were sent to Fort Worth for fuselage assembly, then shipped back to Europe for final assembly of EPAF aircraft at the Belgian plant on 15 February 1978; deliveries to the Belgian Air Force began in January 1979. The first Royal Netherlands Air Force aircraft was delivered in June 1979. In 1980, the first aircraft were delivered to the Royal Norwegian Air Force by Fokker and to the Royal Danish Air Force by SABCA.[42][43]

During the late 1980s and 1990s, Turkish Aerospace Industries (TAI) produced 232 Block 30/40/50 F-16s on a production line in Ankara under license for the Turkish Air Force. TAI also produced 46 Block 40s for Egypt in the mid-1990s and 30 Block 50s from 2010 onwards. Korean Aerospace Industries opened a production line for the KF-16 program, producing 140 Block 52s from the mid-1990s to mid-2000s (decade). If India had selected the F-16IN for its Medium Multi-Role Combat Aircraft procurement, a sixth F-16 production line would have been built in India.[44] In May 2013, Lockheed Martin stated there were currently enough orders to keep producing the F-16 until 2017.[45]

One change made during production was augmented pitch control to avoid deep stall conditions at high angles of attack. The stall issue had been raised during development but had originally been discounted. Model tests of the YF-16 conducted by the Langley Research Center revealed a potential problem, but no other laboratory was able to duplicate it. YF-16 flight tests were not sufficient to expose the issue; later flight testing on the FSD aircraft demonstrated a real concern. In response, the area of each horizontal stabilizer was increased by 25% on the Block 15 aircraft in 1981 and later retrofitted to earlier aircraft. In addition, a manual override switch to disable the horizontal stabilizer flight limiter was prominently placed on the control console, allowing the pilot to regain control of the horizontal stabilizers (which the flight limiters otherwise lock in place) and recover. Besides reducing the risk of deep stalls, the larger horizontal tail also improved stability and permitted faster takeoff rotation.[46][47]

In the 1980s, the Multinational Staged Improvement Program (MSIP) was conducted to evolve the F-16's capabilities, mitigate risks during technology development, and ensure the aircraft's worth. The program upgraded the F-16 in three stages. The MSIP process permitted the quick introduction of new capabilities, at lower costs and with reduced risks compared to traditional independent upgrade programs.[48] In 2012, the USAF had allocated $2.8 billion (~$3.67 billion in 2023) to upgrade 350 F-16s while waiting for the F-35 to enter service.[49] One key upgrade has been an auto-GCAS (Ground collision avoidance system) to reduce instances of controlled flight into terrain.[50] Onboard power and cooling capacities limit the scope of upgrades, which often involve the addition of more power-hungry avionics.[51]

Lockheed won many contracts to upgrade foreign operators' F-16s. BAE Systems also offers various F-16 upgrades, receiving orders from South Korea, Oman, Turkey, and the US Air National Guard;[52][53][54] BAE lost the South Korean contract because of a price breach in November 2014.[55] In 2012, the USAF assigned the total upgrade contract to Lockheed Martin.[56] Upgrades include Raytheon's Center Display Unit, which replaces several analog flight instruments with a single digital display.[57]

In 2013, sequestration budget cuts cast doubt on the USAF's ability to complete the Combat Avionics Programmed Extension Suite (CAPES), a part of secondary programs such as Taiwan's F-16 upgrade.[58] Air Combat Command's General Mike Hostage stated that if he only had money for a service life extension program (SLEP) or CAPES, he would fund SLEP to keep the aircraft flying.[59] Lockheed Martin responded to talk of CAPES cancellation with a fixed-price upgrade package for foreign users.[60] CAPES was not included in the Pentagon's 2015 budget request.[61] The USAF said that the upgrade package will still be offered to Taiwan's Republic of China Air Force, and Lockheed said that some common elements with the F-35 will keep the radar's unit costs down.[62] In 2014, the USAF issued a RFI to SLEP 300 F-16 C/Ds.[63]

To make more room for assembly of its newer F-35 Lightning II fighter aircraft, Lockheed Martin moved the F-16 production from Fort Worth, Texas to its plant in Greenville, South Carolina.[3] Lockheed delivered the last F-16 from Fort Worth to the Iraqi Air Force on 14 November 2017, ending 40 years of F-16 production there. The company resumed production in 2019, though engineering and modernization work will remain in Fort Worth.[64] A gap in orders made it possible to stop production during the move; after completing orders for the last Iraqi purchase,[65] the company was negotiating an F-16 sale to Bahrain that would be produced in Greenville. This contract was signed in June 2018,[2] and the first planes rolled off the Greenville line in 2023.[66]

The F-16 is a single-engine, highly maneuverable, supersonic, multirole tactical fighter aircraft. It is much smaller and lighter than its predecessors but uses advanced aerodynamics and avionics, including the first use of a relaxed static stability/fly-by-wire (RSS/FBW) flight control system, to achieve enhanced maneuver performance. Highly agile, the F-16 was the first fighter aircraft purpose-built to pull 9-g maneuvers and can reach a maximum speed of over Mach 2. Innovations include a frameless bubble canopy for better visibility, a side-mounted control stick, and a reclined seat to reduce g-force effects on the pilot. It is armed with an internal 20 mm M61 Vulcan cannon in the left wing root and has multiple locations for mounting various missiles, bombs and pods. It has a thrust-to-weight ratio greater than one, providing power to climb and vertical acceleration.[67]

The F-16 was designed to be relatively inexpensive to build and simpler to maintain than earlier-generation fighters. The airframe is built with about 80% aviation-grade aluminum alloys, 8% steel, 3% composites, and 1.5% titanium. The leading-edge flaps, stabilators, and ventral fins make use of bonded aluminum honeycomb structures and graphite epoxy lamination coatings. The number of lubrication points, fuel line connections, and replaceable modules is significantly less than in preceding fighters; 80% of the access panels can be accessed without stands.[44] The air intake was placed so it was rearward of the nose but forward enough to minimize air flow losses and reduce aerodynamic drag.[68]

Although the LWF program called for a structural life of 4,000 flight hours, capable of achieving 7.33 g with 80% internal fuel; GD's engineers decided to design the F-16's airframe life for 8,000 hours and for 9-g maneuvers on full internal fuel. This proved advantageous when the aircraft's mission changed from solely air-to-air combat to multirole operations. Changes in operational use and additional systems have increased weight, necessitating multiple structural strengthening programs.[69]

The F-16 has a cropped-delta wing incorporating wing-fuselage blending and forebody vortex-control strakes; a fixed-geometry, underslung air intake (with splitter plate[70]) to the single turbofan jet engine; a conventional tri-plane empennage arrangement with all-moving horizontal "stabilator" tailplanes; a pair of ventral fins beneath the fuselage aft of the wing's trailing edge; and a tricycle landing gear configuration with the aft-retracting, steerable nose gear deploying a short distance behind the inlet lip. There is a boom-style aerial refueling receptacle located behind the single-piece "bubble" canopy of the cockpit. Split-flap speedbrakes are located at the aft end of the wing-body fairing, and a tailhook is mounted underneath the fuselage. A fairing beneath the rudder often houses ECM equipment or a drag chute. Later F-16 models feature a long dorsal fairing along the fuselage's "spine", housing additional equipment or fuel.[44][71]

Aerodynamic studies in the 1960s demonstrated that the "vortex lift" phenomenon could be harnessed by highly swept wing configurations to reach higher angles of attack, using leading edge vortex flow off a slender lifting surface. As the F-16 was being optimized for high combat agility, GD's designers chose a slender cropped-delta wing with a leading-edge sweep of 40° and a straight trailing edge. To improve maneuverability, a variable-camber wing with a NACA 64A-204 airfoil was selected; the camber is adjusted by leading-edge and trailing edge flaperons linked to a digital flight control system regulating the flight envelope.[44][69] The F-16 has a moderate wing loading, reduced by fuselage lift.[72] The vortex lift effect is increased by leading-edge extensions, known as strakes. Strakes act as additional short-span, triangular wings running from the wing root (the junction with the fuselage) to a point further forward on the fuselage. Blended into the fuselage and along the wing root, the strake generates a high-speed vortex that remains attached to the top of the wing as the angle of attack increases, generating additional lift and allowing greater angles of attack without stalling. Strakes allow a smaller, lower-aspect-ratio wing, which increases roll rates and directional stability while decreasing weight. Deeper wing roots also increase structural strength and internal fuel volume.[69][73]

Early F-16s could be armed with up to six AIM-9 Sidewinder heat-seeking short-range air-to-air missiles (AAM) by employing rail launchers on each wingtip, as well as radar-guided AIM-7 Sparrow medium-range AAMs in a weapons mix.[74] More recent versions support the AIM-120 AMRAAM, and US aircraft often mount that missile on their wingtips to reduce wing flutter.[75] The aircraft can carry various other AAMs, a wide variety of air-to-ground missiles, rockets or bombs; electronic countermeasures (ECM), navigation, targeting or weapons pods; and fuel tanks on 9 hardpoints – six under the wings, two on wingtips, and one under the fuselage. Two other locations under the fuselage are available for sensor or radar pods.[74] The F-16 carries a 20 mm (0.79 in) M61A1 Vulcan cannon, which is mounted inside the fuselage to the left of the cockpit.[74]

The F-16 is the first production fighter aircraft intentionally designed to be slightly aerodynamically unstable, also known as relaxed static stability (RSS), to both reduce drag and improve maneuverability.[76] Most aircraft are designed to have positive static stability, which induces the aircraft to return to straight and level flight attitude if the pilot releases the controls. This reduces maneuverability as the inherent stability has to be overcome and increases a form of drag known as trim drag. Aircraft with relaxed stability are designed to be able to augment their stability characteristics while maneuvering to increase lift and reduce drag, thus greatly increasing their maneuverability. At Mach 1, the F-16 gains positive stability because of aerodynamic changes.[77][78][79]

To counter the tendency to depart from controlled flight and avoid the need for constant trim inputs by the pilot, the F-16 has a quadruplex (four-channel) fly-by-wire (FBW) flight control system (FLCS). The flight control computer (FLCC) accepts pilot input from the stick and rudder controls and manipulates the control surfaces in such a way as to produce the desired result without inducing control loss. The FLCC conducts thousands of measurements per second on the aircraft's flight attitude to automatically counter deviations from the pilot-set flight path. The FLCC further incorporates limiters governing movement in the three main axes based on attitude, airspeed, and angle of attack (AOA)/g; these prevent control surfaces from inducing instability such as slips or skids, or a high AOA inducing a stall. The limiters also prevent maneuvers that would exert more than a 9-g load.[80][81]

Flight testing revealed that "assaulting" multiple limiters at high AOA and low speed can result in an AOA far exceeding the 25° limit, colloquially referred to as "departing"; this causes a deep stall; a near-freefall at 50° to 60° AOA, either upright or inverted. While at a very high AOA, the aircraft's attitude is stable but control surfaces are ineffective. The pitch limiter locks the stabilators at an extreme pitch-up or pitch-down attempting to recover. This can be overridden so the pilot can "rock" the nose via pitch control to recover.[82]

Unlike the YF-17, which had hydromechanical controls serving as a backup to the FBW, General Dynamics took the innovative step of eliminating mechanical linkages from the control stick and rudder pedals to the flight control surfaces.[83] The F-16 is entirely reliant on its electrical systems to relay flight commands, instead of traditional mechanically linked controls, leading to the early moniker of "the electric jet" and aphorisms among pilots such as "You don't fly an F-16; it flies you."[84] The quadruplex design permits "graceful degradation" in flight control response in that the loss of one channel renders the FLCS a "triplex" system.[85][86] The FLCC began as an analog system on the A/B variants but has been supplanted by a digital computer system beginning with the F-16C/D Block 40.[87][88] The F-16's controls suffered from a sensitivity to static electricity or electrostatic discharge (ESD) and lightning.[89] Up to 70–80% of the C/D models' electronics were vulnerable to ESD.[90]

A key feature of the F-16's cockpit is the exceptional field of view. The single-piece, bird-proof polycarbonate bubble canopy provides 360° all-round visibility, with a 40° look-down angle over the side of the aircraft, and 15° down over the nose (compared to the common 12–13° of preceding aircraft); the pilot's seat is elevated for this purpose. Additionally, the F-16's canopy omits the forward bow frame found on many fighters, which is an obstruction to a pilot's forward vision.[44][91] The F-16's ACES II zero/zero ejection seat is reclined at an unusual tilt-back angle of 30°; most fighters have a tilted seat at 13–15°. The tilted seat can accommodate taller pilots and increases g-force tolerance; however, it has been associated with reports of neck aches, possibly caused by incorrect headrest usage.[92] Subsequent U.S. fighters have adopted more modest tilt-back angles of 20°.[44][93] Because of the seat angle and the canopy's thickness, the ejection seat lacks canopy-breakers for emergency egress; instead the entire canopy is jettisoned prior to the seat's rocket firing.[94]

The pilot flies primarily by means of an armrest-mounted side-stick controller (instead of a traditional center-mounted stick) and an engine throttle; conventional rudder pedals are also employed. To enhance the pilot's degree of control of the aircraft during high-g combat maneuvers, various switches and function controls were moved to centralized hands on throttle-and-stick (HOTAS) controls upon both the controllers and the throttle. Hand pressure on the side-stick controller is transmitted by electrical signals via the FBW system to adjust various flight control surfaces to maneuver the F-16. Originally, the side-stick controller was non-moving, but this proved uncomfortable and difficult for pilots to adjust to, sometimes resulting in a tendency to "over-rotate" during takeoffs, so the control stick was given a small amount of "play". Since the introduction of the F-16, HOTAS controls have become a standard feature on modern fighters.[citation needed]

The F-16 has a head-up display (HUD), which projects visual flight and combat information in front of the pilot without obstructing the view; being able to keep their head "out of the cockpit" improves the pilot's situation awareness.[95] Further flight and systems information are displayed on multi-function displays (MFD). The left-hand MFD is the primary flight display (PFD), typically showing radar and moving maps; the right-hand MFD is the system display (SD), presenting information about the engine, landing gear, slat and flap settings, and fuel and weapons status. Initially, the F-16A/B had monochrome cathode-ray tube (CRT) displays; replaced by color liquid-crystal displays on the Block 50/52.[44][96] The Mid-Life Update (MLU) introduced compatibility with night-vision goggles (NVG). The Boeing Joint Helmet Mounted Cueing System (JHMCS) is available from Block 40 onwards for targeting based on where the pilot's head faces, unrestricted by the HUD, using high-off-boresight missiles like the AIM-9X.[97] The newer Scorpion Helmet Mounted Display is also available and would later replace the JHMCS in U.S. service.[98]

In November 2024 it was announced that the US Air Force had awarded a $9 million contract to Danish defense company Terma A/S, to supply its 3-D audio system for the aircraft, with a program of upgrades over the following two years. The system will provide high-fidelity digital audio by spatially separating radio signals, aligning audio with threat directions, and integrating active noise reduction.[99]

The F-16A/B was originally equipped with the Westinghouse AN/APG-66 fire-control radar. Its slotted planar array antenna was designed to be compact to fit into the F-16's relatively small nose. In uplook mode, the APG-66 uses a low pulse-repetition frequency (PRF) for medium- and high-altitude target detection in a low-clutter environment, and in look-down/shoot-down employs a medium PRF for heavy clutter environments. It has four operating frequencies within the X band, and provides four air-to-air and seven air-to-ground operating modes for combat, even at night or in bad weather. The Block 15's APG-66(V)2 model added more powerful signal processing, higher output power, improved reliability, and increased range in cluttered or jamming environments. The Mid-Life Update (MLU) program introduced a new model, APG-66(V)2A, which features higher speed and more memory.[100]

The AN/APG-68, an evolution of the APG-66, was introduced with the F-16C/D Block 25. The APG-68 has greater range and resolution, as well as 25 operating modes, including ground-mapping, Doppler beam-sharpening, ground moving target indication, sea target, and track while scan (TWS) for up to 10 targets. The Block 40/42's APG-68(V)1 model added full compatibility with Lockheed Martin Low Altitude Navigation and Targeting Infrared for Night (LANTIRN) pods, and a high-PRF pulse-Doppler track mode to provide Interrupted Continuous Wave guidance for semi-active radar homing (SARH) missiles like the AIM-7 Sparrow. Block 50/52 F-16s initially used the more reliable APG-68(V)5 which has a programmable signal processor employing Very High Speed Integrated Circuit (VHSIC) technology. The Advanced Block 50/52 (or 50+/52+) is equipped with the APG-68(V)9 radar, with a 30% greater air-to-air detection range and a synthetic aperture radar (SAR) mode for high-resolution mapping and target detection-recognition. In August 2004, Northrop Grumman was contracted to upgrade the APG-68 radars of Block 40/42/50/52 aircraft to the (V)10 standard, providing all-weather autonomous detection and targeting for Global Positioning System (GPS)-aided precision weapons, SAR mapping, and terrain-following radar (TF) modes, as well as interleaving of all modes.[44]

The F-16E/F is outfitted with Northrop Grumman's AN/APG-80 active electronically scanned array (AESA) radar.[101] Northrop Grumman developed the latest AESA radar upgrade for the F-16 (selected for USAF and Taiwan's Republic of China Air Force F-16 upgrades), named the AN/APG-83 Scalable Agile Beam Radar (SABR).[102][103] In July 2007, Raytheon announced that it was developing a Next Generation Radar (RANGR) based on its earlier AN/APG-79 AESA radar as a competitor to Northrop Grumman's AN/APG-68 and AN/APG-80 for the F-16.[44] On 28 February 2020, Northrop Grumman received an order from USAF to extend the service lives of their F-16s to at least 2048 with AN/APG-83 as part of the service-life extension program (SLEP).[104]

The initial powerplant selected for the single-engined F-16 was the Pratt & Whitney F100-PW-200 afterburning turbofan, a modified version of the F-15's F100-PW-100, rated at 23,830 lbf (106.0 kN) thrust. During testing, the engine was found to be prone to compressor stalls and "rollbacks", wherein the engine's thrust would spontaneously reduce to idle. Until resolved, the Air Force ordered F-16s to be operated within "dead-stick landing" distance of its bases.[16] It was the standard F-16 engine through the Block 25, except for the newly built Block 15s with the Operational Capability Upgrade (OCU). The OCU introduced the 23,770 lbf (105.7 kN) F100-PW-220, later installed on Block 32 and 42 aircraft: the main advance being a Digital Electronic Engine Control (DEEC) unit, which improved reliability and reduced stall occurrence. Beginning production in 1988, the "-220" also supplanted the F-15's "-100", for commonality. Many of the "-220" engines on Block 25 and later aircraft were upgraded from 1997 onwards to the "-220E" standard, which enhanced reliability and maintainability; unscheduled engine removals were reduced by 35%.[105]

The F100-PW-220/220E was the result of the USAF's Alternate Fighter Engine (AFE) program (colloquially known as "the Great Engine War"), which also saw the entry of General Electric as an F-16 engine provider. Its F110-GE-100 turbofan was limited by the original inlet to a thrust of 25,735 lbf (114.47 kN), the Modular Common Inlet Duct allowed the F110 to achieve its maximum thrust of 28,984 lbf (128.93 kN). (To distinguish between aircraft equipped with these two engines and inlets, from the Block 30 series on, blocks ending in "0" (e.g., Block 30) are powered by GE, and blocks ending in "2" (e.g., Block 32) are fitted with Pratt & Whitney engines.)[105][106]

The Increased Performance Engine (IPE) program led to the 29,588 lbf (131.61 kN) F110-GE-129 on the Block 50 and 29,160 lbf (129.7 kN) F100-PW-229 on the Block 52. F-16s began flying with these IPE engines in the early 1990s. Altogether, of the 1,446 F-16C/Ds ordered by the USAF, 556 were fitted with F100-series engines and 890 with F110s.[44] The United Arab Emirates' Block 60 is powered by the General Electric F110-GE-132 turbofan with a maximum thrust of 32,500 lbf (145 kN), the highest thrust engine developed for the F-16.[107]

The F-16 is being used by the active-duty USAF, Air Force Reserve, and Air National Guard units, the USAF aerial demonstration team, the U.S. Air Force Thunderbirds, and as an adversary-aggressor aircraft by the United States Navy at the Naval Strike and Air Warfare Center.[citation needed]

The U.S. Air Force, including the Air Force Reserve and the Air National Guard, flew the F-16 in combat during Operation Desert Storm in 1991 and in the Balkans later in the 1990s. F-16s also patrolled the no-fly zones in Iraq during Operations Northern Watch and Southern Watch and served during the War in Afghanistan and the War in Iraq from 2001 and 2003 respectively. In 2011, Air Force F-16s took part in the intervention in Libya.[108]

On 11 September 2001, two unarmed F-16s were launched in an attempt to ram and down United Airlines Flight 93 before it reached Washington D.C. during the 11 September 2001 terrorist attacks, but Flight 93 was prematurely brought down by the hijackers after passengers attacked the cockpit, so the F-16s were retasked to patrol the local airspace and later escorted Air Force One back to Washington.[109][110][importance?]

The F-16 had been scheduled to remain in service with the U.S. Air Force until 2025.[111] Its replacement is planned to be the F-35A variant of the Lockheed Martin F-35 Lightning II, which is expected to gradually begin replacing several multirole aircraft among the program's member nations. However, owing to delays in the F-35 program, all USAF F-16s will receive service life extension upgrades.[112] In 2022, it was announced the USAF would continue to operate the F-16 for another two decades.[113]

The F-16's first air-to-air combat success was achieved by the Israeli Air Force (IAF) over the Bekaa Valley on 28 April 1981, against a Syrian Mi-8 helicopter, which was downed with cannon fire.[115] On 7 June 1981, eight Israeli F-16s, escorted by six F-15s, executed Operation Opera, their first employment in a significant air-to-ground operation. This raid severely damaged Osirak, an Iraqi nuclear reactor under construction near Baghdad, to prevent the regime of Saddam Hussein from using the reactor for the creation of nuclear weapons.[116]

The following year, during the 1982 Lebanon War Israeli F-16s engaged Syrian aircraft in one of the largest air battles involving jet aircraft, which began on 9 June and continued for two more days. Israeli Air Force F-16s were credited with 44 air-to-air kills during the conflict.[115][117]

In January 2000, Israel completed a purchase of 102 new F-16I aircraft in a deal totaling $4.5 billion.[118] F-16s were also used in their ground-attack role for strikes against targets in Lebanon. IAF F-16s participated in the 2006 Lebanon War and the 2008–09 Gaza War.[119] During and after the 2006 Lebanon war, IAF F-16s shot down Iranian-made UAVs launched by Hezbollah, using Rafael Python 5 air-to-air missiles.[120][121][122]

On 10 February 2018, an Israeli Air Force F-16I was shot down in northern Israel when it was hit by a relatively old model S-200 (NATO name SA-5 Gammon) surface-to-air missile of the Syrian Air Defense Force.[123] The pilot and navigator ejected safely in Israeli territory. The F-16I was part of a bombing mission against Syrian and Iranian targets around Damascus after an Iranian drone entered Israeli airspace and was shot down.[124] An Israel Air Force investigation determined on 27 February 2018 that the loss was due to pilot error since the IAF determined the air crew did not adequately defend themselves.[125]

On 16 July 2024, the last single-seat F-16C Barak-1 (‘Lightning’ in Hebrew) were retired; the IAF continue to use the F-16D Brakeet and F-16I Sufa two-seat variants.[126]

During the Soviet–Afghan War, PAF F-16As shot down between 20 and 30 Soviet and Afghan warplanes; the political situation however resulted in PAF officially recognizing only 9 kills which were made inside Pakistani airspace.[127] From May 1986 to January 1989, PAF F-16s from the Tail Choppers and Griffin squadrons using mostly AIM-9 Sidewinder missiles, shot down four Afghan Su-22s, two MiG-23s, one Su-25, and one An-26.[128] Most of these kills were by missiles, but at least one, a Su-22, was destroyed by cannon fire. One F-16 was lost in these battles. The downed F-16 was likely hit accidentally by the other F-16.[129]

On 7 June 2002, a Pakistan Air Force F-16B Block 15 (S. No. 82-605) shot down an Indian Air Force unmanned aerial vehicle, an Israeli-made Searcher II, using an AIM-9L Sidewinder missile, during a night interception near Lahore.[130]

The Pakistan Air Force has used its F-16s in various foreign and internal military exercises, such as the "Indus Vipers" exercise in 2008 conducted jointly with Turkey.[131][failed verification]

Between May 2009 and November 2011[update], the PAF F-16 fleet flew more than 5,500 sorties[needs update] in support of the Pakistan Army's operations against the Taliban insurgency in the FATA region of North-West Pakistan. More than 80% of the dropped munitions were laser-guided bombs.[132][133]

On 27 February 2019, following six Pakistan Air Force airstrikes in Jammu and Kashmir, India, Pakistani officials said that two of its fighter jets shot down one MiG-21 and one Su-30MKI belonging to the Indian Air Force.[134][135][136][137] Indian officials only confirmed the loss of one MiG-21 but denied losing any Su-30MKI in the clash and claimed the Pakistani claims as dubious.[138][139] Additionally Indian officials also claimed to have shot down one F-16 belonging to the Pakistan Air Force.[140][141] This was denied by the Pakistani side,[142] considered dubious by neutral sources,[143][144] and later backed by a report by Foreign Policy magazine, reporting that the US had completed a physical count of Pakistan's F-16s and found none missing.[145] A report by The Washington Post noted that the Pentagon and State Department refused public comment on the matter but did not deny the earlier report.[146]

The Turkish Air Force acquired its first F-16s in 1987. F-16s were later produced in Turkey under four phases of Peace Onyx programs. In 2015, they were upgraded to Block 50/52+ with CCIP by Turkish Aerospace Industries.[147] Turkish F-16s are being fitted with indigenous AESA radars and EW suite called SPEWS-II.[148]

On 18 June 1992, a Greek Mirage F1 crashed during a dogfight with a Turkish F-16.[149][150][151] On 8 February 1995, a Turkish F-16 crashed into the Aegean Sea after being intercepted by Greek Mirage F1 fighters.[152][153]

Turkish F-16s have participated in Bosnia-Herzegovina and Kosovo since 1993 in support of United Nations resolutions.[154]

On 8 October 1996, seven months after the escalation a Greek Mirage 2000 reportedly fired an R.550 Magic II missile and shot down a Turkish F-16D over the Aegean Sea.[155][156] The Turkish pilot died, while the co-pilot ejected and was rescued by Greek forces.[151][157][158] In August 2012, after the downing of an RF-4E on the Syrian coast, Turkish Defence Minister İsmet Yılmaz confirmed that the Turkish F-16D was shot down by a Greek Mirage 2000 with an R.550 Magic II in 1996 near Chios island.[159] Greece denies that the F-16 was shot down.[160] Both Mirage 2000 pilots reported that the F-16 caught fire and they saw one parachute.[161][162]

On 23 May 2006, two Greek F-16s intercepted a Turkish RF-4 reconnaissance aircraft and two F-16 escorts off the coast of the Greek island of Karpathos, within the Athens FIR. A mock dogfight ensued between the two sides, resulting in a midair collision[163] between a Turkish F-16 and a Greek F-16. The Turkish pilot ejected safely, but the Greek pilot died owing to damage caused by the collision.[164][165]

Turkey used its F-16s extensively in its conflict with Kurdish insurgents in southeastern parts of Turkey and Iraq. Turkey launched its first cross-border raid on 16 December 2007, a prelude to the 2008 Turkish incursion into northern Iraq, involving 50 fighters before Operation Sun. This was the first time Turkey had mounted a night-bombing operation on a massive scale, and also the largest operation conducted by the Turkish Air Force.[166]

During the Syrian Civil War, Turkish F-16s were tasked with airspace protection on the Syrian border. After the RF-4 downing in June 2012 Turkey changed its rules of engagement against Syrian aircraft, resulting in scrambles and downings of Syrian combat aircraft.[167] On 16 September 2013, a Turkish Air Force F-16 shot down a Syrian Arab Air Force Mil Mi-17 helicopter near the Turkish border.[168] On 23 March 2014, a Turkish Air Force F-16 shot down a Syrian Arab Air Force MiG-23 when it allegedly entered Turkish air space during a ground attack mission against Al Qaeda-linked insurgents.[169] On 16 May 2015, two Turkish Air Force F-16s shot down a Syrian Mohajer 4 UAV firing two AIM-9 missiles after it trespassed into Turkish airspace for 5 minutes.[170][171] A Turkish Air Force F-16 shot down a Russian Air Force Sukhoi Su-24 on the Turkey-Syria border on 24 November 2015.[172]

On 1 March 2020, two Syrian Sukhoi Su-24s were shot down by Turkish Air Force F-16s using air-to-air missiles over Syria's Idlib Governorate.[173] All four pilots safely ejected.[174] On 3 March 2020, a Syrian Arab Army Air Force L-39 combat trainer was shot down by a Turkish F-16 over Syria's Idlib province.[175] The pilot died.[176]

As a part of Turkish F-16 modernization program new air-to-air missiles are being developed and tested for the aircraft. GÖKTUĞ program led by TUBITAK SAGE has presented two types of air-to-air missiles named as Bozdogan (Merlin) and Gokdogan (Peregrine). While Bozdogan has been categorized as a Within Visual Range Air-to-Air Missile (WVRAAM), Gokdogan is a Beyond Visual Range Air-to-Air Missile (BVRAAM). On 14 April 2021, first live test exercise of Bozdogan have successfully completed and the first batch of missiles are expected to be delivered throughout the same year to the Turkish Air Force.[177][178]

On 16 February 2015, Egyptian F-16s struck weapons caches and training camps of the Islamic State (ISIS) in Libya in retaliation for the murder of 21 Egyptian Coptic Christian construction workers by masked militants affiliated with ISIS. The airstrikes killed 64 ISIS fighters, including three leaders in Derna and Sirte on the coast.[179]

The Royal Netherlands Air Force, Belgian Air Component, Royal Danish Air Force and Royal Norwegian Air Force all fly the F-16.[180] All F-16s in most European air forces are equipped with drag chutes specifically to allow them to operate from automobile highways.[181]

A Yugoslavian MiG-29 was shot down by a Dutch F-16AM during the Kosovo War in 1999.[182] Belgian and Danish F-16s also participated in joint operations over Kosovo during the war.[182] Dutch, Belgian, Danish, and Norwegian F-16s were deployed during the 2011 intervention in Libya and in Afghanistan.[183] In Libya, Norwegian F-16s dropped almost 550 bombs and flew 596 missions,[184] some 17% of the total strike missions[185] including the bombing of Muammar Gaddafi's headquarters.[186]

In late March 2018, Croatia announced its intention to purchase 12 used Israeli F-16C/D "Barak"/"Brakeet" jets, pending U.S. approval.[187] Acquiring these F-16s would allow Croatia to retire its aging MiG-21s.[188] In January 2019, the deal was canceled because U.S. would only allow the resale if Israel stripped the planes of all the modernized electronics, while Croatia insisted on the original deal with all the upgrades installed.[189] At the end of November 2021, Croatia signed with France instead, for 12 Rafales.[190]

On 11 July 2018, Slovakia's government approved the purchase of 14 F-16 Block 70/72 to replace its aging fleet of Soviet-made MiG-29s.[191] A contract was signed on 12 December 2018 in Bratislava.[192]

In May 2023, an international coalition consisting of the United Kingdom, the Netherlands, Belgium and Denmark announced their intention to train Ukrainian Air Force pilots on the F-16 ahead of possible future deliveries to increase the Ukrainian Air Force capabilities in the current Russo-Ukrainian War. The U.S. confirmed that it would approve the re-export from these countries to Ukraine.[193] Denmark has agreed to help train Ukrainians on their usage of the fighter. Denmark's acting Defence Minister Troels Lund Poulsen said that Denmark "will now be able to move forward for a collective contribution to train Ukrainian pilots to fly F-16s".[194] On 6 July 2023, Romania announced that it will host the future training center after the meeting of the Supreme Council of National Defense.[195] During the 2023 Vilnius summit, a coalition was formed consisting of Denmark, the Netherlands, Belgium, Canada, Luxembourg, Norway, Poland, Portugal, Romania, Sweden, the United Kingdom, and Ukraine.[196] A number of Ukrainian pilots began training in Denmark and the U.S.[197][198] The European F-16 Training Center, organized by Romania, the Netherlands, and Lockheed Martin through several subcontractors, officially opened on 13 November 2023. It is located at the Romanian Air Force's 86th Air Base,[199] and Ukrainian pilots began training there in September 2024.[200] On 17 August 2023, the U.S. approved the transfer of F-16s from the Netherlands and Denmark to Ukraine after the Ukrainian pilots have completed their training.[201] The Netherlands and Denmark have announced that together they will donate up to 61 F-16AM/BM Block 15 MLU fighters to Ukraine once pilot training has been completed.[202][203]

On 13 May 2024, Danish Prime Minister Mette Frederiksen said that "F-16 from Denmark will be in the air over Ukraine within months." Denmark is sending 19 F-16s in total.[204] By the end of July 2024, the first F-16s were delivered to Ukraine.[205]

On 4 August 2024, President Zelensky announced to the public that the F-16 was now in operational service with Ukraine. Zelensky stated at an opening ceremony that: "F-16s are in Ukraine. We did it. I am proud of our guys who are mastering these jets and have already started using them for our country,".[206]

On 26 August 2024, F-16s were reportedly used to intercept Russian cruise missiles for the first time.[207] Also on 26 August, a Ukrainian F-16 crashed and the pilot, Oleksii Mes, was killed while intercepting Russian aerial targets during the cruise missile strikes. The cause is under investigation.[208]


On 13 December 2024, the Ukrainian Air Force claimed an F-16 shot down six Russian cruise missiles. Two were shot down with "medium-range missiles", another two with "short-range missiles" and two were claimed to be downed by the 20 mm cannon.[209]

Ukraine has lost two F-16[210]. On 30 August 2024, the Commander of the Ukrainian Air Force, Mykola Oleshchuk, was dismissed by President Zelenskyy and replaced by Lieutenant General Anatolii Kryvonozhko,[211] which was partially attributed to "indications" that the F-16 that crashed on 26 August was shot down in "a friendly fire incident". Ukrainian parliamentarian Maryana Bezuhla and Oleshchuk had previously argued over the cause of the F-16 loss.[212][213]

On 12 April 2025, Ukraine stated pilot Pavlo Ivanov was killed in action flying an F-16.[214][215] BBC Ukraine reported that Russia had fired three missiles at the F-16, which was probably flying over the Sumy region, including S-400 ground-to-air and R-37 air-to-air missiles.[216]

Venezuela Air Force have flown the F-16 on combat missions.[180] During the November 1992 Venezuelan coup attempt, two F-16A belonging to the government loyalist managed to shoot down two OV-10 Bronco and an AT-27 Tucano flown by the rebels and establishing aerial superiority for the government forces.[217]

Two F-16B of the Indonesian Air Force intercepted and engaged several US Navy F/A-18 Hornets over the Java Sea in the 2003 Bawean incident.[218]

The Royal Moroccan Air Force and the Royal Bahraini Air Force each lost a single F-16C, both shot down by Houthi anti-aircraft fire during the Saudi Arabian-led intervention in Yemen, respectively on 11 May 2015 and on 30 December 2015.[219]

On 11 October 2023, Deputy Assistant Secretary for Regional Security Mira Resnick confirmed to Jorge Argüello, Argentinean ambassador to the US, that the State Department has approved the transfer of 38 F-16s from Denmark.[220] On 16 April 2024, it was announced by defense minister Luis Petri that the country went through with the purchase of 24+1 Danish F-16s, that are to be brought up to date before they are sent to Argentina.[221] The 25th plane, an F-16B MLU Block 10, meant for mechanics training, came disassembled in an Argentinian C-130 in late December 2024.[222] Six F-16s a year are to be delivered from Denmark to Argentina until all are delivered, with the first batch expected around November 2025.[221]

In 2019, the US State Department approved the possible sale of 8 F-16 Block 70s to Bulgaria,[223] and the deal was approved by the Bulgarian parliament, and President Rumen Radev.[224] In November 2022, the purchase of a further 8 F-16 Block 70 fighters, spares, weapons and other systems was approved for delivery in 2027.[225] The Bulgarian Air Force expects delivery of the first eight new F-16 Block 70s by 2025 and the second batch of eight F-16 Block 70s is expected in 2027.[226]

In 2024, Argentina selected a bid for 24 F-16AM/BM aircraft from Denmark, instead of one from JF-17s from China/Pakistan.[227] The first aircraft, a F-16B, was unveiled in Buenos Aires on 24 February 2025.[228]

In 2021, the Defense Security Cooperation Agency approved the Philippines' purchase of 12 F-16s worth an estimated US$2.43 billion. However, the Philippines has yet to complete this deal due to financial constraints with negotiations ongoing.[229][230] In April 2025, the possible sale of 20 F-16s were approved.[231]

In 2025, multiple news channels reported that Vietnam is finalizing an agreement to purchase at least 24 F-16s, possibly the F-16V variant.[232][233]

In January 2021, Canadian defence contractor Top Aces announced that they had taken delivery of the first civilian owned F-16s to their US HQ in Mesa, Arizona.[234] In an approval process that had taken years, they had purchased a batch of 29 F-16A/B Netz from the Israeli Air Force, including several that had taken part in Operation Opera. A year later, the first of these aircraft had finished the extensive AAMS mission system upgrades including AESA radar, HMCS, ECM, and Tactical Datalink. In late 2022 they began regular operations flying as contracted aggressors for USAF F-22 and F-35 squadrons in Luke AFB and Eglin AFB, as well as supporting exercises in other USAF and USMC bases.[235]

F-16 models are denoted by increasing block numbers to denote upgrades. The blocks cover both single- and two-seat versions. A variety of software, hardware, systems, weapons compatibility, and structural enhancements have been instituted over the years to gradually upgrade production models and retrofit delivered aircraft.[citation needed]

While many F-16s were produced according to these block designs, there have been many other variants with significant changes, usually because of modification programs. Other changes have resulted in role-specialization, such as the close air support and reconnaissance variants. Several models were also developed to test new technology. The F-16 design also inspired the design of other aircraft, which are considered derivatives. Older F-16s are being converted into QF-16 drone targets.[236]

As of 2024, there have been 2,145 F-16s in active service around the world.[286][287]

The F-16 has been involved in over 670 hull-loss accidents as of January 2020.[300][301]

As newer variants have entered service, many examples of older F-16 models have been preserved for display worldwide, particularly in Europe and the United States.

Data from USAF sheet,[67] International Directory of Military Aircraft,[76] Flight Manual for F-16C/D Block 50/52+[319]
General characteristics

Performance

Armament

Avionics

Related development

Aircraft of comparable role, configuration, and era


Related listsThe Georgia Tech Yellow Jackets football program represents the Georgia Institute of Technology in the NCAA Football Bowl Subdivision in the sport of American football. The Yellow Jackets college football team competes in the Football Bowl Subdivision (FBS) of the National Collegiate Athletic Association (NCAA) and the Atlantic Coast Conference (ACC). Georgia Tech has fielded a football team since 1892 and as of 2023, it has an all-time record of 761–544–43.[4] The Yellow Jackets play in Bobby Dodd Stadium at Hyundai Field in Atlanta, Georgia, holding a stadium max capacity of 51,913.

The Yellow Jackets claim four national championships across four decades. The program has also won 16 conference titles. Among the team's former coaches are John Heisman, for whom the Heisman Trophy is named, and Bobby Dodd, for whom the Bobby Dodd Coach of the Year Award and the school's stadium are named. Heisman led the team to the most lopsided game in football history, 222–0, and both Heisman and Dodd led Tech's football team to national championships. Dodd also led the Jackets on their longest winning streak — 8 straight games — against the University of Georgia in Tech's most time-endured rivalry, called Clean, Old-Fashioned Hate. For his part, Heisman led Georgia Tech to an undefeated 12–0–1 record in the Georgia Tech–Clemson football rivalry.

A number of successful collegiate and professional football players have also played for Tech. The program has 48 first-team All-Americans and over 150 alumni who have played in the NFL. Among the most lauded and most notable players the school has produced are Maxie Baughan, Calvin Johnson, Demaryius Thomas, Keith Brooking, Joe Hamilton, Joe Guyon, Pat Swilling and Billy Shaw.

In the 21st century, Georgia Tech has won their Coastal Division and appeared in the ACC Championship Game four times since 2006. In addition to its conference and national championships, legendary coaches, and talented players, Tech's football program has been noted for its many historic traditions and improbable game finishes throughout the years, including its famed fight song Ramblin' Wreck from Georgia Tech, its famous blocked field goal return against No. 9 Florida State in 2015, and its comeback win over No. 17 Miami in 2023.[5]

Tech began its football program with several students forming a loose-knit troop of footballers called the Blacksmiths. On November 5, 1892, Tech played its first football game against Mercer University. The team lost to Mercer 12–6 in Macon, Georgia.[n 1] Tech played two other games during their first season and lost both of them for a season record of 0–3. Discouraged by these results, the Blacksmiths sought a coach to improve their record. Leonard Wood, an Army officer and Atlantan, heard of Tech's football struggles and volunteered to player-coach the team.[7] Over the span of 1892–1903, Tech only won 8 games, tied in 5, and lost 32.[8] In 1893, Tech played against the University of Georgia for the first time. Tech defeated Georgia 28–6 for the school's first-ever victory. The angry Georgia fans threw stones and other debris at the Tech players during and after the game. The poor treatment of the Blacksmiths by the Georgia faithful gave birth to the rivalry now known as Clean, Old-Fashioned Hate.[9][10] In 1902, Jesse Thrash was the team's first All-Southern selection. He began the season as a sub and closed it as the undisputed star of the Tech team.[11]

Oliver Jones Huie was selected by Ga Tech's athletic association to coach the football team for the 1903 season when the team won 3 and lost 5 games. A professional coach was desperately needed if Tech wished to build a truly competitive football program. The first game of the 1903 season was a 73–0 destruction at the hands of John Heisman's Clemson; shortly after the season, Tech offered Heisman a coaching position.

John Heisman put together 16 consecutive non-losing seasons, amassed 104 wins, including three undefeated campaigns and a 32-game undefeated streak. From 1915 to 1918 Georgia Tech went 30–1–2 and outscored opponents 1611–93 utilizing his jump shift offense.[n 2] He would also muster a 5-game winning streak against the hated Georgia Bulldogs from 1904 to 1908 before incidents led up to the cutting of athletic ties with Georgia in 1919.[9] Heisman was hired by Tech for $2,250 a year and 30% of the home ticket sales. Heisman would not disappoint the Tech faithful as his first season was an 8–1–1 performance, the first winning season since 1893.[12] One source relates: "The real feature of the season was the marvelus advance made by the Georgia School of Technology which burst from fetters that kept it in the lowest class for ten years."[13]

His team posted victories over Georgia, Tennessee, University of Florida at Lake City, and Cumberland, and a tie with his last employer, Clemson. He suffered just one loss, to another first year coach, Mike Donahue of Auburn. The 1905 team went 6–0–1. The 1906 team beat Auburn for the first time. Stars of this early period for Tech include Lob Brown and Billy Wilson. The 1907 and 1908 teams were led by "Twenty Percent" Davis. Pat Patterson was All-Southern in 1910. Patterson was captain in 1911, a season in which future coach William Alexander was a reserve quarterback. Heisman helped students construct Grant Field in 1913, when Alf McDonald was quarterback. The 1915 team went undefeated.

Arguably the most notable game of Heisman's career was the most lopsided victory in college football history. In 1916, Cumberland College ended its football program and attempted to cancel a scheduled game with Heisman's Jackets. Heisman, however, was seeking vengeance for a 22–0 baseball loss to Cumberland in the spring of 1916, a game in which Heisman suspected Cumberland of hiring professional players to pose as Cumberland students. Heisman refused the game's cancellation and Cumberland mustered up a group of commonfolk to play Tech.[14] Tech won 222–0.[15] Neither team achieved a first down other than a touchdown, as Cumberland either punted or turned the ball over before a first down and Tech scored on almost every play from scrimmage.[14] Jim Preas, Tech's kicker, kicked 16 point after tries, which is still a record for a single game.

In 1917 Tech won its first national championship behind the backfield of Everett Strupper, Joe Guyon, Al Hill, and Judy Harlan. It was the first national title for a Southern team, and for many years the "Golden Tornado" was considered the finest team the region ever produced. Strupper and captain Walker Carpenter were the first two players from the Deep South ever selected first-team All-American. Heisman challenged Pop Warner's undefeated Pittsburgh team to a decisive national championship game, but he declined. In the next season of 1918, Tech lost a lopsided game to Pitt 32–0. Center Bum Day became the first player from the south selected for Walter Camp's first team. In 1919, Auburn upset Tech for the SIAA crown. By 1919, Heisman had divorced his wife and felt that he would embarrass his wife socially if he remained in Atlanta.[16] Heisman moved to Pennsylvania, leaving Tech in the hands of William Alexander.[17]

William Alexander had attended Georgia Tech and after graduating as valedictorian of his class in 1912, taught mathematics at Tech and served as Heisman's assistant coach.[17] In 1920, he was given the job of head coaching Tech's football team. Alexander retained Heisman's 'jump shift' offense, and in his first season he saw Tech win an SIAA title behind captain Buck Flowers, the first Georgia Tech player inducted into the College Football Hall of Fame. Tech suffered its only loss again to Warner's Pitt, and finished the season with a win over rival Auburn.[18][19] Tackle Bill Fincher made Camp's first team All-America.[n 3]

The 1921 and 1922 teams also claimed SIAA titles. The 1921 team suffered its only loss to undefeated, eastern power Penn State. Tech was captained by fullback Judy Harlan. Future Tech fullback Sam Murray was asked about a certain strong runner in the 1930s, "He's good. But if I were playing again, I would have one wish – never to see bearing down upon me a more fearsome picture of power than Judy Harlan blocking for Red Barron."[21] Barron ran for 1,459 yards on the season.[22]

From 1923 to 1925, though Tech failed to claim a conference title, it had one of its best-ever players: fullback Doug Wycoff, "the outstanding back of the South for the past two years."[23] Coach Alexander recalled "The work of Douglas Wycoff against Notre Dame two years in succession was brilliant in the extreme, as was his plunging against Penn. State when we defeated them twice."[24][n 4]  Tech and UGA renewed their annual rivalry game in 1925 after an eight-year hiatus. Quarterback Ike Armstrong thought the game clock read five seconds remaining in the game when in actuality it was five minutes. Williams set up his offense for a field goal and kicked it to put Tech up 3–0 on first down. Luckily for Williams, Tech won 3–0.

In 1927, Alexander instituted "the Plan." Georgia was highly rated to start the 1927 season, known as the "dream and wonder team", and justified their rating throughout the season going 9–0 in their first 9 games. Alexander's plan was to minimize injuries by benching his starters early no matter the score of every game before the UGA finale. On December 3, 1927, UGA rolled into Atlanta on the cusp of a national and conference title. Tech's well rested starters were helped by the rain and shut out the Bulldogs 12–0, ending any chance of UGA's first national title, while netting the SIAA title.[26]

Alexander's 1928 team amassed a perfect record and won the school's second national title. The team was led at center by captain Peter Pund and upset Notre Dame. "I sat at Grant Field and saw a magnificent Notre Dame team suddenly recoil before the furious pounding of one man–Pund, center", said legendary coach Knute Rockne. "Nobody could stop him. I counted 20 scoring plays that this man ruined."[27] The 1928 team was also the very first Tech team to attend a bowl game. The team was invited to the Rose Bowl to play California.[n 5] The game was a defensive struggle, with the first points scored after a Georgia Tech fumble. The loose ball was scooped up by California center Roy Riegels and then accidentally returned in the wrong direction. Riegels returned the ball all the way to California's 3-yard line. After Riegels was finally stopped by his own teammate at the 1-yard line, he was swarmed by a group of Tech players. The Bears opted to punt from the end zone. The punt was blocked and converted by Tech into a safety giving Tech a 2–0 lead.[n 6] Cal scored a touchdown and a point after but Tech would score another touchdown to win the game 8–7. This victory made Tech the 10–0 undefeated national champion of 1928.[28][29][n 7] Coach Alexander found campus spirit to be particularly low during the Great Depression. His  football program (and the other athletic teams) had very few student fans attending the games. He helped to establish a spirit organization known as the Yellow Jacket Club in 1930 to bolster student spirit.[31] The group would later become the Ramblin' Reck Club. Georgia Tech football declined following the 1928 championship, and did not post another winning record until 1937. The 1939 team was SEC co-champion. The only retired jersey in Georgia Tech football history is No. 19.[32] The number belonged to Tech halfback Clint Castleberry. Castleberry played on the No. 5 ranked 1942 Tech team as a true freshman and was third place in the 1942 Heisman Trophy voting. After ending his freshman year at Tech, Castleberry elected to join the war effort and signed up for the Army Air Corps. While co-piloting a B-26 Marauder over Africa, Castleberry, his crew, and another B-26 disappeared and were never heard from again.[n 8] Castleberry has been memorialized on Grant Field ever since, with a prominent No. 19 on display in the stadium.[32] The 1943 and 1944 teams won SEC titles. Coach Alexander finally retired in 1944 after winning 134 games as head coach and taking Tech to the Rose Bowl, Orange Bowl, Cotton Bowl Classic, and Sugar Bowl. To this day, Alexander has the second most victories of any Tech football coach. The record for most coaching victories in Tech history is still held by Alexander's then coordinator and eventual successor Bobby Dodd.

President Blake R Van Leer believed athletics were an important part of collegiate life, he championed that belief with public support of coaches like Dodd. Van Leer was recorded being proud of Dodd's accomplishments, celebrating him among peers and being a supporting friend.[34] Bobby Dodd took over the Georgia Tech football program following Coach Alexander's retirement in 1944. He did not believe in intense physical practices but rather precise and well executed practices. Dodd's philosophy translated to winning. He set the record for career wins at Tech at 165 career coaching wins including a 31-game winning streak from 1951 to 1952.[35] He also managed to capture two Southeastern Conference Titles and the 1952 National Title, which concluded a 12–0 perfect season and Sugar Bowl conquest of previously undefeated, seventh ranked Ole Miss[35] in a season that also included victories over Orange Bowl champions, 9th ranked, Alabama; 15th ranked Gator Bowl champions Florida Gators football; 16th ranked Duke; and a 7–4 rival Georgia. While 9–0 Michigan State would capture the AP and UP titles, the Yellow Jackets' were ranked first in the International News Service poll. Dodd also understood the deep-seated rivalry with the University of Georgia. His teams won 8 games in a row over the Bulldogs from 1949 to 1956 outscoring the Bulldogs 176–39 during the winning streak.[36] This 8–game winning streak against Georgia remains the longest winning streak by either team in the series. Dodd would finish his career with a 12–9 record against the Bulldogs.[36] In 1956, much controversy preceded the 1956 Sugar Bowl. Segregationists tried to keep Pitt fullback/linebacker Bobby Grier from playing because he was black. Georgia's governor Marvin Griffin privately met with Coach Dodd and Georgia Tech President Van Leer, where he told them the game could go on without pushback. Immediately after, the governor issued a telegram stating the South stands in Armageddon and publicly threatened to remove funding if Georgia Tech's president Van Leer did not cancel the game. Dodd, along with his team, publicly supported moving forward with the game. President Van Leer threatened to resign in a show of support. Ultimately, Bobby Grier played, making this the first integrated Sugar Bowl and is regarded as the first integrated bowl game in the Deep South.[5]

Dodd's tenure included Georgia Tech's withdrawal from the Southeastern Conference.[35] The initial spark for Dodd's withdrawal was a historic feud with Alabama Crimson Tide Coach Bear Bryant.[37] The feud began when Tech was visiting the Tide at Legion Field in Birmingham in 1961. After a Tech punt, Alabama fair-caught the ball. Chick Granning of Tech was playing coverage and relaxed after the signal for the fair catch. Darwin Holt of Alabama continued play and smashed his elbow into Granning's face causing severe fracturing in his face, a broken nose, and blood-filled sinuses. Granning was knocked unconscious and suffered a severe concussion, the result of which left him unable to play football ever again. Dodd sent Bryant a letter asking Bryant to suspend Holt after game film indicated Holt had intentionally injured Granning.[37] Bryant never suspended Holt. The lack of discipline infuriated Dodd and sparked Dodd's interest in withdrawing from the SEC.[38] Another issue of concern for Dodd was Alabama's and other SEC schools' over-recruitment of players.[37] Universities would recruit more players than they had roster space for. During the summer practice sessions, the teams in question would cut the players well after signing day thus preventing the cut players from finding new colleges to play for. Dodd appealed the SEC administration to punish the "tryout camps" of his fellow SEC members but the SEC did not. Finally, Dodd withdrew Georgia Tech from the SEC in 1964.[37] Tech would remain an independent like Notre Dame and Penn State (at the time) during the final four years of Dodd's coaching tenure. In 1967, Dodd passed the head coach position to his favorite coordinator, Bud Carson. Dodd simply retained his athletic director position, which he had acquired in 1950. He would not retire from athletic directing until 1976.

Bud Carson was Tech's defensive coordinator in 1966. His job was to appease the Tech fan base Bobby Dodd had accumulated. Carson was not the charismatic leader like Dodd but rather a strategy man that enjoyed intense game planning. Carson's most notable achievements included recruiting Tech's first ever African American scholarship athlete and being the first Tech head coach to be fired. Carson recruited Eddie McAshan to play quarterback in 1970.[39] After several Summer practices, McAshan won the starting quarterback job and became the first African American quarterback to start for a major Southeastern university.[39] This decision initially polarized Georgia Tech's fan base, but after winning his first 4 starts and leading Tech to a 9–3 season after three straight 4–6 seasons, McAshan won the hearts of the Tech faithful.  McAshan's besting of UGA in the annual rivalry game made McAshan a fixture on campus. The following season, however, led to Carson's demise. In 1971, Tech went 6–6 and a fan base used to Bobby Dodd's 8 wins per season average forced Carson out by James E. Boyd's hand. Carson went on to form the Steel Curtain Pittsburgh Steelers defense.

Bill Fulcher supplanted Bud Carson. Fulcher appeared to be the right choice but quit after two seasons, overwhelmed by racial incidents. Fulcher's tenure included a terrible feud with Eddie McAshan, which peaked before the 1972 UGA game. McAshan had requested additional tickets for the game so that his family could attend. Fulcher refused the ticket request and McAshan sat out of practice in protest.[39] Fulcher responded by suspending the quarterback for the UGA game and the upcoming Liberty Bowl. The story exploded on the national scene when Jesse Jackson attended the UGA game, allowing McAshan to sit with him outside of the stadium in protest.[39]

Alumnus Pepper Rodgers was hired soon after Fulcher quit, hired away from UCLA. Like Carson and Fulcher,  he simply could not return Tech to its national prominence of Dodd's era; in six seasons, his overall record was 34–31–2 (.522).[40] Rodgers' flamboyant demeanor shortened his welcome at the school, and athletic director Doug Weaver replaced him with Bill Curry after the 1979 season. Homer Rice became athletic director and attempted to reinvigorate Tech's program by joining the Atlantic Coast Conference in 1980.

Alumnus Bill Curry had no experience as a head coach, but was a refreshing change after the flamboyant Rodgers. Curry's early years saw Tech reach its lowest point in modern history. His first two Tech teams in 1980 and 1981 went 2–19–1 (.114), with the only bright spots being a brilliant 24–21 road victory over Bear Bryant's Alabama team at Legion Field to open the 1981 season and a 3–3 slug fest in 1980 with top-ranked Notre Dame at Grant Field. Things had gotten so bad, they could only get better.[41]

Curry slowly rebuilt the team, restored a winning mentality to the Georgia Tech fan base; Tech won nine games in 1985, including a 17–14 victory over Michigan State in the All American Bowl. Tech's 1984–1985 teams featured the "Black Watch" defense; created by defensive coordinator Don Lindsey, it featured linebackers Ted Roof and Jim Anderson, safety Mark Hogan, and lineman Pat Swilling.[42][43] The elite defensive players were awarded black stripes down the center of their helmets and black GT emblems on the side of their helmets.[43] Curry's leadership and ability to build a winning program sparked interest from the Crimson Tide and Alabama hired Curry away from Tech in 1986.[44]

After Curry's departure, Tech hired the talented Maryland Terrapins Coach Bobby Ross,[45] who departed a Maryland athletic program in turmoil after the Len Bias tragedy.[46] Bobby Ross came from Maryland after winning three ACC titles over four years. Ross' first season at Tech experienced a severe talent vacuum after Curry's departure, and the players Ross inherited resisted the changes he demanded. The team only won two games, and Ross contemplated ending his coaching career after a humbling loss to Wake Forest in 1987. Ross decided to remain at Tech and continued to rebuild Tech's program. The turning point came in 1989 with the recruitment of Shawn Jones and several other key freshman. After two seasons and only five total wins, Jones helped the Jackets rebound at the end of the 1989 season.[47] In Jones' sophomore season, Tech powered through their schedule and won the ACC. The four-game unbeaten streak in 1989 extended all the way through 1990 and into the 1991 Citrus Bowl. The key victory in the streak was a huge 41–38 come from behind upset victory over then No.1 ranked Virginia in Charlottesville before a nationwide TV audience. Tech demolished Nebraska 45–21 in the 1991 Citrus Bowl, finishing the season 11–0–1, and earning a share of the 1990 National Title with the Colorado Buffaloes.[48][49] Tech's winning streak ended against Penn State in the 1991 Kick Off Classic. Ross and Jones never replicated that 1990 season but managed to win 8 games in 1991 making Shawn Jones one of the most heralded quarterbacks in Tech history. Ross was offered a head coach position after the 1991 season for the San Diego Chargers, which he took.[50]

After first considering Ross assistant coaches, Ralph Friedgen and George O'Leary, Tech hired Bill Lewis away from East Carolina soon after Ross' departure.[51] When Lewis was hired, the Tech faithful hoped he would continue to build on Ross' success.  He had just led East Carolina to an 11–1 record and a final ranking of ninth in the nation. However, Lewis' first season at Tech in  1992 saw the Jackets collapse to only a 5–6 record just two years removed from a national championship. Preseason All-American Shawn Jones suffered from nagging injuries, leaving Tech's offense inept. After Jones' fourth year ran out, redshirt freshman Donnie Davis stepped in to fill his shoes in 1993, which saw another 5–6 season. In just two years, Lewis had completely squandered the successful momentum established by Bobby Ross. During the Summer of '94, George O'Leary was rehired as defensive coordinator. With Davis injured in spring practice, Lewis recruited Tom Luginbill as his replacement. Luginbill was a proficient passer at Palomar College, a junior college in California, and his first two games in 1994 showed promise.  Tech almost upset Arizona who was projected as the No. 1 team in the nation by Sports Illustrated and won 45–26 over Western Carolina.  However, Tech lost its next six games before Lewis was fired with three games remaining in the season.[52] O'Leary was named interim coach for the rest of the season.[53]

Georgia Tech lost their final three games, including a 48–10 drubbing at the hands of Georgia. Despite this, Tech dropped the "interim" tag from O'Leary's title and named him head coach in 1995. O'Leary's first season saw Senior Donnie Davis return as starter and Tech won 6 games. O'Leary's second season saw the emergence of Joe Hamilton as starter when Brandon Shaw struggled in his first two starts. Hamilton would eventually lead the Jackets back to bowl contention and Tech attended its first bowl in six years, the 1997 Carquest Bowl. Hamilton's prowess as a runner and passer thrilled the Georgia Tech fans. Offensive coordinator Ralph Friedgen utilized a complex offense with Hamilton that featured option football mixed in with complex timing routes. Hamilton racked up yardage, touchdowns, and wins for Tech. In 1998, Hamilton and Tech's high powered offense won 10 games and a season ending victory over Notre Dame in the Gator Bowl. Hamilton's senior year put him on the national stage. He was a leading candidate for the Heisman Trophy against rushing phenomenon Ron Dayne. Hamilton passed for over 3,000 yards and rushed for over 700 yards.[54] But while Hamilton dazzled, the Georgia Tech defense was a liability (they allowed around 28 points per game), and may have ultimately cost Hamilton the 1999 Heisman Trophy. In a late-season, nationally televised game against Wake Forest, Tech gave up 26 points and Hamilton threw two interceptions and no touchdowns. As an indirect result, Dayne went on to win the Heisman (Joe was runner-up). Hamilton's Georgia Tech career ended on a sour note in the 2000 Gator Bowl against the Miami, where the Jackets lost 28–13.[55] The following season, redshirt junior George Godsey, a more traditional pocket passer, succeeded Hamilton at the helm of Tech's powerful offense. The drop-off was minimal—Godsey continued where Hamilton left off, winning 9 games in 2000 and 8 games in 2001. In 2000, Godsey also led Tech to their third straight victory over the archrival Georgia Bulldogs.[56]

The end of the 2001 season saw George O'Leary entertain a coaching offer from Notre Dame after Bob Davie announced resignation as Irish head coach.[57] O'Leary was eventually awarded the position, but it was revoked shortly thereafter when Notre Dame discovered that O'Leary had fabricated several aspects of his resume.[58] He claimed to have played three years for the University of New Hampshire and to have attained a master's degree from New York University; in actuality, he had attended NYU but did not graduate, and he never played a down of New Hampshire football.[59][60] Following O'Leary's departure, Mac McWhorter was named interim head coach for Georgia Tech's bowl game, a victory over Stanford in the 2001 Seattle Bowl.

The following spring, Chan Gailey was hired to replace O'Leary as Georgia Tech's head coach.[61] Chan Gailey came to Georgia Tech in 2002 after head coaching stints with the Dallas Cowboys, Samford Bulldogs, and Troy Trojans. Gailey's first team in 2002 managed to win seven games under the quarterbacking of A.J. Suggs. The most notable game of the 2002 season was an upset of National Title Contender North Carolina State. Georgia Tech rallied in the fourth quarter to upset NC State and end Philip Rivers's Heisman Trophy hopes. In 2003, eleven Georgia Tech players were found academically ineligible.[62] Despite the academic losses and the playing of true freshman Reggie Ball, Gailey would lead Tech to a seven-win season and humiliation of Tulsa in the Humanitarian Bowl. P.J. Daniels racked up over 300 yards rushing in the effort.

2004 and 2005 saw Georgia Tech improve talent and skill wise but Tech won seven games again. Star Calvin Johnson arrived as a true freshman in 2004. His performance against Clemson in 2004 helped cement Johnson's place in the annals of all-time Tech greats. Two off-the-field problems affected the Yellow Jackets' 2005 season. First, Reuben Houston, a starting cornerback, was arrested for possession of over 100 pounds of marijuana. Houston was dismissed from the football team immediately following this arrest but a later court order forced Coach Gailey to allow Houston to return to the team. Houston would see little playing time following the court order.[63][64] At the end of the 2005 season, an NCAA investigation found that 11 ineligible players had played for the Yellow Jackets between the 1998 and 2005 seasons.[65] These players played while not making progress towards graduation on the NCAA-approved schedule. The football victories for that season were initially revoked, and Georgia Tech was put on two years of NCAA probation. Twelve football scholarships were stricken from Georgia Tech's allotment for the 2006 and 2007 freshman classes.[66] The Georgia Tech Athletic Department appealed this decision by the NCAA, and the records were restored but scholarship reductions and probation remained.[67] Athletic Director Dave Braine retired in January 2006, and Dan Radakovich was hired as athletic director. Gailey's most successful year at Georgia Tech was in 2006 with nine victories and the ACC Coastal Division championship. The Yellow Jackets football team reached its first New Year's Bowl since the 1999 Gator Bowl and played the West Virginia Mountaineers in the Gator Bowl. Tashard Choice led the ACC in rushing yards and Calvin Johnson led the ACC in receptions and receiving yardage. After an impressive 33–3 victory at Notre Dame to open the 2007 season, the team slid to finish 7–6. On the morning of Monday, November 26, 2007, Gailey was fired from the Yellow Jackets, two days after another heartbreaking loss to the University of Georgia.[68] The Yellow Jackets' Athletic Department hired Paul Johnson, then the head coach at Navy and former Georgia Southern head coach, as Gailey's replacement on December 7, 2007.[69]

On Friday, December 7, 2007, less than two weeks after Georgia Tech announced the firing of Chan Gailey, Paul Johnson was announced as the new Georgia Tech head football coach.[69] Johnson was hired under a seven-year contract worth more than $11 million. Johnson immediately began installing his unique flexbone option offense at Georgia Tech.[70] By the regular season's end, Johnson had led the Yellow Jackets to a 9–3 record including an ACC Coastal Division Co-Championship and a 45–42 win in Athens, Georgia over arch-rival UGA, Tech's first win against the Bulldogs since 2000.[71] In recognition of his accomplishments in his first season, Johnson was named 2008 ACC Coach of the Year by the Atlantic Coast Sports Media Association as well as the CBSSports.com coach of the year.[72][73]


Several weeks after Johnson's defeat of rival Georgia, Georgia Tech rewarded Johnson with a new contract worth $17.7 million, a 53% raise that made him the second highest paid coach in the ACC before he had even completed his first year in the conference.[74] In 2009, Johnson led the Yellow Jackets to historic wins over Florida State in Tallahassee, No. 4 Virginia Tech (breaking an 0–17 losing streak to top five opponents at Grant Field in the past 47 years), and Virginia in Charlottesville. The jackets went on to defeat the Clemson Tigers to make them ACC champions, a title that would be vacated on July 14, 2011, due to NCAA infractions.[75] The Yellow Jackets went on to lose to Iowa in the Orange Bowl, 24–14. Georgia Tech had another significant win over the No. 5 Clemson Tigers on October 29, 2011, giving the Tigers their first defeat of the season and enabling QB Tevin Washington to rush for 176 yards on 27 carries and a touchdown, breaking a school record.[76] In 2012, Georgia Tech was declared the winner of the ACC Coastal Division on November 19, 2012, clinching it with a victory over Duke 42–24 and finishing with a 5-3 ACC record. Georgia Tech played against Florida State in the 2012 ACC Championship Game, which was coach Paul Johnson's second appearance in the title game. The Yellow Jackets lost to the Seminoles 21–15.[77][78][79]
The 2014 Yellow Jackets, despite being predicted to finish 5th in Coastal Division by ESPN, garnered a 10–2 regular season record (6-2 ACC),[80] including wins over then No. 19 Clemson and No. 9 Georgia to finish the regular season ranked No. 11 by the recently created College Football Playoff Committee. The highlight of the season was an overtime thriller that lead to the defeat of the Bulldogs in Athens, featuring Harrison Butker's 53-yard field goal that sent the game into overtime, a 1-yard rushing touchdown by RB Zack Laskey, and a game clinching interception of UGA quarterback Hutson Mason's throw by cornerback D.J. White.[81] Georgia Tech met No. 4 Florida State in the 2014 ACC Championship Game in Charlotte, North Carolina, losing 37–35.[82] Following their conference championship, Florida State was chosen in the top four (ranked No. 3), under which circumstance the Orange Bowl selected Georgia Tech (now No. 12) as its replacement to face the No. 7 Mississippi State Bulldogs on December 31, 2014.[83] Justin Thomas led the Jackets to a dominating 49–34 win for the Yellow Jackets, finishing the season 11–3, No. 8 in AP poll and No. 7 in the American Coaches Poll.

The 2015 season showed the Yellow Jackets a 3–9 record, after numerous injuries throughout the entire year. Their only notable win was a 22–16 upset over No. 9 Florida State on Tech's Homecoming Night, when the Yellow Jackets blocked an attempted field goal by Florida State Kicker Roberto Aguayo, which was picked up by Lance Austin and returned for the game-winning touchdown. This was later coined the "Miracle on Techwood Drive".[84][85] 2015 year marked the first year since 1996 that Georgia Tech did not make a bowl appearance. The next year, 2016, marked a bounce-back season, with the Yellow Jackets, led by team captain Justin Thomas, posting a 9–4 record, including a win over Kentucky in the TaxSlayer Bowl. 2016 also saw a 28–27 victory over Georgia in Athens featuring a 14-point comeback in the 4th quarter topped off by a 6-yard TD rush on third down by Qua Searcy, with 30 seconds left in the game. The Yellow Jackets took a step back in 2017, finishing 5–6 (4–4 ACC) with close losses to Tennessee (42–41 in 2OT) at the Chick Fil A Kickoff Game in the newly constructed Mercedes-Benz Stadium, and at Miami (25–24). Despite starting the 2018 season 1–3, the Yellow Jackets rallied to finish the regular season 7–5. The most notable victory was that against rival Virginia Tech, making Georgia Tech the only conference opponent to win three consecutive games in Lane Stadium against Virginia Tech.[86] The season ended with the 2018 Quick Lane Bowl, where the Jackets fell 34–10 to the Minnesota Golden Gophers.[87] Johnson announced his retirement on November 28, 2018, effective following the team's bowl game.[88] Geoff Collins was named Johnson's replacement on December 7, 2018.[89]

Geoff Collins was announced on December 7, 2018, as the new head coach, to replace the retiring Paul Johnson, starting the 2019 season.[90] Collins was hired under a seven-year contract worth more than $23 million.[91] Geoff Collins, a native of Conyers, Georgia, was previously the head coach at Temple, defensive coordinator at Mississippi State and Florida, and previously worked with Georgia Tech as a graduate assistant and recruiting coordinator.[92] In his first season the Jackets experienced several significant losses. A loss against The Citadel was the Jackets' first loss against an FCS opponent since 1983, and a 45–0 loss to Virginia Tech was the Jackets' first shutout loss at Bobby Dodd Stadium since 1957.[93][94] Geoff Collins was fired from Georgia Tech along with athletic director Todd Stansbury on September 25, 2022, after three 3-win seasons and a 1–3 start in 2022. His final record at Georgia Tech was 10–28, one of the worst coaching records in Georgia Tech history.[95]

Brent Key, a Georgia Tech alumnus and football letterwinner who at the time was the OL coach, was named the interim for the rest of the 2022 season.

Key led the Yellow Jackets to a 4–4 record over the final eight games of the 2022 season. The four wins included two road victories over nationally ranked opponents – a 26–21 win at No. 24 Pitt in his first game at the helm on October 1 and a 21–17 triumph at No. 13 North Carolina on November 19. The Jackets overcame a plethora of injuries (which included its top two quarterbacks.) to finish 5–7 overall and 4–4 in Atlantic Coast Conference play after a 1–3 start. The overall and conference win totals were Tech's highest since 2018, as was its fourth-place finish in the ACC Coastal Division standings. Key's Jackets also defeated the three teams that finished ahead of them in the coastal division. (North Carolina, Pitt and Duke).

On Tuesday November 29, 2022 the interim tag was stripped and Key was named Georgia Tech's 21st head football coach. One of Key's most memorable wins came on October 7, 2023, when following an embarrassing loss to Bowling Green the week prior, the Yellow Jackets defeated the then 17th ranked Miami Hurricanes'. Miami could have won the game by taking a knee, but they ran the ball before fumbling with 26 seconds left. Tech drove 74 yards in four plays to win the game on a last second 44-yard touchdown pass from Haynes King to Christian Leary. It was Key's third win over a ranked ACC opponent on the road.

Tech finished the 2023 regular season at 6–6 and made its first bowl since 2018. The Jackets went 5–3 in ACC play and finished 4th in the conference. They then defeated UCF 30–17 in the Gasparilla Bowl to finish 7–6. It was their first winning season since 2018 and their first bowl win since 2016.

The Yellow Jackets began the 2024 campaign against the 9th-ranked Florida State Seminoles in Week 0. Tech and FSU played in the Aer Lingus Classic at Aviva Stadium in Dublin, Ireland. Tech upset the Seminoles 24–21, with the game-winning field goal being kicked by sophomore kicker Aiden Birr with no time left on the clock. Tech then beat crosstown foe Georgia State 35–12 the next week in the first ever meeting between the two Atlanta programs. 

Tech started 5–2 through the first seven games, then starting quarterback Haynes King got injured in the win over North Carolina which led to a two game losing skid. Haynes King returned for the homecoming game against #4 Miami, however Tech also played freshman Aaron Philo for long passing situations as King's right shoulder was not fully healed. The Yellow Jackets pulled off their biggest upset in 15 years when they knocked the Hurricanes from the ranks of the unbeaten, winning 28–23 in front of a raucous crowd.

Tech's encore performance yielded a narrow 30-29 win against the NC State Wolfpack on November 21, 2024. On November 29, 2024, the Yellow Jackets battled #6 Georgia, losing 44-42 after 8 overtimes, the second most overtimes played in FBS history as of December 2024.[96] In his first full game since recovering from a shoulder injury, quarterback Haynes King produced 413 yards of total offense - comprising 303 passing and 110 rushing yards - and 5 touchdowns. Eric Singleton, Jr. had 8 catches for 86 yards, with Jamal Haynes carrying the ball 13 times for 91 yards.[97]

[98]
[99]

Georgia Tech has been named national champions seven times by NCAA-designated major selectors, including the Coaches' Poll national championship in 1990.[100]: 111–114 [101] Georgia Tech claims the 1917, 1928, 1952, and 1990 championships.[102]: 149–150 

Georgia Tech has won 16 conference championships, nine outright and seven shared. Their 2009 ACC championship was later vacated by the NCAA.[103][104]

† Co-champions 
‡ Vacated by the NCAA

Georgia Tech has won five division championships, with four of those leading to an appearance in the ACC Championship Game.

† Co-champions

List of Georgia Tech head coaches.[105]

† Interim

Georgia Tech has appeared in 47 bowl games and compiled a record of 26–21.[106] Georgia Tech's first four bowl game appearances, the Rose Bowl (1929), Orange Bowl (1940), Cotton Bowl Classic (1943), and Sugar Bowl (1944), marked the first time a team had competed in all four of the Major Bowl Games.[107]

† Interim

‡ New Year's Six Bowl Game

The Yellow Jackets play their home games at Bobby Dodd Stadium at Hyundai Field in Atlanta, Georgia. Upon his hiring in 1904, John Heisman insisted that the Institute acquire its own football field. Grant Field was constructed to appease Heisman as well as bring a true home field advantage to Tech football.[108]

From 1893 to 1912, the team used area parks such as Brisbane Park, Ponce de Leon Park, and Piedmont Park as the home field.[109] Georgia Tech took out a seven-year lease on what is now the southern end of Grant Field, although the land was not adequate for sports, due to its unleveled, rocky nature. In 1905, Heisman had 300 convict laborers clear rocks, remove tree stumps, and level out the field for play; Tech students then built a grandstand on the property. The land was purchased by 1913, and John W. Grant donated $15,000 towards the construction of the field's first permanent stands; the field was named Grant Field in honor of the donor's deceased son, Hugh Inman Grant.[109][110]

The stadium now sits amongst a unique urban skyline and is among the oldest Division I FBS football stadiums. In fact, the only Division I stadiums older are Franklin Field at the University of Pennsylvania and Harvard Stadium.[111] Grant Field was natural grass until 1971. The astroturf was replaced by grass in 1995.[108] The stadium officially holds 55,000 but has held up to 56,412 in 2005[112] and 56,680 in 2006.[113]

On February 26, 2019, Georgia Tech officials unveiled plans to hold one home game per season from 2020 to 2024 (five games total) at Mercedes-Benz Stadium, located less than one mile from the Georgia Tech main campus, with the series dubbed "Mayhem at MBS". However, due to the COVID-19 pandemic, on July 30, 2020, the 2020 game against Notre Dame was moved back to Bobby Dodd Stadium and the agreement with MBS was extended to six years, running through 2026, comprising six games.[114][115][116][117]

The interlocking GT logo was created in 1967 at the request of Bobby Dodd. One of the varsity players was asked to design a logo for the helmets. Several variations of the design were submitted, including a yellow jacket design. The yellow jacket was not submitted because to make the insect look mean it would have to be stinging and therefore flying backwards. The interlocking GT was selected during the summer of 1967 and formalized into decals for the helmets. Over the years it became the official logo for Georgia Tech Athletics.[118]

When head coach Paul Johnson was hired in 2008, the Yellow Jackets adopted a new uniform style. One year later, the uniforms were altered to change the yellow to gold. A year after that, the uniforms were altered again. This time, the team adopted separate white uniforms for both home and away games, while retaining the previous styles' navy and gold jerseys for occasions when the Yellow Jackets could not wear white at home.[citation needed]

In 2018, after nearly 40 years of being with Russell Athletic, Georgia Tech switched to Adidas. With the change came more consistent branding across all sports and a custom shade of gold for the team as well as new uniforms that entwine progressive and traditional elements. The uniforms were updated in 2022 with a more classic look.

The Crimson Tide and the Yellow Jackets have played 52 times in a rivalry that dates back to 1902.[119] With the exception of a four-year break during World War II (1943–1946), they squared off annually from 1922 to 1963 as members of the Southern Conference (1922–1932) and Southeastern Conference (1933–1963). The rivalry continued for one season after Georgia Tech withdrew from the SEC in 1964, then was renewed again with games in six-straight seasons from 1979 to 1984. With 52 previous meetings, Alabama is Georgia Tech's fifth-most-common all-time opponent (behind only Georgia – 114 meetings, Auburn – 92, Duke – 87 and Clemson – 85).[120] Alabama and Georgia Tech announced in January 2020 that they are set to renew the rivalry after 36 years in 2030 and 2031.[121] Alabama leads the series 28–21–3, and the Yellow Jackets won the last matchup 16–6 in 1984.

The Yellow Jackets have played the Auburn Tigers more than 90 times in football, and the series of football games between the two is the second-oldest in the Southeast. Auburn Univ. or A.P.I. is by far Georgia Tech's second-most-often played opponent in football.[122] The rivalry is also intense in basketball, baseball, etc.

The first game took place on November 25, 1892, in Atlanta, Georgia. They played in the SIAA until it was defunct in 1922, before joining the Southeastern Conference. This rivalry lost some luster when the Georgia Tech Athletics discarded its membership in the Southeastern Conference in 1963 to become an independent institute. However, the Yellow Jackets continued their annual series of football games with the Auburn Tigers through 1987. Georgia Tech and Auburn play football games in occasional years, and games in other sports regularly. Even though the Yellow Jackets have joined the Atlantic Coast Conference for all sports in recent decades, from a historical perspective, the Auburn Tigers are Georgia Tech's second-highest sports rivalry, behind only the Georgia Bulldogs.[122] Auburn leads in the all-time series 47–41–4 with the last game played in 2005.[123]

The Georgia Tech Yellow Jackets and the Clemson Tigers have the fourth-most-played series in Georgia Tech football history. They have been rivals since 1898 and Clemson is Tech's closest opponent, geographically, in the Atlantic Coast Conference. Also, in the ACC's new two-division arrangement, each team has one football opponent in the opposite division which has been selected as the two teams' official cross-division rival, they played every year until 2024. The Yellow Jackets and the Clemson Tigers are one of these six pairs.[124] In addition to their geographical closeness and the Heisman connection, the Georgia Tech – Clemson pairing is also a logical one because of both schools' long history in engineering, technology, and science education. Recently, the game has become known for last-minute, extremely close finishes.  From 1996 to 2001, each of the six games was decided by exactly three points.

In 1977 (before the Yellow Jackets had even joined the ACC), this football series was being considered for termination by the administration of Georgia Tech. Clemson football fans, in an effort to show their economic impact on the Atlanta, Ga., area, brought with them to Atlanta large stockpiles of two-dollar bills that were stamped with Clemson Tiger Paws.[125] Georgia Tech leads Clemson in the all-time series 50–36–2.[126]

Georgia Tech's fight songs and cheers are tailored to belittle the Georgia Bulldogs, and the perennial catch-phrase for Georgia Tech fans for many decades has been "To Hell With georgia [sic]". Georgia Tech and the Univ. of Georgia have played each other in football over 100 times (and hundreds more times in basketball, baseball, track and field, tennis, etc.) and this rivalry has become known as Clean, Old-Fashioned Hate. They have been heated rivals since 1893. The annual football game is by far the most important game on the schedule for most Georgia Tech sports fans. The winner of this game takes home the Georgia State Governor's Cup. Georgia Tech trails Georgia in the all-time series 72–41–5 through the 2024 season.[127]

Georgia Tech and Tennessee hadn't met since 1987 until losing a heart breaking Labor Day game in Atlanta in 2017 that renewed the rivalry between the two.[128] When Georgia Tech was part of the Southeastern Conference they played annually. After Georgia Tech left the SEC in 1964, the teams still met until 1987. The series dates back to 1902 and Tennessee leads the series 25–17–2 with the last game played in the 2017 season.[129]

The Yellow Jackets and the Vanderbilt Commodores first met in 1892 in Atlanta, Georgia with Vanderbilt winning 20–10.[130]  Since 1924, the winning team in the series has received a silver-plated cowbell with the year and final score of each game engraved on it. The trophy was created by Ed F. Cavaleri was described by the Atlanta Constitution as “a faithful Georgia Tech supporter though he did not attend the Jacket institution,” according to Georgia Tech's website. Cavaleri purchased a cowbell at an Atlanta hardware store to use as a noise-maker while on his way to a game in 1924. The Commodores defeated Georgia Tech 3–0, however another fan in attendance suggested that Cavaleri award the bell to the winning team. The tradition was born and Cavaleri attended every game between the two teams from 1924 to 1967.[131]  The cowbell has a gold plate screwed into each side, with “GEORGIA TECH-VANDERBILT FOOTBALL TROPHY” inscribed at the top. Three columns list the year of each game, Georgia Tech's points scored and Vanderbilt's points scored. The results of the games from 1924 to 1967 are engraved on one side; the results from 2002, 2003, 2009 and 2016 are on the other.[132] Georgia Tech is 20–15–3 against Vanderbilt in 38 games. The Yellow Jackets won the last matchup 38–7 in 2016.

The rivalry with Virginia Tech has grown considerably since Virginia Tech entered the ACC. In previous years, the teams played infrequently. The intra-conference game has often seen both teams ranked and the outcome has played a key part in determining the winner of the ACC Coastal Division. Since the ACC switched to Division format in 2005, the winner of this game has gone on to win the Coastal Division all but once, with VT winning six times and GT winning four times.[133] Dubbed the Battle of the Techs, the game has seen some very close, very intense match-ups.[134]

Virginia Tech leads the series 12–8.[135]

The Blue Devils and the Yellow Jackets have played 90 times in a series that dates back to 1933 and every year uninterrupted until 2023.[136] There was a long period of Duke dominance in the series from 1936 to 1945. The Blue Devils won all but one matchup including a six-game win streak, the longest in the series for Duke. The win streak also came in the glory days for Duke football, as the 1930s and 1940s featured the best Duke football teams. From 1946 to 1984, the series would be rather back and forth, teams exchanging periods of dominance over the other. Heading into the 1984 season, the series was deadlock at 25–25–1. But since then it has been nearly all Georgia Tech. In the 36 matchups since 1984, the Jackets have walked away with 26 victories, the Blue Devils have won just ten.[137] Duke is Georgia Tech's third-most common opponent all-time (behind only Georgia – 113 meetings and Auburn – 92).[138] Georgia Tech leads the series 55–35-1. This game decided the ACC Coastal Division champion in 2014. Although Duke won the game 31–25, they had a loss to Miami beforehand followed by losses to Virginia Tech and rival North Carolina, which allowed the Yellow Jackets to claim the division title and a trip to Charlotte for the ACC Championship as they just had 2 conference losses whereas Duke had 3. Georgia Tech won the last matchup 24–14 in Atlanta in 2024. The series will continue in 2025 in Durham and in 2026 in Atlanta. It is occasionally missed due to the ACC's new scheduling format, not being played in 2023 and 2027. [139][140]

This series began in 1922. The Fighting Irish were a longtime rival of the Yellow Jackets and the two teams met periodically on an annual basis over the years, particularly from 1963 to 1981 when both schools were independents following Tech's departure from the Southeastern Conference.  The 1975 Georgia Tech-Notre Dame game marked the sole appearance in an Irish uniform of Rudy Ruettiger, the subject of the film Rudy.  When Georgia Tech joined the Atlantic Coast Conference beginning in 1982, they were forced to end the series after 1981 because of scheduling difficulties. Consequently, the two teams have met very infrequently since then. Georgia Tech was the opponent in the inaugural game in the newly expanded Notre Dame Stadium in 1997, then a year later they met again in the Gator Bowl. The Fighting Irish and Yellow Jackets met in the 2006 and 2007 season openers and split both games. The rivalry resumed in 2015 with a 30–22 Irish win in South Bend, and will continue on a semi-regular basis as Georgia Tech and Notre Dame are scheduled to face off five times in the next ten years starting in 2020.[141] Notre Dame is played Georgia Tech at Mercedes-Benz Stadium in 2024, beating Tech 31–13. Georgia Tech traveled to South Bend in 2021, and is set to return for the next contest again Notre Dame in 2027. [142] Notre Dame leads the series 29–6–1.[143]

The Yellow Jackets and the Tulane Green Wave first met on November 4, 1916, in Atlanta, Georgia. Tulane was the opponent at Bobby Dodd Stadium for the Jackets’ first-ever televised football game — a 13–7 win over the Green Wave on WSB-TV on Oct. 2, 1948.[144]  Tulane is the seventh-most frequent opponent for Georgia Tech (50 meetings).[145]  Tulane and Georgia Tech spent most of their athletic histories as members of the same conference: they were among the first to join the SIAA in 1894 then Georgia Tech left in 1921 and Tulane in 1922 to join the Southern Conference. Both schools moved yet again in 1932 to charter the Southeastern Conference, of which they were members until Tech's departure in 1963 to become independent. Tulane followed suit in 1966, but they played each other yearly until 1982.[citation needed] Georgia Tech is now a member of the Atlantic Coast Conference, while Tulane is a member of the American Athletic Conference. The rivalry was renewed on September 6, 2014, in the first football game played on Tulane's campus since Tulane Stadium was torn down in 1980. Georgia Tech leads the series 37–13. The Yellow Jackets won the last matchup 65–10 in 2015.

Georgia Tech has had several players receive votes in the Heisman Trophy balloting. Eddie Prokop finished fifth in the 1943 Heisman voting,[152] Lenny Snow was fourteenth in 1966,[153] Eddie Lee Ivery was eighth in 1978,[154] and Calvin Johnson was tenth in 2006.[155] Billy Lothridge is the only Tech player to receive votes in multiple years. He was eighth in 1962 and runner-up in 1963.[153] Clint Castleberry was the only freshman in the history of the Heisman to finish as high as third until Herschel Walker's third-place finish in 1980.[156] Castleberry and Walker, however, were both surpassed in 2004 by true freshman Adrian Peterson's Heisman runner-up season. Joe Hamilton tied Lothridge's runner-up status in 1999.[157]

Georgia Tech has fielded 50 First Team All-Americans. The first All-Americans at Tech were Walker Carpenter and Everett Strupper in 1917 while the most recent were Durant Brooks in 2007,[158] Michael Johnson in 2008, Derrick Morgan in 2009, and Shaquille Mason in 2014.

Four Georgia Tech players have been awarded the highest collegiate award possible for their position.  Joe Hamilton won the Davey O'Brien Award after his senior season in 1999, Calvin Johnson won the Fred Biletnikoff Award after his junior season in 2006, and Durant Brooks and Pressley Harvin III won the Ray Guy Award in 2007 and 2020 respectively. Hamilton and Johnson were the only Tech players to be named ACC Player of the Year until Jonathan Dwyer received the honor in 2008.[159]

Georgia Tech has had four coaches and 14 players inducted into the College Football Hall of Fame just down the street in Atlanta.[160][161] Coaches Heisman, Alexander, Dodd, and Johnson were inducted in the 1954, 1951, 1993, and 2023 classes respectively.

Georgia Tech has over 150 alumni that have played in the National Football League.[162]  Tech has had ten players selected in the first round of the NFL draft since its inception in 1937.[163]  The first Georgia Tech player ever to be drafted was Middleton Fitzsimmons in 1937. He was drafted 2nd in the 10th round by the Chicago Bears.[164]  The first Tech player selected in the first round was Eddie Prokop in 1945 and the most recent first round Yellow Jackets were Demaryius Thomas and Derrick Morgan in 2010.[163]

Three Yellow Jackets have been inducted into the Pro Football Hall of Fame.[165] Joe Guyon played professional football from 1920 to 1927. Guyon was a collegiate teammate of Jim Thorpe at Carlisle Indian Industrial School before transferring to Georgia Tech. His playing career began with the Canton Bulldogs and finished with the New York Giants. He was inducted into the Hall of Fame in the class of 1966.[166] Billy Shaw played professional football for the Buffalo Bills from 1961 to 1969. He was inducted into the Hall of Fame in the class of 1999.[167] Calvin Johnson played for the Detroit Lions from 2007 to 2015. He was inducted into the Hall of Fame in the class of 2021, his first year of eligibility.[168]

Announced schedules as of April 9, 2025.[169][170] With the ACC announcing a 17-team schedule, Georgia Tech plays the following games with no annual rival. Georgia Tech plays each team in the ACC at minimum once every 4 years.

1. The 2025 game with Notre Dame will be played at Mercedes-Benz Stadium in  Atlanta, GA

"It seems to us that one name is left out in this collection, who may have been the best all-around player the South has had. "We have reference to Doug Wycoff of Tech who, for three straight years, was practically the unanimous all-Southern football choice, despite the fact that Georgia Tech had very lean years during his period of play at this institution. If Wycoff had been flanked by such a pair of halfbacks as Red Barron and Buck Flowers, or Thomason and Mizell while he was with the Jackets, he would have been an all-American. As it was he had to carry all of the offensive load and on the defense he was a wheelhorse. He was a great punter and passer. If Wycoff was not the best all-around player the South had produced then he was very close to the peak."
Seifried, C.S., & Kellison, T. (2019). The Modernization of Grant Field at Bobby Dodd Stadium. The Georgia Historical Quarterly, 103(2), 94-126.

Seifried, C.S., & Kellison, T. (2019). The Modernization of Grant Field at Bobby Dodd Stadium. The Georgia Historical Quarterly, 103(2), 94-126.Grey's Anatomy is an American medical drama television series focusing on the personal and professional lives of surgical interns, residents, and attendings at the fictional Seattle Grace Hospital, later named the Grey Sloan Memorial Hospital. The series premiered on March 27, 2005, on ABC as a mid-season replacement. The show's title is a reference to Gray's Anatomy, a classic human anatomy textbook.[1] Writer Shonda Rhimes developed the pilot and served as showrunner, head writer, and executive producer until stepping down in 2015.[2] Set in Seattle, Washington, the series is filmed primarily in Los Angeles, California, and Vancouver, British Columbia.

The original cast consisted of nine star-billed actors: Ellen Pompeo, Sandra Oh, Katherine Heigl, Justin Chambers, T. R. Knight, Chandra Wilson, James Pickens Jr., Isaiah Washington, and Patrick Dempsey. For most of its run, the series revolves around Dr. Meredith Grey (Pompeo), chronicling her progression from surgical intern to fully-qualified doctor to the hospital's chief of general surgery. The cast has undergone major changes throughout the series' run, with only three original members remaining by the 19th season – Pompeo, Wilson, and Pickens. Pompeo stepped back from the series in its 19th season, at which point the show shifted to more of an ensemble format. ABC announced the show had been renewed for a twenty-first season in April 2024. In April 2025, the show was renewed for a twenty-second season. Grey's Anatomy has two spin-off series: Private Practice (2007–2013) and Station 19 (2018–2024).

Grey's Anatomy is the longest-running scripted primetime show currently airing on ABC, and the longest scripted primetime series carried by ABC. Its success catapulted many series regulars, including Pompeo, Oh and Dempsey, to worldwide recognition; they were among the five highest-earning television actors in 2013.[3][4] Once among the overall top-ten shows in the United States, the show's ratings have fallen, although as of 2017[update] it was still one of the highest-rated shows among the 18–49 demographic.[5] The show also does well on streaming television; as of February 2023[update], Grey's Anatomy was ranked the 10th most popular on-demand program.

Grey's Anatomy has been well received by critics throughout much of its run and has been included in various critics' year-end top 10 lists. Since its inception, the show has been described by the media outlets as a television "phenomenon" or a "juggernaut", owing to its longevity and dominant ratings. It is considered to have had a significant effect on popular culture and has received numerous awards, including the Golden Globe Award for Best Television Series – Drama and a total of 38 Primetime Emmy Award nominations, including 2 for Outstanding Drama Series. The cast members have also received accolades for their individual performances.

The series follows the life of Meredith Grey (Ellen Pompeo), the daughter of world renowned general surgeon Ellis Grey (Kate Burton), starting from her acceptance into the surgical residency program at the fictional Seattle Grace Hospital (later named Seattle Grace Mercy West, and finally, Grey Sloan Memorial Hospital). During her time as an intern, Grey works alongside fellow physicians Cristina Yang (Sandra Oh), Izzie Stevens (Katherine Heigl), Alex Karev (Justin Chambers) and George O'Malley (T. R. Knight), who each struggle to balance their personal lives with hectic schedules and stressful residency requirements. During their internship, they are overseen by Miranda Bailey (Chandra Wilson), a senior resident, who works with attending physicians Derek Shepherd (Patrick Dempsey), head of neurosurgery and Meredith's love interest, and Preston Burke (Isaiah Washington), head of cardiothoracic surgery and Yang's eventual fiancé. Richard Webber (James Pickens Jr.), Chief of Surgery and attending general surgeon, is the former lover of Ellis Grey. During the first six seasons, Burke, O'Malley, and Stevens all depart the series.

The series also shows the drama, emotions, and romance in Meredith's life. In addition to Webber, Burke, and Shepherd, the surgical wing is primarily supervised by Addison Montgomery (Kate Walsh), Shepherd's ex-wife and the head of OB/GYN, neonatal and fetal surgery who leaves for Los Angeles at the end of Season 3; Mark Sloan (Eric Dane), head of plastic surgery and also certified in ENT surgery; Callie Torres (Sara Ramirez), a resident who later becomes head of orthopedic surgery and leaves Seattle for New York at the end of Season 12; Erica Hahn (Brooke Smith), as head of cardiothoracic surgery, who leaves Seattle Grace in Season 5 after a disagreement with Torres; Owen Hunt (Kevin McKidd), as head of trauma surgery who later marries and divorces Yang; Arizona Robbins (Jessica Capshaw), as head of pediatric surgery, and later head of maternal / fetal surgery who marries Torres; Teddy Altman (Kim Raver), as head of cardiothoracic surgery who departs at the end of Season 8 but returns in Season 14; and Amelia Shepherd (Caterina Scorsone), Derek's sister, who is hired to replace him as head of neurosurgery.

Lexie Grey (Chyler Leigh), Meredith's paternal half-sister, joins the residency program in season 4 until her death with her love interest Sloan in a plane crash at the end of Season 8, after which Seattle Grace is renamed Grey Sloan Memorial Hospital in their memory. Former Mercy-West residents April Kepner (Sarah Drew) and Jackson Avery (Jesse Williams) join Seattle Grace following a hospital merger in Season 6. Other additions include Leah Murphy (Tessa Ferrer), who departs at the end of Season 10 but returns during Season 13; Shane Ross (Gaius Charles), who leaves with Yang for Zürich, Switzerland, in the Season 10 finale; Stephanie Edwards (Jerrika Hinton), who resigns at the end of Season 13; Jo Wilson (Camilla Luddington), who marries Karev; Andrew DeLuca (Giacomo Gianniotti), the former love interest of Meredith's maternal half sister Maggie Pierce (Kelly McCreary) who also served as head of cardiothoracic surgery; and Ben Warren (Jason George), an anesthesiologist-turned resident-turned firefighter[6] who marries Bailey. Season 11 sees the death of Derek Shepherd, and in season 12, attending cardiovascular surgeon Nathan Riggs (Martin Henderson) joins the show, and later briefly becomes Meredith's love interest. In the early episodes of Season 14, Thomas Koracick (Greg Germann), an attending neurosurgeon, begins making appearances and Riggs leaves the series to start a life with Owen's long-lost sister Megan (Abigail Spencer); by the season finale, Robbins, Kepner and Warren also depart the show. Midway through Season 16, Cormac Hayes (Richard Flood) becomes the new chief of pediatric surgery and Karev departs to reunite with Stevens. During Season 17, DeLuca is stabbed while chasing a child abductor/ human trafficker and despite the efforts of Hunt and Altman, he dies. Avery and Koracick also depart in Season 17, with Avery moving to Boston to take over his family's charitable foundation and Koracick leaving to assist him. In Season 18, Hayes moves back to his home country, Hunt and Altman flee the country, Marsh returns to Minnesota, Webber and Fox go on sabbatical, and Bailey resigns as chief of surgery, leaving Grey as chief of surgery. In Season 19, new interns Simone Griffith (Alexis Floyd), Benson "Blue" Kwan (Harry Shum Jr.), Jules Millin (Adelaide Kane), Mika Yasuda (Midori Francis) and Lucas Adams (Niko Terho), Derek and Amelia's nephew, join the program; Meredith moves to Boston and Pierce departs to move to Chicago. In Season 20, Marsh resigns from his role as Residency Director and moves to Boston with Meredith; leaving Bailey to be the new Residency Director.

Grey's Anatomy chronicles the lives of surgical interns, residents, and attendings at the fictional Grey Sloan Memorial Hospital (formerly Seattle Grace Hospital, Season 1–6; Seattle Grace Mercy West Hospital, Season 6–9; and then Grey Sloan Memorial Hospital, Season 9–present), as the interns progressively evolve into seasoned doctors under the guidance of their residents, attendings, and chiefs of surgery. Each episode usually opens with a voice-over narration from Meredith Grey or another season regular, often foreshadowing the episode's theme.[7][8] Each season typically mirrors the physicians' academic year, with each completed year elevating the residents to a higher level in the surgical field.[9] The season finale often culminates in a dramatic event, such as a character's death or departure.[10] The series focuses on the doctors' professional lives as surgeons, but also gives significant attention to their personal lives.[7] Character development and relationships frequently take precedence over medical ethics concerns. While the physicians perform intricate surgeries and treat their patients' illnesses, they also exhibit a competitive drive and seek recognition.[11]

Upon arrival at the hospital each morning, residents may compete over who gets assigned a challenging case, often generating tension.[12] A hospital superior typically assigns cases, further intensifying the dynamic between residents and their superiors.[13] Episodes shift between the doctors' interactions with patients and their personal dynamics with colleagues. Once assigned a case, each doctor works to diagnose the patient, typically with the support of an attending physician, which usually results in surgery.[14] The show often portrays the doctors forming personal connections with their patients, with many cases indirectly reflecting the doctors' own personal struggles. One notable instance is Isobel "Izzie" Stevens' engagement to her patient, Denny Duquette, in Season 2.[15] Relationships between the doctors, whether romantic or platonic, frequently develop and create conflicts between their personal and professional lives. Emotional moments are often underscored by an indie rock soundtrack, something that has become a hallmark of the series.[16] Each episode concludes with a voice-over, generally providing contrast or a follow-up to the initial narration.[8]

This section includes characters who have appeared in the series as credited cast.



The five characters who are first introduced in the series premiere as surgical interns are Meredith Grey, Cristina Yang, Izzie Stevens, Alex Karev, and George O'Malley.[17] They are initially mentored by Dr. Miranda Bailey, a senior resident who becomes the hospital's Chief Resident,[18] and later an attending general surgeon, in Season 6.[19] The surgical program is initially headed by Richard Webber, the Chief of Surgery, who has a pre-existing personal relationship with Meredith, having had an affair with her mother, famed general surgeon Ellis Grey, when Meredith was a child. In Webber's employ are attending neurosurgeon Derek Shepherd, dubbed 'McDreamy' by the residents, and attending cardiothoracic surgeon Preston Burke. Shepherd is introduced as Meredith's love-interest, while Burke begins a relationship with Cristina.[20]

Introduced in the show's second season are obstetrician-gynecologist and neonatal surgeon, Addison Montgomery,[21] plastic and ENT surgeon Mark Sloan (nicknamed 'McSteamy' by the interns), from New York,[22] and orthopedic surgeon Callie Torres.[23] Montgomery is Shepherd's estranged wife who arrives in Seattle seeking reconciliation with him,[24] Sloan is Shepherd's former best-friend, who aided the breakdown of his marriage by having an affair with Montgomery,[25] while Torres is introduced as a love-interest, and eventual wife for O'Malley. The penultimate episode of Season 3 introduces Lexie Grey, Meredith's half-sister who unexpectedly decides to pursue her internship at Seattle Grace Hospital after her mother's sudden death, and begins an on-again, off-again relationship with Sloan.[26] Burke and Yang, having been engaged,[27] endeavor to plan their wedding, while Montgomery departs the show in the Season 3 finale, relocating to California, seeking a new life, thus becoming the lead character of the spin-off Private Practice. The Season 3 finale also shows Burke's exit from the show, after leaving Yang at the altar on their wedding day.[28]

Grey, Yang, Stevens, and Karev are all promoted to residents in the Season 4 premiere,[29] while O'Malley is forced to repeat his internship year, following his failing of the intern exam.[28] Subsequently, Torres and O'Malley divorce, due to him having an affair with Stevens, initially concealing it from Torres.[30][31] Early in the fourth season, cardiothoracic surgeon Erica Hahn becomes Torres's love-interest.[32] During the fifth season, Hahn departs from the series after a disagreement with Torres,[33] and O'Malley retakes his intern exam, passes and joins his fellow physicians as a resident.[34] Two new characters are introduced, former United States Army trauma surgeon Dr. Owen Hunt,[35] and pediatric surgeon Dr. Arizona Robbins.[36] Hunt becomes a love-interest for Yang,[37] while Robbins begins a relationship with Torres.[38] When Stevens is diagnosed with Stage 4 metastatic melanoma,[39] she and Karev get married at the conclusion of the fifth season.[40] In addition, Meredith and Shepherd marry, with their vows written on a blue post-it note.[41]

O'Malley dies in the Season 6 premiere, due to injuries sustained from saving a woman from being hit by a bus,[19] and Stevens later departs Seattle following a communication breakdown with her then-husband Karev following the Seattle Grace merger with Mercy West Hospital.[15] New characters are introduced as Seattle Grace Hospital merges with Mercy West Hospital.[42] Residents April Kepner and Jackson Avery transfer to Seattle Grace Hospital from Mercy West Hospital, and Avery has a brief relationship with Lexie, until she reunites with Sloan.[43] Subsequently, Teddy Altman is introduced as the new Chief of Cardiothoracic Surgery.[44] Ben Warren is also introduced in the sixth season, who becomes a romantic interest for Bailey throughout the series, eventually marrying her. In the Season 6 finale, a deceased patient's grieving husband embarks on a shooting spree at the hospital, injuring Karev, Shepherd, and Hunt.[45][46] In the shooting's emotional reverberations, Yang and Hunt abruptly marry, not wanting to risk separation.[47] Torres and Robbins eventually wed, in Season 7, officiated by Bailey.[48] In Season 8, Webber steps down as Chief of Surgery and allocates his job to Hunt.[49] As the final year of residency for Meredith, Yang, Karev, Kepner and Avery is coming to a close, the doctors are all planning to relocate to different hospitals to pursue their specialty careers.[50] However, all plans are put on hold when several doctors from Seattle Grace Mercy West Hospital are engaged in a plane crash, which kills Lexie and endangers Meredith, Shepherd, Yang, Sloan and Robbins in the Season 8 finale.[10] The season finale also sees Altman being courteously fired by Hunt as she struggles to decide whether or not to take the job as Chief at the United States Army Medical Command (MEDCOM).[10]

In the Season 9 premiere, Sloan dies due to sustained injuries from the plane crash following a brief relapse of temporary health ("the surge") and the remaining characters work through their post traumatic stress and Robbins's loss of limb by way of suing Seattle Grace Mercy West, as the hospital was responsible for putting the surgeons on the plane. Meanwhile, Jo Wilson, Stephanie Edwards, Leah Murphy and Shane Ross are introduced as a new group of interns. The ninth season continues with the struggle of the lawsuit and the animosity that it creates within the hospital, Yang and Hunt eventually divorce in order to help the lawsuit.[51][52] The doctors who were on the plane win the lawsuit, but the payout bankrupts the hospital. They all club together and buy Seattle Grace Mercy West, with the help of the Harper Avery Foundation, and they become the Board of Directors, once being called the "Grey-Sloan 7".[53][54][55] One of the changes they implement is renaming the hospital to Grey Sloan Memorial Hospital in memory of Lexie Grey and Mark Sloan. Robbins cheats on Torres with a visiting facial reconstruction surgeon and Webber gets electrocuted in the Season 9 finale, although survives the incident. Grey's Anatomy saw the departure of one of its major players, Cristina Yang, played by Sandra Oh, as well as the departure of interns Murphy and Ross, in the Season 10 finale. Amelia Shepherd, the sister of Derek, joined the main cast in Season 11, transitioning over from being a main character of the spin-off series, Private Practice which had recently ended. The 11th season also saw the introduction of the new Chief of Cardiothoracic Surgery, Maggie Pierce, Richard's secret daughter with Ellis Grey. Towards the end of the 11th season, Derek Shepherd witnesses a car accident and pulls over to help the injured, but his car is hit by a truck with him inside as he attempts to leave the scene. He later dies at another hospital following the doctors' mishandling of his injuries. Season 12 saw Andrew DeLuca join the main cast as a new intern at the hospital, as well as Nathan Riggs as a new cardiothoracic surgeon with a pre-existing off-screen history with Hunt. The Season 12 finale saw the departure of one of the show's longest-running characters, Callie Torres, played by Sara Ramirez. Edwards departed at the end of Season 13, deciding to quit her job as a surgeon after surviving a major fire at the hospital. Riggs departed and Altman returned to Seattle toward the beginning of Season 14. The fourteenth season also saw the introduction of Tom Koracick who worked as a new neurosurgeon alongside Amelia Shepherd, Levi Schmitt who is a new intern at the hospital, as well as Andrew's sister, Carina DeLuca, an obstetrician who enters into a relationship with Robbins. During the fourteenth season, Warren transitions out to become a main character of the series' second spin-off, Station 19, after his character decides to quit working at the hospital to become a firefighter. Carina DeLuca eventually joins him as a main character of that spin-off series. Long-time characters, Robbins and Kepner depart the series in the Season 14 finale to pursue other career opportunities. Season 15 saw the introduction of Atticus "Link" Lincoln, as a new orthopedic surgeon, while Season 16 saw the introduction of Cormac Hayes, a new pediatric surgeon, and Winston Ndugu, a new cardiothoracic surgeon. The sixteenth season was the last to feature Alex Karev, who moved to Kansas to reunite with Stevens, leaving Grey as the last remaining intern from the original cast. Season 17 saw Nick Marsh, a transplant surgeon, join the ensemble of main characters as a new romantic interest for Meredith Grey. The seventeenth season also saw the departure of Andrew DeLuca, after he was stabbed and ultimately died after pursuing a sex trafficker, and was also the last season to feature both Jackson Avery, who moved to Boston in order to run a family foundation and Koracick, who also moved to Boston to assist Avery, as main characters. Hayes departed the series in Season 18 after deciding to return to Ireland with his kids.

Season 18 saw the introduction of five new interns with Simone Griffith, Mika Yasuda, Benson Kwan, Jules Millin and Lucas Adams, the latter of which is revealed to be a nephew of Amelia and Derek Shepherd. Meredith Grey, one of the last remaining original characters of the series, as well as Pierce, both depart Seattle during the course of Season 19 and appear in recurring and guest capacities afterwards, respectively. Yusda and Schmitt depart in season 21, with Schmitt moving to Texas for his ped's fellowship and Yusda quits the program following her sister's death.

With the drama's setting being a hospital, numerous medical personnel appear regularly on the show, as well as several other recurring characters. Joe (Steven W. Bailey), is first shown as the owner of the Emerald City Bar and Grill, across the street from the hospital, which is a common relaxation area for the physicians.[21] Also introduced in the pilot, is the legendary former general surgeon, Dr. Ellis Grey (Kate Burton), Meredith's Alzheimer's-stricken mother, who appeared on the show until her death in season 3.[17] In the first season, Olivia Harper (Sarah Utterback), a nurse who appeared on the show occasionally until getting laid off in the merger with Mercy West,[42] engages in a sexual relationship with O'Malley, giving him syphilis.[21] Serving as an assistant and secretary to the Chief of Surgery, former nurse Patricia (Robin Pearson Rose), has appeared on the show since its debut.[17] Tyler Christian (Moe Irvin), a hospital nurse, makes occasional appearances throughout the series.[17] Within the second season, Bailey becomes pregnant by her husband, Tucker Jones (Cress Williams),[56] who makes frequent appearances on Grey's Anatomy, until their divorce in season 5.[41] While Bailey takes a sabbatical, due to her pregnancy, the cheerful Dr. Sydney Heron (Kali Rocha), fills her position as the resident supervising Grey, Yang, Stevens, Karev and O'Malley,[57] and makes occasional appearances until the fifth season.[31]

Thatcher and Susan Grey (Jeff Perry and Mare Winningham), Meredith's estranged father and step-mother, are introduced in season 2,[58] with Susan making appearances until her death in season 3,[59] and Thatcher continuing to appear in the series until his death in season 15.[60] Adele Webber (Loretta Devine), is introduced as Richard's wife,[61] who eventually acquires Alzheimer's, in the seventh season,[62] and continued to make appearances until her death in season 9.[63] Introduced as Preston's mother, Jane Burke (Diahann Carroll) makes occasional appearances until the fourth season. Denny Duquette (Jeffrey Dean Morgan), a patient with congestive heart failure, originates as one of Burke's patients,[64] who goes on to propose to Stevens, after weeks of bonding between the two.[65] Facing death, Stevens cuts Duquette's left ventricular assist device (LVAD), to elevate his position on the United Network for Organ Sharing (UNOS) transplant list.[66] This ultimately ends in his death season 2 finale, marking his initial departure from the show,[61] and placing Stevens on disciplinary probation.[67] Initially conceived as a veterinarian hired for Shepherd's dog, Doc,[58] Dr. Finn Dandridge (Chris O'Donnell) soon becomes a love-interest for Meredith, while Shepherd is with Montgomery.[68] Dandridge is included in a multi-episode story arc in seasons 2 and 3, consisting of 9 episodes, ending when Meredith reunites with Shepherd.[69]

In season 3, George's father, Harold O'Malley (George Dzundza), is diagnosed with cancer and dies, with his wife Louise (Debra Monk) and George's brothers Jerry (Greg Pitts) and Ronny (Tim Griffin) by his side.[70] Louise goes on to appear occasionally, and was last seen in season 8.[71] A ferryboat accident brings along Rebecca Pope (Elizabeth Reaser), who is initially introduced as a pregnant amnesiac Jane Doe patient.[72] Pope eventually embarks on a relationship with Karev, until she is diagnosed with a personality disorder in season 4, and makes her final departure.[32] Amidst the crisis of the ferryboat crash, Meredith falls into the water at the disaster site.[72] Although rescued, she goes into cardiac arrest, waking up in what appears to be limbo.[73] Within the limbo, Meredith is entertained by deceased acquaintances Duquette and Dylan Young (Kyle Chandler), who was killed during a bomb crisis in the second season,[74] until eventually being resuscitated.[75] Seeking a cure to her depression, Meredith undergoes therapy sessions with the hospital psychiatrist, Dr. Katharine Wyatt (Amy Madigan),[76] who in addition, serves as a psychiatrist to Hunt.[77]

The season 4 premiere introduces several new interns, to be trained under Meredith, Yang, Stevens, Karev, and eventually O'Malley.[b] Among them are Dr. Steve Mostow (Mark Saul) who continues to make appearances,[29] and Dr. Sadie Harris (Melissa George) who formed a friendship with Meredith while the two were in college.[78] Harris is fired in the fifth season, due to not actually having a medical degree, and departs the show immediately after.[38] Meredith and Shepherd's relationship reaches a crisis, and the two separate, leaving Shepherd to entertain a relationship with Rose (Lauren Stamile), a nurse.[31] Rose appears frequently until season 5, when Derek and Meredith decide to rekindle their flame.[35] Throughout the fifth season, Stevens experiences full-out hallucinations of Duquette,[36] signaling that she is ill,[79] and once she is lucid, he departs, marking his final appearance.[77] Following the announcement of her relationship with Robbins (Jessica Capshaw), Callie's father Carlos Torres (Hector Elizondo) initially contests his daughter's concurrence in homosexuality,[80] but eventually accepts it,[43] and he reappears several times throughout the series.[48]

The hospital's merging with Mercy West introduces new residents: Dr. Reed Adamson (Nora Zehetner) and Dr. Charles Percy (Robert Baker),[43] but the two are both murdered in the season 6 finale.[45][46] Also introduced in the sixth season is Dr. Ben Warren (Jason George), an anesthesiologist[81] and eventual husband to Dr. Miranda Bailey,[10] as well as Sloan Riley (Leven Rambin), Dr. Mark Sloan's estranged pregnant daughter who seeks kinship with him.[44] Robbins receives a grant to aid children in Malawi, which leads to a falling-out between her and Torres.[82] While in Malawi, Robbins is replaced by Dr. Robert Stark (Peter MacNicol),[83] a pediatric surgeon with an interest in Dr. April Kepner,[84] who appears occasionally until season 8. Following the breakdown of Dr. Torres's relationship with Dr. Robbins, Dr. Torres becomes pregnant with Dr. Sloan's baby.[85] Torres's relationship with Robbins is subsequently mended,[86] and the couple endeavors to raise their new daughter, Sofia Robbin Sloan Torres, with the help of Dr. Sloan.[87] Dr. Lucy Fields (Rachael Taylor), an obstetrician-gynecologist, is introduced in the seventh season, and serves as a love-interest for Dr. Alex Karev,[88] until eventually taking up Robbins' job in Africa.[89] Shepherd and Meredith also become new parents, with their adoption of Zola, a baby girl from Malawi.[c][89] Conceived as a patient with a tumor condition who later develops diabetes, Henry Burton (Scott Foley) befriends Dr. Altman and eventually joins her in marriage only to get treated using her medical insurance,[90] until he dies while undergoing heart surgery in season 8.[91] Dr Catherine Avery (played by Debbie Allen), a urologist, is introduced in the show's eighth season and subsequently makes recurring appearances in all subsequent seasons thus far. Her character becomes a romantic interest for Dr Richard Webber, and eventually his second wife following the death of his first wife, Adele.

In the season 9 premiere, interns Dr. Jo Wilson (Camilla Luddington), Dr. Shane Ross (Gaius Charles), Dr. Stephanie Edwards (Jerrika Hinton), Dr. Leah Murphy (Tessa Ferrer) and Dr. Heather Brooks (Tina Majorino) are introduced.[92][93][94] Steven Culp and William Daniels play Dr. Parker and Dr. Craig Thomas, respectively.[95][96] Dr. Parker is Chief of Cardiothoracic Surgery and Dr. Craig Thomas (William Daniels) is an attending cardiothoracic surgeon at Mayo Clinic, where Cristina worked temporarily. Dr. Alana Cahill (Constance Zimmer)[97] introduced in season 9 is appointed to cut costs at the hospital and she eventually decides the best course of action would be to seek out a new buyer[98] until the 4 crash survivors and Torres on the behalf of Sloan pool their money together in a bid to purchase the hospital themselves. Kepner starts dating a paramedic named Matthew Taylor (Justin Bruening)[99] and they form a deep relationship over the course of seasons 9 and 10 and she eventually agreed to marry before reconciling with Avery in the middle of her wedding ceremony to Matthew.[100] Lauren Boswell (Hilarie Burton) is introduced as a craniofacial surgeon consulting on a case at Grey Sloan Memorial[101][102] who showed romantic interest in Arizona and eventually ended up having a one-night stand with her.[103]

Dr. Heather Brooks dies in the premiere of season 10. She goes to search for Dr. Webber and finds him lying in the basement of the hospital. Trying to save Dr. Webber, she accidentally steps into a puddle and electrocutes herself while hitting her head as she falls.[104] Also introduced in season 10 was Dr. Karev's estranged father Jimmy Evans (James Remar)[105] who tries to form a relationship with his son but fails repeatedly, and dies in a botched surgery.[106][107] The conclusion of season 10 has Cristina leaving Grey Sloan for Dr. Burke's job as head of Klausman Institute for Medical Research in Zurich, while Dr. Ross makes a last-minute decision to follow her in order to study under her. Dr. Maggie Pierce (Kelly McCreary)[108] unknowingly drops a bombshell that she is the child of Dr. Webber and the late Dr. Grey, and was given up at birth for adoption. Dr. Meredith Grey has to accommodate another half-sister in her life.[109] Also, Dr. Yang privately gives her shares in the hospital to Dr. Karev, also giving him her seat on the board. But Dr. Webber all but promises the seat to Dr. Bailey, so the board has to decide between them. Season 11 begins with new surgical residents coming to the hospital. Introduced in season 11 is Dr. Nicole Herman (Geena Davis),[110] who is Chief of Fetal Surgery at Grey Sloan Memorial. Dr. Herman selects Robbins for a fetal surgery fellowship and becomes her mentor. Herman features in a 12-episode arc before departing in episode 14.[111][112] Season 14 sees the introduction of Taryn Helm (played by Jaicy Elliot), a new intern who eventually becomes a resident, with recurring appearances in the series in all subsequent seasons thus far.

Shonda Rhimes wanted to make a show that she would enjoy watching,[113] and thought it would be interesting to create a show about "smart women competing against one another".[114]


When asked how she decided to develop a medical drama, Rhimes responded: 
I was obsessed with the surgery channels... My sisters and I would call each other up and talk about operations we'd seen on the Discovery Channel. There's something fascinating about the medical world—you see things you'd never imagine, like the fact that doctors talk about their boyfriends or their day while they're cutting somebody open. So when ABC asked me to write another pilot, the [operating room] seemed like the natural setting.[115]

The series was pitched to ABC Entertainment, which gave it the green light. It was picked up as a mid-season replacement for Boston Legal in the 2005 television season.[116] Francie Calfo, executive vice president of development at ABC Entertainment, noted that ABC was looking for a medical show that stood apart from the others airing at the time. She pointed out: 
Medical shows are hard, and it was hard trying to figure out where ours could be different. But where everybody else is speeding up their medical shows, [Rhimes] found a way to slow it down, so you get to know the characters. There's definitely a strong female appeal to it.[114]
ER is high-speed medicine. The camera flies around, adrenaline is rushing. My show is more personal. The idea for the series began when a doctor told me it was incredibly hard to shave her legs in the hospital shower. At first, that seemed like a silly detail. But then I thought about the fact that it was the only time and place this woman might have to shave her legs. That's how hard the work is.

While creating the characters and writing the initial script, the series' writers did not have specific character descriptions in mind, instead hoping to cast the best actor available for each role. Shonda Rhimes has expressed that if the network had not permitted her to create characters this way, she would have been hesitant to continue with the series.[115] Female roles, in particular, were crafted to be multi-faceted and complex characters.


Rhimes offered her insight:
I wanted to create a world in which you felt as if you were watching very real women. Most of the women I saw on TV didn't seem like people I actually knew. They felt like ideas of what women are. They never got to be nasty or competitive or hungry or angry. They were often just the loving wife or the nice friend. But who gets to be the bitch? Who gets to be the three-dimensional woman?[115]
The show's title, Grey's Anatomy, was devised as a play on words, referencing both Henry Gray's classic English medical textbook, Gray's Anatomy (first published in 1858 and still in print), and the title character Dr. Meredith Grey (Ellen Pompeo).[117][118][119][120] Almost all episodes are titled after songs, with notable examples including "Into You Like a Train" "Drowning on Dry Land" and "How to Save a Life". However, the episode titled "1-800-799-7233" breaks from this pattern, referencing the National Domestic Violence Hotline. This title change was suggested by actor Giacomo Gianniotti (who plays Andrew DeLuca), according to showrunner Krista Vernoff.[121]

Before the series debuted on March 27, 2005, a few early screenings were shown to close friends and family of the producers and actors. The show was initially scheduled to run in the Boston Legal time slot for four weeks.[114] Due to its high ratings and viewership, ABC decided to keep it in that time slot for the remainder of the season.[114] ABC Entertainment President, Steve McPherson, explained the scheduling change: "Ultimately, we decided that, without having adequate lead time or marketing dollars to devote to moving either show so late in the season, we'd continue to let [Grey's Anatomy] build on its tremendous momentum through May."[122]

Prior to its broadcast, there were discussions about changing the show's title from Grey's Anatomy to Complications, though this change never took place.[123]

List of showrunners throughout the series' run:

Grey's Anatomy is produced by Shondaland and 20th Television, in association with Lionsgate Television (formerly known as The Mark Gordon Company) and, until 2024, ABC Signature (formerly Touchstone Television until 2007 and ABC Studios between 2007 and 2020).[89] Shonda Rhimes, Betsy Beers, Krista Vernoff, Mark Gordon, Rob Corn, and Mark Wilding have all served as executive producers throughout the series.[124] In later seasons, Steve Mulholland, Kent Hodder, Nancy Bordson, James D. Parriott, and Peter Horton have also been executive producers, with Allan Heinberg joining the team in 2006.[124] By Season 8, the executive producers were Rhimes, Beers, Gordon, Vernoff, Corn, Wilding, and Heinberg.[125]

Rhimes serves as the series' head writer, or its most prolific writer. In the show's early years, she often promoted the series by answering fan questions on her Twitter account.[126] Other key members of the writing staff include Vernoff, Wilding, Peter Nowalk, Stacy McKee, William Harper, Zoanne Clack, Tony Phelan, Joan Rater, and Debora Cahn.[124] From seasons 2 through 7, the writers maintained a blog titled Grey Matter, where the writer of an episode provided insights into the writing process.[127]

Directors vary by episode, with Rob Corn being the most frequent director, followed by Tom Verica. Horton, Edward Ornelas, and Jessica Yu have also directed numerous episodes. Cast members Chandra Wilson, Debbie Allen and Kevin McKidd have each directed multiple episodes as well.[128]

Susan Vaill has edited Grey's Anatomy since its inception, with David Greenspan joining as an editor in 2006.[129][130] Casting directors Linda Lowy and John Brace have been part of the production team since 2005. The show's production design is led by Donald Lee Harris, with Brian Harms assisting as art director. Mimi Melgaard heads costume design, with Thomas Houchins supervising costumes. Ellen Vieira serves as the makeup artist, while Jerilynn Stevens is the hairstylist. The Director of Photography is Herbert Davis, and the music coordinator is Danny Lux.[129]

The show's medical authenticity is ensured by consultants Karen Lisa Pike, M.D. and Linda Klein, R.N.[131] In an interview, Ellen Pompeo (who plays Meredith Grey) shared that the cast frequently consults the medical team for accuracy, asking about procedures and surgeries to ensure authenticity. Many cast members observed real-life surgeries to prepare for their roles.

In addition to their work on the show, the production staff participates in a Grey's Anatomy softball team, which competes against teams from other television shows, such as CSI: Crime Scene Investigation.[132]

She brought this energy that felt very fresh. From the beginning, I've been shaping Cristina around Sandra a little bit. One of my favorite things to do is take as much of her dialogue out of a scene as possible because she does so much nonverbally. Then I just watch what she manages to do without having a word to say.

Grey's Anatomy used a color-blind casting technique, resulting in a racially diverse ensemble. Each role was cast without the character's race being pre-determined, keeping Rhimes's vision of diversity.[134] The production staff began casting with the program's title character, Dr. Meredith Grey,[115] which Rhimes said was a challenging role to cast.[135] "I kept saying we need a girl like that girl from Moonlight Mile (2002)[a]," said Rhimes, "and after a while, they were like, 'We think we can get that girl from Moonlight Mile."[135] The next to be cast, Sandra Oh (Dr. Cristina Yang), was initially invited to audition for the character of Bailey, but pressed to read for the role of Cristina instead.[136] Many actors read for the role of Dr. Derek Shepherd. Rob Lowe was offered the role, but turned it down as he had already agreed to star in another medical drama Dr. Vegas. When Patrick Dempsey read for the part, "he was just perfect," according to Rhimes.[135]

The only character developed with a racial description in mind was Dr. Miranda Bailey, who is portrayed by Chandra Wilson. Her character was first described as a tiny blonde with curly hair, but when Wilson began speaking, Rhimes reported: "[Wilson] is exactly who Miranda is."[115] James Pickens Jr. was selected to appear as Dr. Richard Webber in the series' pilot and first season.[137] Katherine Heigl wanted to portray Dr. Izzie Stevens as a brunette but was requested to retain her natural blonde for the part.[138] Isaiah Washington, who portrayed Dr. Preston Burke, initially read for the role of Shepherd but was cast as Burke, because the actor who had been intended to play Burke, Paul Adelstein, had to withdraw due to commitments to the movie Be Cool.[134][139] T. R. Knight signed on for the pilot as Dr. George O'Malley, expecting that the role might be short-lived, because he liked that the character was multi-faceted.[140] Rounding out the season 1 cast was Justin Chambers as Dr. Alex Karev, a character who was not originally included in the show's pilot, but added through digital editing and additional scenes.[141][142]

Rhimes later shifted focus away from physical descriptions of characters, instead emphasizing the feelings and qualities they conveyed.[143]

Season 2 marked the introduction of attending doctors Mark Sloan (Eric Dane) and Callie Torres (Sara Ramirez). They were initially cast as recurring characters, but both were given star billing at the opening of Season 3.[144][145] Ramirez was cast after ABC executives offered them a role in the network show of their choice;[146] Dane had previously auditioned unsuccessfully for a role in the pilot episode.[145] Dr. Addison Montgomery (Kate Walsh) also joined the show in Season 2, after making a guest appearance in the season 1 finale[147]

In October 2006, Washington allegedly insulted Knight with a homophobic slur, during an on-set altercation with Dempsey, and ABC terminated Washington's contract at the end of Season 3.[148] Washington returned for a guest appearance in season 10. At the conclusion of Season 3, Walsh departed the show to pursue the Grey's Anatomy spin-off, Private Practice, but continues to make guest appearances.[149]

Chyler Leigh joined the cast as a main character in Season 4 as Dr. Lexie Grey, Meredith's half-sister. Leigh had appeared as a guest-star in the final 2 episodes of Season 3.[150] On the selection of Leigh for the role of Lexie, Rhimes said: "Chyler stood out ... It felt like she could be Meredith's sister, but she had a depth that was very interesting."[151]

Dr. Erica Hahn (Brooke Smith), who first appeared on Grey's Anatomy in Season 2, returned as a series regular in Season 4.[152] Shortly after the announcement that Smith would be a regular member of the cast, Entertainment Weekly's Michael Ausiello, reported that her character, Hahn, would depart from Grey's Anatomy on November 6, 2008.[153] E! Online's Kristin Dos Santos asserted that Smith's dismissal from the show had been forced by the ABC network, as part of an attempt to "de-gay" Grey's Anatomy.[154] Rhimes countered these claims, saying that "we did not find that the magic and chemistry with Brooke's character would sustain in the long run".[153]

Season 5 introduced actor Kevin McKidd (Dr. Owen Hunt), who was signed as a series regular after originally being cast for a specific story-arc.[155] In addition, Jessica Capshaw (Dr. Arizona Robbins) was originally introduced for a three-episode arc, but her contract was extended until the end of the season; she became a series regular in the sixth season.[156]

Knight departed the show at the conclusion of Season 5, citing an unhappiness with the development and lack of screen-time for his character.[157] Directly following Knight's departure, Entertainment Weekly reported that Heigl had not returned to the set as scheduled after her maternity leave.[158] It was later confirmed that Heigl would not return to the show at all.[159]

Kim Raver, who was cast as recurring character Dr. Teddy Altman in season 6, was given star billing later in the season.[160] Sarah Drew (Dr. April Kepner) and Jesse Williams (Dr. Jackson Avery), who both made their series debuts as recurring characters in the sixth season, and received star-billing in the seventh.[161][162]

The six original actors' contracts expired after Season 8, but in May 2012, Pompeo, Oh, Chambers, Wilson, Pickens Jr. and Dempsey renewed their contracts with the show for another two years.[163] At the conclusion of season 8, Leigh's character Lexie Grey departed from the show at Leigh's request, and with Rhimes's agreement.[164][165] Raver's character Teddy Altman was also written out of the show during the Season 8 finale. Rhimes said that Raver had been offered a contract renewal, but declined.[166]

In July 2012, Dane (Sloan) confirmed that he was departing the show to pursue other projects; he made his final appearances in the first two episodes of Season 9.[167] With the start of season 10, Camilla Luddington, Gaius Charles, Jerrika Hinton and Tessa Ferrer were introduced to the show as series regulars. They were first introduced to the show in season 9 as the new interns. On August 13, 2013, Oh (Cristina) announced that the show's tenth season would be her final one because she had decided to pursue more diverse career opportunities.[168][169] In March 2014, it was announced that Isaiah Washington, who portrayed Preston Burke in the first three seasons of the show, would make a guest appearance to coincide with the departure of series regular Sandra Oh, his former on-screen love-interest.[170] Neither Charles's nor Ferrer's contracts were renewed for Season 11.[171]

On May 2, 2014, it was announced that, in addition to Pompeo and Dempsey, all the original remaining cast members—apart from Oh—signed 2-year deals, extending their contracts through seasons 11 and 12. Despite joining the series in season 2, Sara Ramirez was on the same negotiation schedule as the season 1 cast and also signed a new two-year deal.[172][173][174] On April 23, 2015, Dempsey departed the show during the show's 11th season, despite the fact that he still had a year left in his contract.[175] Dempsey decided to leave because the show was too time-consuming. He wanted to spend more time with his family, so in season 11 his character died. In the show he was brain dead from a car accident and then his wife decided to pull the plug on him. On the night of the Season 12 finale, May 19, 2016, Sara Ramirez announced that they would be leaving the show following the decision not to renew their contract.[176]

On January 17, 2018, it was announced by ABC that Ellen Pompeo's contract had been renewed through Season 16. Not only did the contract renewal insure that Pompeo will return as Meredith Grey, but it made her a producer of Grey's Anatomy and a co-executive producer of Station 19. The deal made Pompeo the highest-paid actress currently on a dramatic TV series, with her making $575,000 per episode and over $20 million yearly.[177] On March 8, 2018, it was announced that series regulars Jessica Capshaw and Sarah Drew would not be returning for Season 15 after executive producers decided to let them go.[178] In May 2018, it was confirmed that Kim Raver, who made returning guest appearances in Season 14, would once again become a series regular, beginning with season 15.[179] In January 2020, Justin Chambers announced that he had departed the series to pursue more diverse career opportunities and that his final episode had aired on November 14, 2019.[180] In March 2021, Giacomo Gianniotti departed the series after wanting to pursue new things such as directing.[181] In May 2021, Jesse Williams and Greg Germann both left the main cast, although Germann is expected to remain on the show in a guest capacity.[182][183] In September 2021, it was announced that Kate Walsh would be returning as Addison Montgomery for multiple episodes in season 18.[184] In July 2022, it was announced that Alexis Floyd, Niko Terho, Adelaide Kane, Midori Francis and Harry Shum Jr. joined the cast as series regular for the nineteenth season.[185] In August 2022, it was announced that Ellen Pompeo will appear in only eight episodes of the upcoming nineteenth season, but that she will continue to narrate the episodes and will remain an executive producer.[186] In May 2024, it was announced that Jake Borelli will leave the series in season 21, along with Midori Francis.[187] In June 2024, it was announced that Jason George would be reupped to a series regular after appearing in a recurring capacity since the series' fifteenth season due to appearing on Station 19.[188] In August 2024, it was announced that Michael Thomas Grant will recur as new hospital chaplain James.[189]

Rhimes initially considered setting the medical drama in her hometown, Chicago, but ultimately decided on Seattle to distinguish Grey's Anatomy from the Chicago-based series ER.[135]

Fisher Plaza, the former headquarters of Fisher Communications (now merged into Sinclair Broadcast Group) and home to Sinclair's ABC-affiliated KOMO radio and television stations in Seattle, is used for some exterior shots of Grey Sloan Memorial Hospital. Notably, air ambulances land on the KOMO-TV news copter's helipad, suggesting the hospital's proximity to the Space Needle (located across the street from Fisher Plaza), the Seattle Monorail, and other iconic landmarks.

However, the hospital used for most exterior and some interior shots is not located in Seattle; these scenes are filmed at the VA Sepulveda Ambulatory Care Center in North Hills, Los Angeles, California, and shots from an interior walkway occasionally show dry California mountains in the background.[190] The exterior of Meredith Grey's house, also known as the Intern House, is a real location. In the show, Grey's home address is 613 Harper Lane, though this is not a real address. The physical house is located at 303 W. Comstock St., in the Queen Anne, Seattle, Washington.


Most scenes are filmed at The Prospect Studios in Los Feliz, Los Angeles, just east of Hollywood, where Grey's Anatomy occupies six sound stages. Some outdoor scenes are shot at Warren G. Magnuson Park in Seattle. Many of the props used in the show are functional medical equipment, including a working MRI machine.[191] When asked about operating room scenes, Sarah Drew (who plays April Kepner) said:
We work with bovine organs, which is cow's organs, The smell is repulsive and makes us all gag. And we use an actual soldering tool to solder the organs. It smells like burning flesh. There's also a lot of silicone and blood matter, red jello mixed with blood and chicken fat. It's pretty gross.[192]
Costumes are used to differentiate between attending surgeons and residents: attending surgeons wear navy blue scrubs, while residents wear light blue scrubs.[193] The series is filmed using a single-camera setup, similar to many other dramas.[194] Grey's Anatomy often utilizes the "walk-and-talk" filming technique, a method popularized by earlier series such as St. Elsewhere, ER, and The West Wing.[195]

Grey's Anatomy's first season commenced airing as a mid-season replacement to Boston Legal on March 27, 2005, and concluded on May 22, 2005. The 9-episode season aired on Sundays in the 10:00pm EST time slot, following Desperate Housewives, although it had initially been slated for a January 2005 debut in the Monday 10:00pm EST time slot.[114] The show was renewed by ABC for a second season,[198] that aired in the same time slot as season 1. Premiering on September 25, 2005, and concluding on May 15, 2006, the season consisted of 27 episodes.[199][200] As the original first season order was for 13 episodes, 5 episodes were held for season 2, as ABC decided to close the first season of Grey's Anatomy on the same night as Desperate Housewives' finale.[201] During season 2, Grey's Anatomy produced 2 specials recapping the events of recent episodes, narrated by Bailey, entitled "Straight to the Heart" and "Under Pressure". The show was renewed for a third season, which was relocated to the coveted Thursday 9:00 pm EST time slot. The show has remained on Thursdays since then.[202] Commencing on September 21, 2006, and ending on May 17, 2007, season 3 consisted of 25 episodes.[203] 2 more specials were produced during the show's third season, entitled "Complications of the Heart" and "Every Moment Counts", which were narrated by Bailey and Morgan, respectively.

ABC renewed Grey's Anatomy for a fourth season, which aired from September 27, 2007, to May 22, 2008, and ultimately consisted of 17 episodes.[204][205] Season 4 had a reduced number of episodes, due to the 2007–08 Writers Guild of America strike, which caused production to cease from February to April, leaving the show with no writing staff during that time.[206] At the beginning of season 4, the show aired its final special entitled "Come Rain Or Shine", created to transition viewers from Grey's Anatomy to Private Practice, which was narrated by the editors of People magazine.[207] The show received a renewal for a fifth season,[208] which premiered on September 25, 2008, and concluded on May 14, 2009, consisting of 24 episodes.[209] The series was renewed for a sixth season consisting of 24 episodes,[210] which commenced on September 24, 2009, and ended on May 20, 2010.[211] During season 6, Grey's Anatomy aired a series of webisodes entitled Seattle Grace: On Call at ABC.com.[212] ABC renewed the show for a seventh season,[213] which premiered on September 23, 2010, and concluded on May 19, 2011, consisting of 22 episodes.[214] This was followed up with Seattle Grace: On Call, Seattle Grace: Message of Hope, aired during the beginning of season 7.[129] Also during season 7, the series produced a musical episode entitled "Song Beneath the Song", featuring songs that became famous through their use in Grey's Anatomy.[215] The show received a 24-episode eighth season renewal,[216] which commenced on September 22, 2011, with a 2-hour episode, and ended on May 17, 2012.[217] Grey's Anatomy was renewed for a ninth season,[218] which premiered on September 27, 2012, and ended on May 16, 2013.[219] Grey's Anatomy was renewed for a tenth season on May 10, 2013[220] and premiered on September 27, 2013, with a 2-hour episode, and ended on May 15, 2014.

On May 8, 2014, ABC renewed the series for an 11th season that aired from September 25, 2014, to May 14, 2015.[221] In addition, the show was relocated to the Thursday 8:00 pm EST time slot. After four seasons outside the Top 25 rated shows, Grey's Anatomy was the No. 15 show in the 2013–2014 season, the show's tenth. The show also re-entered the Top 5 shows in the 18–49 viewer demographic. On May 7, 2015, ABC renewed the series for a 12th season that premiered on September 24, 2015, and concluded on May 19, 2016.[222] The 13th season aired from September 22, 2016, to May 18, 2017.[223] The 14th season aired from September 28, 2017, to May 17, 2018. The 15th season aired from September 27, 2018, to May 16, 2019. The 16th season aired from September 26, 2019, to April 9, 2020. The 17th season aired from November 12, 2020, to June 3, 2021. On May 10, 2021, the series was renewed for an 18th season,[224] which premiered on September 30, 2021.[225] On January 10, 2022, the series was renewed for a 19th season,[226] which premiered on October 6, 2022.[227] On March 24, 2023, ABC renewed the series for a 20th season with Meg Marinis now serving as the showrunner.[228] The 20th season premiered on March 14, 2024.[229] In April 2024, ABC announced the show had been picked up for a 21st season, with Marinis remaining as the show-runner.[230] In a statement, Rhimes expressing her gratitude for Marnis, who joined the series during the third season, telling The Hollywood Reporter: "Meg Marinis' storytelling is a gift that continues to keep the show vibrant, compelling and alive, and I can't wait to see what she has in store for next season."[231] On April 3, 2025, ABC renewed the series for a 22nd season.[232]

Grey's Anatomy episodes air regularly on ABC in the United States. Each episode is approximately 43 minutes long and is broadcast in both high and standard-definition.[233] The series' episodes are also available for download in both qualities on the iTunes Store and Amazon Video.[234][235] ABC Video-On-Demand offers recent episodes for temporary viewing, while episodes can also be streamed on ABC's official Grey's Anatomy website, as well as on Hulu and Xfinity.[236][237][238]

In 2009, ABC signed a deal to allow episodes to stream on Netflix.[239] The show is syndicated on Lifetime, with one-hour blocks airing weekdays at 1:00 pm, 2:00 pm, and 3:00 pm EST.[240] In certain regions, such as Germany and the United Kingdom, the full series is available on Disney+.

Since its debut, Buena Vista Home Entertainment has released the first 13 seasons on DVD to regions 1, 2, and 4.[241] The season 1 DVD, released on February 14, 2006, features an alternate title sequence, bloopers, behind-the-scenes footage, audio commentaries, and an extended edition of the pilot episode.[242] The season 2 DVD, released on September 12, 2006, which includes extended episodes, an interview with Wilson, deleted scenes, a set tour, a "Q&A" with the cast, and a segment on the special effects creation.[243] The season 3 DVD was released on September 11, 2007, with bonus features including extended episodes, an interview with star Dempsey, audio commentaries, and bloopers.[244]

The season 4 DVD released on September 9, 2008, features an interview with Heigl and Chambers, extended episodes, bloopers, and deleted scenes.[245] The season 5 DVD was released on September 15, 2009, and includes unaired scenes, bloopers, and extended episodes.[246] The season 6 DVD, released on September 14, 2010, features deleted scenes, an extended finale, and bloopers.[247] The season 7 DVD, released on September 13, 2011, includes an extended edition of and a behind-the-scenes featurette on the musical episode, bloopers, as well as deleted scenes.[248] In addition, the season 8 DVD was released on September 4, 2012, with several bonus features and deleted scenes.[241]

The season 9 DVD released on August 27, 2013, with several bonus features and deleted scenes. The season 10 DVD was released on September 2, 2014, with new several bonus features and deleted scenes. The season was officially released on DVD as a 6-disc box-set under the title of Grey's Anatomy: The Complete Tenth Season – Live For The Moments on September 2, 2014. In view of the departure of the character of Cristina Yang in the season finale, the DVD set featured an extended episode Do You Know? and a special feature from Sandra Oh titled "An Immeasurable Gift".[249] Season 11 released on DVD as a 6-disc box set on August 18, 2015, with interviews with new series regular Caterina Scorsone and a special feature for Dempsey's departure, titled "How To Say Goodbye Dr. Derek Shepherd". Season 12 released on DVD as a 6-disc box set on August 30, 2016. Season 13 released on DVD as a 6-disc box set on August 29, 2017.

Grey's Anatomy offers a different and perhaps more valuable idea of what it means to be strong: the capacity to suffer terribly, break down completely, and then get up again, confident that you're bigger than the sum of the tragedies you've suffered—because everyone else is, too. The layers of history have grown pretty dense and rich, and the friendships that form around them as the characters suffer and survive is the glue of the series, and what—despite a few inevitable hit-or-miss patches—elevates it to something special.

Grey's Anatomy has been well received among critics. As of August 2023, the show holds an average score of 84% on Rotten Tomatoes.[294] Emily VanDerWerff of The A.V. Club gave an insight on the series' overwhelming success and the lows, writing that the quality arc is "all over the place". She noted the steady build-up in the first season; the series skyrocketing into a phenomenon in the second season; the gradual dip in season three; and "some seriously bumpy moments" in the fourth season, which was interrupted by the writers' strike. VanDerWerff felt that the "climb begins again in season five".[295] Samantha Highfill of Entertainment Weekly in a review wrote, "I believe the show's best season to date is season 2. Let me make it clear that I'm not saying seasons 3 through 9 were bad. In my opinion, there have only been a few lulls in the show's history, and most of them didn't last a full season." adding, "I still enjoy the show, and I'll honestly never stop watching. By any standards, Grey's Anatomy has been successful television, ranking highly in the ratings for nine seasons and entering the cultural lexicon via phrases as cloying yet catchy as 'McDreamy', the show has had its periods of being intensely irritating, and it has had its periods when it seems as if Shonda Rhimes has taken leave of her faculties, but it's also got an amazingly high batting average, particularly with every solid season that passes along in this second act of its run." The site lauded the show saying, "On average, it's been very good TV, filled with interesting, driven characters who run the gamut of professions within the show's hospital setting. It's been, by turns, a good soap, a good romantic comedy, a good medical drama, and a good interpersonal show about an unexpected workplace family."[296]

The first season received positive reviews which steadily built up, with Gary Levin of USA Today calling Grey's Anatomy one of the top shows on television.[297] The New York Daily News named Grey's Anatomy a "winner", whereas Newsday expressed a positive opinion by stating "You simply can't stop watching." The Washington Post's Tom Shales was critical of season one, finding it reminiscent of ER and commenting that: "The show is much more a matter of commercial calculation than an honest attempt to try something fresh and different."[298] Shortly after its initial airing, the Chicago Tribune's Maureen Ryan called Grey's Anatomy the new Friends (a concluded National Broadcasting Company (NBC) sitcom following the lives of a group of young adults, that for all of its 10-year run was in the top-5 for viewer ratings).[113] The second season received high critical acclaim: top critics like Emily VanDerWerff of The A.V. Club called the show a "phenomenon", adding the show was, "one of the best TV shows around", while Samantha Highfill of Entertainment Weekly later during the tenth season called the second "the show's best season to date". However, Kevin Carr of 7M Pictures opined that Grey's Anatomy is a mere combination of Scrubs, ER, Sex and the City and The Love Boat.[299] It further garnered positive reviews: Christopher Monfette of IGN added "The second season of this medical drama expertly wove its signature elements of complex relationships, whimsical banter and challenging life-lessons; all to a montage-fetish, indie-rock soundtrack."[300] Todd Gilchrist, also from IGN, called the season "terrific" adding, "Indeed, one of the best currently on television. While it remains to be seen what the creators do with it, now that it's become an outright event program, the season demonstrates that Rhimes and co. know what to do with the opportunities presented them. whether you're male or female, this is the kind of entertainment that small-screen devotees and folks fed up with television need to see."[301]

The title character of Grey's Anatomy, Meredith, has received both widespread critical acclaim and weary feedback by critics along the course of the show, with the development of the character garnering praise from majority critics. Earlier reactions for Meredith were mixed; in a 2006 review, Alessandra Stanley of The New York Times dubbed to her as "the heroine of Grey's Anatomy".[303] A reviewer for BuddyTV praised the distinct uniqueness in the character calling Meredith an "unconventional heroine" adding that the character was, "Neither black nor white but always ... wait for it ... many shades of grey. The reviewer and to add that even in her lighter moments, she has still been "dark and twisty."[304] The sentiment was shared by Glenn Diaz who remarked, "You gotta love Mer when she's gloomy." When Pompeo did not receive a nomination at the 61st Primetime Emmy Awards, for her work as Meredith. Mary McNamara of the Los Angeles Times suggested that Pompeo, "has worked very hard ... to make Meredith Grey an interesting character", and should have received a nomination.[305] IGN's Monfette, less impressed by the character, criticized her storyline as "some bizarrely under-developed sub-plot about depression and giving Derek a season's worth of reconsidering to do."[300] Robert Rorke of the New York Post was critical of Meredith's relationship with Derek Shepherd, writing: "She used to be the queen of the romantic dilemmas, but lately, she's been a little dopey, what with the endless 'McDreamy' soliloquies."[306] The development of the character has received praise from critics. Reviewing the first part of the eighth season, TV Fanatic wrote: "this season belongs to Meredith Grey. She is the heart and soul of the show and has been outstanding. This is a character that used to be so dark and twisty and has now grown into a more mature woman. Ellen Pompeo has been at the top of her game this season." Rick Porter reviewing the episode "How to Save a Life" from the 11th season for Zap2it wrote, "Without Meredith, and without one of Pompeo's strongest performances in her long time on the show, "How to Save a Life" would have run the risk of coming across as a baldly manipulative death episode, the likes of which the show has done several times before. He added "'How to Save a Life" may not be the ideal Emmy-submission episode for Ellen Pompeo, considering Meredith is off screen for more than half of it. But it's among the best work she's ever done on the show."[307] Janalen Samson of BuddyTV lauded the Meredith's development throughout the series saying, "When one considers how this character has grown over eleven seasons, it really is amazing. Kudos to Ellen Pompeo for her fine work. She's actually done the impossible, because I actually care what happens to Meredith Grey."[304] Reviewing the season 12 premiere, "Sledgehammer", critics including Alex Hawkins of the Western Gazette again highlighted Pompeo's being due for an Emmy Award.

The majority of the supporting cast of Grey's Anatomy have been well received as well, with the New York Post's Rorke deeming Stevens to be "the heart-and-soul" of Grey's Anatomy, whereas Eyder Peralta of the Houston Chronicle was critical of her character development, stating: "[She's] the reason I don't watch Grey's Anatomy anymore."[308] Kelli Catana of The Huffington Post named Yang "the best damn character" and deemed "the Grey/Yang relationship the most true friendship on network television."[309] Television Without Pity writer Lauren Shotwell claimed Yang is "the only one of these five [residents] that regularly acts like an actual doctor". Analyzing Alex Karev, Rachel Simon called him "underrated", and she pointed out that his personal growth never seems to get acknowledged, as "Alex has evolved, slowly and realistically, into a genuinely good person whose faults don't miraculously disappear, but take a backseat to much better qualities."[310] Robert Bianco of USA Today said Dempsey has a "seemingly effortless way of humanizing Derek's 'dreamy' appeal with ego and vanity".[311] His friendship with Mark Sloan has been well received. Victor Balta said "they've demonstrated an easy chemistry that makes for some of the great comic relief around Seattle Grace Hospital."[312] Addison Montgomery was deemed "sassy and bright and interesting."[313] TV Guide said of Walsh's stint on Grey's Anatomy that she "adds spice to an already hot show."[314] Callie Torres, after having previously received mixed reviews, was praised for her bisexual storyline. Critics added that the character was anchored by stellar performances by Sara Ramirez. Lexie Grey, having initially been criticized, became a critics' favorite in the series. Alex Keen of The Trades wrote that Lexie's "presence and confidence have increased quite a bit ... and actress, Chyler Leigh, does a fantastic job of making this progression feel seamless. Since the series has defused the tension between Little Grey and Big Grey (aka Meredith), Lexie has clear sailing through the season and steals the show as one of the best current characters on the series."[315]

With the departure of several cast members throughout the seasons, many new characters have been added to the drama's ensemble. McKidd and Capshaw were referred to as "fresh additions" to the series, by Monfette of IGN.[316] Matt Roush of TV Guide commented: "Hunt is the most encouraging thing to happen to Grey's Anatomy in quite a while."[317] Matt Mitovich of TV Guide noted that Robbins "quickly established herself as a fan favorite",[318] describing her as "a breath of fresh air in the often-angsty halls of Seattle Grace.[319] On April Kepner and Jackson Avery, Courtney Morrison of TVFanatic wrote, "April has grown since her character was introduced ... she's honest. A girl with principles is a girl you want to do well." He described her and Avery as "a couple for whom viewers can root". Speaking of the new cast members, in addition to the remaining original ones, Robert Bianco from USA Today called them the show's "best ensemble in years".[320]

Regarding season 3, Bill Carter of The New York Times called Grey's Anatomy "television's hottest show", adding: "[No show] is expected to challenge Grey's Anatomy for prime-time pre-eminence."[321] Contrasting with Carter's view, Monfette of IGN said that it speedily found itself "mired in the annoying and absurd", adding: "This third season may very well represent a case of over-writing a concept that has, perhaps tragically, run bone-dry on narrative fuel."[300] At the conclusion of season 3, Entertainment Weekly's Gregory Kirschling said "the show lacked a defining happy, warm-gooseflesh moment", adding that the season "didn't leave you dying for the [next] season premiere".[322] Speaking of the fourth season, Laura Burrows of IGN said the series became "a little more than mediocre, but less than fantastic", adding: "This season proved that even strong chemistry and good acting cannot save a show that suffers from the inevitable recycled plot."[323]

In contrast to the moderately negative feedback the third and fourth seasons received, Alan Sepinwall of The Star-Ledger said of the fifth season: "Overall, it feels more like the good old days than Grey's Anatomy has in a long time." Misha Davenport from the Chicago Sun-Times said season 5 "hits on all the things the show does so well", adding: "There is romance, heartbreak, humor and a few moments that will move fans to tears."[324] Brian Lowry of Variety, less impressed, opinionated that the season 5 displayed the show running out of storylines.[325] Speaking of the sixth season, Bianco of USA Today wrote: "Grey's has always loved grand gestures. You like them or you don't; the only real question is whether the show pulls them off or it doesn't. This year, it did."[326]

The series has a score of 66 out of 100 on Metacritic, based on five reviews for season 7.[327] In response to the season, Bianco from USA Today commented: "Happily, it now seems to have landed on solid ground." Also of the seventh season, Entertainment Weekly's Jennifer Armstrong said: "It's in the shooting's emotional reverberations that the show is regenerating after the past few hit-and-miss seasons," whereas Verne Gay of Newsday commented: "Unfortunately, they've settled on far-too-easy and facile answers for the most part."[328] HitFix gave a positive review saying that, "season 7 overall has been one of the show's strongest ever." and added, "There was a time when Grey's Anatomy was this show where I suffered through a lot of stuff that made me cringe to get to those genius melodrama moments it could do so well. Over the last couple of years, it's evolved into a show that's much more consistent in tone, where it may not move me as often as it did in the early years but also very rarely makes me question my reasons for watching."[329] Speaking of season 8, Entertainment Weekly's Mandi Bierly called it a "so-so season",[330] and Lesley Goldberg of The Hollywood Reporter called it "emotional".[331] Also acknowledging the fan base, Verne Gay of Newsday wrote "Grey's has had a good season and has an intensely loyal fan base to prove it" regarding the eighth season.[332]

The ninth season received positive reviews, with Rob Salem of Toronto Star calling it "a solid return to form."[333] Praising the friendship between Meredith and Cristina of Entertainment Weekly wrote, "There's still one good reason to keep watching: Where else can you find such deep friendships between female co-workers".[334] The tenth season was also marked with praise, with Annie Barrett for Entertainment Weekly writing "There's true sorrow here along with the passion, which keeps their dynamic so intriguing to me."[335] Caroline Siede from The A.V. Club wrote in her review for the tenth season "At its best, Grey's Anatomy is about everyday bravery, sacrifice, and courage. At its worst, it's a melodramatic, moralizing soap opera. Both sides are on display as the show heads confidently into its 10th season."[336] Many sources, including Rachel Simon of Bustle and Nicole Pomarico of Wetpaint, claimed that Sandra Oh's performance during her final season on Grey's Anatomy is worthy of an Emmy nomination.[337][338]

Bryce Olin of Netflix ranked Grey's 9th among the 50 Best TV Shows on Netflix stating, "It's a tough call, but based on Grey's casting choices and revolutionary portrayals of female doctors in the series, I'm willing to argue that Grey's Anatomy is the best medical drama of all time. Obviously, Shonda Rhimes didn't reinvent the wheel with the series, but there's no denying its popularity." adding, "I understand its significance in the pop culture sphere." He also stated that the show could go higher in the ranks with the upcoming season stating, "Apparently, Grey's Anatomy fans are passionate about their show, although it seems like they've been closeted for the last few years. I'd love to move Grey's Anatomy even higher on the ranking, but I'll have wait until the eleventh season comes to Netflix."[339][340]

The show was criticized for its handling of a controversy: the Grey's Anatomy scandal revolved around actor Isaiah Washington using a homophobic slur on set. Mainstream media coverage scapegoated this scandal as "black masculinity for perpetrating homophobia, thus containing both sexual and racial difference in the name of tolerance."[341]

Critics included Grey's Anatomy in top ten lists for five of its 14 seasons; these are listed below in order of rank.



Grey's Anatomy makes an impact on how people's perception of the world is created.

Grey's Anatomy has been recognized as a significant cultural phenomenon. Entertainment Weekly's executive editor, Lori Majewski, remarked, "Grey's Anatomy isn't just a show, it's a phenomenon. When [the] final shows air, every place in New York City is empty. You could get a table at the best restaurants."[348] Similarly, Jace Lacob of The Daily Beast compared its success to that of Friends, calling it a "cultural phenomenon".[349] Media analyst Steve Sternberg of Magna Global USA explained the show's broad appeal, noting that "roughly 80 percent of households during prime time only have one TV set on. People are looking for shows they can watch with other household members."[114]

In its early seasons, Grey's Anatomy introduced the trend of "Mc-labeling", beginning with the character Derek Shepherd being dubbed "McDreamy". This trend became so pervasive that The National Post labeled it a "phenomenon".[350] It was later parodied on shows like ER and Degrassi: The Next Generation.[351] Mark Lawson of The Guardian credited Grey's Anatomy with popularizing the "songtage", or musical montage segments.[16] This trend was parodied by Mad TV in 2006, which spoofed the show's emotional moments and musical montages.[352]

The series has also been praised for redefining what is considered "good" television. The A.V. Club writes, "Since The Sopranos burst onto the scene, we've too often classified a show as 'good' based on how closely it adhered to the dark, violent, male-centric template set out by that particular show. It's time for that to end. At its best, Grey's Anatomy has been among the very best shows on TV, and at its worst, it's been at least fascinating to watch. TV is at its best when it emotionally connects, and even when it seems to be otherwise merrily hurtling off a cliff, Grey's Anatomy is nothing but emotional connection."

The show was ranked No. 66 on Entertainment Weekly's "New TV Classics" list in 2007 and was declared the third-highest rated show on IMDb for the first ten years of the platform (2002–2012).[353][354] The premise of Grey's Anatomy inspired the creation of A corazón abierto, a Colombian adaptation, which in turn spawned a Mexican version of the same name.[355][356]

A study conducted by Brian Quick from the University of Illinois suggested that the show's portrayal of doctors as "smart, good looking, capable, and interesting" led viewers to associate real-world doctors with those traits.[347] However, Dr. Karen Zink, a surgical resident, deemed the show's portrayal of interns unrealistic, commenting, "None of [the characters] have bags under their eyes. They all leave the hospital dressed cute, with their hair done and makeup on. That is so far away from the reality of interns."[357]

The show has even had real-world life-saving impacts. In 2011, a woman in Sheboygan, Wisconsin survived an asthma attack after her daughter and a friend performed cardiopulmonary resuscitation (CPR) they learned from Grey's Anatomy.[358] In 2017, an Israeli woman saved her husband by performing cardiac massage she learned from the show.[359] She continued the procedure for 20 minutes until paramedics arrived and transferred the husband to Shaare Zedek Medical Center.

In the 14th season, the episode titled "1-800-799-7233", named after the National Domestic Violence Hotline, was well received by audiences, who used social media to raise awareness about domestic violence and the hotline itself.[360]

Grey's Anatomy has received high viewership and ratings since its debut. The first 4 seasons of the program each ranked in the top 10 among all viewers, reaching its peak Nielsen ratings in the second season, attracting an average of 19.44 million viewers per episode, and ranking at #5 overall. Following the show's time-slot being relocated, overall rankings steadily declined, dropping below the top 10 in its fifth season. Grey's Anatomy made its greatest fall from its sixth to seventh season, slipping from #16 to #31. The series is on a steady decline in terms of overall viewership and rankings, yet Grey's Anatomy still holds value in charts when numbers are pulled from the digital video recorder (DVR). It was the most recorded show between 2007 and 2011, based on cumulative totals, and has been for several years in a row.[361]

The most-watched episode of the series is "It's the End of the World", with 37.88 million viewers, aided by a lead-in from Super Bowl XL.[362] Grey's Anatomy was the most expensive program on television in the 2007–08 season measured by advertising revenue, with earnings of US$400,000 per 30 seconds.[363] The show was named the fourth (behind Desperate Housewives, Two and a Half Men, and American Idol),[364] and the fifth-highest (behind Glee, Two and a Half Men, The X Factor and American Idol) revenue-earning show, with the earnings of US$2.67 million and US$2.75 million per half hour in 2011 and 2012 respectively.[365] While Grey's Anatomy is no longer ranked in the top numbers for overall ratings, the show's ranking in the key 18–49 demographic has remained high. As of season 8, the series is the highest-rated drama on television in the target demographic.[366] In 2016, a New York Times study of the 50 TV shows with the most Facebook Likes found that Grey's Anatomy was "most popular in a swath of the middle of the country, particularly in areas with a lower percentage of college graduates".[367]

Below is a table of Grey's Anatomy's seasonal rankings in the U.S. television market, based on average total viewers per episode. Each U.S. network television season starts in September and ends in late May, which coincides with the completion of May sweeps.

Grey's Anatomy has received numerous awards and nominations since its inception. The show has been nominated for 39 Primetime Emmy Awards, having been nominated for at least one every year until 2013.[194] At the 57th Primetime Emmy Awards in 2005, Oh was nominated for Outstanding Supporting Actress in a Drama Series, which she went on to be nominated for every year until 2009, and Horton was nominated for Outstanding Directing for a Drama Series.[194] The following year, at the 58th Primetime Emmy Awards, the series received a nomination for Outstanding Drama Series, which they were nominated for again in 2007.[194] Also in 2006, Wilson was nominated for Outstanding Supporting Actress in a Drama Series, which she went on to be nominated for every year until 2009, and Kyle Chandler was nominated for Outstanding Guest Actor in a Drama Series.[194] The 58th ceremony also honored Rhimes and Vernoff, who were both nominated for Outstanding Writing for a Drama Series.[194] Rhimes, whose career kicked off in 1995, has since produced yet another ABC series, Scandal, which began on air in 2012 and ended in 2019. Beginning in 2005, Rhimes has been continually nominated for numerous awards, including 3 Emmy Awards: first in 2006 for a dramatic series and a separate nomination for writing a dramatic series, followed by a third nomination in 2007 for a dramatic series.[194]

In 2007, at the 59th Primetime Emmy Awards, Heigl won the award for Outstanding Supporting Actress in a Drama Series, while Knight was nominated for Outstanding Supporting Actor in a Drama Series.[194] Numerous guest actresses have been nominated for Outstanding Guest Actress in a Drama Series, including Burton in 2006 and 2007, Christina Ricci in 2006, Reaser in 2007, Diahann Carroll in 2008, and Sharon Lawrence in 2009, but the only actress to have won the award is Devine in 2011, who was nominated again in 2012.[194] The show has also been nominated for 13 Creative Arts Emmy Awards, having won 2 of them: Outstanding Casting for a Drama Series and Outstanding Makeup for a Single-Camera Series (Non-Prosthetic)[194]

The show has received 10 Golden Globe Award nominations since its premiere. At the 63rd Golden Globe Awards, in 2006, the series was nominated for Best Drama Series,[437] Dempsey was nominated for Best Actor in a Drama Series,[437] which he was nominated for again in 2007,[438] and Oh won the award for Best Supporting Actress in a Series, Miniseries, or Television Film.[439] The following year, at the 64th Golden Globe Awards, in 2007, Pompeo was nominated for Best Actress in a Drama Series,[438] and the show won Best Drama Series.[440] At the 65th Golden Globe Awards, in 2008, Heigl was nominated for Best Supporting Actress in a Series, Miniseries, or Television Film,[441] while the series in whole was nominated for Best Drama Series.[441]

The series has won People's Choice Awards for Best Drama 5 times in 2007, 2013, 2015, 2016 and 2017 and has been nominated for several other People's Choice Awards, with nominations received by Oh[442] as well as multiple wins from Dempsey,[443] Pompeo winning in recent years 2013 and 2015,[444][445] Heigl,[443] Wilson,[443] Demi Lovato,[442] for guest starring, and the drama in whole for Favorite TV Drama.[446] In 2007, Rhimes and the female cast were the recipient of the Women in Film Lucy Award, in recognition of the excellence and innovation in the show as a creative work that has enhanced the perception of women through the medium of television.[447] The series has been honored with numerous NAACP Image Award nominations, many having been won, including 5 awards for Outstanding Drama Series.[448] Grey's Anatomy has also received several Screen Actors Guild Awards, with nominations received by Dempsey, as well as wins from Oh,[449] Wilson,[450] and the main cast for Outstanding Performance by an Ensemble in a Drama Series.[450] The series won a GLAAD Media Award for Outstanding Drama Series in 2012, and has received nominations for 8 consecutive years from 2010 to 2017, 2019, 2022, 2023, 2024.[451]

Grey's Anatomy has spawned several spin-offs and adaptations.

On February 21, 2007, The Wall Street Journal reported that ABC was pursuing a spin-off medical drama television series for Grey's Anatomy featuring Walsh's character, Addison Montgomery.[149] Subsequent reports confirmed the decision, stating that an expanded two-hour broadcast of Grey's Anatomy would serve as a backdoor pilot for the proposed spin-off. The cast of Grey's Anatomy was reportedly unhappy about the decision, as all hoped the spin-off would have been given to them. Pompeo commented that she felt, as the star, she should have been consulted,[452] and Heigl disclosed that she had hoped for a spin-off for Stevens.[453] The backdoor pilot that aired on May 3, 2007, sees Montgomery take a leave of absence from Seattle Grace Hospital, to visit her best-friend from Los Angeles, Naomi Bennett (Merrin Dungey, later Audra McDonald), a reproductive endocrinology and infertility specialist. While in Los Angeles, she meets Bennett's colleagues at the Oceanside Wellness Center.[59] The two-hour broadcast entitled "The Other Side of This Life" served as the twenty-second and twenty-third episodes of the third season, and was directed by Michael Grossman, according to Variety.[454] The cast included Amy Brenneman, Paul Adelstein, Tim Daly, Taye Diggs, Chris Lowell, and Merrin Dungey.[455]

KaDee Strickland's character, Charlotte King, who would be introduced in the spin-off's first-season premiere, did not appear in the backdoor pilot. Her addition to the main cast was announced on July 11, 2007, prior to the commencement of the first season.[456] She did not have to audition for the role, but was cast after a meeting with Rhimes.[457] Also not present in the backdoor pilot was McDonald, due to her character, Naomi Bennett, being portrayed by a different actress, Merrin Dungey. However, on June 29, 2007, ABC announced that Dungey would be replaced, with no reason given for the change.[458] The drama was titled Private Practice, and its premiere episode followed the second part of the season debut of Dancing with the Stars, and provided a lead-in to fellow freshman series Dirty Sexy Money. Pushing Daisies, a third new series for the evening, rounded out the lineup as a lead-in to Private Practice.[204] The series ended its run on January 22, 2013, after 6 seasons.[459][460]

On May 16, 2017, Channing Dungey announced at the ABC Upfronts that the network ordered another Grey's Anatomy spin-off, this one focusing on firefighters in Seattle. The series premiered mid-season in 2018. Stacy McKee, long-term Grey's writer and executive producer, serves as the spin-off's showrunner.[461] The new show was introduced Season 14, Episode 13, when a house fire brings the firefighters to Grey Sloan Memorial Hospital. In July 2017, it was announced that Jaina Lee Ortiz was the first actress cast in the spin-off series.[462] In September 2017, it was announced that Jason George, who has played Dr. Ben Warren since season 6, would be leaving Grey's Anatomy to move to the spin-off. He continued to be a series regular on Grey's Anatomy until the spin-off began production.[463] In October 2017, it was announced that 5 new series regulars for the spin-off had been cast being Grey Damon, Jay Hayden, Okieriete Onaodowan, Danielle Savre and Barrett Doss. It was also announced that the spin-off had a 10-episode order for the first season.[464][465] Later in October 2017, it was announced that Miguel Sandoval was cast as the Captain of the firehouse.[466]

On January 9, 2018, it was announced by Sarah Drew on her Instagram page that a 6-episode spin-off series following the new interns of Grey Sloan Memorial would be released for streaming on the ABC app and abc.com on Thursday, January 11, 2018. Grey's Anatomy: B-Team stars Sophia Taylor Ali (Dahlia Qadri), Jake Borelli (Levi Schmitt), Alex Blue Davis (Casey Parker), Jaicy Elliot (Taryn Helm), Rushi Kota (Vik Roy) and Jeanine Mason (Samantha "Sam" Bello) with special guest appearances made by Justin Chambers (Alex Karev), Kelly McCreary (Maggie Pierce), Kevin McKidd (Owen Hunt) and James Pickens Jr. (Richard Webber). The 6 episodes in this series were written by Barbara Kaye Friend with Grey's Anatomy series regular Sarah Drew (April Kepner) making her directorial debut directing each of them.[467]

Grey's Anatomy has several crossover storylines with both spin-offs Private Practice and Station 19 throughout its run.

In 2006, Doktorlar ("Doctors") an adaptation of the series, was made by the Turkish network Show TV. It aired for 4 seasons.

In 2010, A Corazón Abierto ("An Open Heart"), an adaptation of the series, was made by the Colombian network RCN TV. It aired for 2 seasons.

In 2011, A Corazón Abierto ("An Open Heart"), an adaptation of the series, was made by the Mexican network TV Azteca. It aired for 2 seasons.

The American Broadcasting Company has partnered with J. Larson CafePress and Barco Uniforms to provide the series' branded merchandise through an online store. The products available include shirts, sweatshirts, kitchenware, homeware, and bags, with the Grey's Anatomy logo on it.[e] Also available are custom unisex scrubs and lab coats in a variety of colors and sizes, designed by Barco.[468] The merchandise released by the company is available for purchase at the Grey's Anatomy official website, and US$1 from every purchase is donated to Barco's Nightingales Foundation.[469]

Five volumes of the Grey's Anatomy Original Soundtrack have been released as of 2011[update]. For the first 2 seasons, the show's main title theme was an excerpt from "Cosy in the Rocket", by British duo Psapp; it is featured on the first soundtrack album released via ABC's corporate cousin, Hollywood Records, on September 27, 2005. The second soundtrack, featuring songs from the series' second season, was released on September 12, 2006,[470] followed by a third soundtrack with music from the third season.[471] Following the seventh season musical episode "Song Beneath the Song", "Grey's Anatomy: The Music Event" soundtrack was released,[472] with volume 4 of the soundtrack released subsequently.[473]

In January 2009, Ubisoft announced that it had signed a licensing agreement with ABC Studios to develop a video game based on Grey's Anatomy.[474] Designed for the Wii, Nintendo DS and PC, Grey's Anatomy: The Video Game was released on March 10, 2009.[475] The game lets the player assume the role of one of the main characters, making decisions for the character's personal and professional life, and competing in a number of minigames.[476] It has been criticized by reviewers because of the simplicity of the mini-games and voice actors who do not play the same characters on the series, with Jason Ocampo of IGN giving it a 6/10 overall rating.[476] The Wii release received mixed reviews,[477] and the Windows release received generally unfavorable reviews.[478]

ABC and Nielsen partnered in 2011 to create a Grey's Anatomy application for Apple's iPad. The application was designed to allow viewers to participate in polls and learn trivial facts as they watch a live episode. It uses Nielsen's Media-Sync software to listen for the episode and to post features as the episode progresses.[479]

The creators of the show set up a real online wedding registry to mark the wedding of Meredith Grey and Derek Shepherd. Instead of buying gifts fans were encouraged to donate money to the American Academy of Neurology Foundation.[480]Guitar Hero  is a series of rhythm games first released in 2005, in which players use a guitar-shaped game controller to simulate playing primarily lead, bass, and rhythm guitar across numerous songs. Players match notes that scroll on-screen to colored fret buttons on the controller, strumming the controller in time to the music in order to score points, and keep the virtual audience excited.  The games attempt to mimic many features of playing a real guitar, including the use of fast-fingering hammer-ons and pull-offs and the use of the whammy bar to alter the pitch of notes. Most games support single player modes, typically a Career mode to play through all the songs in the game, as well as competitive and cooperative multiplayer modes. With the introduction of Guitar Hero World Tour in 2008, the game includes support for a four-player band including vocals and drums. The series initially used mostly cover versions of songs created by WaveGroup Sound, but most recent titles feature soundtracks that are fully master recordings, and in some cases, special re-recordings, of the songs. Later titles in the series feature support for downloadable content in the form of new songs.

In 2005, RedOctane, a company specializing in the manufacture of unique game controllers, was inspired to create Guitar Hero based on its experience creating hardware for Konami's GuitarFreaks arcade game. It enlisted Harmonix, which had previously developed several music video games, for development assistance. The first game in the series was made on a budget of US$1 million. The series became extremely successful, leading to the acquisition of RedOctane by Activision in 2007. Harmonix was acquired by MTV Games and went on to create the Rock Band series of music games in the same vein as Guitar Hero. Activision brought Neversoft (primarily known for their Tony Hawk series of skateboarding games) on board for future development duties. Additional companies, such as Budcat Creations and Vicarious Visions, have assisted in the adaptation of the games for other systems.

The series has twenty-five releases, including the two spin-offs, the DJ Hero series and Band Hero. The Guitar Hero franchise was a primary brand during the emergence of the popularity of rhythm games as a cultural phenomenon in North America. Such games have been utilized as a learning and development tool for medical purposes. The first game in the series was considered by several journalists to be one of the most influential video games of the first decade of the 21st century. The series has sold more than 25 million units worldwide, earning US$2 billion at retail.

Despite early success, the series, along with the overall rhythm game genre, suffered from poor sales starting in 2009. Despite asserting consumer research suggested continued solid demand for the series, Activision later stated that the series was on hiatus for 2011, amid the development of a seventh main installment that was later cancelled as the emerging product was considered to be of poor quality. Activision later shut down sales of the series' downloadable content, although users who purchased material from it previously may still play what they bought.

Guitar Hero Live, released in October 2015, was the first new title in the series in five years, considered to be a reboot of the series and developed by FreeStyleGames, which had developed the DJ Hero games. Following a lukewarm reception and sales, Activision laid off many of the game's developers and sold the studio to Ubisoft, later shutting down the game's streaming DLC service.

Guitar Hero was created from a partnership between RedOctane, then their own company that produced specialized video game controllers, and Harmonix, a music video game development company who had previously produced Frequency, Amplitude, and Karaoke Revolution. RedOctane was seeking to bring in a GuitarFreaks-like game, highly popular in Japan at the time, into Western markets, and approached Harmonix about helping them to develop a music game involving a guitar controller. Both companies agreed to it, and went on to produce Guitar Hero in 2005.[1] The title was highly successful, leading to the development of its successful sequel, Guitar Hero II, in 2006. While the original controllers for the first Guitar Hero game were designed by Ryan Lesser, Rob Kay, Greg LoPiccolo, and Alex Rigopulous of Harmonix and built by the Honeybee Corporation of China, subsequent iterations and future controllers were developed inhouse at RedOctane.[2]

Both RedOctane and Harmonix experienced changes in 2006. RedOctane was bought by Activision in June — who spent US$100 million to acquire the Guitar Hero franchise[3] — while it was announced in October that Harmonix would be purchased by MTV Games. As a result of the two purchases, Harmonix would no longer develop future games in the Guitar Hero series. Instead, that responsibility would go to Neversoft, a subsidiary of Activision known for developing the Tony Hawk's series of skateboarding games.[4] Neversoft was chosen to helm the Guitar Hero series after Neversoft founder, Joel Jewett, admitted to the RedOctane founders, Kai and Charles Huang, that his development team for Tony Hawk's Project 8 went to work on weekends just to play Guitar Hero.[5] Activision CEO Bobby Kotick believed that Neversoft would help them bring great games to the series, but on reflection, stated that had Activision explored Harmonix further as a continued developer for the series, things "may have turned out differently".[6] In addition, Activision began seeking other markets for the game; a Nintendo DS version of the series was developed by Vicarious Visions, while a Guitar Hero Mobile series was created for mobile phones. The company also began considering the expansion of the series to band-specific titles with Guitar Hero: Aerosmith. Later, in November 2008, Activision acquired Budcat Creations, another development studio that had helped with the PlayStation 2 versions of Guitar Hero III and World Tour, announcing that they would be helping to develop another game in the Guitar Hero series.[7]

In 2007, Harmonix and MTV Games released a new music title through rival publisher Electronic Arts, called Rock Band. It expanded upon the gameplay popularized by the Guitar Hero series by adding drum and microphone instruments, allowing players to simulate playing songs as bands. Activision followed suit with the release of Guitar Hero World Tour in 2008, which supported multiple instruments. In 2009, Activision tripled its Guitar Hero offerings, and in addition to further continuation of the existing main series with Guitar Hero 5 and expansions, they introduced the titles Band Hero, geared towards more family-friendly pop music, and DJ Hero, a game based on turntablism and featuring a number of mixes. With the release of Guitar Hero 5, Activision considered the series to have moved away from its heavy metal basis into a broader selection of music. Guitar Hero 5 is the first game in the series to use a new version of the series' logo; previous games used a logo in a font with sharper "points" on the letters, which was considered "idiosyncratic with a vengeance" to match the games' emphasis on heavy metal music. Activision used the services of the Pentagram design studio to refashion the game's logo. Pentagram developed a new font, removing some of the "aggressive odd" features to make the typeface more suitable and amendable to design feature incorporation to other games such as Band Hero and DJ Hero.[8][9]

The results of the expanded offerings did not contribute well to the series, alongside the late-2000s recession; sales of most rhythm games including Guitar Hero and DJ Hero did not meet expectations, falling about 50% short of projected targets.[10][11][12] Activision announced it would be cutting back to only 10 SKUs within 2010 instead of the 25 in 2009.[13] Though RedOctane and Neversoft continued to develop the 6th main game, Guitar Hero: Warriors of Rock, until its completion, both studios were later shuttered by Activision, moving key personnel into Activision directly for future game development, and in the case of Neversoft, closing its Guitar Hero division, while transferring future development duties for the series to Vicarious Visions, another Activision studio which had been fundamental in building the Wii and Nintendo DS versions of the games.[14] In November 2010, Activision also closed Budcat Creations, the arm of the publisher that was primarily responsible for porting the Guitar Hero games to the PlayStation 2.[15]

Ahead of Activision's 2010 fourth quarter financial report in February 2011, Activision disbanded its Guitar Hero business unit and announced that it would cease development of the planned 2011 Guitar Hero game.[16][17] Activision cited "continued declines in the music genre" to explain its decision.[18] The closure also affected the DJ Hero series, as Activision stated that there were no plans to publish a music game during 2011.[19] Activision's vice president Dan Winters later clarified that the company was "just putting Guitar Hero on hiatus" and that they were "just not making a new game for next year, that's all".[20][21]

In a July 2011 interview with Forbes, Kotick stated that while the publisher was "going to stop selling Guitar Hero altogether", they were "going to go back to the studios and we're going to use new studios and reinvent" the series,[22] but a former team member of Vicarious Visions stated that as of 2012, all development of Guitar Hero had come to an end within Activision.[23] Another source close to Vicarious Visions had reported to Kotaku that while Guitar Hero 7 was in development under an Activision studio, the game was considered a "disaster".[24] The cancelled game omitted the additional instruments and used only a guitar peripheral, redesigning the unit to include a 6-button mechanism replacing the strum bar; the resulting unit was considered too expensive to manufacture and purchase.[24] The developers had also started the game development from scratch to try to create new characters and venues that would be more reactive to the actual songs being played to give the feel of a music video, but ultimately this proved too much of a challenge and had to be scrapped.[24] Further, with a limited budget, the song selection was limited to "low-budget" hits of the 1990s, or at times reusing songs that had previously been included in Guitar Hero games.[24] Though the team had a two-year development cycle, it was closed down after Activision president Eric Hirshber had seen the current state of the project at the one-year point.[24]

Another potential Guitar Hero project was discovered by the archival site Unseen64 for a game titled Hero World, a massively multiplayer online game that would link the Guitar Hero and DJ Hero games. The game had been developed by FreeStyleGames, sometime after the release of DJ Hero 2, with the main development duties passed to Virtual Fairground, using their platform The Ride, an Adobe Flash-based platform that would let the game be played in a web browser. The game was cancelled in 2011 along with other pending Guitar Hero projects.[25]

No further downloadable content for either Guitar Hero or DJ Hero was made after February 2011,[26] though Activision committed to releasing content that was already in development by that time due to fan response;[27] later, in a move described by Game Informer as "the final nail in [the series'] coffins",[28] Activision announced it would discontinue all DLC sales for the series without revoking access to tracks already bought as of March 31, 2014.[29] Though Activision had moved away from the Guitar Hero series, the lessons learned helped them and developer Toys for Bob to handle the manufacturing and outsourcing  issues that came with the highly successful Skylanders toy and video game franchise.[30]

In April 2015, Activision announced a new entry in the series, titled Guitar Hero Live.[31] The title was developed by Activision's internal studio FreeStyleGames, who previously had worked on the DJ Hero spinoff titles. FreeStyleGames were given free rein to reboot the Guitar Hero series for next-generation consoles. One of their first innovations was to drop the standard five-button guitar controller, ultimately designing a six-button guitar controller, with two rows of three buttons each, allowing them to mimic actual guitar fingering. Guitar Hero Live was released with both a career and an online mode. The career mode used full-motion video taken from the perspective of a lead guitarist underneath the note highway, to create an immersive experience to the player. The online mode, called GHTV, discarded the previous downloadable content approach and used a music video channel approach to stream playable songs to players, adding new songs to the catalog on a weekly basis. The game was released in October 2015.

Though the game was praised as a reinvention of the Guitar Hero series, the game did not sell as well as Activision expected; due to lowered forecasts, Activision let go of about half of FreeStyleGames' developers.[32] In January 2017, Ubisoft acquired FreeStyleGames from Activision, with unclear consequences for the game.[33] Activision shut down GHTV on December 1, 2018, reducing the available songs from almost 500 to the 42 present on-disc.[34][35]

In 2020 online servers for all Guitar Hero games were shut down on PS3.[36] Between this and the closure of Wii online servers for all games, they are now only playable online on Xbox 360.

The original Guitar Hero was released on the PlayStation 2 in November 2005. Guitar Hero is notable because it comes packaged with a controller peripheral modeled after a black Gibson SG guitar. Rather than a typical gamepad, this guitar controller is the primary input for the game. Playing the game with the guitar controller simulates playing an actual guitar, except it uses five colored "fret buttons" and a "strum bar" instead of frets and strings. The development of Guitar Hero was inspired by Konami's GuitarFreaks video game, which at the time, had not seen much exposure in the North American market; RedOctane, already selling guitar-shaped controllers for imported copies of GuitarFreaks, approached Harmonix about creating a game to use an entirely new Guitar controller. The concept was to have the gameplay of Amplitude with the visuals of Karaoke Revolution, both of which had been developed by Harmonix.[37][38][39][40]  The game was met with critical acclaim and received numerous awards for its innovative guitar peripheral and its soundtrack, which comprised 47 playable rock songs (most of which were cover versions of popular songs from artists and bands from the 1960s through modern rock). Guitar Hero has sold nearly 1.5 million copies to date.[41]

The popularity of the series increased dramatically with the release of Guitar Hero II for the PlayStation 2 in 2006. Featuring improved multiplayer gameplay, an improved note-recognizing system, and 64 songs, it became the fifth best-selling video game of 2006.[42] The PlayStation 2 version of the game was offered both separately and in a bundle with a cherry red Gibson SG guitar controller. Guitar Hero II was later released for the Xbox 360 in April 2007 with an exclusive Gibson Explorer guitar controller and an additional 10 songs, among other features. About 3 million units of Guitar Hero II have sold on the PlayStation 2 and Xbox 360.[43]

Guitar Hero III: Legends of Rock was released in late 2007 for the PlayStation 2, PlayStation 3, Xbox 360, Wii, Microsoft Windows, and Mac OS X platforms. The title is the first installment of the series to include wireless guitars bundled with the game and also the first to release a special bundle with two guitars.  The game includes Slash and Tom Morello as playable characters in addition to the existing fictional avatars; both guitarists performed motion capture to be used for their characters' animation in the game. The game opened to critically acclaimed reviews from critics and fans alike, with most calling it the greatest entry in the series. As PGNx Media stated on Metacritic, "Guitar Hero III: Legends of Rock is undoubtedly the best game in the series."[44] The game later became one of the best selling video games of all time and the first ever game to reach 1 billion dollars in sales.

Guitar Hero World Tour, previously named Guitar Hero IV, is the fourth full game in the series and was released on October 26, 2008 for PlayStation 2, PlayStation 3, Xbox 360, and Wii.  Analysts had expected that future Guitar Hero games in 2008 would include additional instrument peripherals to compete against Rock Band;[45] Guitar Hero World Tour was confirmed as in development following the announcement of the merger between Activision and Vivendi Games in December 2007.[46]  Activision's CEO Bobby Kotick announced on April 21, 2008 that Guitar Hero World Tour will branch out into other instruments including vocals.[47] Guitar Hero World Tour includes drums and vocals, and can be bought packaged with a new drum set controller, a microphone, and the standard guitar controller.[48]  A larger number of real-world musicians appear as playable characters, including Jimi Hendrix, Billy Corgan, Hayley Williams, Zakk Wylde, Ted Nugent, Travis Barker, Sting, and Ozzy Osbourne.  Guitar Hero World Tour also features custom song creation that can be shared with others.[48]

Guitar Hero 5, the fifth main entry in the series, was confirmed in December 2008.[49] It was released on September 1, 2009, and includes 85 songs from 83 different artists. The game includes new game modes and features, including its 'Party Mode,' which gives players the ability to drop-in and out and change difficulties in the middle of a song. Artists including Johnny Cash, Matt Bellamy, Carlos Santana, Kurt Cobain and Shirley Manson appear as playable characters in the game.[50]

Guitar Hero: Warriors of Rock, the sixth main console game in the series, was released on September 28, 2010. It is the last game in the series developed by Neversoft's Guitar Hero division prior to its dissolution, with Vicarious Visions assisting on the Wii version with added Nintendo DS functionality. The game has been described as returning to the roots of the Guitar Hero series; while it still allows for full band play, the soundtrack's focus is on rock and roll music and an emphasis on guitar "shredding".[51] The game introduced a career-based "Quest Mode", narrated by Gene Simmons, that guides the players to complete songs to unlock "warriors of rock" to join them in saving "demigod of rock" and his guitar from his imprisonment by "the Beast".[52][53]

Following a five-year hiatus, as described below, Activision announced Guitar Hero Live for release in late 2015 on most seventh-generation and eighth-generation consoles. Live was developed to rebuild the game from the ground up, and while the gameplay remains similar to the earlier titles, focusing primarily on the lead guitar, it uses a 3-button guitar controller with each button having "up" and "down" positions, making for more complex tabulators. The game uses live footage of a rock concert, taken from the perspective of the lead guitarist, to provide a more immersive experience.[54]

Guitar Hero Encore: Rocks the 80s for the PlayStation 2, which was released in July 2007, was the final game Harmonix developed for the series.[55] Though it was produced after Harmonix was purchased by MTV Games, they were contractually obligated to finish the game, which as suggested by its name primarily features songs from the 1980s. The game was criticized for its small selection of songs, and in one case compared unfavorably to Lou Reed's album Metal Machine Music for allegedly being a mere contractual obligation project.

Guitar Hero: Aerosmith was the first Guitar Hero game to center on one specific artist. On September 4, 2007, Billboard announced that Aerosmith was "working closely with the makers of Guitar Hero IV", which would be "dedicated to the group's music".[56]  On February 15, 2008, Activision announced that Guitar Hero: Aerosmith would be released on June 29, 2008.[57][58][59] Guitar Hero: Aerosmith was developed by Neversoft for the Xbox 360 and PlayStation 3 versions, by Vicarious Visions for the Wii, and by Budcat Creations for the PlayStation 2.[60] The game's setlist is mostly Aerosmith songs, with other songs from Joe Perry's solo work or artists that have inspired or performed with Aerosmith, including Run-DMC.

Guitar Hero: Metallica, the next entry in the series to center on one artist, was released on March 29, 2009. It is based on the full band experience of World Tour, and educates players on Metallica's history and music in the same manner as Guitar Hero: Aerosmith did for Aerosmith.[61] The game also debuted a new difficulty for drums, called Expert+ (read "expert-plus"), intended to incorporate a second bass drum pedal for songs that would otherwise be too difficult to play on drums. The game received generally well reviews with fans praising the song selection and the heavily enforced Metallica theme.

Guitar Hero Smash Hits (titled Guitar Hero Greatest Hits in Europe and Australia) was released in June 2009. It features full-band versions of 48 songs from earlier Guitar Hero games that only used the guitar controller. Unlike the previous versions, each of the songs is based on a master recording that includes some live tracks.[62] The game follows a similar model as Guitar Hero: Metallica, and was developed Beenox for the PlayStation 3, Xbox 360, PlayStation 2, and Wii. It received a lukewarm reception with many calling it unnecessary and a "cash grab."[63]


DJ Hero was announced by Activision in May 2009. Prior to the announcement, the company had purchased FreeStyleGames, a small developer of music games, to help produce localized downloadable content for Guitar Hero games and a then-unannounced music game, later revealed to be DJ Hero.[64] DJ Hero uses a special turntable-based controller for players to perform with on various song mixes in the game. The game also incorporates the use of a Guitar Hero controller on ten specially arranged tracks; Bright suggested that future Guitar Hero games after Guitar Hero 5 could include the use of the turntable control.[65]

Band Hero was announced in May 2009 and features "Top 40" hits aimed at family audiences, using the full band play style of Guitar Hero 5.[66] The game was also developed for the Nintendo DS, using the Guitar Hero: On Tour Guitar Grip, a new "drum skin" to fit the DS Lite, and the DS's microphone to support the full band experience.[67] Musician Taylor Swift appears as a playable character in the game,[68] as do the members of No Doubt.

Guitar Hero: Van Halen was released on December 22, 2009,[69] though customers that purchased Guitar Hero 5 under a special promotion received a copy of the game early. Like the other games oriented around a specific artist, Guitar Hero: Van Halen mainly uses songs by the band Van Halen, including three guitar solos by Eddie Van Halen, in addition to guest acts such as Queen, Weezer, Blink-182, Foo Fighters, The Offspring, and Queens of the Stone Age.[66][70]

A sequel to DJ Hero, DJ Hero 2, was officially announced in June 2010 for release in the last quarter of 2010, featuring more than 70 mashups from over 85 artists.[71] The game includes several new gameplay modes, including an "Empire" career mode, head-to-head DJ battles, social multiplayer modes, and a jump-in and out Party Play mode similar to Guitar Hero 5.  The game also includes more vocal options for singing and rapping to songs, and a freestyle mode for players.[71]

Guitar Hero: On Tour was released on the Nintendo DS hand-held system on June 22, 2008. The game includes a peripheral, dubbed the "Guitar Grip", a rectangular device that fits into the second slot of the Nintendo DS or DS Lite.  The peripheral only features the first four fret buttons and a strap so the Nintendo DS can be held sideways comfortably for play. The game also includes a guitar pick shaped stylus for use with strumming in the game, which players move across the touchscreen.[72] Guitar Hero: On Tour was developed by Vicarious Visions, who also ported the Guitar Hero games to Nintendo's Wii console.

A sequel, Guitar Hero On Tour: Decades, was released in November 2008, featuring music spanning four decades.[73] A third title in the series, Guitar Hero On Tour: Modern Hits, was announced following various rumors of its existence,[74][75] and was released in June 2009, featuring songs recorded since the year 2000.[63][76][77] Both games use the "Guitar Grip" controller, and allow two players to compete against each other using any version of the On Tour series, with songs being shared between versions.[73]

Band Hero was also ported to the Nintendo DS by Vicarious Visions, expanding the play to include vocals (through the DS microphone) and drumming. The drumming uses a special "drum skin" adapter designed for the Nintendo DS Lite to map the unit's face buttons to four drum pads. However, the peripheral is not compatible with the original Nintendo DS model or the Nintendo DSi. However, since the drum skin is not electronic but a rubber cover switch that duplicates certain buttons on the DS Lite, a player can simply press the buttons in time to play the drums. The game includes four-player local wireless play in a similar manner as Guitar Hero 5 allowing any combination of instruments to be used. The game has a set of 30 songs; some are from Band Hero and others are from several Guitar Hero games' set lists.

Guitar Hero: On Tour does not work on the Nintendo DSi and Nintendo 3DS because unlike the Nintendo DS, they do not have Game Boy Advance slots. Band Hero is limited to vocals and drums on the two consoles for the same reason.

Guitar Hero III Mobile was released for mobile phones in 2007 and 2008, and was developed by MachineWorks Northwest LLC.  The base version of the game includes 15 songs from both Guitar Hero II and Guitar Hero III, and has released a three-song add-on pack every month since January 2008.  The title has been downloaded by users one million times, with both Verizon and Hands-On Mobile claiming that over 250,000 songs are played a day on the platform.[78]
The two companies produced two other mobile-based Guitar Hero games; Guitar Hero III: Backstage Pass, released in July 2008, adds role-playing elements to manage the band's success in addition to the core rhythm game,[79] while the mobile version of Guitar Hero World Tour, released in December 2008, expands each included track for play on both lead guitar and drums, mimicking the expansion of the console series to the full band.[80]

Glu Mobile developed the mobile version of Guitar Hero 5, released in the last quarter of 2009.[81]


Activision and RedOctane also worked with Basic Fun, Inc. to produce Guitar Hero Carabiner, a handheld electronic game that features excerpts of several songs taken from the first two games.[82][83]


Activision and Konami, who had previously worked together to make sure that the Guitar Hero series meets with Konami's patents on music games, in conjunction with Raw Thrills, developed an arcade console version of the game, titled Guitar Hero Arcade, distributed to arcades in early 2009.  The game is completely based on the Guitar Hero III gameplay, but reducing some of the features such as the use of the Whammy bar, Star Power Button (Star Power may only be activated by lifting the Guitar) and Practice Modes, but keeping the ability to download new songs for the cabinet from the Internet.[84] The arcade game has come under some scrutiny by the American Society of Composers, Authors and Publishers (ASCAP), who believe the use of the game in arcades is equivalent to "public performances" and seek additional fees to be paid by operators of the game.[85]

The double release of Guitar Hero 5 and Band Hero in 2009 were the last of the series' games to be released on PlayStation 2.[86] It was expected that the 2010 entry for Guitar Hero, Warriors of Rock, would be the final entry developed by Neversoft, based on claims that Neversoft would be letting go of its Guitar Hero division, with Vicarious Visions likely poised to take over future development.[14] Further industry rumors pointed at the closure of RedOctane Studios and Underground Development (the development studio for Guitar Hero: Van Halen) as further results from the scaling-back; Activision moved the controller hardware development within their own division to continue to support the series,[87] with RedOctane founders Kai and Charles Huang remaining with Activision.[88] A week prior to these announcements, the Guitar Hero division CEO at Activision, Dan Rosensweig, left the company, leading to some speculation on whether Rosensweig's departure influenced these changes.[89] Activision and RedOctane had trademarked the titles "Guitar Villain", "Drum Villain", "Keyboard Hero" and "Sing Hero".[90][91] RedOctane originally trademarked the titles "Drum Hero" and "Band Hero", but the work performed towards the Drum Hero title was eventually folded into the gameplay for Guitar Hero World Tour, and Band Hero became its own game.[48] Later, as of October 2009, Activision reapplied for a Drum Hero trademark.[92] Pi Studios, which had previously helped to port Rock Band to the Wii, had started work on the karaoke title Sing Hero before Activision cancelled its development.[93] Dance Hero was also said to have been in development as of 2011.[94]

Dave Mustaine, frontman for Megadeth, stated he had been in talks with Activision and Neversoft for a Guitar Hero-related product.[95] It was later revealed that Mustaine was working with Activision for music in Guitar Hero: Warriors of Rock, including an original track ("Sudden Death") recorded specifically for the game.

Two Guitar Hero products that were announced but never released were a Red Hot Chili Peppers-themed title[96] and a PlayStation Portable title that would have featured a drum component.[97]

The core gameplay of the Guitar Hero games is a rhythm video game similar to Konami's GuitarFreaks[39] and to a lesser extent Harmonix's previous music games such as Frequency and Amplitude. The guitar controller is recommended for play, although a standard console controller can be used instead.[98][99] However, the guitar controller has been required for play ever since the inclusion of drum and vocal parts in the series. The game supports toggling the handedness of the guitar, allowing both left-handed and right-handed players to utilize the guitar controller.[98][99]

While playing the game, an extended guitar neck is shown vertically on the screen (the frets horizontal), often called the "note highway", and as the song progresses, colored markers or "gems" indicating notes travel down the screen in time with the music; the note colors and positions match those of the five fret keys on the guitar controller. Once the note(s) reach the bottom, the player must play the indicated note(s) by holding down the correct fret button(s) and hitting the strumming bar in order to score points. Success or failure will cause the on-screen Rock Meter to change, gauging the current health of the performance (denoted by red, yellow, and green sections). Should the Rock Meter drop below the red section, the song will automatically end, with the player booed off the stage by the audience. Successful note hits will add to the player's score, and by hitting a long series of consecutive successful note hits, the player can increase their score multiplier. There is a window of time for hitting each note, similar to other rhythm games such as Dance Dance Revolution, but unlike these games, scoring in Guitar Hero is not affected by accuracy; as long as the note is hit within that window, the player receives the same number of points.[98][99]

Selected special segments of the song will have glowing notes outlined by stars: successfully hitting all notes in this series will fill the "Star Power Meter". The Star Power Meter can also be filled by using the whammy bar during sustained notes within these segments. Once the Star Power Meter is at least half full, the player can activate "Star Power" by pressing the select button or momentarily lifting the guitar into a vertical position. When Star Power is activated, the scoring multiplier is doubled until Star Power is depleted. The Rock Meter also increases more dramatically when Star Power is activated, making it easier for the player to make the Rock Meter stay at a high level. Thus, Star Power can be used strategically to play difficult sections of a song that otherwise might cause the player to fail.[98][99] In the earlier entries of the series (up until Guitar Hero: Aerosmith), activating Star Power meant that players could not accrue more Star Power until the Star Power meter was fully drained and the effect ended. Starting with Guitar Hero: World Tour, more Star Power can be collected even if the effect is active by completing more Star Power phrases, extending the Star Power's duration by doing so. When playing in cooperative play (with a bassist/rhythm guitarist in Guitar Hero II through Guitar Hero: Aerosmith or as a band in Guitar Hero: World Tour), Star Power is shared between all the players and activation of Star Power is dependent on all players simultaneously activating it.

Notes can be a single note, or composed of two to five notes that make a chord. Both single notes and chords can also be sustained, indicated by a colored line following the note marker; the player can hold the sustained note(s) keys down for the entire length for additional points. During a sustained note, a player may use the whammy bar on the guitar to alter the tone of the note. Also, regardless of whether sustains are hit early or late, if the fret is held for the full duration of the hold, the game will always award the same amount of score increase for the note. In addition, the games support virtual implementations of "hammer-ons" and "pull-offs", guitar-playing techniques that are used to successfully play a fast series of notes by only changing the fingering on the fret buttons without having to strum each note. Sequences where strumming is not required are indicated on-screen by notes with a white outline at the top of the marker instead of the usual black one, with Guitar Hero III: Legends of Rock adding a white-glowing effect to make these notes clearer.[98][99] Guitar Hero World Tour features transparent notes that are connected by a purple outline; players may either simply tap the correct fret for these notes without strumming or utilize a touchpad on World Tour's guitar controller to mimic the slide technique. In addition, notes can now be played while a sustained note is being played. World Tour also adds an open string note for bass players, represented by a line across the fret instead of any note gems, that is played by strumming without holding down any fret buttons (the sixth installment, Warriors of Rock, features an open note sustain for bass instruments as well).

Guitar Hero World Tour introduced drums and vocal tracks in addition to lead and bass guitar.  Drum tracks are played similar to guitar tracks; the player must strike the appropriate drum head or step down on the bass drum pedal on the controller when the note gems pass the indicated line.  Certain note gems, when using a drum controller that is velocity-sensitive, are "armored", requiring the player to hit the indicated drum pad harder to score more points.  Vocal tracks are played similar to games such as Karaoke Revolution where the player must match the pitch and the pacing of the lyrics to score points. Guitar Hero 5 allows players to create a band of up to four players using any combination of instruments.[100]

While the song is playing, the background visuals feature the players' chosen avatar, along with the rest of the band performing in one of several real and fictional venues. The reaction of the audience is based on the performance of the player judged by the Rock Meter. Guitar Hero II added special lighting and other stage effects that were synchronized to the music to provide a more complete concert experience.[98][99] The games developed by Neversoft feature a simple storyline, usually about a band's quest for fame, which is told through animations played throughout the game. These animations were created by Chris Prynoski and his studio, Titmouse, Inc., who have also done animations for the animated show Metalocalypse.[101]

The main mode of play in the Guitar Hero games is Career Mode, where the player and in-game band travel between various fictional performance arenas and perform sets of four to six songs. It is by completing songs in this mode that the songs are unlocked for play across the rest of the game. Players can choose their on-stage character, their guitar of choice, and the venue in which they wish to play. In this mode, the player can earn money from his/her performances that is redeemable at the in-game store, where bonus songs, additional guitars and finishes, your characters clothing and bonus content can be unlocked. Quick Play mode is a quicker method of playing songs, as it allows the player to select a track and difficulty, selecting the character, venue, and guitar and guitar skin for the player based on the song chosen. After successfully completing a song, the player is given a score, a percentage of how many notes they hit and a rating from three to five stars, and two in rare cases depending on his/her final score on the song, with money being awarded in Guitar Hero World Tour.[98][99]

The games have also added multiplayer modes.  Cooperative modes allow two players to play lead and either bass or rhythm guitar on the same song, working together towards the same score.  A competitive Face-Off mode allows two players to play against each other at different difficulty levels, each attempting to earn the best score on a song. Each player plays different portions of the song. There is also a Pro Face-Off mode, where two players battle at the same difficulty level. Unlike standard Face-off, each player attempts to play all of the notes in a song, while still trying to earn the highest score. In Guitar Hero World Tour this was advanced on, as players could play a Pro Face-Off game against each other on any difficulty level, the lower your difficulty, the more points were awarded so a player on a low difficulty could potentially beat a player on a more challenging difficulty. Guitar Hero III introduced Boss Battles, in which two players face off against each other, attempt to collect "distractions" to throw at their opponent, trying to make them fail.  With Guitar Hero World Tour, up to four players can play cooperatively on lead and bass guitar, drums, and vocals, while a total of eight players can compete in a Battle of the Bands.  The Xbox 360, PlayStation 3, and Wii versions of the games support multiplayer modes over their respective network services.

The four difficulty levels for each song afford the player a learning curve in order to help him/her progress in skill. The first difficulty level, Easy, only focuses on the first three fret buttons while displaying a significantly reduced number of notes for the player to play. Medium introduces the fourth (blue) fret button, and Hard includes the final fret button while adding additional notes. The addition of the orange fret button forces players to move their fingers up and down the neck. Expert does not introduce any other frets to learn, but adds more notes in a manner designed to challenge the player and to simulate the player's hands to move in a sequence similar to a real guitar. A difficulty added in World Tour is Beginner, which only requires the player to strum to the basic rhythm; holding the fret buttons becomes unnecessary.[98][99] Another new difficulty only for drums was added to Metallica known as Expert+, which uses the double bass pedal.

Guitar Hero: Warriors of Rock is the sixth installment in the franchise and introduced a new take on the Career mode of previous games. Rather than being a quest for fame and glory with the band travelling through different venues, Warriors of Rock features the "Quest Mode" as the primary campaign mode. Quest Mode tells the story of an ancient warrior who was defeated by a powerful monster and his mystical guitar was lost. The player must amass a team of rockers to help recover this guitar and defeat the monster (called "The Beast"). As the player progresses through the mode, the rockers joining them will transform based on the number of stars earned from songs played. These transformations will empower the player with extra abilities in a song such as constant score multipliers or Star Power bonuses. These abilities are each unique to the individual rockers and by using them effectively, it is possible now to earn up to forty stars for a single song.

When playing through Career mode or in other parts of the Guitar Hero games, the player has the option to select one of several pre-created avatar characters, who will be shown performing on stage as the player attempts a song, but otherwise has no effect on the gameplay.  A certain number of characters are available at the start of the game, but the player must spend in-game money earned by successful performances to unlock other characters.  Many of the characters reappear throughout the series, with the character roster changing as new characters are added or removed. Standby characters that have appeared in nearly all the games include the metalhead Axel Steel, extreme/Viking/thrash metalhead Lars Ümlaut, punk rocker Johnny Napalm, gothic rocker Pandora, alternative rocker Judy Nails, and hard rocker Casey Lynch. The developers utilized these characters in more detail within Warriors of Rock, where each was given a unique setlist and venue based on their musical style, as well as a unique power within the game's Quest mode.

Several games in the series feature caricatures of celebrity artists, such as Slash, Tom Morello and Bret Michaels  in Guitar Hero III, Ozzy Osbourne, Ted Nugent and Jimi Hendrix in World Tour, Kurt Cobain in Guitar Hero 5, and Taylor Swift and the band No Doubt in Band Hero. The band-specific games, Aerosmith, Metallica, and Van Halen also feature the members of the respective bands. However, in late 2009, both Courtney Love and the members of No Doubt sought legal action against Activision for the misuse of their in-game characters singing or performing songs by other artists, which the musicians believe fell outside of their contract.

The ability for the players to create their own avatars was added in Guitar Hero World Tour, and was based on Neversoft's existing character creation tools from the Tony Hawk series. Later games on the Xbox 360 and Wii allowed players to use the respective console's avatars as members of the band. In addition to unlocking characters, in-game money can be used to buy clothing, accessories and instruments that they are seen playing with.  The guitars can also be customized with special finishes purchasable through the in-game store.  Guitar Hero World Tour includes the ability to fully customize any component of the guitar.  The in-game store in the series is also used to unlock bonus songs or special videos with interviews about the game or with the artists involved.

Most of the games in the Guitar Hero series feature a selection of songs ranging from the 1960s to present day rock music from both highly successful artists and bands and independent groups.  Guitar Hero Encore: Rocks the 80s features songs primarily from the 1980s, while  Guitar Hero: Aerosmith, Metallica, and Van Halen feature music from the respective bands and groups that inspired or worked with the bands. Songs with profanities have been censored.

Many of the Guitar Hero games developed for the recent generation of consoles (Xbox 360, PlayStation 3, and Wii) support downloadable content, allowing players to purchase new songs to play in the respective titles.  Songs each cost approximately $2 through the various online stores for the console's platform.  Prior to Guitar Hero 5, downloadable content for earlier games will not work in other games in the series, save for songs from Metallica's Death Magnetic, which were available for Guitar Hero III, World Tour, and Metallica.[102] Existing World Tour downloadable content for World Tour will be forward-compatible with Guitar Hero 5, Band Hero[100] and Guitar Hero Warriors of Rock, and for a small fee, some songs from both Guitar Hero World Tour and Guitar Hero Smash Hits can be exported to both Guitar Hero 5 and Band Hero, limited by music licensing.[103] Activision has also stated that they are considering a monthly subscription service to deliver downloadable content to user for future games.[104] Guitar Hero World Tour introduced a music creation mode that will allow players to create and share songs (excluding vocals) via the "GHTunes" service, which was also used in all other Guitar Hero games and Band Hero since its inclusion. The creation tools were improved with Guitar Hero 5 and Band Hero to allow longer songs and other means of generating songs in real-time.

In the first two games and the 2007 expansion Guitar Hero Encore: Rocks the 80s, the majority of the songs on the main career mode set lists are covers of the original song; for example, a song may be presented as "Free Bird as made famous by Lynyrd Skynyrd".[105] Guitar Hero III: Legends of Rock introduces a much larger range of original recordings, and World Tour featured a setlist that contained all master recordings.  The covers throughout the games are mostly recreated by WaveGroup Sound who has worked before to create songs for Beatmania, Dance Dance Revolution, and Karaoke Revolution,[106] making small changes to the guitar portions to make them more adaptable for gameplay.[107]  Almost all of the unlockable bonus songs are songs performed by the original artist for the game (the only exception is the song "She Bangs the Drums" by The Stone Roses, which is featured in Guitar Hero III: Legends of Rock).

Prior to the release of Guitar Hero III: Legends of Rock, Activision worked with the iTunes Store to provide more than 1300 tracks of Guitar Hero-related music across more than 20 compilations, including most of the tracks from the games in the series, called "Guitar Hero Essentials". These compilations, such as "Killer Guitar Solos" and "Guitar Anthems of the '80s", include songs related to but not contained within the Guitar Hero series. Dusty Welch of RedOctane stated, "Where there's music, there's Guitar Hero, and with iTunes, we are able to provide fans with a central location for downloading their favorite rock anthems."[108] Following the merger of Activision and Blizzard, the new company announced plans to create an alternative to iTunes based on the Guitar Hero brand that would allow for downloading songs and their associated note tracks for the Guitar Hero games.[109]

Games in the Guitar Hero series have been generally well received by critics. The initial games were highly praised by reviewers.[153][154][155][156] Neversoft's first entry to the series, Guitar Hero III, was considered to be too difficult, with many difficult songs presenting players with "walls of notes"; the developers later acknowledged this.[157][158][159] Subsequent efforts in Guitar Hero: Aerosmith and Guitar Hero World Tour were seen to have some improvements, with Guitar Hero: Metallica considered to be a well-polished title and, at that time, the best Guitar Hero title Neversoft has produced.[160][161] Guitar Hero 5's improvements toward social gameplay were complemented by reviewers and considered a further improvement upon the series. Entertainment Weekly put it on its end-of-the-decade, "best-of" list, saying, "An addictive videogame provides the illusion of musical mastery for even the least gifted:. How do you get to Carnegie Hall? Tap, tap, tap."[162]

Upon release, the first game was seen as an unexpected hit, earning over US$45 million with about 1.5 million copies sold.[163][164][165] Guitar Hero II was significantly more financially successful, with over 1.3 million copies sold and sales over US$200 million.[163][166] Guitar Hero III, according to Activision, was the first single video game to sell more than US$1 billion at retail, with nearly 3.5 million copies sold during the first seven months of 2008.[167][168] World Tour continued the series' high sales records with 3.4 million units sold in the United States during 2008.[169] More than 60 million downloadable tracks have been purchased across the series as of February 2010.[13] Both Guitar Hero III and World Tour were listed on a March 2011 list from the NPD Group of top-grossing games in unadjusted sales in the United States since 1995; Guitar Hero III tops the list with total sales of $830.9 million.[170]

Overall, the Guitar Hero series has sold more than 25 million units worldwide, earning US$2 billion at retail. Activision claimed the series to be the 3rd largest game franchise in 2009 after the Mario and Madden NFL franchises.[171][172][173]

The Guitar Hero series has made a significant cultural impact, becoming a "cultural phenomenon".[43][174] The series has helped to rekindle music education in children, influenced changes in both the video game and music industry, has found use in health and treatment of recovering patients, and has become part of the popular culture vernacular. Several journalists, including 1UP.com,[175] Wired,[176] G4TV,[177] the San Jose Mercury News,[178] Inc.,[179] The Guardian,[180] and Advertising Age,[181] considered Guitar Hero to be one of the most influential products of the first decade of the 21st century, attributing it as the spark leading to the growth of the rhythm game market, for boosting music sales for both new and old artists, for introducing more social gaming concepts to the video game market, and, in conjunction with the Wii, for improving interactivity with gaming consoles. Guitar Hero has seen a revitalization in the form of a software clone called Clone Hero, as observed in 2020.[182][183][184]

Sony's PlayStation 3 console has no compatibility with the PlayStation 2 Guitar Hero controller on the system. While Guitar Hero and Guitar Hero II are fully backward-compatible through the hardware PlayStation 2 emulation in the initial North American release of the console, it was impossible at launch to use the guitar controller to play either game. Kai Huang, of RedOctane, states that they are "working on that with Sony right now – looking at how we can get all the PlayStation 2 guitars that are out there, and all the owners of them, to use them on the PlayStation 3."[185] Nyko, an accessories company, was poised to make a special PlayStation 2 controller adapter for the PlayStation 3, but put the product on hold due to technical difficulties. Tac, another accessories company, also made a PlayStation 2 controller adapter for a PlayStation 3 game console so players could use their Guitar Hero guitar controllers that were made for the PlayStation 2 with a PlayStation 3.[186] However, the May 2007 PlayStation 3 V1.80 system update has made the guitar controller compatible with generic PlayStation 2 controller to USB adapters when playing Guitar Hero and Guitar Hero II.[187] In addition, Pelican Accessories has released a special controller adapter that supports both games, including the ability to switch the handedness of the guitar.[188]

Gibson Guitar Corporation, whose guitar likenesses have appeared in the Guitar Hero series from the first game to Guitar Hero Aerosmith, informed Activision on January 7, 2008, that it believed the games infringe its U.S. patent 5,990,405. Gibson claimed that this covers technology that simulates a concert performance via pre-recorded audio and a musical instrument. In response, Activision filed a suit seeking a declaration that it was not in violation of the Gibson patent; Activision also asserted that Gibson had given an implied license by waiting to assert the patent and that the patent was invalid.[189]  On March 17, 2008, Gibson sued six retailers (GameStop, Amazon.com, Wal-Mart, Target, Toys "R" Us and Kmart) for selling Guitar Hero products.[190][191]  Subsequently, on March 21, 2008, Gibson also filed a lawsuit against EA, MTV, and Harmonix over their game Rock Band also for violation of its patent, to which a Harmonix spokesperson stated that Gibson's claims are "completely without merit".[192]  Activision lawyer Mary Tuck stated in their legal filings that they believe that Gibson initiated the lawsuit due to the fact that "Activision was not [interested] in renewing the License and Marketing Support Agreement" with Gibson Guitars.[193]
In February 2009, the United States District Court for the Central District of California ruled against Gibson in their case against Activision, stating that the controllers are not musical instruments but "toys that represent other items", and that Gibson's patent only covers instruments that send out analog signals.[194][195] Activision and Gibson settled the suit following this ruling.[196]

Activision, through John Devecka,[197] owns all of Devecka Enterprises' US and international patents[198][199][200] that deal with music games. All patents issued by the USPTO are presumed valid.[201][202]

In February 2010, Activision was sued by the Patent Compliance Group (PCG) for releasing Guitar Hero products with false patent claims, with the PCG asserting that games like Guitar Hero 5 and Band Hero were marked with up to 10 patents that are not used within the games along with several other improper patent pending claims. PCG claimed that "Acts of false marketing deter innovation and stifle competition in the marketplace." PCG's qui tam lawsuit was seeking up to $500 per unit sold if Activision was found liable.[203] However, by June 2010, PCG had withdrawn the case without prejudice.[204]

Aside from being a defendant, Activision has also acted as a plaintiff. In 2007, the company issued a cease and desist letter to Nicholas Piegdon, the developer of an open source piano teaching program called Piano Hero, demanding he change the name to something less similar to that of the Guitar Hero series. He complied and renamed the software Synthesia.[205]

Many critics believed that the number of releases of Guitar Hero games was "milking" the brand name and oversaturating the market.[12][206][207] PaRappa the Rapper creator Masaya Matsuura stated that the video game market was growing stale and needed to move beyond games that simply challenge the player to mimic the playing of licensed music.[208] Ryan Geddes of IGN stated that he "hit the wall with play-along music games", and challenged the game makers to explore other ways to combine music and video games.[209] Analysts stated that such games must continue to innovate instead of just providing more songs in order to prevent "genre fatigue".[210] Jesse Divnich of Electronic Entertainment and Design Research commented that, much like Dance Dance Revolution, Guitar Hero and other music games explosively grew initially due to significant new features from other games but have become stagnant due to focusing on content over features, and suggested that for the genre to continue to grow, they must look to incremental changes as done with the first-person shooter genre.[211] Former CEO for RedOctane, Kelly Sumner, believed that Activision "abused" the series, as "they tried to get too much out of the franchise too quickly".[212]

The series has also been criticized for its release model in contrast to the Rock Band series, causing some players to hold contempt towards Activision.[213][214] Harmonix considered the Rock Band series as a "music platform", and  supported it with downloadable content and the ability to import songs from its games and expansions into most other games of the series.[215] Critics argued that Guitar Hero should have been doing the same, either through releasing expansions that could be incorporated into the main games of the series, or by issuing the songs as downloadable content.[216][217][218][219] The release of Guitar Hero: Smash Hits, reworking older songs from the series to full four-instrument band support but otherwise adding no additional material, was called "the definition of 'milking'" by reviewers, with no observable technical limitation as to why the songs could not be added as downloadable content.[213][217][220][221][222] Ars Technica recognized that licensing issues might have limited when songs from one single game could be played in others of the series (such as the case for The Beatles: Rock Band), but that such cross-compatibility should have been a high priority for rhythm games.[223] Furthermore, some expansions were praised for the additional content beyond the note-matching gameplay; Guitar Hero: Metallica is considered to be one of the series' best works to be developed by Neversoft in part due to the care that the developers took with imaging the band and the available extras for the game.[216] Activision later revealed that both Guitar Hero 5 and Band Hero would support playing songs from both Guitar Hero World Tour (both on-disc and downloadable content) and Guitar Hero Smash Hits, with music licensing being the only limiting factor on which songs could be made forward-compatible.[103]

The large number of Guitar Hero and Rock Band titles on the market is considered to be partially responsible for the sharp decline of music game sales in the latter half of 2009, along with the effects of the late-2000s recession.[10][12][206][224] The market for rhythm games was $1.4 billion in 2008, but dropped to $700 million (a 50% decrease) in 2009 even though more titles were available that year.[225] Former Neversoft project director Brian Bright noted that at one point in 2009, they were responsible for the release of three games that year (Guitar Hero 5, Metallica, and Band Hero) and supporting other studios for the development of two additional games, causing the studio to lose focus both in development and marketing efforts.[226] According to Bright, sales of all the Guitar Hero games released in 2009 totaled the number of sales of the 2008 title World Tour, demonstrating the dilution of the marketing.[226] Though Activision had originally planned on tripling the offerings of the Guitar Hero series in 2010,[227][228][229] the company readjusted their plans, reducing the number of offerings and focusing more on selling digital downloadable content for the series.[230] Only two titles, Guitar Hero: Warriors of Rock and DJ Hero 2 were released in 2010,[13] both scheduled for the "back half of 2010".[231] Analysts predicted that the market would evolve to support a smaller number of titles each year, averaging at a "healthy" value in the range of $500–600 million in revenues annually.[225] Kotick believed that part of the downfall of Guitar Hero was due to Activision's introduction of DJ Hero, which they gave too much focus and left the core Guitar Hero games without the "nourishment and care" needed to continue to innovate in the series.[22]

Activision Publishing chief executive Mike Griffith, in response to questions about Activision's approach to the Guitar Hero market, noted that Guitar Hero continues to outsell the Rock Band series in both number of sales and revenue, with consumers continuing to buy the separate games on the market, and considered the market acceptance of the multiple games as validation for their model.[232] Activision also claimed to have conducted research that showed there was solid consumer interest in products such as DJ Hero and Band Hero, each of which had a markedly different focus than the main installments of the series; series spokesperson Eric Hollreiser suggested the experimental approach would attract players who wanted different styles of music or a different peripheral with which to relate to the music.[210] Regardless, after releasing 25 different SKUs (between games and bundle packages) in 2009, Activision opted to reduce that number to 10 in 2010, recognizing the music game genre was not as profitable as it once was.[13] Activision later opted to suspend future development of the series in early 2011 citing weak sales in the rhythm game genre, a move that many journalists attributed to Activision's earlier oversaturation.[233][234][235][236]Gun violence is a term of political, economic and sociological interest referring to the tens of thousands of annual firearms-related deaths and injuries occurring in the United States.[2]

In 2016, a U.S. male aged 15–24 was 70 times more likely to be killed with a gun than a French male or British male.[3]

In 2022, up to 100 daily fatalities and hundreds of daily injuries were attributable to gun violence in the United States.[4] In 2018, the most recent year for which data are available, the Centers for Disease Control and Prevention's (CDC) National Center for Health Statistics reported 38,390 deaths by firearm, of which 24,432 were suicides.[5][6] The national rate of firearm deaths rose from 10.3 people for every 100,000 in 1999 to 11.9 people per 100,000 in 2018, equating to over 109 daily deaths (or about 14,542 annual homicides).[7][8][9][10] In 2010, there were 19,392 firearm-related suicides, and 11,078 firearm-related homicides in the U.S.[11] In 2010, 358 murders were reported involving a rifle while 6,009 were reported involving a handgun; another 1,939 were reported with an unspecified type of firearm.[12] In 2011, a total of 478,400 fatal and nonfatal violent crimes were committed with a firearm.[13] In 2023, 350 shootings occurred in K-12 schools with an additional 30 on college campuses. This marked the highest number of shootings recorded on school grounds[14]

According to a Pew Research Center report, gun deaths among America's children rose 50% from 2019 to 2021.[15]

Firearms are overwhelmingly used in more defensive scenarios (self-defense and home protection) than offensive scenarios in the United States.[16][17] In 2021, The National Firearms Survey, currently the nation's largest and most comprehensive study into American firearm ownership, found that privately owned firearms are used in roughly 1.7 million defensive usage cases (self-defense from an attacker/attackers inside and outside the home) per year across the nation, compared to the Centers for Disease Control and Prevention's (C.D.C.) report of 20,958 homicides in that same year.[18][19][20]

Legislation at the federal, state, and local levels has attempted to address gun violence through methods including restricting firearms purchases by youths and other "at-risk" populations, setting waiting periods for firearm purchases, establishing gun buyback programs, law enforcement and policing strategies, stiff sentencing of gun law violators, education programs for parents and children, and community outreach programs.

Some medical professionals express concern regarding the prevalence and growth of gun violence in America, even comparing gun violence in the United States to a disease or epidemic.[21] Relatedly, recent polling suggests up to 26% of Americans believe guns are the number one national public health threat.[22]

The Congressional Research Service in 2009 estimated that among US population of 306 million people, there were 310 million firearms in the U.S., not including military armaments. Of these, 114 million were handguns, 110 million were rifles, and 86 million were shotguns.[25][26]
Accurate figures for civilian gun ownership are difficult to determine.[27] The percentage of Americans and American households who claim to own guns has been in long-term decline, according to the General Social Survey poll. It found that gun ownership by households may have declined from about half, in the late 1970s and early 1980s, down to 32% in 2015. The percentage of individual owners may have declined from 31% in 1985 to 22% in 2014.[28]

Gun ownership figures are generally estimated via polling, by such organizations as the General Social Survey (GSS), Harris Interactive, and Gallup. There are significant disparities in the results across polls by different organizations, calling into question their reliability.[29] In Gallup's 1972 survey, 43% reported having a gun in their home. GSS's 1973 survey resulted in 49% reporting a gun in the home. In 1993, Gallup's poll results were 51%. GSS's 1994 poll showed 43%.[30] In 2012, Gallup's survey showed 47% of Americans reporting having a gun in their home,[31] while the GSS in 2012 reports 34%.[30] In 2018 it was estimated that U.S. civilians own 393 million firearms,[32] and that 40% to 42% of the households in the country have at least one gun. However, record gun sales followed in the subsequent years.[33][34][35]

In 1997, estimates were about 44 million gun owners in the United States. These owners possessed around 192 million firearms, of which an estimated 65 million were handguns.[36] A National Survey on Private Ownership and Use of Firearms (NSPOF), conducted in 1994, estimated that Americans owned 192 million guns: 36% rifles, 34% handguns, 26% shotguns, and 4% other types of long guns.[36] Most firearm owners owned multiple firearms, with the NSPOF survey indicating 25% of adults owned firearms.[36] Throughout the 1970s and much of the 1980s, the estimated rate of gun ownership in the home ranged from 45 to 50%.[30] After highly publicized mass murders, it is consistently observed that there are rapid increases in gun purchases and large crowds at gun vendors and gun shows, due to fears of increased gun control .[37][38][39][40][41]

Gun ownership rates vary across geographic regions, ranging from 2004 estimates of 25% in the Northeastern United States to 60% in the East South Central States.[42] A 2004 Gallup poll estimated that 49% of men reported gun ownership, compared to 33% of women, while 44% of whites owned a gun, compared to 24% of nonwhites.[43] An estimated 56% of those living in rural areas owned a gun, compared to 40% of suburbanites and 29% of those in urban areas.[43] Approximately 53% of Republicans owned guns, compared to 36% of political independents and 31% of Democrats.[43]

One criticism of the GSS survey and other proxy measures of gun ownership, is that they do not provide adequate macro-level detail to allow conclusions on the relationship between overall firearm ownership and gun violence.[44] Gary Kleck compared various survey and proxy measures and found no correlation between overall firearm ownership and gun violence.[45][46] Studies by David Hemenway and his colleagues, which used GSS data and the fraction of suicides committed with a gun as a proxy for gun ownership rates, found a strong positive correlation between gun ownership and homicide in the United States.[47][48] A 2006 study by Philip J. Cook and Jens Ludwig, also using the percentage of suicides committed with a gun as a proxy, found that gun prevalence correlated with increased homicide rates.[49]

The effectiveness and safety of guns used for personal defense is debated. Studies place the instances of guns used in personal defense as low as 65,000 times per year, and as high as 2.5 million times per year. Under President Bill Clinton, the Department of Justice conducted a survey in 1994 that placed the usage rate of guns used in personal defense at 1.5 million times per year, based on an extrapolation from 45 survey respondents reporting using a firearm for self-defense, but noted this was likely to be an overestimate due to the low sample size.[46] A May 2014 Harvard Injury Control Research Center (HICRC) survey of 150 firearms researchers found that only 8% of them agreed that 'In the United States, guns are used in self-defense far more often than they are used in crime'.[50]

HICRC random-respondent national surveys were conducted in 1996 and 1999 to investigate the use of guns in self-defense. Survey participants were asked open-ended questions about defensive gun use incidents and detailed questions about both gun victimization and self-defense gun use. Self-reported defensive gun use incidents were then examined by five criminal court judges, who were asked to determine whether these self-defense gun uses were likely to be legal. The surveys found that far more respondents reported having been threatened or intimidated with a gun, than having used a gun to protect themselves, even after having excluded many of these responses; and, a majority of the reported self-defense gun uses were rated by a majority of judges as probably illegal. This was true even when it was assumed that the respondent had a permit to own and carry the gun, and that the event was described honestly. The conclusion being from this report that most self-described 'defensive' gun uses, are gun uses in escalating arguments, and are both socially undesirable and illegal.[51][52]

Further studies by HICRC found the following: firearms in the home are used more often to intimidate intimates than to thwart crime;[53] gun use in self-defense is rare and not more effective at preventing injury than other protective actions;[54] and a study of hospital gun-shot appearances does not back up the claim of millions of defensive gun use, as virtually all criminals with a gunshot wound go to hospital;[55][56] with virtually all having been shot whilst the victim of crime and not shot whilst offending.[57][51]

Between 1987 and 1990, David McDowall et al. found that guns were used in defense during a crime incident 64,615 times annually (258,460 times total over the whole period).[58] This equated to two times out of 1,000 criminal incidents (0.2%) that occurred in this period, including criminal incidents where no guns were involved at all.[58] For violent crimes, assault, robbery, and rape, guns were used 0.8% of the time in self-defense.[58] Of the times that guns were used in self-defense, 71% of the crimes were committed by strangers, with the rest of the incidents evenly divided between offenders that were acquaintances or persons well known to the victim.[58] In 28% of incidents where a gun was used for self-defense, victims fired the gun at the offender.[58] In 20% of the self-defense incidents, the guns were used by police officers.[58] During this same period, 1987 to 1990, there were 11,580 gun homicides per year (46,319 total),[59] and the National Crime Victimization Survey estimated that 2,628,532 nonfatal crimes involving guns occurred.[58]

McDowall's study for the American Journal of Public Health contrasted with a 1995 study by Gary Kleck and Marc Gertz, which found that 2.45 million crimes were thwarted each year in the U.S. using guns, and in most cases, the potential victim never fired a shot.[60] The results of the Kleck studies have been cited many times in scholarly and popular media.[61][62][63][64][65][66][67] The methodology of the Kleck and Gertz study has been criticized by some researchers[68][69] but also defended by gun-control advocate Marvin Wolfgang.[70]

Using cross-sectional time-series data for U.S. counties from 1977 to 1992, Lott and Mustard of the Law School at the University of Chicago found that allowing citizens to carry concealed weapons deters violent crimes and appears to produce no increase in accidental deaths. They claimed that if those states which did not have right-to-carry concealed gun provisions had adopted them in 1992, approximately 1,570 murders, 4,177 rapes, and over 60,000 aggravated assaults would have been avoided yearly.[71]

On the other hand, regarding the efficacy of laws allowing use of firearms for self-defense like stand your ground laws, a 2018 RAND Corporation review of existing research concluded that "there is moderate evidence that stand-your-ground laws may increase homicide rates and limited evidence that the laws increase firearm homicides in particular."[72] In 2019, RAND authors published an update, writing "Since publication of RAND's report, at least four additional studies meeting RAND's standards of rigor have reinforced the finding that "stand your ground" laws increase homicides. None of them found that "stand your ground" laws deter violent crime. No rigorous study has yet determined whether "stand your ground" laws promote legitimate acts of self-defense.[73]

In the U.S., most people who die of suicide use a gun, and most deaths by gun are suicides.

In 2010, there were 19,392 firearm-related suicides in the U.S.[11] In 2017, over half of the nation's 47,173 suicides involved a firearm.[78][79] In 2010, the U.S. Department of Justice reported that about 60% of all adult firearm deaths were by suicide, 61% more than deaths by homicide.[80] One study found that military veterans used firearms in about 67% of suicides in 2014.[81] Firearms are the most lethal method of suicide, with a lethality rate 2.6 times higher than suffocation, the second-most lethal method.[82] From 1999 to 2020, youth firearm suicide death rates increased on average 1.0% per year.[83] American Indian and Alaska Native adolescents had the highest absolute increase in firearm suicide (3.83 per 100 000 population), followed by White (0.69 per 100 000 population), Black (0.67 per 100 000 population), Asian and Pacific Islander (0.64 per 100 000 population), and Hispanic or Latino (0.18 per 100 000 population) individuals.[83]

In the United States, access to firearms is associated with an increased risk of suicide.[84] A 1992 case-control study in the New England Journal of Medicine showed an association between estimated household firearm ownership and suicide rates, finding that individuals living in a home where firearms are present are more likely to successfully commit suicide than those individuals who do not own firearms, by a factor of 3 or 4.[2][85] A 2006 study by researchers from the Harvard School of Public Health found a significant association between changes in estimated household gun ownership rates and suicide rates in the United States among men, women, and children.[86]

A 2007 study by the same research team found that in the United States, estimated household gun ownership rates were strongly associated with overall suicide rates and gun suicide rates, but not with non-gun suicide rates.[87] A 2013 study reproduced this finding, even after controlling for different underlying rates of suicidal behavior by states.[88] A 2015 study also found a strong association between estimated gun ownership rates in American cities and rates of both overall and gun suicide, but not with non-gun suicide.[89] Correlation studies comparing different countries do not always find a statistically significant effect.[90]: 30 

A 2016 cross-sectional study showed a strong association between estimated household gun ownership rates and gun-related suicide rates among men and women in the United States. The same study found a strong association between estimated gun ownership rates and overall suicide rates, but only in men.[91] During the 1980s and early 1990s, there was a strong upward trend in adolescent suicides with guns[92] as well as a sharp overall increase in suicides among those age 75 and over.[93] A 2018 study found that temporary gun seizure laws were associated with a 13.7% reduction in firearm suicides in Connecticut and a 7.5% reduction in firearm suicides in Indiana.[94]

David Hemenway, professor of health policy at Harvard University's School of Public Health, and director of the Harvard Injury Control Research Center and the Harvard Youth Violence Prevention Center, stated

Differences in overall suicide rates across cities, states and regions in the United States are best explained not by differences in mental health, suicide ideation, or even suicide attempts, but by availability of firearms. Many suicides are impulsive, and the urge to die fades away. Firearms are a swift, lethal method of suicide with a high case-fatality rate.[95]
There are over twice as many gun-related suicides as gun-related homicides in the United States.[96] Firearms are the most popular method of suicide due to the lethality of the weapon. 90% of all suicides attempted using a firearm result in a fatality, as opposed to less than 3% of suicide attempts involving cutting or drug-use.[88] The risk of someone attempting suicide is 4.8 times greater if they are exposed to a firearm on a regular basis; for example, in the home.[97]

Unlike other high-income OECD countries, most homicides in the U.S. are gun homicides.[77] In the U.S. in 2011, 67 percent of homicide victims were killed using a firearm: 66 percent of single-victim homicides and 79 percent of multiple-victim homicides.[99] Between 1968 and 2011, about 1.4 million people died from firearms in the U.S. This number includes all deaths resulting from a firearm, including suicides, homicides, and accidents.[100]

In 2017, compared to 22 other high-income nations, the U.S. gun-related homicide rate was 25 times higher.[95] Although the US has half the population of the other 22 nations combined, among those 22 nations studied, the U.S. had 82 percent of gun deaths, 90 percent of all women killed with guns, 91 percent of children under 14 and 92 percent of young people between ages 15 and 24 killed with guns, with guns being the leading cause of death for children.[95] The ownership and regulation of guns are among the most widely debated issues in the US.

In 1993, there were seven gun homicides for every 100,000 people. By 2013, that figure had fallen to 3.6, according to Pew Research.[101]

The Centers for Disease Control reports that there were 11,078 gun homicides in the U.S. in 2010.[11] This is higher than the FBI's count.[12] The CDC stated there were 14,414 (or 4.4 per 100,000 population) homicides by firearm in 2018, and stated that there were a total of 19,141 homicides (5.8 per 100,000 population) in 2019.[102] Gun-related deaths among children in the U.S. in 2021 was 4,752, surpassing the record total seen during the first year of the pandemic, a new analysis of Centers for Disease Control and Prevention data found.[103]

The police chief of Washington, DC attributes the 203 homicides in 2022 to an influx of guns from out-of-town, marking the first time in nearly 20 years that the nation's capital exceeded the 200 homicide threshold in consecutive years. According to the Metropolitan Police Department, Washington last experienced such violence in 2002 and 2003, when it recorded 262 and 246 homicides, respectively. Property crime has decreased by 3% and violent crime decreased by 7% overall since 2021.[104][105]

In 2021, a little above 80% of all murders (20,958 out of 26,031) in the US involved a firearm‍— the highest percentage since at least 1968, the earliest year for which the CDC has online records. A little under 55% of all suicides (26,328 out of 48,183) in 2021 involved a gun, the highest percentage since 2001.[106]

In the 19th century, gun violence played a role in civil disorder such as the Haymarket riot.[107] Homicide rates in cities such as Philadelphia were significantly lower in the 19th century than in modern times.[108] During the 1980s and early 1990s, homicide rates surged in cities across the United States (see applicable graphs).[109] Handgun homicides accounted for nearly all of the overall increase in the homicide rate, from 1985 to 1993, while homicide rates involving other weapons declined during that time frame.[44]

The rising trend in homicide rates during the 1980s and early 1990s was most pronounced among lower income and especially unemployed males. Youths and Hispanic and African American males in the U.S. were the most represented, with the injury and death rates tripling for black males aged 13 to 17 and doubling for black males aged 18 to 24.[110][111] The rise in crack cocaine use in cities across the U.S. has been cited as a factor for increased gun violence among youths during this time period.[112][113][114] After 1993, gun violence in the United States began a period of dramatic decline.[115][116]

Prevalence of homicide and violent crime is higher in statistical metropolitan areas of the U.S. than it is in non-metropolitan counties;[117] the vast majority of the U.S. population lives in statistical metropolitan areas.[118] In metropolitan areas, the 2013 homicide rate was 4.7 per 100,000 compared with 3.4 in non-metropolitan counties.[119] More narrowly, the rates of murder and non-negligent manslaughter are identical in metropolitan counties and non-metropolitan counties.[120]
In 2005, in U.S. cities with populations greater than 250,000, the mean homicide rate was 12.1 per 100,000.[121] According to 2005 FBI statistics, the highest per capita rates of gun-related homicides in 2005 were in Washington, D.C. (35.4/100,000), Puerto Rico (19.6/100,000), Louisiana (9.9/100,000), and Maryland (9.9/100,000).[122] In 2017, according to the Associated Press, Baltimore broke a record for homicides.[123] [citation needed]

In 2005, the 17-24 age group was significantly over-represented in violent crime statistics, particularly homicides involving firearms.[124] In 2005, 17- to 19-year-olds were 4.3% of the overall population of the U.S.[125] but 11.2% of those killed in firearm homicides.[126] This age group accounted for 10.6% of all homicide offenses.[127] The 20-24-year-old age group accounted for 7.1% of the population,[125] but 22.5% of those killed in firearm homicides.[126] The 20-24 age group accounted for 17.7% of all homicide offenses.[127]

African American populations in the United States disproportionately represent the majority of firearms injury and homicide compared to other racial groupings.[128][8] Although mass shootings are covered extensively in the media, mass shootings in the United States account for only a small fraction of gun-related deaths.[129] Regardless, mass shootings occur on a larger scale and much more frequently than in other developed countries. School shootings are described as a "uniquely American crisis", according to The Washington Post in 2018.[130] Children at U.S. schools have active shooter drills.[131] According to USA Today in 2019, "About 95% of public schools now have students and teachers practice huddling in silence, hiding from an imaginary gunman."[131]
Those under 17 are not over-represented in homicide statistics. In 2005, 13-16-year-olds accounted for 6% of the overall population of the U.S., but only 3.6% of firearm homicide victims,[126] and 2.7% of overall homicide offenses.[127]

People with a criminal record are more likely to die as homicide victims.[110] Between 1990 and 1994, 75% of all homicide victims age 21 and younger in the city of Boston had a prior criminal record.[132] In Philadelphia, the percentage of those killed in gun homicides that had prior criminal records increased from 73% in 1985 to 93% in 1996.[110][133] In Richmond, Virginia, the risk of gunshot injury is 22 times higher for those males involved with crime.[134]

It is significantly more likely that a death will result when either the victim or the attacker has a firearm.[135][136] The mortality rate for gunshot wounds to the heart is 84%, compared to 30% for people who suffer stab wounds to the heart.[137]

In the United States, states with higher gun ownership rates have higher rates of gun homicides and homicides overall, but not higher rates of non-gun homicides.[138][139][140] Higher gun availability is positively associated with homicide rates.[141][142][143]

Some studies suggest that the concept of guns can prime aggressive thoughts and aggressive reactions. An experiment by Berkowitz and LePage in 1967 examined this "weapons effect." Ultimately, when study participants were provoked, their reaction was substantially more aggressive when a gun was visibly present in the room, in contrast with a more benign object like a tennis racket.[144] Other similar experiments like those conducted by Carson, Marcus-Newhall and Miller yield similar results.[145] Such results imply that the presence of a gun in an altercation could elicit an aggressive reaction, which may result in homicide.[144][145]

In 2023, the U.S. ranked 4th out of 34 developed nations in rate of homicides committed with a firearm, according to Organisation for Economic Co-operation and Development (OECD) data.[citation needed] Mexico, Turkey, and Estonia are ranked ahead of the U.S. in incidence of homicides. However, according to comprehensive research by the University of Sydney, the firearm-related homicide rates in Estonia and Turkey are both below the US, at 0.78 in Turkey and 0 in Estonia, while being 5.9 in the US, with Estonia registering zero in 2015.[147]

In 2016, a U.S. male aged 15–24 was 70 times more likely to be killed with a gun than his counterparts in any of the other eight largest industrialized nations in the world (the G8).[3] In 2013, in a broader comparison of 218 countries, the U.S. was ranked 111.[148] In 2010, the U.S.' homicide rate was 7 times higher than the average for populous developed countries in the OECD, and its firearm-related homicide rate was 25.2 times higher.[149] In 2013, the United States' firearm-related death rate was 10.64 deaths for every 100,000 inhabitants, a figure very close to Mexico's 11.17, although in Mexico firearm deaths are predominantly homicides whereas in the United States they are predominantly suicides.[150] Although Mexico has strict gun laws, the laws restricting carry are often unenforced, and the laws restricting manufacture and sale are often circumvented by trafficking from the United States and other countries.[151]

Canada and Switzerland each have much looser gun control regulation than the majority of developed nations, although significantly more than in the United States, and have firearm death rates of 2.22 and 2.91 per 100,000 citizens, respectively. By comparison Australia, which imposed sweeping gun control laws in response to the Port Arthur massacre in 1996, has a firearm death rate of 0.86 per 100,000. In the United Kingdom the rate is 0.26.[152]

In 2014, there were 8,124 gun homicides in the U.S.[152] In 2015, there were 33,636 deaths due to firearms in the U.S., with homicides accounting for 13,286 of those, while guns were used to kill about 50 people in the U.K., a country with population one-fifth of the size of the U.S. population.[3] More people are typically killed with guns in the U.S. in a day—about 85—than in the U.K. in a year, if suicides are included.[3] With deaths by firearm reaching almost 40,000 in the U.S. in 2017, their highest level since 1968, almost 109 people died per day.[9]

A study conducted by the Journal of the American Medical Association determined that worldwide yearly gun deaths had reached 250,000 by 2018 and that the United States was one of only six countries that collectively accounted for roughly half of those fatalities.[153][154]
According to the 2023 Small Arms Survey, there are about 120 guns for every 100 Americans. In other words, there are more civilian guns in the United States than there are people. The rate of deaths from gun violence in the United States is eight times greater than in Canada, which has the seventh-highest rate of gun ownership in the world.[155]

The definition of a mass shooting remains under debate. The precise inclusion criteria are disputed, and there no broadly accepted definition exists.[158][159] Mother Jones, using their standard of a mass shooting where a lone gunman kills at least four people in a public place for motivations excluding gang violence or robbery,[160] concluded that between 1982 and 2006 there were 40 mass shootings, an average of 1.6 per year. From 2007 to May 2018, there were 61 mass shootings, an average of 5.4 per year.[161] More broadly, the frequency of mass shootings steadily declined throughout the 1990s and early 2000s, then increased dramatically.[162][163]

Studies indicate that the rate at which public mass shootings occur has tripled since 2011. Between 1982 and 2011, a mass shooting occurred roughly once every 200 days. Between 2011 and 2014, that rate accelerated greatly with at least one mass shooting occurring every 64 days in the United States.[164] In "Behind the Bloodshed", a report by USA Today, said that there were mass killings every two weeks and that public mass killings account for 1 in 6 of all mass killings (26 killings annually would thus be equivalent to 26/6, 4 to 5, public killings per year).[165]

Mother Jones listed seven mass shootings in the U.S. for 2015.[160] The average for the period 2011–2015 was about 5 a year.[166] An analysis by Michael Bloomberg's gun violence prevention group, Everytown for Gun Safety, identified 110 mass shootings, defined as shootings in which at least four people were murdered with a firearm, between January 2009 and July 2014. At least 57% were related to domestic or family violence.[167]

Other media outlets have reported that hundreds of mass shootings take place in the United States in a single calendar year, citing a crowd-funded website known as Shooting Tracker which defines a mass shooting as having four or more people injured.[168] In December 2015, The Washington Post reported that there had been 355 mass shootings in the United States so far that year.[169] In August 2015, The Washington Post reported that the United States was averaging one mass shooting per day.[170] An earlier report had indicated that in 2015 alone, there had been 294 mass shootings that killed or injured 1,464 people.[171] Shooting Tracker and Mass Shooting Tracker, the two sites that the media have been citing, have been criticized for using a criterion much more inclusive than that used by the government—they count four victims injured as a mass shooting—thus producing much higher figures.[172][173]

Handguns figured in the Virginia Tech shooting, the Binghamton shooting, the 2009 Fort Hood shooting, the 2012 Oikos University shooting, and the 2011 Tucson shooting, but both a handgun and a rifle were used in the Sandy Hook Elementary School shooting.[174] The Aurora theater shooting and the Columbine High School massacre were committed by assailants armed with multiple weapons. AR-15 style rifles have been used in a number of the deadliest mass shooting incidents, and have come to be widely characterized as the weapon of choice for perpetrators of mass shootings,[175][176][177][178][179][180][181][182] despite statistics which show that handguns are the most commonly used weapon type in mass shootings.[183]

The number of public mass shootings has increased substantially over several decades, with a steady increase in gun-related deaths.[184][185] Although mass shootings are covered extensively in the media, they account for a small fraction of gun-related deaths,[129] only 1 percent of all gun deaths between 1980 and 2008.[186] Between January 1 and May 18, 2018, 31 students and teachers were killed inside U.S. schools, exceeding the number of U.S. military service members who died in combat and noncombat roles during the same period.[187]

The perpetrators and victims of accidental and negligent gun discharges may be of any age. Accidental injuries are most common in homes where guns are kept for self-defense.[188] The injuries are self-inflicted in half of the cases.[188]
On January 16, 2013, President Barack Obama issued 23 Executive Orders on Gun Safety,[189] one of which was for the Center for Disease Control (CDC) to research causes and possible prevention of gun violence. The five main areas of focus were gun violence, risk factors, prevention/intervention, gun safety and how media and violent video games influence the public. They also researched the area of accidental firearm deaths. According to this study not only have the number of accidental firearm deaths been on the decline over the past century but they now account for less than 1% of all unintentional deaths, half of which are self-inflicted.[190]

In the United States, states with higher levels of gun ownership were associated with higher rates of gun assault and gun robbery.[138] However it is unclear if higher crime rates are a result of increased gun ownership or if gun ownership rates increase as a result of increased crime.[191]

In 2000, the costs of gun violence in the United States were estimated to be on the order of $100 billion per year, plus the costs associated with the gun violence avoidance and prevention behaviors.[194]

In 2010, gun violence cost U.S. taxpayers about $516 million in direct hospital costs.[195]

At least eleven assassination attempts with firearms have been made on U.S. presidents (over one-fifth of all presidents); four sitting presidents have been killed, three with handguns and one with a rifle.

Abraham Lincoln survived an earlier attack,[196] but was killed using a .44-caliber Derringer pistol fired by John Wilkes Booth.[197] James A. Garfield was shot two times and mortally wounded by Charles J. Guiteau using a .44-caliber revolver on July 2, 1881. He would die of pneumonia the same year on September 19. On September 6, 1901, William McKinley was fatally wounded by Leon Czolgosz when he fired twice at point-blank range using a .32-caliber revolver. Struck by one of the bullets and receiving immediate surgical treatment, McKinley died 8 days later of gangrene infection.[197] John F. Kennedy was killed by Lee Harvey Oswald with a bolt-action rifle on November 22, 1963.[198]

Andrew Jackson, Harry S. Truman, and Gerald Ford (the latter twice) survived unharmed from assassination attempts involving firearms.[199][200][201]

Ronald Reagan was critically wounded in the March 30, 1981 assassination attempt by John Hinckley, Jr. with a .22-caliber revolver. He is the only U.S. president to survive being shot while in office.[202] Former president Theodore Roosevelt was shot and wounded right before delivering a speech during his 1912 presidential campaign. Despite bleeding from his chest, Roosevelt refused to go to a hospital until he delivered the speech.[203] On February 15, 1933, Giuseppe Zangara attempted to assassinate president-elect Franklin Delano Roosevelt, who was giving a speech from his car in Miami, Florida, with a .32-caliber pistol.[204] Roosevelt was unharmed, but Chicago mayor Anton Cermak died in the attempt, and several other bystanders received non-fatal injuries.[205]

Response to these events has resulted in federal legislation to regulate the public possession of firearms. For example, the attempted assassination of Franklin Roosevelt contributed to passage of the National Firearms Act of 1934,[205] and the Kennedy assassination (along with others) resulted in the Gun Control Act of 1968. The GCA is a federal law signed by President Lyndon Johnson that broadly regulates the firearms industry and firearms owners. It primarily focuses on regulating interstate commerce in firearms by largely prohibiting interstate firearms transfers except among licensed manufacturers, dealers, and importers.[206]

A quarter of robberies of commercial premises in the U.S. are committed with guns.[207] Fatalities are three times as likely in robberies committed with guns than where other, or no, weapons are used,[207][208][209] with similar patterns in cases of family violence.[210] Criminologist Philip J. Cook hypothesized that if guns were less available, criminals might commit the same crime, but with less-lethal weapons.[211] He finds that the level of gun ownership in the 50 largest U.S. cities correlates with the rate of robberies committed with guns, but not with overall robbery rates.[212][213] He also finds that robberies in which the assailant uses a gun are more likely to result in the death of the victim, but less likely to result in injury to the victim.[214] Overall robbery and assault rates in the U.S. are comparable to those in other developed countries, such as Australia and Finland, with much lower levels of gun ownership.[211][215] A 2000 study showed a strong association between the availability of illegal guns and violent crime rates, but not between legal gun availability and violent crime rates.[216]

Firearms are the leading cause of death for ages 16–19 in United States since 2020; with the US accounting for 97% of gun-related deaths of late-teens among similarly large and wealthy countries.[218][219] According to the U.S. Bureau of Justice Statistics, from 1980 to 2008, 84% of white homicide victims were killed by white offenders and 93% of black homicide victims were killed by black offenders.[220]

African-Americans, who were only 13% of the U.S. population in 2010, were 55% of the victims of gun homicide. In 2017 African-American males aged 15 to 34 years were the most frequent victims of firearm homicide in the United States with 81 deaths per 100,000 population.[221][8] Non-Hispanic whites were 65% of the U.S. population in 2010, but only 25% of the victims. Hispanics were 16% of the population in 2010 and 17% of victims.[222]

According to a 2021 CDC study, the male gun homicide rate was over five times the female gun homicide rate. The highest gun homicide rate was among those age 25–44. Non-Hispanic blacks had the highest gun homicide rate in every age group, with a rate 13 times higher than whites in the 25-44 age group.[223] According to ABC News, so far, more than 11,500 Americans killed by firearms in 2023.[224]

With a rise in gun violence and mass shootings in the United States, many surveys have been conducted throughout the recent years to examine the public opinion on certain gun policies and prevention methods in an effort to gain an understanding on the major trends in public opinion. Americans have found to have a range of opinions regarding this issue.

Across different studies conducted, it has been found that US public opinion varies based on gender, age, gun ownership status, occupation, education, political affiliation among many other demographics. However, most Americans support some form of restrictions and limitations with firearms, whether they are gun owners or not.

A study conducted by Berry College's Department of Political Science utilized data from surveys that were administered from 1999 to 2001, 2011, 2012, 2015, 2017 and 2018. They compared the attitude of the massacre generation which refers to people born after the Columbine high school shooting in 1999 to the older generation. An age effect was only seen in studies conducted after 2012.[225] Results from these surveys indicated that the younger generation are more likely to believe that the government can effectively prevent future mass shootings with more gun prevention laws.[225] The data also suggested that the younger generation are more likely to attribute mass shootings to lack of government regulation.[225]

Another study was conducted in April 2015 which measured public opinion of carrying firearms in public places. Results from the study showed that overall, less than one third of the adults in the US supported carrying firearms in public spaces.[226] Support was greater in gun owners compared to non- gun owners.[226] Support for carrying firearms in public was lowest for schools, bars, and sport stadiums.[226] According to the data, 18.2 percent of the respondents supported carrying guns in bars, 17.1 percent supported carrying guns in sport stadiums and 18.8 percent supported carrying guns in schools.[226] Support for carrying firearms was greatest in restaurants and retail stores. 32.9 percent of the respondents support carrying guns in restaurants and 30.8 percent support carrying guns in retail stores.[226] From this study it was concluded that most people in the United States, even most gun owners, are in support of limiting the places gun owners are allowed to carry their weapons.

Another study that was conducted in 2015 by Johns Hopkins Bloomberg School of Public Health revealed that the majority of Americans supported various gun laws and there was minimal difference between gun owners and non-gun owners for a majority of the policies.[227] Support for "prohibiting a person subject to a temporary domestic violence restraining order from having a gun" was around 77.5 percent among gun owners and around 79.6 percent among non-gun owners.[227] Overall, support for a policy that authorizes law enforcement to remove firearms from a person temporarily who may be a threat to themselves or others was 70.9 percent, non- gun owner support was 71.8 percent and gun owner support was 67 percent.[227] The study examined a comparison between public opinion on gun policy immediately after the 2013 school shooting at Sandy Hook Elementary school in Newton, Connecticut and 2 years after the shooting. In most cases there was only a slight change in opinion. For example, overall support for prohibiting a person under the age of 21 from having a gun only decreased 4 percent.[227]

A national study of gun policy was conducted in 2019 examining the trends in data from surveys that were administered by the Johns Hopkins Center for Gun Policy and Research in 2012, 2015, 2017 and 2019. This study analyzed how the attitude towards certain gun policies changed overtime based on political party affiliation and gun ownership status. The study has found that the majority of the people supported a range of gun policies whether they were gun owners or not. From 2015 to 2019, there was an overall increase in support among American adults for 18- gun policies.[228] For instance, support for requiring purchaser licensing and safe gun storage laws increased 5 percent. There was a 4 percent increase in support for universal background checks.[228] Moreover, data showed that a majority of Republicans and Independents supported all except one of the 18 policies.[228] The data reveal high support for safety training among gun owners and non-gun owners. The results of the study indicated that overall 81 percent of the respondents supported the requirement of a safety test for those who have applied for a license to carry firearms in public, in which the support was 73 percent from gun owners and 83 percent from non-gun owners.[228] Additionally, 36 percent of the participants in the study supported permitting a person to carry a concealed gun on a college campus and only 31 percent supported permitting someone to carry guns in elementary school.[228] Overall support for prohibiting a person convicted of a violent crime from carrying a gun in public for 10 years was around 78 percent, where gun owner support was around 71 percent and non-gun owner support was around 80%.[228] Data from this study suggests that both gun owners and non- gunowners support a range of gun policies.

A study conducted in 2021 examines American public opinion on several gun violence prevention funding policies among different racial and ethnic groups.[229] Support for funding community-based prevention programs that provide social support was 71 percent among blacks, 68 percent among whites and 69 percent among Hispanics.[229] Moreover, support for funding hospital- based gun violence prevention programs that provide counseling to people to reduce an individual's risk of future violence was 57 percent among whites, 66 percent among blacks and 57 percent among Hispanics.[229] Support for redirecting government funding from police to social programs was 35% among whites, 60% among blacks and 43% among Hispanics.[229] Overall, data revealed that black support for most of the policies examined was greater than white support, however the differences were minimal.[229]
Public opinion polls show Americans are about evenly split on banning guns like the AR-15, with recent polls showing support for the ban has dipped slightly.[230]

In the midst of a recent surge in mass shootings, including a record 46 school shootings in 2022, an April 2023 Fox News poll found registered voters overwhelmingly supported a wide variety of gun restrictions:

Public policy as related to preventing gun violence is an ongoing political and social debate regarding both the restriction and availability of firearms within the United States. Policy at the Federal level is/has been governed by the Second Amendment, National Firearms Act, Gun Control Act of 1968, Firearm Owners Protection Act, Brady Handgun Violence Prevention Act, Violent Crime Control and Law Enforcement Act, and the Domestic Violence Offender Act. Gun policy in the U.S. has been revised many times with acts such as the Firearm Owners Protection Act, which loosened provisions for gun sales while banning civilian ownership of machine guns made after 1986.[234]

At the federal, state and local level, gun laws such as handgun bans have been overturned by the Supreme Court in cases such as District of Columbia v. Heller and McDonald v. Chicago. These cases hold that the Second Amendment protects an individual right to possess a firearm. D.C. v. Heller only addressed the issue on Federal enclaves, while McDonald v. Chicago addressed the issue as relating to the individual states.[235]

Gun control proponents often cite the relatively high number of homicides committed with firearms as reason to support stricter gun control laws.[236] Policies and laws that reduce homicides committed with firearms prevent homicides overall; a decrease in firearm-related homicides is not balanced by an increase in non-firearm homicides.[237] Firearm laws are a subject of debate in the U.S., with firearms used for recreational purposes as well as for personal protection.[2] Gun rights advocates cite the use of firearms for self-protection, and to deter violent crime, as reasons why more guns can reduce crime.[238] Gun rights advocates also say criminals are the least likely to obey firearms laws, and so limiting access to guns by law-abiding people makes them more vulnerable to armed criminals.[58]

In a survey of 41 studies, half of the studies found a connection between gun ownership and homicide but these were usually the least rigorous studies. Only six studies controlled at least six statistically significant confounding variables, and none of them showed a significant positive effect. Eleven macro-level studies showed that crime rates increase gun levels (not vice versa). The reason that there is no opposite effect may be that most owners are noncriminals and that they may use guns to prevent violence.[239]

The United States Constitution enshrines the right to gun ownership in the Second Amendment of the United States Bill of Rights to ensure the security of a free state through a well regulated Militia. It states: "A well regulated Militia, being necessary to the security of a free State, the right of the people to keep and bear Arms shall not be infringed." The Constitution makes no distinction between the type of firearm in question or state of residency.[241]

Gun dealers in the U.S. are prohibited from selling handguns to those under the age of 21, and long guns to those under the age of 18.[211] In 2017, the National Safety Council released a state ranking on firearms access indicators such as background checks, waiting periods, safe storage, training, and sharing of mental health records with the NICS database to restrict firearm access.[243]

Assuming access to guns, the top ten guns involved in crime in the United States show a definite tendency to favor handguns over long guns. The top ten guns used in crime, as reported by the ATF in 1993, were the Smith & Wesson .38 Special and .357 revolvers; Raven Arms .25 caliber, Davis P-380 .380 caliber, Ruger .22 caliber, Lorcin L-380 .380 caliber, and Smith & Wesson semi-automatic handguns; Mossberg and Remington 12 gauge shotguns; and the Tec DC-9 9 mm handgun.[244] An earlier 1985 study of 1,800 incarcerated felons showed that criminals preferred revolvers and other non-semi-automatic firearms over semi-automatic firearms.[245] In Pittsburgh a change in preferences towards pistols occurred in the early 1990s, coinciding with the arrival of crack cocaine and the rise of violent youth gangs.[246] Background checks in California from 1998 to 2000 resulted in 1% of sales being initially denied.[247] The types of guns most often denied included semiautomatic pistols with short barrels and of medium caliber.[247] A 2018 study determined that California's implementation of comprehensive background checks and misdemeanor violation policies was not associated with a net change in the firearm homicide rate over the ensuing 10 years.[248] A 2018 study found no evidence of an association between the repeal of comprehensive background check policies and firearm homicide and suicide rates in Indiana and Tennessee.[249]

Among juveniles (minors under the age of 16, 17, or 18, depending on legal jurisdiction) serving in correctional facilities, 86% had owned a gun, with 66% acquiring their first gun by age 14.[250] There was also a tendency for juvenile offenders to have owned several firearms, with 65% owning three or more.[250] Juveniles most often acquired guns illegally from family, friends, drug dealers, and street contacts.[250] Inner city youths cited "self-protection from enemies" as the top reason for carrying a gun.[250] In Rochester, New York, 22% of young males have carried a firearm illegally, most for only a short time.[251] There is little overlap between legal gun ownership and illegal gun carrying among youths.[251]

A 2011 study indicated that in states where local background checks for gun purchases are completed, the suicide rate was lower than states without.[252]

Gun rights advocates argue that policy aimed at the supply side of the firearms market is based on limited research.[2] One consideration is that 60–70% of firearms sales in the U.S. are transacted through federally licensed firearm dealers, with the remainder taking place in the "secondary market", in which previously owned firearms are transferred by non-dealers.[36][254][255][256] Access to secondary markets is generally less convenient to purchasers, and involves such risks as the possibility of the gun having been used previously in a crime.[257] Unlicensed private sellers were permitted by law to sell privately owned guns at gun shows or at private locations in 24 states as of 1998.[258] Regulations that limit the number of handgun sales in the primary, regulated market to one handgun a month per customer have been shown to be effective at reducing illegal gun trafficking by reducing the supply into the secondary market.[259] Taxes on firearm purchases are another means for government to influence the primary market.[260]

Criminals tend to obtain guns through multiple illegal pathways, including large-scale gun traffickers, who tend to provide criminals with relatively few guns.[261] Federally licensed firearm dealers in the primary (new and used gun) market are regulated by the Bureau of Alcohol, Tobacco, Firearms, and Explosives (ATF). Firearm manufacturers are required to mark all firearms manufactured with serial numbers. This allows the ATF to trace guns involved in crimes back to their last Federal Firearms License (FFL) reported change of ownership transaction, although not past the first private sale involving any particular gun. A report by the ATF released in 1999 found that 0.4% of federally licensed dealers sold half of the guns used criminally in 1996 and 1997.[262][263] This is sometimes done through "straw purchases."[262] State laws, such as those in California, that restrict the number of gun purchases in a month may help stem such "straw purchases."[262] States with gun registration and licensing laws are generally less likely to have guns initially sold there used in crimes.[264] Similarly, crime guns tend to travel from states with weak gun laws to states with strict gun laws.[265][266][267] An estimated 500,000 guns are stolen each year, becoming available to prohibited users.[254][260] During the ATF's Youth Crime Gun Interdiction Initiative (YCGII), which involved expanded tracing of firearms recovered by law enforcement agencies,[268] only 18% of guns used criminally that were recovered in 1998 were in possession of the original owner.[269] Guns recovered by police during criminal investigations were often sold by legitimate retail sales outlets to legal owners, and then diverted to criminal use over relatively short times ranging from a few months to a few years,[132][269][270] which makes them relatively new compared with firearms in general circulation.[260][271]

A 2016 survey of prison inmates by the Bureau of Justice Statistics found that 43% of guns used in crimes were obtained from the black market, 25% from an individual, 10% from a retail source (including 0.8% from a gun show), and 6% from theft.[272]



The first Federal legislation related to firearms was the Second Amendment to the United States Constitution ratified in 1791. For 143 years, this was the only major Federal legislation regarding firearms. The next Federal firearm legislation was the National Firearms Act of 1934, which created regulations for the sale of firearms, established taxes on their sale, and required registration of some types of firearms such as machine guns.[274]

Following the Robert F. Kennedy and Martin Luther King Jr. assassinations, the Gun Control Act of 1968 was enacted. This Act regulated gun commerce, restricting mail order sales, and allowing shipments only to licensed firearm dealers. The Act also prohibited sale of firearms to felons, those under indictment, fugitives, illegal aliens, drug users, those dishonorably discharged from the military, and those in mental institutions.[211] The law also restricted importation of so-called Saturday night specials and other types of guns, and limited the sale of automatic weapons and semi-automatic weapon conversion kits.[262]

The Firearm Owners Protection Act, also known as the McClure-Volkmer Act, was passed in 1986. It changed some restrictions in the 1968 Act, allowing federally licensed gun dealers and individual unlicensed private sellers to sell at gun shows, while continuing to require licensed gun dealers to require background checks.[262] The 1986 Act also restricted the Bureau of Alcohol, Tobacco and Firearms from conducting punitively repetitive inspections, reduced the amount of record-keeping required of gun dealers, raised the burden of proof for convicting gun law violators, and changed restrictions on convicted felons from owning firearms.[262] In addition it also banned new machine guns for sale to the public, but grandfathered in any that were already registered.

In the years following the passage of the Gun Control Act of 1968, people buying guns were required to show identification and sign a statement affirming that they were not in any of the prohibited categories.[211] Many states enacted background check laws that went beyond the federal requirements.[276] The Brady Handgun Violence Prevention Act passed by Congress in 1993 imposed a waiting period before the purchase of a handgun, giving time for, but not requiring, a background check to be made.[277] The Brady Act also required the establishment of a national system to provide instant criminal background checks, with checks to be done by firearms dealers.[278] The Brady Act only applied to people who bought guns from licensed dealers, whereas felons buy some percentage of their guns from black market sources.[270] Restrictions, such as waiting periods, impose costs and inconveniences on legitimate gun purchasers, such as hunters.[260] A 2000 study found that the implementation of the Brady Act was associated with "reductions in the firearm suicide rate for persons aged 55 years or older but not with reductions in homicide rates or overall suicide rates."[279]

The Violent Crime Control and Law Enforcement Act, enacted in 1994, included the Federal Assault Weapons Ban, and was a response to public fears over mass shootings.[282] This provision prohibited the manufacture and importation of some firearms with certain features such as a folding stock, pistol grip, flash suppressor, and magazines holding more than ten rounds.[282] A grandfather clause was included that allowed firearms manufactured before 1994 to remain legal. A short-term evaluation by University of Pennsylvania criminologists Christopher S. Koper and Jeffrey A. Roth did not find any clear impact of this legislation on gun violence.[283] Given the short study time period of the evaluation, the National Academy of Sciences advised caution in drawing any conclusions.[260] In September 2004, the assault weapon ban expired, with its sunset clause.[284]

The Domestic Violence Offender Gun Ban, the Lautenberg Amendment, prohibited anyone previously convicted of a misdemeanor or felony crime of domestic violence from shipment, transport, ownership and use of guns or ammunition. This was ex post facto, in the opinion of Representative Bob Barr.[285] This law also prohibited the sale or gift of a firearm or ammunition to such a person. It was passed in 1996, and became effective in 1997. The law does not exempt people who use firearms as part of their duties, such as police officers or military personnel with applicable criminal convictions; they may not carry firearms.

In the immediate aftermath of Hurricane Katrina, police and National Guard units in New Orleans confiscated firearms from private citizens in an attempt to prevent violence. In reaction, Congress passed the Disaster Recovery Personal Protection Act of 2006 in the form of an amendment to Department of Homeland Security Appropriations Act, 2007. Section 706 of the Act prohibits federal employees and those receiving federal funds from confiscating legally possessed firearms during a disaster.[286]

On January 5, 2016, President Obama unveiled his new strategy to curb gun violence in America. His proposals focus on new background check requirements that are intended to enhance the effectiveness of the National Instant Criminal Background Check System (NICS), and greater education and enforcement efforts of existing laws at the state level.[287][288] In an interview with Bill Simmons of HBO, President Obama also confirmed that gun control will be the "dominant" issue on his agenda in his last year of presidency.[289][290]

All 50 U.S. states allow for the right to carry firearms. A majority of states either require a shall-issue permit or allow carrying without a permit and a minority require a may-issue permit. Right-to-carry laws expanded in the 1990s as homicide rates from gun violence in the U.S. increased, largely in response to incidents such as the Luby's shooting of 1991 in Texas which directly resulted in the passage of a carrying concealed weapon, or CCW, law in Texas in 1995.[291] As Rorie Sherman, staff reporter for the National Law Journal wrote in an article published on April 18, 1994, "It is a time of unparalleled desperation about crime. But the mood is decidedly 'I'll do it myself' and 'Don't get in my way.'"[292]

The result was laws, or the lack thereof, that permitted persons to carry firearms openly, known as open carry, often without any permit required, in 22 states by 1998.[293] Laws that permitted persons to carry concealed handguns, sometimes termed a concealed handgun license, CHL, or concealed pistol license, CPL in some jurisdictions instead of CCW, existed in 34 states in the U.S. by 2004.[2] Since then, the number of states with CCW laws has increased; as of 2014[update], all 50 states have some form of CCW laws on the books.[294]

Economist John Lott has argued that right-to-carry laws create a perception that more potential crime victims might be carrying firearms, and thus serve as a deterrent against crime.[295] Lott's study has been criticized for not adequately controlling for other factors, including other state laws also enacted, such as Florida's laws requiring background checks and waiting period for handgun buyers.[296] When Lott's data was re-analyzed by some researchers, the only statistically significant effect of concealed-carry laws found was an increase in assaults,[296] with similar findings by Jens Ludwig.[297] Lott and Mustard's 1997 study has also been criticized by Paul Rubin and Hashem Dezhbakhsh for inappropriately using a dummy variable; Rubin and Dezhbakhsh reported in a 2003 study that right-to-carry laws have much smaller and more inconsistent effects than those reported by Lott and Mustard, and that these effects are usually not crime-reducing.[298] Since concealed-carry permits are only given to adults, Philip J. Cook suggested that analysis should focus on the relationship with adult and not juvenile gun incident rates.[211] He found no statistically significant effect.[211] A 2004 National Academy of Sciences survey of existing literature found that the data available "are too weak to support unambiguous conclusions" about the impact of right-to-carry laws on rates of violent crime.[2] NAS suggested that new analytical approaches and datasets at the county or local level are needed to adequately evaluate the impact of right-to-carry laws.[299] A 2014 study found that Arizona's SB 1108, which allowed adults in the state to concealed carry without a permit and without passing a training course, was associated with an increase in gun-related fatalities.[300] A 2018 study by Charles Manski and John V. Pepper found that the apparent effects of RTC laws on crime rates depend significantly on the assumptions made in the analysis.[301] A 2019 study found no statistically significant association between the liberalization of state level firearm carry legislation over the last 30 years and the rates of homicides or other violent crime.[302]

 Child Access Prevention (CAP) laws, enacted by many states, require parents to store firearms safely, to minimize access by children to guns, while maintaining ease of access by adults.[303] CAP laws hold gun owners liable should a child gain access to a loaded gun that is not properly stored.[303] The Centers for Disease Control and Prevention (CDC) said that, on average, one child died every three days in accidental incidents in the U.S. from 2000 to 2005.[304] In most states, CAP law violations are considered misdemeanors.[303] Florida's CAP law, enacted in 1989, permits felony prosecution of violators.[303] Research indicates that CAP laws are correlated with a reduction in unintentional gun deaths by 23%,[305] and gun suicides among those aged 14 through 17 by 11%.[306] A study by Lott did not detect a relationship between CAP laws and accidental gun deaths or suicides among those age 19 and under between 1979 and 1996.[307] However, two studies disputed Lott's findings.[306][308] A 2013 study found that CAP laws are correlated with a reduction of non-fatal gun injuries among both children and adults by 30–40%.[303] In 2016 the American Academy of Pediatrics found that safe gun storage laws were associated with lower overall adolescent suicide rates.[309] Research also indicated that CAP laws were most highly correlated with reductions of non-fatal gun injuries in states where violations were considered felonies, whereas in states that considered violations as misdemeanors, the potential impact of CAP laws was not statistically significant.[310]

Some local jurisdictions in the U.S. have more restrictive laws, such as Washington, D.C.'s Firearms Control Regulations Act of 1975, which banned residents from owning handguns, and required permitted firearms be disassembled and locked with a trigger lock. On March 9, 2007, a U.S. Appeals Court ruled the Washington, D.C., handgun ban unconstitutional.[311] The appeal of that case later led to the Supreme Court's ruling in District of Columbia v. Heller that D.C.'s ban was unconstitutional under the Second Amendment.

Despite New York City's strict gun control laws, guns are often trafficked in from other parts of the U.S., particularly the southern states.[263][312] Results from the ATF's Youth Crime Gun Interdiction Initiative indicate that the percentage of imported guns involved in crimes is tied to the stringency of local firearm laws.[268]

Violence prevention and educational programs have been established in many schools and communities across the United States. These programs aim to change personal behavior of both children and their parents, encouraging children to stay away from guns, ensure parents store guns safely, and encourage children to solve disputes without resorting to violence.[313] Programs aimed at altering behavior range from passive (requiring no effort on the part of the individual) to active (supervising children, or placing a trigger lock on a gun).[313] The more effort required of people, the more difficult it is to implement a prevention strategy.[314][315] Prevention strategies focused on modifying the situational environment and the firearm itself may be more effective.[313] Empirical evaluation of gun violence prevention programs has been limited.[2] Of the evaluations that have been done, results indicate such programs have minimal effectiveness.[313]

Speak Up is a national youth violence prevention initiative created by The Center to Prevent Youth Violence,[316] which provides young people with tools to improve the safety of their schools and communities. The SPEAK UP program is an anonymous, national hot-line for young people to report threats of violence in their communities or at school. The hot-line is operated in accordance with a protocol developed in collaboration with national education and law enforcement authorities, including the FBI. Trained counselors, with access to translators for 140 languages, collect information from callers and then report the threat to appropriate school and law enforcement officials.[317][318][non-primary source needed]

One of the most widely used parent counseling programs is Steps to Prevent Firearm Injury program (STOP), which was developed in 1994 by the American Academy of Pediatrics and the Brady Center to Prevent Gun Violence (the latter of which was then known as the Center to Prevent Handgun Violence).[313][319] STOP was superseded by STOP 2 in 1998, which has a broader focus including more communities and health care providers.[313] STOP has been evaluated and found not to have a significant effect on gun ownership or firearm storage practices by inner-city parents.[319] Marjorie S. Hardy suggests further evaluation of STOP is needed, as this evaluation had a limited sample size and lacked a control group.[313] A 1999 study found no statistically significant effect of STOP on rates of gun ownership or better gun storage.[319]

Prevention programs geared towards children have also not been greatly successful.[313] Many inherent challenges arise when working with children, including their tendency to perceive themselves as invulnerable to injury,[320] limited ability to apply lessons learned,[321][322] their innate curiosity,[321] and peer pressure.

The goal of gun safety programs, usually administered by local firearms dealers and shooting clubs, is to teach older children and adolescents how to handle firearms safely.[313] There has been no systematic evaluation of the effect of these programs on children.[313] For adults, no positive effect on gun storage practices has been found as a result of these programs.[36][323] Also, researchers have found that gun safety programs for children may likely increase a child's interest in obtaining and using guns, which they cannot be expected to use safely all the time, even with training.[324]

One approach taken is gun avoidance, such as when encountering a firearm at a neighbor's home. The Eddie Eagle Gun Safety Program, administered by the National Rifle Association (NRA), is geared towards younger children from pre-kindergarten to sixth grade, and teaches kids that real guns are not toys by emphasizing a "just say no" approach.[313] The Eddie Eagle program is based on training children in a four-step action to take when they see a firearm: (1) Stop! (2) Don't touch! (3) Leave the area. (4) Go tell an adult. Materials, such as coloring books and posters, back the lessons up and provide the repetition necessary in any child-education program. ABC News challenged the effectiveness of the "just say no" approach promoted by the NRA's Eddie the Eagle program in an investigative piece by Diane Sawyer in 1999.[325] Sawyer's piece was based on an academic study conducted by Dr. Marjorie Hardy.[326] Dr. Hardy's study tracked the behavior of elementary age schoolchildren who spent a day learning the Eddie the Eagle four-step action plan from a uniformed police officer. The children were then placed into a playroom which contained a hidden gun. When the children found the gun, they did not run away from the gun, but rather, they inevitably played with it, pulled the trigger while looking into the barrel, or aimed the gun at a playmate and pulled the trigger. The study concluded that children's natural curiosity was far more powerful than the parental admonition to "Just say no".[327]

Programs targeted at entire communities, such as community revitalization, after-school programs, and media campaigns, may be more effective in reducing the general level of violence that children are exposed to.[328][329] Community-based programs that have specifically targeted gun violence include Safe Kids/Healthy Neighborhoods Injury Prevention Program in New York City,[330][331] and Safe Homes and Havens in Chicago.[313] Evaluation of such community-based programs is difficult, due to many confounding factors and the multifaceted nature of such programs.[313] A Chicago-based program, "BAM" (Becoming a Man) has produced positive results, according to the University of Chicago Crime Lab, and is expanding to Boston in 2017.[332]

The March for Our Lives was a student-led demonstration in support of legislation to prevent gun violence in the United States. It took place in Washington, D.C., on March 24, 2018, with over 880 sibling events throughout the U.S.[333][334][335][336][337] It was planned by Never Again MSD in collaboration with the nonprofit organization.[338] The demonstration followed the Marjory Stoneman Douglas High School shooting in Parkland, Florida on February 14, 2018, which was described by several media outlets as a possible tipping point for gun control legislation.[339][340][341]

Sociologist James D. Wright suggests that to convince inner-city youths not to carry guns "requires convincing them that they can survive in their neighborhood without being armed, that they can come and go in peace, that being unarmed will not cause them to be victimized, intimidated, or slain."[250] Intervention programs, such as CeaseFire Chicago, Operation Ceasefire in Boston and Project Exile in Richmond, Virginia during the 1990s, have been shown to be effective.[2][342] Other intervention strategies, such as gun "buy-back" programs have been demonstrated to be ineffective.[343]

Gun "buyback" programs are a strategy aimed at influencing the firearms market by taking guns "off the streets".[343] Gun "buyback" programs have been shown to be effective to prevent suicides, but ineffective to prevent homicides[344][345] with the National Academy of Sciences citing theory underlying these programs as "badly flawed."[343] Guns surrendered tend to be those least likely to be involved in crime, such as old, malfunctioning guns with little resale value, muzzleloading or other black-powder guns, antiques chambered for obsolete cartridges that are no longer commercially manufactured or sold, or guns that individuals inherit but have little value in possessing.[345] Other limitations of gun buyback programs include the fact that it is relatively easy to obtain gun replacements, often of better guns than were relinquished in the buyback.[343] Also, the number of handguns used in crime (about 7,500 per year) is very small compared to about 70 million handguns in the U.S.. (i.e., 0.011%).[343]

"Gun bounty" programs launched in several Florida cities have shown more promise. These programs involve cash rewards for anonymous tips about illegal weapons that lead to an arrest and a weapons charge. Since its inception in May 2007, the Miami program has led to 264 arrests and the confiscation of 432 guns owned illegally and $2.2 million in drugs, and has solved several murder and burglary cases.[346]

In 1995, Operation Ceasefire was established as a strategy for addressing youth gun violence in Boston. Violence was particularly concentrated in poor, inner-city neighborhoods including Roxbury, Dorchester, and Mattapan.[347] There were 22 youths (under the age of 24) killed in Boston in 1987, with that figure rising to 73 in 1990.[347] Operation Ceasefire entailed a problem-oriented policing approach, and focused on specific places that were crime hot spots—two strategies that when combined have been shown to be quite effective.[348][349][350] Particular focus was placed on two elements of the gun violence problem, including illicit gun trafficking[351] and gang violence.[347] Within two years of implementing Operation Ceasefire in Boston, the number of youth homicides dropped to ten, with only one handgun-related youth homicide occurring in 1999 and 2000.[262] The Operation Ceasefire strategy has since been replicated in other cities, including Los Angeles.[352] Erica Bridgeford, spearheaded a "72-hour ceasefire" in August 2017, but the ceasefire was broken with a homicide. Councilman Brandon Scott, Mayor Catherine Pugh and others talked of community policing models that might work for Baltimore.[353][123]

Project Exile, conducted in Richmond, Virginia during the 1990s, was a coordinated effort involving federal, state, and local officials that targeted gun violence. The strategy entailed prosecution of gun violations in Federal courts, where sentencing guidelines were tougher. Project Exile also involved outreach and education efforts through media campaigns, getting the message out about the crackdown.[354] Research analysts offered different opinions as to the program's success in reducing gun crime. Authors of a 2003 analysis of the program argued that the decline in gun homicide was part of a "general regression to the mean" across U.S. cities with high homicide rates.[355] Authors of a 2005 study disagreed, concluding that Richmond's gun homicide rate fell more rapidly than the rates in other large U.S. cities with other influences controlled.[354][356]

Project Safe Neighborhoods (PSN) is a national strategy for reducing gun violence that builds on the strategies implemented in Operation Ceasefire and Project Exile.[357] PSN was established in 2001, with support from the Bush administration, channelled through the United States Attorney's Offices in the United States Department of Justice. The Federal government has spent over US$1.5 billion since the program's inception on the hiring of prosecutors, and providing assistance to state and local jurisdictions in support of training and community outreach efforts.[358][359]

In 2016, Chicago saw a 58% increase in homicides.[360] In response to the spike in gun violence, a group of foundations and social service agencies created the Rapid Employment and Development Initiative (READI) Chicago.[361] A Heartland Alliance program,[362] READI Chicago targets those most at risk of being involved in gun violence – either as perpetrator or a victim.[363] Individuals are provided with 18 months of transitional jobs, cognitive behavioral therapy and legal and social services.[363] Individuals are also provided with 6 months of support as they transition to full-time employment at the end of the 18 months.[363] The University of Chicago Crime Lab is evaluating READI Chicago's impact on gun violence reduction.[364] The evaluation, expected to be completed in Spring 2021, is showing early signs of success.[365] Eddie Bocanegra, senior director of READI Chicago, hopes that the early success of READI Chicago will result in funding from the City of Chicago.[364]

The National Incident-Based Reporting System (NIBRS) is used by law enforcement agencies in the United States for collecting and reporting data on crimes.[366] The NIBRS is one of four subsets of the Uniform Crime Reporting (UCR) program.

The FBI states the UCR Program is retiring the SRS and will transition to a NIBRS-only data collection by January 1, 2021.[366] Additionally, the FBI states NIBRS will collect more detailed information, including incident date and time, whether reported offenses were attempted or completed, expanded victim types, relationships of victims to offenders and offenses, demographic details, location data, property descriptions, drug types and quantities, the offender's suspected use of drugs or alcohol, the involvement of gang activity, and whether a computer was used in the commission of the crime.[366]

Though NIBRS will be collecting more data the reporting if the firearm used was legally or illegally obtained by the suspect will not be identified. Nor will the system have the capability to identify if a legally obtained firearm used in the crime was used by the owner or registered owner, if required to be registered. Additionally, the information of how an illegally obtained firearm was acquired will be left to speculation. The absence of collecting this information into NIBRS the reported "gun violence" data will remain a gross misinterpretation lending anyone information that can be skewed to their liking/needs and not pinpoint where actual efforts need to be directed to curb the use of firearms in crime.

In the United States, research into firearms and violent crime is fraught with difficulties, associated with limited data on gun ownership and use,[42] firearms markets, and aggregation of crime data.[2] Research studies into gun violence have primarily taken one of two approaches: case-control studies and social ecology.[2] Gun ownership is usually determined through surveys, proxy variables, and sometimes with production and import figures. In statistical analysis of homicides and other types of crime which are rare events, these data tend to have poisson distributions, which also presents methodological challenges to researchers. With data aggregation, it is difficult to make inferences about individual behavior.[367] This problem, known as ecological fallacy, is not always handled properly by researchers; this leads some to jump to conclusions that their data do not necessarily support.[368]

In 1996 the NRA lobbied Congressman Jay Dickey (R-Ark.) to include budget provisions that prohibited the Centers for Disease Control (CDC) from advocating or promoting gun control and that deleted $2.6 million from the CDC budget, the exact amount the CDC had spent on firearms research the previous year. The ban was later extended to all research funded by the Department of Health and Human Services (DHHS). According to an article in Nature, this made gun research more difficult, reduced the number of studies, and discouraged researchers from even talking about gun violence at medical and scientific conferences. In 2013, after the December 2012 Sandy Hook Elementary School shooting, President Barack Obama ordered the CDC to resume funding research on gun violence and prevention, and put $10 million in the 2014 budget request for it.[369] However, the order had no practical effect, as the CDC refused to act without a specific appropriation to cover the research, and Congress repeatedly declined to allocate any funds. As a result, the CDC has not performed any such studies since 1996.[370]High-Definition Multimedia Interface (HDMI) is a proprietary audio/video interface for transmitting uncompressed video data and compressed or uncompressed digital audio data from a source device, such as a display controller, to a computer monitor, video projector, digital television, or digital audio device.[3] HDMI is a digital replacement for analog video standards.

HDMI implements the ANSI/CTA-861 standard, which defines video formats and waveforms, transport of compressed and uncompressed LPCM audio, auxiliary data, and implementations of the VESA EDID.[4][5]: p. III  CEA-861 signals carried by HDMI are electrically compatible with the CEA-861 signals used by the Digital Visual Interface (DVI). No signal conversion is necessary, nor is there a loss of video quality when a DVI-to-HDMI adapter is used.[5]: §C  The Consumer Electronics Control (CEC) capability allows HDMI devices to control each other when necessary and allows the user to operate multiple devices with one handheld remote control device.[5]: §6.3 

Several versions of HDMI have been developed and deployed since the initial release of the technology, occasionally introducing new connectors with smaller form factors, but all versions still use the same basic pinout and are compatible with all connector types and cables. Other than improved audio and video capacity, performance, resolution and color spaces, newer versions have optional advanced features such as 3D, Ethernet data connection, and CEC extensions.

Production of consumer HDMI products started in late 2003.[6] In Europe, either DVI-HDCP or HDMI is included in the HD ready in-store labeling specification for TV sets for HDTV, formulated by EICTA with SES Astra in 2005. HDMI began to appear on consumer HDTVs in 2004 and camcorders and digital still cameras in 2006.[7][8] As of January 2021[update], nearly 10 billion HDMI devices have been sold.[9]

The HDMI founders were Hitachi, Panasonic, Philips, Silicon Image, Sony, Thomson, and Toshiba.[1] Digital Content Protection, LLC provides HDCP (which was developed by Intel) for HDMI.[10] HDMI has the support of motion picture producers Fox, Universal, Warner Bros. and Disney, along with system operators DirecTV, EchoStar (Dish Network) and CableLabs.[3]

The HDMI founders began development on HDMI 1.0 on April 16, 2002, with the goal of creating an AV connector that was backward-compatible with DVI.[11][12] At the time, DVI-HDCP (DVI with HDCP) and DVI-HDTV (DVI-HDCP using the CEA-861-B video standard) were being used on HDTVs.[12][13][14] HDMI 1.0 was designed to improve on DVI-HDTV by using a smaller connector and adding audio capability, enhanced Y′CBCR capability, and consumer electronics control functions.[12][13]

The first Authorized Testing Center (ATC), which tests HDMI products, was opened by Silicon Image on June 23, 2003, in California, United States.[15] The first ATC in Japan was opened by Panasonic on May 1, 2004, in Osaka.[16] The first ATC in Europe was opened by Philips on May 25, 2005, in Caen, France.[17] The first ATC in China was opened by Silicon Image on November 21, 2005, in Shenzhen.[18] The first ATC in India was opened by Philips on June 12, 2008, in Bangalore.[19] The HDMI website contains a list of all the ATCs.[20]

According to In-Stat, the number of HDMI devices sold was 5 million in 2004, 17.4 million in 2005, 63 million in 2006, and 143 million in 2007.[21][22][23] HDMI has become the de facto standard for HDTVs, and according to In-Stat, around 90% of digital televisions in 2007 included HDMI.[21][24][25][26][27] In-Stat has estimated that 229 million HDMI devices were sold in 2008.[28] On April 8, 2008, there were over 850 consumer electronics and PC companies that had adopted the HDMI specification (HDMI adopters).[29][30] On January 7, 2009, HDMI Licensing, LLC announced that HDMI had reached an installed base of over 600 million HDMI devices.[30] In-Stat estimated that 394 million HDMI devices would sell in 2009 and that all digital televisions by the end of 2009 would have at least one HDMI input.[30]

On January 28, 2008, In-Stat reported that shipments of HDMI were expected to exceed those of DVI in 2008, driven primarily by the consumer electronics market.[21][31]

In 2008, PC Magazine awarded a Technical Excellence Award in the Home Theater category for an "innovation that has changed the world" to the CEC portion of the HDMI specification.[32] Ten companies were given a Technology and Engineering Emmy Award for their development of HDMI by the National Academy of Television Arts and Sciences on January 7, 2009.[33]

On October 25, 2011, the HDMI Forum was established by the HDMI founders to create an open organization so that interested companies can participate in the development of the HDMI specification.[34][35] All members of the HDMI Forum have equal voting rights, may participate in the Technical Working Group, and if elected can be on the Board of Directors.[35] There is no limit to the number of companies allowed in the HDMI Forum though companies must pay an annual fee of US$15,000 with an additional annual fee of $5,000 for those companies that serve on the Board of Directors.[35] The Board of Directors is made up of 11 companies who are elected every two years by a general vote of HDMI Forum members.[35] All future development of the HDMI specification take place in the HDMI Forum and are built upon the HDMI 1.4b specification.[35] Also on the same day HDMI Licensing, LLC announced that there were over 1,100 HDMI adopters and that over 2 billion HDMI-enabled products had shipped since the launch of the HDMI standard.[1][34] From October 25, 2011, all development of the HDMI specification became the responsibility of the newly created HDMI Forum.[34]

On January 8, 2013, HDMI Licensing, LLC announced that there were over 1,300 HDMI adopters and that over 3 billion HDMI devices had shipped since the launch of the HDMI standard.[36][37] The day also marked the 10th anniversary of the release of the first HDMI specification.[36][37]

As of January 2021[update], nearly 10 billion HDMI devices had been sold.[9]

The HDMI specification defines the protocols, signals, electrical interfaces and mechanical requirements of the standard.[5]: p. V  The maximum pixel clock rate for HDMI 1.0 is 165 MHz, which is sufficient to allow 1080p and WUXGA (1920×1200) at 60 Hz. HDMI 1.3 increases that to 340 MHz, which allows for higher resolution (such as WQXGA, 2560×1600) across a single digital link.[38] An HDMI connection can either be single-link (type A/C/D) or dual-link (type B) and can have a video pixel rate of 25 MHz to 340 MHz (for a single-link connection) or 25 MHz to 680 MHz (for a dual-link connection). Video formats with pixel rates below 25 MHz (like 480i at 13.5 MHz) are transmitted over TMDS links using a pixel-repetition scheme.[5]: §§3, 6.4 

HDMI uses the Consumer Technology Association/Electronic Industries Alliance 861 standards. HDMI 1.0 to HDMI 1.2a uses the EIA/CEA-861-B video standard, HDMI 1.3 uses the CEA-861-D video standard, and HDMI 1.4 uses the CEA-861-E video standard.[5]: p. III  The CEA-861-E document defines "video formats and waveforms; colorimetry and quantization; transport of compressed and uncompressed LPCM audio; carriage of auxiliary data; and implementations of the Video Electronics Standards Association (VESA) Enhanced Extended Display Identification Data Standard (E-EDID)".[39] On July 15, 2013, the CEA announced the publication of CEA-861-F, a standard that can be used by video interfaces such as DVI, HDMI, and LVDS.[40] CEA-861-F adds the ability to transmit several Ultra HD video formats and additional color spaces.[40]

To ensure baseline compatibility between different HDMI sources and displays (as well as backward compatibility with the electrically compatible DVI standard) all HDMI devices must implement the sRGB color space at 8 bits per component.[5]: §6.2.3  Ability to use the Y′CBCR color space and higher color depths ("deep color") is optional. HDMI permits sRGB 4:4:4 chroma subsampling (8–16 bits per component), xvYCC 4:4:4 chroma subsampling (8–16 bits per component), Y′CBCR 4:4:4 chroma subsampling (8–16 bits per component), or Y′CBCR 4:2:2 chroma subsampling (8–12 bits per component). The color spaces that can be used by HDMI are ITU-R BT.601, ITU-R BT.709-5 and IEC 61966-2-4.[5]: §§6.5,6.7.2 

For digital audio, if an HDMI device has audio, it is required to implement the baseline format: stereo (uncompressed) PCM. Other formats are optional, with HDMI allowing up to 8 channels of uncompressed audio at sample sizes of 16 bits, 20 bits, or 24 bits, with sample rates of 32 kHz, 44.1 kHz, 48 kHz, 88.2 kHz, 96 kHz, 176.4 kHz, or 192 kHz.[5]: §7  HDMI also carries any IEC 61937-compliant compressed audio stream, such as Dolby Digital and DTS, and up to 8 channels of one-bit DSD audio (used on Super Audio CDs) at rates up to four times that of Super Audio CD.[5]: §7  With version 1.3, HDMI allows lossless compressed audio streams Dolby TrueHD and DTS-HD Master Audio.[5]: §7  As with the Y′CBCR video, audio capability is optional. Audio return channel (ARC) is a feature introduced in the HDMI 1.4 standard.[41] "Return" refers to the case where the audio comes from the TV and can be sent "upstream" to the AV receiver using the HDMI cable connected to the AV receiver.[41] An example given on the HDMI website is that a TV that directly receives a terrestrial/satellite broadcast, or has a video source built in, sends the audio "upstream" to the AV receiver.[41]

The HDMI standard was not designed to pass closed caption data (for example, subtitles) to the television for decoding.[42] As such, any closed caption stream must be decoded and included as an image in the video stream(s) prior to transmission over an HDMI cable to appear on the DTV. This limits the caption style (even for digital captions) to only that decoded at the source prior to HDMI transmission. This also prevents closed captions when transmission over HDMI is required for upconversion. For example, a DVD player that sends an upscaled 720p/1080i format via HDMI to an HDTV has no way to pass Closed Captioning data so that the HDTV can decode it, as there is no line 21 VBI in that format.

HDMI has three physically separate communication channels, which are the VESA DDC, TMDS and the optional CEC.[5]: §8.1  HDMI 1.4 added ARC and HEC.[41][43]

The Display Data Channel (DDC) is a VESA standard communications channel based on the I2C bus specification. HDMI specifically requires the device implement the Enhanced Display Data Channel (E-DDC), which is used by the HDMI source device to read the E-EDID data from the HDMI sink device to learn what audio/video formats it can take.[5]: §§8.1, CEC-1.2–CEC-1.3  HDMI requires that the E-DDC implement I2C standard mode speed (100 kbit/s) and allows it to optionally implement fast mode speed (400 kbit/s).[5]: §4.2.8 

I2C address 0x74 on the DDC channel is actively used for High-bandwidth Digital Content Protection (HDCP).

Transition-minimized differential signaling (TMDS) on HDMI interleaves video, audio and auxiliary data using three different packet types, called the video data period, the data island period and the control period. During the video data period, the pixels of an active video line are transmitted. During the data island period (which occurs during the horizontal and vertical blanking intervals), audio and auxiliary data are transmitted within a series of packets. The control period occurs between video and data island periods.[5]: §5.1.2 

Both HDMI and DVI use TMDS to send 10-bit characters that are encoded using 8b/10b encoding that differs from the original IBM form for the video data period and 2b/10b encoding for the control period. HDMI adds the ability to send audio and auxiliary data using 4b/10b encoding for the data island period. Each data island period is 32 pixels in size and contains a 32-bit packet header, which includes 8 bits of BCH ECC parity data for error correction and describes the contents of the packet. Each packet contains four subpackets, and each subpacket is 64 bits in size, including 8 bits of BCH ECC parity data, allowing for each packet to carry up to 224 bits of audio data. Each data island period can contain up to 18 packets. Seven of the 15 packet types described in the HDMI 1.3a specifications deal with audio data, while the other 8 types deal with auxiliary data. Among these are the general control packet and the gamut metadata packet. The general control packet carries information on AVMUTE (which mutes the audio during changes that may cause audio noise) and color depth (which sends the bit depth of the current video stream and is required for deep color). The gamut metadata packet carries information on the color space being used for the current video stream and is required for xvYCC.[5]: §§5.2–5.3,6.5.3,6.7.2,6.7.3 

Consumer Electronics Control (CEC) is an HDMI feature designed to allow the user to command and control up to 15 CEC-enabled devices, that are connected through HDMI,[44][45] by using only one of their remote controls (for example by controlling a television set, set-top box, and DVD player using only the remote control of the TV). CEC also allows for individual CEC-enabled devices to command and control each other without user intervention.[5]: §CEC-3.1 

It is a one-wire bidirectional serial bus that is based on the CENELEC standard AV.link protocol to perform remote control functions.[46] CEC wiring is mandatory, although implementation of CEC in a product is optional.[5]: §8.1  It was defined in HDMI Specification 1.0 and updated in HDMI 1.2, HDMI 1.2a and HDMI 1.3a (which added timer and audio commands to the bus).[5]: §§CEC-1.2, CEC-1.3, CEC-3.1, CEC-5  USB to CEC adapters exist that allow a computer to control CEC-enabled devices.[47][48][49][50]

Introduced in HDMI 1.4, HDMI Ethernet and Audio Return Channel (HEAC) adds a high-speed bidirectional data communication link (HEC) and the ability to send audio data upstream to the source device (ARC). HEAC utilizes two lines from the connector: the previously unused Reserved pin (called HEAC+) and the Hot Plug Detect pin (called HEAC−).[51]: §HEAC-2.1  If only ARC transmission is required, a single mode signal using the HEAC+ line can be used, otherwise, HEC is transmitted as a differential signal over the pair of lines, and ARC as a common mode component of the pair.[51]: §HEAC-2.2 


Audio Return Channel (ARC) and Enhanced Audio Return Channel (eARC)

ARC (Audio Return Channel) is an audio link introduced in 2009 with the HDMI 1.4 standard meant to replace other cables between the TV and the A/V receiver or speaker system.[41] This direction is used when the TV is the one that generates or receives the video stream instead of the other equipment.[41] A typical case is the running of an app on a smart TV such as Netflix, but reproduction of audio is handled by the other equipment.[41] Without ARC, the audio output from the TV must be routed by another cable, typically TOSLink or RCA, into the speaker system.[52] ARC supports compressed audio formats such as PCM (stereo), Dolby Digital, Dolby Digital Plus and DTS up to 5.1 channels, with Dolby Atmos metadata in Dolby codecs.[53]

eARC (Enhanced Audio Return Channel) was introduced in 2017 with the HDMI 2.1 standard. eARC has higher bandwidth (37 Mbps) and adds support for uncompressed surround sound, Dolby TrueHD and DTS-HD Master Audio passthrough with support for up to 32 channels. eARC requires an "Ultra High Speed", "Premium High Speed with Ethernet", or "High Speed with Ethernet" HDMI cable.[54][55]


HDMI Ethernet Channel (HEC)

HDMI Ethernet Channel technology consolidates video, audio, and data streams into a single HDMI cable, and the HEC feature enables IP-based applications over HDMI and provides a bidirectional Ethernet communication at 100 Mbit/s.[43] The physical layer of the Ethernet implementation uses a hybrid to simultaneously send and receive attenuated 100BASE-TX-type signals through a single twisted pair.[56][57]

HDMI is backward compatible with single-link Digital Visual Interface digital video (DVI-D or DVI-I, but not DVI-A or dual-link DVI). No signal conversion is required when an adapter or asymmetric cable is used, so there is no loss of video quality.[5]: appx. C 

From a user's perspective, an HDMI display can be driven by a single-link DVI-D source, since HDMI and DVI-D define an overlapping minimum set of allowed resolutions and frame-buffer formats to ensure a basic level of interoperability. In the reverse case, a DVI-D monitor has the same level of basic interoperability unless content protection with High-bandwidth Digital Content Protection (HDCP) interferes—or the HDMI color encoding is in component color space Y′CBCR instead of RGB, which is not possible in DVI. An HDMI source, such as a Blu-ray player, may require an HDCP-compliant display, and refuse to output HDCP-protected content to a non-compliant display.[58] A further complication is that there is a small amount of display equipment, such as some high-end home theater projectors, designed with HDMI inputs but not HDCP-compliant.

Any DVI-to-HDMI adapter can function as an HDMI-to-DVI adapter (and vice versa).[59] Typically, the only limitation is the gender of the adapter's connectors and the gender of the cables and sockets it is used with.

Features specific to HDMI, such as remote-control and audio transport, are not available in devices that use legacy DVI-D signalling. However, many devices output HDMI over a DVI connector (e.g., ATI 3000-series and NVIDIA GTX 200-series video cards),[5]: appx. C [60] and some multimedia displays may accept HDMI (including audio) over a DVI input. Exact capabilities beyond basic compatibility vary. Adapters are generally bi-directional.

High-bandwidth Digital Content Protection (HDCP) is a newer form of digital rights management (DRM). Intel created the original technology to make sure that digital content followed the guidelines set by the Digital Content Protection group.

HDMI can use HDCP to encrypt the signal if required by the source device. Content Scramble System (CSS), Content Protection for Recordable Media (CPRM) and Advanced Access Content System (AACS) require the use of HDCP on HDMI when playing back encrypted DVD Video, DVD Audio, HD DVD and Blu-ray Disc. The HDCP repeater bit controls the authentication and switching/distribution of an HDMI signal. According to HDCP Specification 1.2 (beginning with HDMI CTS 1.3a), any system that implements HDCP must do so in a fully compliant manner. HDCP testing that was previously only a requirement for optional tests such as the "Simplay HD" testing program is now part of the requirements for HDMI compliance.[5]: §9.2 [61][62] HDCP accommodates up to 127 connected devices with up to 7 levels, using a combination of sources, sinks and repeaters.[63] A simple example of this is several HDMI devices connected to an HDMI AV receiver that is connected to an HDMI display.[63]

Devices called HDCP strippers can remove the HDCP information from the video signal so the video can play on non-HDCP-compliant displays,[64] though a fair use and non-disclosure form must usually be signed with a registering agency before use.

There are five HDMI connector types. Type A/B are defined in the HDMI 1.0 specification, type C is defined in the HDMI 1.3 specification, and type D/E are defined in the HDMI 1.4 specification.










The HDMI alternate mode lets a user connect the reversible USB-C connector with the HDMI source devices (mobile, tablet, laptop). This cable connects to video display/sink devices using any of the native HDMI connectors. This is an HDMI cable, in this case a USB-C to HDMI cable.[72]

An HDMI cable is composed of four shielded twisted pairs, with impedance of the order of 100 Ω (±15%), plus seven separate conductors. HDMI cables with Ethernet differ in that three of the separate conductors instead form an additional shielded twisted pair (with the CEC/DDC ground as a shield).[51]: §HEAC-2.9 

Although no maximum length for an HDMI cable is specified, signal attenuation (dependent on the cable's construction quality and conducting materials) limits usable lengths in practice[73][74] and certification is difficult to achieve for lengths beyond 13 m.[75] HDMI 1.3 defines two cable categories: Category 1-certified cables, which have been tested at 74.25 MHz (which would include resolutions such as 720p60 and 1080i60), and Category 2-certified cables, which have been tested at 340 MHz (which would include resolutions such as 1080p60 and 4K30).[5]: §4.2.6 [66][76] Category 1 HDMI cables are marketed as "Standard" and Category 2 HDMI cables as "High Speed".[3] This labeling guideline for HDMI cables went into effect on October 17, 2008.[77][78] Category 1 and 2 cables can either meet the required parameter specifications for inter-pair skew, far-end crosstalk, attenuation and differential impedance, or they can meet the required non-equalized/equalized eye diagram requirements.[5]: §4.2.6  A cable of about 5 meters (16 feet) can be manufactured to Category 1 specifications easily and inexpensively by using 28 AWG (0.081 mm2) conductors.[73] With better quality construction and materials, including 24 AWG (0.205 mm2) conductors, an HDMI cable can reach lengths of up to 15 meters (49 feet).[73] Many HDMI cables under 5 meters of length that were made before the HDMI 1.3 specification can work as Category 2 cables, but only Category 2-tested cables are guaranteed to work for Category 2 purposes.[79]

As of the HDMI 2.1b specification, the following cable types are defined for HDMI in general:[80]

An HDMI extender is a single device (or pair of devices) powered with an external power source or with the 5V DC from the HDMI source.[81][82][83] Long cables can cause instability of HDCP and blinking on the screen, due to the weakened DDC signal that HDCP requires. HDCP DDC signals must be multiplexed with TMDS video signals to comply with HDCP requirements for HDMI extenders based on a single Category 5/Category 6 cable.[84][85] Several companies offer amplifiers, equalizers and repeaters that can string several standard HDMI cables together. Active HDMI cables use electronics within the cable to boost the signal and allow for HDMI cables of up to 30 meters (98 feet);[81] those based on HDBaseT can extend to 100 meters; HDMI extenders that are based on dual Category 5/Category 6 cable can extend HDMI to 250 meters (820 feet); while HDMI extenders based on optical fiber can extend HDMI to 300 meters (980 feet).[82][83]

The HDMI specification is not an open standard; manufacturers need to be licensed by HDMI LA in order to implement HDMI in any product or component. Companies that are licensed by HDMI LA are known as HDMI Adopters.[86]

DVI is the only interface that does not require a license for interfacing HDMI.[citation needed]

While earlier versions of HDMI specs are available to the public for download, only adopters have access to the latest standards (HDMI 1.4b/2.1). Only adopters have access to the compliance test specification (CTS) that is used for compliance and certification. Compliance testing is required before any HDMI product can be legally sold.

There are two annual fee structures associated with being an HDMI adopter:

The annual fee is due upon the execution of the Adopter Agreement, and must be paid on the anniversary of this date each year thereafter.

The royalty fee structure is the same for all volumes. The following variable per-unit royalty is device-based and not dependent on number of ports, chips or connectors:

Use of HDMI logo requires compliance testing. Adopters need to license HDCP separately.

The HDMI royalty is only payable on licensed products that will be sold on a stand-alone basis (i.e., that are not incorporated into another licensed product that is subject to an HDMI royalty). For example, if a cable or IC is sold to an adopter who then includes it in a television subject to a royalty, then the cable or IC maker would not pay a royalty, and the television manufacturer would pay the royalty on the final product. If the cable is sold directly to consumers, then the cable would be subject to a royalty.[87]

HDMI devices and cables are designed based on the HDMI Specification, a document published by HDMI Licensing (through version 1.4b) or the HDMI Forum (from version 2.0 onward). The HDMI Specification defines the minimum baseline requirements that all HDMI devices must adhere to for interoperability, as well as a large set of optional features that HDMI devices may support. The specification is periodically updated to add clarifications or define new capabilities that HDMI devices may implement. Each new version of the specification expands the list of possible features, but does not mandate support for new features in all devices or establish any "classes" of HDMI products which must support certain capabilities. Version numbers do not refer to classes or tiers of products with certain levels of feature support, and as such, HDMI specification "version numbers" are not a method of describing support for specific features or describing the capabilities of an HDMI device or cable.[88][89][90]

In 2009, HDMI Licensing banned the use of "version numbers" in labeling HDMI products.[91] Instead, HDMI devices should explicitly declare which features and capabilities they support. For HDMI cables, a speed rating system was established since feature support is not dependent on the cable (apart from inline Ethernet and ARC); the cable only affects the maximum possible speed of the connection.[89] HDMI cables should be labeled with the appropriate speed certification (i.e. Standard Speed, High Speed, or Ultra High Speed), not a "version number".[88]

HDMI 1.0 was released on December 9, 2002, and is a single-cable digital audio/video connector interface. The link architecture is based on DVI, using exactly the same video transmission format but sending audio and other auxiliary data during the blanking intervals of the video stream. HDMI 1.0 allows a maximum TMDS clock of 165 MHz (4.95 Gbit/s bandwidth per link), the same as DVI. It defines two connectors called type A and type B, with pinouts based on the Single-Link DVI-D and Dual-Link DVI-D connectors respectively, though the type B connector was never used in any commercial products. HDMI 1.0 uses TMDS encoding for video transmission, giving it 3.96 Gbit/s of video bandwidth (1920 × 1080 or 1920 × 1200 at 60 Hz) and 8-channel LPCM/192 kHz/24-bit audio. HDMI 1.0 requires support for RGB video, with optional support for Y′CBCR 4:4:4 and 4:2:2 (mandatory if the device has support for Y′CBCR on other interfaces). Color depth of 10 bpc (30 bit/px) or 12 bpc (36 bit/px) is allowed when using 4:2:2 subsampling, but only 8 bpc (24 bit/px) color depth is permitted when using RGB or Y′CBCR 4:4:4. Only the Rec. 601 and Rec. 709 color spaces are supported. HDMI 1.0 allows only specific pre-defined video formats, including all the formats defined in EIA/CEA-861-B and some additional formats listed in the HDMI Specification itself. All HDMI sources/sinks must also be capable of sending/receiving native Single-Link DVI video and be fully compliant with the DVI Specification.[92]

HDMI 1.1 was released on May 20, 2004, and added support for DVD-Audio.

HDMI 1.2 was released on August 8, 2005, and added the option of One Bit Audio, used on Super Audio CDs, at up to 8 channels. To make HDMI more suitable for use on PC devices, version 1.2 also removed the requirement that only explicitly supported formats be used. It added the ability for manufacturers to create vendor-specific formats, allowing any arbitrary resolution and refresh rate rather than being limited to a pre-defined list of supported formats. In addition, it added explicit support for several new formats including 720p at 100 and 120 Hz and relaxed the pixel format support requirements so that sources with only native RGB output (PC sources) would not be required to support Y′CBCR output.[93]: §6.2.3 

HDMI 1.2a was released on December 14, 2005 and fully specifies Consumer Electronic Control (CEC) features, command sets and CEC compliance tests.[93]

HDMI 1.3 was released on June 22, 2006, and increased the maximum TMDS clock to 340 MHz (10.2 Gbit/s).[5][38][94] Like previous versions, it uses TMDS encoding, giving it a maximum video bandwidth of 8.16 Gbit/s (sufficient for 1920 × 1080 at 144 Hz or 2560 × 1440 at 75 Hz). It added support for 10 bpc, 12 bpc, and 16 bpc color depth (30, 36, and 48 bit/px), called deep color. It also added support for the xvYCC color space, in addition to the ITU-R BT.601 and BT.709 color spaces supported by previous versions, and added the ability to carry metadata defining color gamut boundaries. It also optionally allows output of Dolby TrueHD and DTS-HD Master Audio streams for external decoding by AV receivers.[95] It incorporates automatic audio syncing (audio video sync) capability.[38] It defined cable Categories 1 and 2, with Category 1 cable being tested up to 74.25 MHz and Category 2 being tested up to 340 MHz.[5]: §4.2.6  It also added the new HDMI type C "Mini" connector for portable devices.[5]: §4.1.1 [96]

HDMI 1.3a was released on November 10, 2006, and had cable and sink modifications for HDMI type C, source termination recommendations, and removed undershoot and maximum rise/fall time limits. It also changed CEC capacitance limits, and CEC commands for timer control were brought back in an altered form, with audio control commands added. It also added the optional ability to stream SACD in its bitstream DST format rather than uncompressed raw DSD.[5] HDMI 1.3a is available to download free of charge, after registration.[97]

HDMI 1.4 was released on June 5, 2009, and first came to market after Q2 of 2009.[66][98][99] Retaining the bandwidth of the previous version, HDMI 1.4 defined standardized timings to use for 4096 × 2160 at 24 Hz, 3840 × 2160 at 24, 25, and 30 Hz, and added explicit support for 1920 × 1080 at 120 Hz with CTA-861 timings.[68]: §6.3.2  It also added an HDMI Ethernet Channel (HEC) that accommodates a 100 Mbit/s Ethernet connection between the two HDMI connected devices so they can share an Internet connection,[43] introduced an audio return channel (ARC),[41] 3D Over HDMI, a new Micro HDMI Connector, an expanded set of color spaces with the addition of sYCC601, Adobe RGB and Adobe YCC601, and an Automotive Connection System.[66][100][101][102][103] HDMI 1.4 defined several stereoscopic 3D formats including field alternative (interlaced), frame packing (a full resolution top-bottom format), line alternative full, side-by-side half, side-by-side full, 2D + depth, and 2D + depth + graphics + graphics depth (WOWvx).[65][104][105] HDMI 1.4 requires that 3D displays implement the frame packing 3D format at either 720p50 and 1080p24 or 720p60 and 1080p24.[105] High Speed HDMI cables as defined in HDMI 1.3 work with all HDMI 1.4 features except for the HDMI Ethernet Channel, which requires the new High Speed HDMI Cable with Ethernet defined in HDMI 1.4.[65][104][105]

HDMI 1.4a was released on March 4, 2010, and added two mandatory 3D formats for broadcast content, which was deferred with HDMI 1.4 pending the direction of the 3D broadcast market.[106][107] HDMI 1.4a has defined mandatory 3D formats for broadcast, game, and movie content.[106] HDMI 1.4a requires that 3D displays implement the frame packing 3D format at either 720p50 and 1080p24 or 720p60 and 1080p24, side-by-side horizontal at either 1080i50 or 1080i60, and top-and-bottom at either 720p50 and 1080p24 or 720p60 and 1080p24.[107]

HDMI 1.4b was released on October 11, 2011,[108] containing only minor clarifications to the 1.4a document. HDMI 1.4b is the last version of the standard that HDMI LA is responsible for. All later versions of the HDMI Specification are produced by the HDMI Forum, created on October 25, 2011.[34][109]

HDMI 2.0, referred to by some manufacturers as HDMI UHD, was released on September 4, 2013.[110]

HDMI 2.0 increases the maximum bandwidth to 18.0 Gbit/s.[110][111][112] HDMI 2.0 uses TMDS encoding for video transmission like previous versions, giving it a maximum video bandwidth of 14.4 Gbit/s. This enables HDMI 2.0 to carry 4K video at 60 Hz with 24 bit/px color depth.[110][113][114] Other features of HDMI 2.0 include support for the Rec. 2020 color space, up to 32 audio channels, up to 1536 kHz audio sample frequency, dual video streams to multiple users on the same screen, up to four audio streams, 4:2:0 chroma subsampling, 25 fps 3D formats, support for the 21:9 aspect ratio, dynamic synchronization of video and audio streams, the HE-AAC and DRA audio standards, improved 3D capability, and additional CEC functions.[110][115][116]

HDMI 2.0a was released on April 8, 2015, and added support for High Dynamic Range (HDR) video with static metadata.[117]

HDMI 2.0b was released March 2016.[118] HDMI 2.0b initially supported the same HDR10 standard as HDMI 2.0a as specified in the CTA-861.3 specification.[115] In December 2016 additional support for HDR Video transport was added to HDMI 2.0b in the CTA-861-G specification, which extends the static metadata signaling to include hybrid log–gamma (HLG).[115][119][120]

HDMI 2.1 was officially announced by the HDMI Forum on January 4, 2017,[121][122] and was released on November 28, 2017.[123] It adds support for higher resolutions and higher refresh rates, including 4K 120 Hz and 8K 60 Hz. HDMI 2.1 also introduces a new HDMI cable category called Ultra High Speed (referred to as 48G during development), which certifies cables at the new higher speeds that these formats require. Ultra High Speed HDMI cables are backwards compatible with older HDMI devices, and older cables are compatible with new HDMI 2.1 devices, though the full 48 Gbit/s bandwidth is only supported with the new cables.

Some systems may not be able to use HDMI 2.1 because the HDMI Forum is preventing its use in open source implementations (such as Linux open source drivers). Users of those systems may need to use DisplayPort instead to access high resolutions and speeds.[124]

The following features were added to the HDMI 2.1 Specification:[123][125]

Video formats that require more bandwidth than 18.0 Gbit/s (4K 60 Hz 8 bpc RGB), such as 4K 60 Hz 10 bpc (HDR), 4K 120 Hz, and 8K 60 Hz, may require the new "Ultra High Speed" or "Ultra High Speed with Ethernet" cables.[122] HDMI 2.1's other new features are supported with existing HDMI cables.

The increase in maximum bandwidth is achieved by increasing both the bitrate of the data channels and the number of channels. Previous HDMI versions use three data channels (each operating at up to 6.0 Gbit/s in HDMI 2.0, or up to 3.4 Gbit/s in HDMI 1.4), with an additional channel for the TMDS clock signal, which runs at a fraction of the data channel speed (one tenth the speed, or up to 340 MHz, for signaling rates up to 3.4 Gbit/s; one fortieth the speed, or up to 150 MHz, for signaling rates between 3.4 and 6.0 Gbit/s). HDMI 2.1 doubles the signaling rate of the data channels to 12 Gbit/s. The structure of the data has been changed to use a new packet-based format with an embedded clock signal, which allows what was formerly the TMDS clock channel to be used as a fourth data channel instead, increasing the signaling rate across that channel to 12 Gbit/s as well. These changes increase the aggregate bandwidth from 18.0 Gbit/s (3 × 6.0 Gbit/s) to 48.0 Gbit/s (4 × 12.0 Gbit/s), a 2.66× improvement in bandwidth. In addition, the data is transmitted more efficiently by using a 16b/18b encoding scheme, which uses a larger percentage of the bandwidth for data rather than DC balancing compared to the TMDS scheme used by previous versions (88.8% compared to 80%). This, in combination with the 2.66× bandwidth, raises the maximum data rate of HDMI 2.1 from 14.4 Gbit/s to 42.6 Gbit/s. Subtracting overhead for FEC, the usable data rate is approximately 42.0 Gbit/s, around 2.92× the data rate of HDMI 2.0.[127][128]

The 48 Gbit/s bandwidth provided by HDMI 2.1 is enough for 8K resolution at approximately 50 Hz, with 8 bpc RGB or Y′CBCR 4:4:4 color. To achieve even higher formats, HDMI 2.1 can use Display Stream Compression (DSC) with a compression ratio of up to 3∶1. Using DSC, formats up to 8K (7680 × 4320) 120 Hz or 10K (10240 × 4320) 100 Hz at 8 bpc RGB/4:4:4 are possible. Using Y′CBCR with 4:2:2 or 4:2:0 chroma subsampling in combination with DSC can allow for even higher formats.[125]

HDMI 2.1a was released on February 15, 2022, and added support for Source-Based Tone Mapping (SBTM).[129][130]

HDMI 2.1b was released on August 10, 2023.[131]

HDMI 2.2 was announced on January 6, 2025. It will be released in the first half of 2025. The maximum allowed bit rate is increased to 96 Gbit/s and support to Latency Indication Protocol (LIP) for improving audio and video synchronization.[132]

The maximum limits for TMDS transmission are calculated using standard data rate calculations.[145] For FRL transmission, the limits are calculated using the capacity computation algorithm provided by the HDMI Specification.[146]: §6.5.6.2.1  All calculations assume uncompressed RGB video with CVT-RB v2 timing. Maximum limits may differ if compression (i.e. DSC) or Y′CBCR 4:2:0 chroma subsampling are used.

Display manufacturers may also use non-standard blanking intervals (a Vendor-Specific Timing Format as defined in the HDMI Specification[5]: §6.1 ) rather than CVT-RB v2 to achieve even higher frequencies when bandwidth is a constraint. The refresh frequencies in the below table do not represent the absolute maximum limit of each interface, but rather an estimate based on a modern standardized timing formula. The minimum blanking intervals (and therefore the exact maximum frequency that can be achieved) will depend on the display and how many secondary data packets it requires, and therefore will differ from model to model.

HDMI 1.0 and 1.1 are restricted to transmitting only certain video formats,[92]: §6.1  defined in EIA/CEA-861-B and in the HDMI Specification itself.[92]: §6.3  HDMI 1.2 and all later versions allow any arbitrary resolution and frame rate (within the bandwidth limit). Formats that are not supported by the HDMI Specification (i.e., no standardized timings defined) may be implemented as a vendor-specific format. Successive versions of the HDMI Specification continue to add support for additional formats (such as 4K resolutions), but the added support is to establish standardized timings to ensure interoperability between products, not to establish which formats are or are not permitted. Video formats do not require explicit support from the HDMI Specification in order to be transmitted and displayed.[93]: §6.1 

Individual products may have heavier limitations than those listed below, since HDMI devices are not required to support the maximum bandwidth of the HDMI version that they implement. Therefore, it is not guaranteed that a display will support the refresh rates listed in this table, even if the display has the required HDMI version.

Uncompressed 8 bpc (24 bit/px) color depth and RGB or Y′CBCR 4:4:4 color format are assumed on this table except where noted.

HDR10 requires 10 bpc (30 bit/px) color depth, which uses 25% more bandwidth than standard 8 bpc video.

Uncompressed 10 bpc color depth and RGB or Y′CBCR 4:4:4 color format are assumed on this table except where noted.

The features defined in the HDMI Specification that an HDMI device may implement are listed below. For historical interest, the version of the HDMI Specification in which the feature was first added is also listed. All features of the HDMI Specification are optional; HDMI devices may implement any combination of these features.

Although the "HDMI version numbers" are commonly misused as a way of indicating that a device supports certain features, this notation has no official meaning and is considered improper by HDMI Licensing.[147] There is no officially-defined correlation between features supported by a device and any claimed "version numbers", as version numbers refer to historical editions of the HDMI Specification document, not to particular classes of HDMI devices. Manufacturers are forbidden from describing their devices using HDMI version numbers, and are required to identify support for features by listing explicit support for them,[148][149] but the HDMI forum has received criticism for lack of enforcement of these policies.[150]

Display Stream Compression (DSC) is a VESA-developed video compression algorithm designed to enable increased display resolutions and frame rates over existing physical interfaces, and make devices smaller and lighter, with longer battery life.[154]

Blu-ray Disc and HD DVD, introduced in 2006, offer high-fidelity audio features that require HDMI for best results. HDMI 1.3 can transport Dolby Digital Plus, Dolby TrueHD, and DTS-HD Master Audio bitstreams in compressed form.[5]: §7  This capability allows for an AV receiver with the necessary decoder to decode the compressed audio stream. The Blu-ray specification does not include video encoded with either deep color or xvYCC; thus, HDMI 1.0 can transfer Blu-ray discs at full video quality.[155]

The HDMI 1.4 specification (released in 2009) added support for 3D video and is used by all Blu-ray 3D compatible players.

The Blu-ray Disc Association (BDA) spokespersons have stated (Sept. 2014 at IFA show in Berlin, Germany) that the Blu-ray, Ultra HD players, and 4K discs are expected to be available starting in the second half to 2015. It is anticipated that such Blu-ray UHD players will be required to include a HDMI 2.0 output that supports HDCP 2.2.

Blu-ray permits secondary audio decoding, whereby the disc content can tell the player to mix multiple audio sources together before final output.[156] Some Blu-ray and HD DVD players can decode all of the audio codecs internally and can output LPCM audio over HDMI. Multichannel LPCM can be transported over an HDMI connection, and as long as the AV receiver implements multichannel LPCM audio over HDMI and implements HDCP, the audio reproduction is equal in resolution to HDMI 1.3 bitstream output. Some low-cost AV receivers, such as the Onkyo TX-SR506, do not allow audio processing over HDMI and are labelled as "HDMI pass through" devices.[157][158] Virtually all modern AV Receivers now offer HDMI 1.4 inputs and outputs with processing for all of the audio formats offered by Blu-ray Discs and other HD video sources. During 2014 several manufacturers introduced premium AV Receivers that include one, or multiple, HDMI 2.0 inputs along with a HDMI 2.0 output(s). However, not until 2015 did most major manufacturers of AV receivers also support HDCP 2.2 as needed to support certain high quality UHD video sources, such as Blu-ray UHD players.

Most consumer camcorders, as well as many digital cameras, are equipped with a mini-HDMI connector (type C connector).

Some cameras also have 4K capability,
although cameras capable of HD video often include an HDMI interface for playback or even live preview, the image processor and the video processor of cameras usable for uncompressed video must be able to deliver the full image resolution at the specified frame rate in real time without any missing frames causing jitter. Therefore, usable uncompressed video out of HDMI is often called "clean HDMI".[159][160]

Personal computer (PCs) with a DVI interface are capable of video output to an HDMI-enabled monitor.[5]: appx. C  Some PCs include an HDMI interface and may also be capable of HDMI audio output, depending on specific hardware.[161] For example, Intel's motherboard chipsets since the 945G and NVIDIA's GeForce 8200/8300 motherboard chipsets are capable of 8-channel LPCM output over HDMI.[161][162] Eight-channel LPCM audio output over HDMI with a video card was first seen with the ATI Radeon HD 4850, which was released in June 2008 and is implemented by other video cards in the ATI Radeon HD 4000 series.[162][163][164][165][166] Linux can drive 8-channel LPCM audio over HDMI if the video card has the necessary hardware and implements the Advanced Linux Sound Architecture (ALSA).[167] The ATI Radeon HD 4000 series implements ALSA.[167][168] Cyberlink announced in June 2008 that they would update their PowerDVD playback software to allow 192 kHz/24-bit Blu-ray Disc audio decoding in Q3-Q4 of 2008.[169] Corel's WinDVD 9 Plus currently has 96 kHz/24-bit Blu-ray Disc audio decoding.[170]

Even with an HDMI output, a computer may not be able to produce signals that implement HDCP, Microsoft's Protected Video Path, or Microsoft's Protected Audio Path.[162][171] Several early graphic cards were labelled as "HDCP-enabled" but did not have the hardware needed for HDCP;[172] this included some graphic cards based on the ATI X1600 chipset and certain models of the NVIDIA Geforce 7900 series.[172] The first computer monitors that could process HDCP were released in 2005; by February 2006 a dozen different models had been released.[173][174] The Protected Video Path was enabled in graphic cards that had HDCP capability, since it was required for output of Blu-ray Disc and HD DVD video. In comparison, the Protected Audio Path was required only if a lossless audio bitstream (such as Dolby TrueHD or DTS-HD MA) was output.[162] Uncompressed LPCM audio, however, does not require a Protected Audio Path, and software programs such as PowerDVD and WinDVD can decode Dolby TrueHD and DTS-HD MA and output it as LPCM.[162][169][170] A limitation is that if the computer does not implement a Protected Audio Path, the audio must be downsampled to 16-bit 48 kHz but can still output at up to 8 channels.[162] No graphic cards were released in 2008 that implemented the Protected Audio Path.[162]

The Asus Xonar HDAV1.3 became the first HDMI sound card that implemented the Protected Audio Path and could both bitstream and decode lossless audio (Dolby TrueHD and DTS-HD MA), although bitstreaming is only available if using the ArcSoft TotalMedia Theatre software.[175][176] It has an HDMI 1.3 input/output, and Asus says that it can work with most video cards on the market.[175][176][177]

Legacy interfaces such as VGA, DVI and LVDS have not kept pace, and newer standards such as DisplayPort and HDMI clearly provide the best connectivity options moving forward. In our opinion, DisplayPort 1.2 is the future interface for PC monitors, along with HDMI 1.4a for TV connectivity.

In September 2009, AMD announced the ATI Radeon HD 5000 series video cards, which have HDMI 1.3 output (deep color, xvYCC wide gamut capability and high bit rate audio), 8-channel LPCM over HDMI, and an integrated HD audio controller with a Protected Audio Path that allows bitstream output over HDMI for AAC, Dolby AC-3, Dolby TrueHD and DTS-HD Master Audio formats.[178][179][180] The ATI Radeon HD 5870 released in September 2009 is the first video card that allows bitstream output over HDMI for Dolby TrueHD and DTS-HD Master Audio.[180] The AMD Radeon HD 6000 series implements HDMI 1.4a. The AMD Radeon HD 7000 series implements HDMI 1.4b.[181]

In December 2010, it was announced that several computer vendors and display makers including Intel, AMD, Dell, Lenovo, Samsung, and LG would stop using LVDS (actually, FPD-Link) from 2013 and legacy DVI and VGA connectors from 2015, replacing them with DisplayPort and HDMI.[182][183]

On August 27, 2012, Asus announced a new 27 in (69 cm) monitor that produces its native resolution of 2560×1440 via HDMI 1.4.[184][185]

On September 18, 2014, Nvidia launched GeForce GTX 980 and GTX 970 (with GM204 chip) with HDMI 2.0 support. On January 22, 2015, GeForce GTX 960 (with GM206 chip) launched with HDMI 2.0 support. On March 17, 2015, GeForce GTX TITAN X (GM200) launched with HDMI 2.0 support. On June 1, 2015, GeForce GTX 980 Ti (with GM200 chip) launched with HDMI 2.0 support. On August 20, 2015, GeForce GTX 950 (with GM206 chip) launched with HDMI 2.0 support.

On May 6, 2016, Nvidia launched the GeForce GTX 1080 (GP104 GPU) with HDMI 2.0b support.[186]

On September 1, 2020, Nvidia launched the GeForce RTX 30 series, the world's first discrete graphics cards with support for the full 48 Gbit/s bandwidth with Display Stream Compression 1.2 of HDMI 2.1.[187][188][189]

Beginning with the seventh generation of video game consoles, most consoles support HDMI. Video game consoles that support HDMI include the Xbox 360 (except most pre-2007 models) (1.2a), Xbox One (1.4b), Xbox One S (2.0a), Xbox One X (2.0b), PlayStation 3 (1.3a), PlayStation 4 (1.4b), PlayStation 4 Pro (2.0a), Wii U (1.4a), Nintendo Switch (1.4b), Nintendo Switch (OLED model) (2.0a), Xbox Series X and Series S (2.1), and PlayStation 5 (2.1).

Some tablet computers implement HDMI using Micro-HDMI (type D) port, while others like the Eee Pad Transformer implement the standard using mini-HDMI (type C) ports. All iPad models have a special A/V adapter that converts Apple's Lightning connector to a standard HDMI (type A) port. Samsung has a similar proprietary thirty-pin port for their Galaxy Tab 10.1 that could adapt to HDMI as well as USB drives. The Dell Streak 5 smartphone/tablet hybrid is capable of outputting over HDMI. While the Streak uses a PDMI port, a separate cradle adds HDMI compatibility. Some tablets running Android OS provide HDMI output using a mini-HDMI (type C) port. Most new laptops and desktops now have built in HDMI as well.

Many mobile phones can produce an output of HDMI video via a micro-HDMI connector, SlimPort, MHL[190][191][192] or other adapter.[193][194][195][196]

HDMI can only be used with older analog-only devices (using connections such as SCART, VGA, RCA, etc.) by means of a digital-to-analog converter or AV receiver, as the interface does not carry any analog signals (unlike DVI, where devices with DVI-I ports accept or provide either digital or analog signals). Cables are available that contain the necessary electronics, but it is important to distinguish these active converter cables from passive HDMI to VGA cables (which are typically cheaper as they don't include any electronics). The passive cables are only useful if a user has a device that is generating or expecting HDMI signals on a VGA connector, or VGA signals on an HDMI connector; this is a non-standard feature, not implemented by most devices.

The HDMI Alternate Mode for USB-C allows HDMI-enabled sources with a USB-C connector to directly connect to standard HDMI display devices, without requiring an adapter.[197] The standard was released in September 2016, and supports all HDMI 1.4b features such as video resolutions up to Ultra HD 30 Hz and CEC.[198] Previously, the similar DisplayPort Alternate Mode could be used to connect to HDMI displays from USB type-C sources, but where in that case active adapters were required to convert from DisplayPort to HDMI, HDMI Alternate Mode connects to the display natively.[199]

The Alternate Mode reconfigures the four SuperSpeed differential pairs present in USB-C to carry the three HDMI TMDS channels and the clock signal. The two Sideband Use pins (SBU1 and SBU2) are used to carry the HDMI Ethernet and Audio Return Channel and the Hot Plug Detect functionality (HEAC+/Utility pin and HEAC−/HPD pin). As there are not enough reconfigurable pins remaining in USB-C to accommodate the DDC clock (SCL), DDC data (SDA), and CEC – these three signals are bridged between the HDMI source and sink via the USB Power Delivery 2.0 (USB-PD) protocol, and are carried over the USB-C Configuration Channel (CC) wire.[197] This is possible because the cable is electronically marked (i.e., it contains a USB-PD node) that serves to tunnel the DDC and CEC from the source over the Configuration Channel to the node in the cable, these USB-PD messages are received and relayed to the HDMI sink as regenerated DDC (SCL and SDA signals), or CEC signals.[197]

As stated at CES in January 2023, HDMI Alternate Mode for USB Type-C is no longer being updated[200] as there are no known products using this protocol, reducing its relevance in the current market. This will reduce consumer confusion as DisplayPort Alternate Mode is the primary video protocol of choice over USB-C.

The DisplayPort audio/video interface was introduced in May 2006 by the Video Electronics Standards Association (VESA). Historically, HDMI Licensing LLC was publicly dismissive of DisplayPort's position in the industry, with its president stating in a 2009 interview that "there are certainly some PCs that have 
DisplayPort connectors on them, but these are niche applications that have not taken hold in the market."[201]

In recent years, DisplayPort connectors have become a common feature of premium[202] products—displays, desktop computers, and video cards; most of the companies producing DisplayPort equipment are in the computer sector. The DisplayPort website states that DisplayPort is expected to complement HDMI,[203] but as of 2016[update] 100% of HD and UHD TVs had HDMI connectivity.[204] DisplayPort supported some advanced features which are useful for multimedia content creators and gamers (e.g., 5K, Adaptive-Sync), which was the reason most GPUs have DisplayPort. These features were added to the official HDMI specification slightly later, but with the introduction of HDMI 2.1, these gaps are already leveled off (e.g., VRR / Variable Refresh Rate).

DisplayPort uses a self-clocking, micro-packet-based protocol that allows for a variable number of differential pair lanes as well as flexible allocation of bandwidth between audio and video, and allows encapsulating multi-channel compressed audio formats in the audio stream.[205][206] DisplayPort 1.2 supports multiple audio/video streams, variable refresh rate (FreeSync), and Dual-mode transmitters compatible with HDMI 1.2 or 1.4.[205][207][208] Revision 1.3 increases overall transmission bandwidth to 32.4 Gbit/s with the new HBR3 mode featuring 8.1 Gbit/s per lane; it requires Dual-mode with mandatory HDMI 2.0 compatibility and HDCP 2.2.[209][210] Revision 1.4 added Display Stream Compression (DSC), support for the BT.2020 color space, and HDR10 extensions from CTA-861.3, including static and dynamic metadata.[211] Revision 1.4a was published in April 2018,[212] updating DisplayPort's DSC implementation from 1.2 to 1.2a.[213] Revision 2.0 increased overall bandwidth from 25.92 to 77.37 Gbit/s, enabling increased resolutions and refresh rates, increasing the resolutions and refresh rates with HDR support, and other related improvements.[214] Revision 2.1 was published in October 2022, incorporating the new DP40 and DP80 cable certifications, which require proper operation at the UHBR10 (40 Gbit/s) and UHBR20 (80 Gbit/s) speeds introduced in version 2.0, and a bandwidth management feature to enable DisplayPort tunnelling to coexist with other I/O data traffic more efficiently over a USB4/USB Type-C connection.[215]

The DisplayPort features an adapter detection mechanism enabling dual-mode operation and the transmission of TMDS signals allowing the conversion to DVI and HDMI 1.2/1.4/2.0 signals using a passive adapter.[216][205] The same external connector is used for both protocols – when a DVI/HDMI passive adapter is attached, the transmitter circuit switches to TMDS mode. DisplayPort Dual-mode ports and cables/adapters are typically marked with the DisplayPort++ logo. Thunderbolt ports with mDP connector also supports Dual-mode passive HDMI adapters/cables. Conversion to dual-link DVI and component video (VGA/YPbPr) requires active powered adapters.[205][216]

The USB 3.1 type-C connector is increasingly the standard video connector, replacing legacy video connectors such as mDP, Thunderbolt, HDMI, and VGA in mobile devices. USB-C connectors can transmit DisplayPort video to docks and displays using standard USB type-C cables or type-C to DisplayPort cables and adapters; USB-C also supports HDMI adapters that actively convert from DisplayPort to HDMI 1.4 or 2.0. DisplayPort Alternate Mode for USB type-C specification was published in 2015. USB type-C chipsets are not required to include Dual-mode, so passive DP-HDMI adapters do not work with type-C sources. A specification for "HDMI Alternate Mode for USB type-C" was released in 2016, but was discontinued in 2023, with HDMI Licensing Administration stating they knew of no adapter having ever been produced.[217]

DisplayPort is royalty-free, though patent pool administrator Via LA attempts to collect a $0.20 per-device charge for a bulk license to patents it regards as essential to the DisplayPort specification,[218] while HDMI has an annual fee of US$10,000 and a per unit royalty rate of between $0.04 and $0.15.[219]

HDMI has had a few advantages over DisplayPort, such as ability to carry Consumer Electronics Control (CEC) signals since its first generation (DisplayPort 1.3, introduced in 2014, is the earliest DisplayPort generation which can carry CEC signals).[220][208][221]

Mobile High-Definition Link (MHL) is an adaptation of HDMI intended to connect mobile devices such as smartphones and tablets to high-definition televisions (HDTVs) and displays.[222][223] Unlike DVI, which is compatible with HDMI using only passive cables and adapters, MHL requires that the HDMI socket be MHL-enabled, otherwise an active adapter (or dongle) is required to convert the signal to HDMI. MHL is developed by a consortium of five consumer electronics manufacturers, several of which are also behind HDMI.[224]

MHL pares down the three TMDS channels in a standard HDMI connection to a single one running over any connector that provides at least five pins.[224] This lets existing connectors in mobile devices – such as micro-USB – be used, avoiding the need for additional dedicated video output sockets.[225] The USB port switches to MHL mode when it detects a compatible device is connected.

In addition to the features in common with HDMI (such as HDCP encrypted uncompressed high-definition video and eight-channel surround sound), MHL also adds the provision of power charging for the mobile device while in use, and also enables the TV remote to control it. Although support for these additional features requires connection to an MHL-enabled HDMI port, power charging can also be provided when using active MHL to HDMI adapters (connected to standard HDMI ports), provided there is a separate power connection to the adapter.

Like HDMI, MHL defines a USB-C Alternate Mode to support the MHL standard over USB-C connections.

Version 1.0 supported 720p/1080i 60 Hz (RGB/4:4:4 pixel encoding) with a bandwidth of 2.25 Gbit/s. Versions 1.3 and 2.0 added support for 1080p 60 Hz (Y′CBCR 4:2:2) with a bandwidth of 3 Gbit/s in PackedPixel mode.[223] Version 3.0 increased the bandwidth to 6 Gbit/s to support Ultra HD (3840 × 2160) 30 Hz video, and also changed from being frame-based, like HDMI, to packet-based.[226]

The fourth version, superMHL, increased bandwidth by operating over multiple TMDS differential pairs (up to a total of six) allowing a maximum of 36 Gbit/s.[227] The six lanes are supported over a reversible 32-pin superMHL connector, while four lanes are supported over USB-C Alternate Mode (only a single lane is supported over micro-USB/HDMI). Display Stream Compression (DSC) is used to allow up to 8K Ultra HD (7680 × 4320) 120 Hz HDR video, and to support Ultra HD 60 Hz video over a single lane.[227]The history of human activity in Indiana, a U.S. state in the Midwest, stems back to the migratory tribes of Native Americans who inhabited Indiana as early as 8000 BC. Tribes succeeded one another in dominance for several thousand years and reached their peak of development during the period of the Mississippian culture. The region entered recorded history in the 1670s, when the first Europeans came to Indiana and claimed the territory for the Kingdom of France. After France ruled for a century (with little settlement in this area), it was defeated by the Kingdom of Great Britain in the French and Indian War (Seven Years' War) and ceded its territory east of the Mississippi River. Britain held the land for more than twenty years, until after its defeat in the American Revolutionary War. Britain then ceded the entire trans-Allegheny region, including what is now Indiana, to the newly formed United States.

The U.S. government divided the trans-Allegheny region into several new territories. The largest of these was the Northwest Territory, which the U.S. Congress subsequently subdivided into several smaller territories. In 1800, Indiana Territory became the first of these new territories established. As Indiana Territory grew in population and development, it was divided in 1805 and again in 1809 until, reduced to its current size and boundaries, it retained the name Indiana and was admitted to the Union on December 11, 1816, as the nineteenth state.

The newly established state government set out on an ambitious plan to transform Indiana from a segment of the frontier into a developed, well-populated, and thriving state. The state founders initiated an internal improvement program that led to the construction of roads, canals, railroads, and state-funded public schools. Despite the noble aims of the project, profligate spending ruined the state's credit. By 1841, the state was near bankruptcy and was forced to liquidate most of its public works. Acting under its new Constitution of 1851, the state government enacted major financial reforms, required that most public offices be filled by election rather than appointment, and greatly weakened the power of the governor. The ambitious development program of Indiana's founders was realized when Indiana became the fourth-largest state in terms of population, as measured by the 1860 United States census.

Indiana became politically influential and played an important role in the Union during the American Civil War. Indiana was the first western state to mobilize for the war, and its soldiers participated in almost every engagement during the war. Following the Civil War, Indiana remained politically important as it became a critical swing state in U.S. presidential elections. It helped decide control of the presidency for three decades.

During the Indiana Gas Boom of the late 19th century, industry began to develop rapidly in the state. The state's Golden Age of Literature began in the same time period, increasing its cultural influence. By the early 20th century, Indiana developed into a strong manufacturing state and attracted numerous immigrants and internal migrants to its industries. It experienced setbacks during the Great Depression of the 1930s. Construction of the Indianapolis Motor Speedway, expansion of the auto industry, urban development, and two wars contributed to the state's industrial growth. During the second half of the 20th century, Indiana became a leader in the pharmaceutical industry due to the innovations of companies such as Indiana based Eli Lilly.

Following the end of the last glacial period, about twenty thousand years ago, Indiana's topography was dominated by spruce and pine forests and was home to mastodon, caribou, and saber-toothed cats. While northern Indiana had been covered by glaciers, southern Indiana remained unaltered by the ice's advance, leaving plants and animals that could sustain human communities.[1][2] Indiana's earliest known inhabitants were Paleo-Indians. Evidence exists that humans were in Indiana as early as the Archaic stage (8000–6000 BC).[3] Hunting camps of the nomadic Clovis culture have been found in Indiana.[4] Carbon dating of artifacts found in the Wyandotte Caves of southern Indiana shows humans mined flint there as early 2000 BC.[5] These nomads ate quantities of freshwater mussels from local streams, as shown by their shell mounds found throughout southern Indiana.[5]

The Early Woodland period in Indiana came between 1000 BC and 200 AD and produced the Adena culture. It domesticated wild squash and made pottery, which were large cultural advances over the Clovis culture. The natives built burial mounds; one of this type has been dated as the oldest earthwork in Anderson's Mounds State Park.[6]

Natives in the Middle Woodland period developed the Hopewell culture and may have been in Indiana as early as 200 BC. The Hopewells were the first culture to create permanent settlements in Indiana. About 1 AD, the Hopewells mastered agriculture and grew crops of sunflowers and squash. Around 200 AD, the Hopewells began to construct mounds used for ceremonies and burials. The Hopewells in Indiana were connected by trade to other native tribes as far away as Central America.[7] For unknown reasons, the Hopewell culture went into decline around 400 and completely disappeared by 500.[8]

The Late Woodland era is generally considered to have begun about 600 AD and lasted until the arrival of Europeans in Indiana. It was a period of rapid cultural change. One of the new developments—which has yet to be explained—was the introduction of masonry, shown by the construction of large, stone forts, many of which overlook the Ohio River. Romantic legend attributed the forts to Welsh Indians, who supposedly arrived centuries before Christopher Columbus reached the Caribbean;[9] however, archaeologists and other scholars have found no evidence for that theory and believe that the cultural development was engendered by the Mississippian culture.[10]

Evidence suggests that after the collapse of the Hopewell, Indiana had a low population until the rise of the Fort Ancient and Mississippian culture around 900 AD.[11] The Ohio River Valley was densely populated by the Mississippians from about 1100 to 1450 AD. Their settlements, like those of the Hopewell, were known for their ceremonial earthwork mounds. Some of these remain visible at locations near the Ohio River. The Mississippian mounds were constructed on a grander scale than the mounds built by the Hopewell. The agrarian Mississippian culture was the first to grow maize in the region. The people also developed the bow and arrow and copper working during this time period.[11]

Mississippian society was complex, dense, and highly developed; the largest Mississippian city of Cahokia (in Illinois) contained as many as 30,000 inhabitants. They had a class society with certain groups specializing as artisans. The elite held related political and religious positions. Their cities were typically sited near rivers. Representing their cosmology, the central developments were dominated by a large central mound, several smaller mounds, and a large open plaza. Wooden palisades were built later around the complex, apparently for defensive purposes.[11] The remains of a major settlement known as Angel Mounds lie east of present-day Evansville.[12] Mississippian houses were generally square-shaped with plastered walls and thatched roofs.[13] For reasons that remain unclear, the Mississippians disappeared in the middle of the 15th century, about 200 years before the Europeans first entered what would become modern Indiana. Mississippian culture marked the high point of native development in Indiana.[11]

It was during this period that American Bison began a periodic east–west trek through Indiana, crossing the Falls of the Ohio and the Wabash River near modern-day Vincennes. These herds became important to civilizations in southern Indiana and created a well-established Buffalo Trace, later used by European-American pioneers moving west.[14]

Before 1600, a major war broke out in eastern North America among Native Americans; it was later called the Beaver Wars. Five American Indian Iroquois tribes confederated to battle against their neighbors. The Iroquois were opposed by a confederation of primarily Algonquian tribes including the Shawnee, Miami, Wea, Pottawatomie, and the Illinois.[15] These tribes were significantly less advanced than the Mississippian culture that had preceded them. The tribes were semi-nomadic, used stone tools rather than copper, and did not create the large-scale construction and farming works of their Mississippian predecessors. The war continued with sporadic fighting for at least a century as the Iroquois sought to dominate the expanding fur trade with the Europeans. They achieved this goal for several decades. During the war, the Iroquois drove the tribes from the Ohio Valley to the south and west. They kept control of the area for hunting grounds.[16][17]

As a result of the war, several tribes, including the Shawnee, migrated into Indiana, where they attempted to resettle in land belonging to the Miami. The Iroquois gained the military advantage after they were supplied with firearms by the Europeans. With their superior weapons, the Iroquois subjugated at least thirty tribes and nearly destroyed several others in northern Indiana.[18]

When the first Europeans entered Indiana during the 1670s, the region was in the final years of the Beaver Wars. The French attempted to trade with the Algonquian tribes in Indiana, selling them firearms in exchange for furs. This incurred the wrath of the Iroquois, who destroyed a French outpost in Indiana in retaliation. Appalled by the Iroquois, the French continued to supply the western tribes with firearms and openly allied with the Algonquian tribes.[19][20] A major battle—and a turning point in the conflict—occurred near present-day South Bend when the Miami and their allies repulsed a large Iroquois force in an ambush.[21] With the firearms they received from the French, the odds were evened. The war finally ended in 1701 with the Great Peace of Montreal. Both Indian confederacies were left exhausted, having suffered heavy casualties. Much of Ohio, Michigan, and Indiana was depopulated after many tribes fled west to escape the fighting.[22]

The Miami and Pottawatomie nations returned to Indiana following the war.[23][24] Other tribes, such as the Algonquian Lenape, were pushed westward into the Midwest from the East Coast by encroachment of European colonists. Around 1770 the Miami invited the Lenape to settle on the White River.[25][note 1] The Shawnee arrived in present-day Indiana after the three other nations.[23] These four nations were later participants in the Sixty Years' War, a struggle between native nations and European settlers for control of the Great Lakes region. Hostilities with the tribes began early. The Piankeshaw killed five French fur traders in 1752 near the Vermilion River. However, the tribes also traded successfully with the French for decades.[26]

French fur traders from Canada were the first Europeans to enter Indiana, beginning in the 1670s.[27] The quickest route connecting the New France districts of Canada and Louisiana ran along Indiana's Wabash River. The Terre Haute highlands were once considered the border between the two French districts.[28] Indiana's geographical location made it a vital part of French lines of communication and trade routes. The French established Vincennes as a permanent settlement in Indiana during European rule, but the population of the area remained primarily Native American.[29] As French influence grew in the region, Great Britain, competing with France for control of North America, came to believe that control of Indiana was important to halt French expansion on the continent.[30]

The first European outpost within the present-day boundaries of Indiana was Tassinong, a French trading post established in 1673 near the Kankakee River.[note 2] French explorer René-Robert Cavelier, Sieur de La Salle, came to the area in 1679, claiming it for King Louis the XIV of France. La Salle came to explore a portage between the St. Joseph and Kankakee rivers,[31] and Father Ribourde, who traveled with La Salle, marked trees along the way. The marks survived to be photographed in the 19th century.[32] In 1681, La Salle negotiated a common defense treaty between the Illinois and Miami nations against the Iroquois.[33]

Further exploration of Indiana led to the French establishing an important trade route between Canada and Louisiana via the Maumee and Wabash rivers. The French built a series of forts and outposts in Indiana as a hedge against the westward expansion of the British colonies from the east coast of North America and to encourage trade with the native tribes. The tribes were able to procure metal tools, cooking utensils, and other manufactured items in exchange for animal pelts. The French built Fort Miamis in the Miami town of Kekionga (modern-day Fort Wayne, Indiana). France assigned Jean Baptiste Bissot, Sieur de Vincennes, as the first agent to the Miami at Kekionga.[34]

In 1717, François-Marie Picoté de Belestre[note 3] established the post of Fort Ouiatenon (southwest of modern-day West Lafayette, Indiana) to discourage the Wea from coming under British influence.[35] In 1732, François-Marie Bissot, Sieur de Vincennes, established a similar post near the Piankeshaw in the town that still bears his name. Although the forts were garrisoned by men from New France, Fort Vincennes was the only outpost to maintain a permanent European presence until the present day.[36] Jesuit priests accompanied many of the French soldiers into Indiana in an attempt to convert the natives to Christianity. The Jesuits conducted missionary activities, lived among the natives and learned their languages, and accompanied them on hunts and migrations. Gabriel Marest, one of the first missionaries in Indiana, taught among the Kaskaskia as early as 1712. The missionaries came to have great influence among the natives and played an important role in keeping the native tribes allied with the French.[37]

During the French and Indian War, the North American front of the Seven Years' War in Europe, the British directly challenged France for control of the region. Although no pitched battles occurred in Indiana, the native tribes of the region supported the French.[38] At the beginning of the war, the tribes sent large groups of warriors to support the French in resisting the British advance and to raid British colonies.[39] Using Fort Pitt as a forward base, British commander Robert Rogers overcame the native resistance and drove deep into the frontier to capture Fort Detroit. The rangers moved south from Detroit and captured many of the key French outposts in Indiana, including Fort Miamis and Fort Vincennes.[40] As the war progressed, the French lost control of Canada after the fall of Montreal. No longer able to effectively fight the British in interior North America, they lost Indiana to British forces. By 1761, the French were entirely forced out of Indiana.[41] Following the French expulsion, native tribes led by Chief Pontiac confederated in an attempt to rebel against the British without French assistance. While Pontiac was besieging British-held Fort Detroit, other tribes in Indiana rose up against the British, who were forced to surrender Fort Miamis and Fort Ouiatenon.[42] In 1763, while Pontiac was fighting the British, the French signed the Treaty of Paris and ceded control of Indiana to the British.[43]

When the British gained control of Indiana, the entire region was in the middle of Pontiac's Rebellion. During the next year, British officials negotiated with the various tribes, splitting them from their alliance with Pontiac. Eventually, Pontiac lost most of his allies, forcing him to make peace with the British on July 25, 1766. As a concession to Pontiac, Great Britain issued a proclamation that the territory west of the Appalachian Mountains was to be reserved for Native Americans.[44] Despite the treaty, Pontiac was still considered a threat to British interests, but after he was murdered on April 20, 1769, the region saw several years of peace.[45]

After Britain established peace with the natives, many of the former French trading posts and forts in the region were abandoned. Fort Miamis was maintained for several years because it was considered to be "of great importance", but even it was eventually abandoned.[46] The Jesuit priests were expelled, and no provisional government was established; the British hoped the French in the area would leave. Many did leave, but the British gradually became more accommodating to the French who remained and continued the fur trade with the Native American nations.[47]

Formal use of the word Indiana dates from 1768, when a Philadelphia-based trading company gave their land claim in the present-day state of West Virginia the name of Indiana in honor of its previous owners, the Iroquois. Later, ownership of the claim was transferred to the Indiana Land Company, the first recorded use of the word Indiana. However, the Virginia colony argued that it was the rightful owner of the land because it fell within its geographic boundaries. The U.S. Supreme Court extinguished the land company's right to the claim in 1798.[48]

In 1773, the territory that included present-day Indiana was brought under the administration of Province of Quebec to appease its French population. The Quebec Act was one of the Intolerable Acts that the thirteen British colonies cited as a reason for the outbreak of the American Revolutionary War. The Thirteen Colonies thought themselves entitled to the territory for their support of Great Britain during the French and Indian War, and were incensed that it was given to the enemy the colonies had been fighting.[49]

Although the United States gained official possession of the region following the conclusion of the American Revolutionary War, British influence on its Native American allies in the region remained strong, especially near Fort Detroit. This influence caused the Northwest Indian War, which began when British-influenced native tribes refused to recognize American authority and were backed in their resistance by British merchants and officials in the area. American military victories in the region and the ratification of the Jay Treaty, which called for British withdrawal from the region's forts, caused a formal evacuation, but the British were not fully expelled from the area until the conclusion of the War of 1812.[50]

After the outbreak of the American Revolutionary War, George Rogers Clark was sent from Virginia to enforce its claim to much of the land in the Great Lakes region.[51] In July 1778, Clark and about 175 men crossed the Ohio River and took control of Kaskaskia, Cahokia, and Vincennes, along with several other villages in British Indiana. The occupation was accomplished without firing a shot because Clark carried letters from the French ambassador stating that France supported the Americans. These letters made most of the French and Native American inhabitants of the area unwilling to support the British.[52]

The fort at Vincennes, which the British had renamed Fort Sackville, had been abandoned years earlier and no garrison was present when the Americans arrived to occupy it. Captain Leonard Helm became the first American commandant at Vincennes. To counter Clark's advance, British forces under Lieutenant Governor Henry Hamilton recaptured Vincennes with a small force. In February 1779, Clark arrived at Vincennes in a surprise winter expedition and retook the town, capturing Hamilton in the process. This expedition secured most of southern Indiana for the United States.[53]

In 1780, emulating Clark's success at Vincennes, French officer Augustin de La Balme organized a militia force of French residents to capture Fort Detroit. While marching to Detroit, the militia stopped to sack Kekionga.[why?] The delay proved fatal when the expedition met Miami warriors led by Chief Little Turtle along the Eel River. The entire militia was killed or captured. Clark organized another assault on Fort Detroit in 1781, but it was aborted when Chief Joseph Brant captured a significant part of Clark's army at a battle known as Lochry's Defeat, near present-day Aurora, Indiana.[51] Other minor skirmishes occurred in Indiana, including the battle at Petit fort in 1780.[54] In 1783, when the war came to an end, Britain ceded the entire trans-Allegheny region to the United States—including Indiana—under the terms of the Treaty of Paris.[55]

Clark's militia was under the authority of the Commonwealth of Virginia, although a Continental Flag was flown over Fort Sackville, which he renamed Fort Patrick Henry in honor of an American patriot. Later that year, the areas formerly known as Illinois Country and Ohio Country were organized as Illinois County, Virginia until the colony relinquished its control of the area to the U.S. government in 1784.[56] Clark was awarded large tracts of land in southern Indiana for his service in the war. Present-day Clark County and Clarksville are named in his honor.[57]

Passage of the Land Ordinance of 1785 and the Northwest Ordinance of 1787 committed the U.S. government to continued plans for western expansion, causing increasing tensions with native tribes who occupied the western lands. In 1785 the conflict erupted into the Northwest Indian War.[58][59] American troops made several unsuccessful attempts to end the native rebellion. During the fall of 1790, U.S. troops under the command of General Josiah Harmar pursued the Miami tribe near present-day Fort Wayne, Indiana, but had to retreat. Major Jean François Hamtramck's expedition to other native villages in the area also failed when it was forced to return to Vincennes due to lack of sufficient provisions.[60][61] In 1791 Major General Arthur St. Clair, who was also the Northwest Territory's governor, commanded about 2,700 men in a campaign to establish a chain of forts in the area near the Miami capital of Kekionga; however, nearly a 1,000 warriors under the leadership of Chief Little Turtle launched a surprise attack on the American camp, forcing the militia's retreat. St. Clair's Defeat remains the U.S. Army's worst by Native Americans in history. Casualties included 623 federal soldiers killed and another 258 wounded; the Indian confederacy lost an estimated 100 men.[62][63]

St. Clair's loss led to the appointment of General "Mad Anthony" Wayne, who organized the Legion of the United States and defeated a Native American force at the Battle of Fallen Timbers in August 1794.[63][64] The Treaty of Greenville (1795) ended the war and marked the beginning of a series of land cession treaties. Under the terms of the Treaty, native tribes ceded most of southern and eastern Ohio and a strip of southeastern Indiana to the U.S. government. This ethnic cleansing opened the area for white settlement. Fort Wayne was built at Kekionga to represent United States sovereignty over the Ohio-Indiana frontier. After the treaty was signed, the powerful Miami nation considered themselves allies of the United States.[65][66] During the 18th century, Native Americans were victorious in 31 of the 37 recorded incidents with white settlers in the territory.[67]

The Congress of the Confederation formed the Northwest Territory under the terms of the Northwest Ordinance on July 13, 1787. This territory, which initially included land bounded by the Appalachian Mountains, the Mississippi River, the Great Lakes, and the Ohio River, was subsequently partitioned into the Indiana Territory (1800), Michigan Territory (1805), and the Illinois Territory (1809), and later became the states of Ohio, Michigan, Indiana, Illinois, Wisconsin, and part of eastern Minnesota. The Northwest Ordinance outlined the basis for government in these western lands and an administrative structure to oversee the territory, as well as a process for achieving statehood, while the Land Ordinance of 1785 called for the U.S. government to survey the territory for future sale and development.[68]

On May 7, 1800, the U.S. Congress passed legislation to establish the Indiana Territory, effective July 4, 1800, by dividing the Northwest Territory in preparation for Ohio's statehood, which occurred in 1803.[69] At the time the Indiana Territory was created, there were only two main American settlements in what became the state of Indiana: Vincennes and Clark's Grant. When the Indiana Territory was established in 1800 its total white population was 5,641; however, its Native American population was estimated to be nearly 20,000, but may have been as high as 75,000.[70][71]

Indiana Territory initially comprised most of the present-day state Indiana excluding a narrow strip of land along the eastern border called "The Gore" (ceded by Ohio in 1803), all of the present-day states of Illinois and Wisconsin, and parts of present-day Michigan and Minnesota.[72][73] The Indiana Territory's boundary was further reduced in 1805 with the creation of the Michigan Territory to the north and again in 1809 when the Illinois Territory was established to the west.[74]

When the Indiana Territory was established in 1800, President John Adams appointed William Henry Harrison as the first governor of the territory. John Gibson, who was appointed the territorial secretary, served as acting governor from July 4, 1800, until Harrison's arrival at Vincennes on January 10, 1801. When Harrison resigned his position, effective December 28, 1812, Gibson served as territorial governor until Thomas Posey was appointed on March 3, 1813. Posey left office on November 7, 1816, when Jonathan Jennings was sworn into office as the first governor of the state of Indiana.[75][76][note 4]

The first territorial capital was established at Vincennes, where it remained from 1800 to 1813, when territorial officials relocated the seat of government to Corydon.[77][78] After the Illinois Territory was formed in 1809, Indiana's territorial legislature became fearful that the outbreak of war on the frontier could cause an attack on Vincennes, located on the western border of the territory, and made plans to move the capital closer to the territory's population center. Governor Harrison also favored Corydon, a town that he had established in 1808 and where he was also a landowner. Construction on the new capitol building began in 1814 and was nearly finished by 1816, when Indiana became a state.[79][80]

The Northwest Ordinance of 1787 made no provision for a popularly elected territorial government in the first or non-representative phase of territorial government (1800 to 1804).[81] Acting as the combined judicial and legislative government, a territorial governor and a General Court, which consisted of a three-member panel of judges, were appointed by the U.S. Congress, and later, the president with congressional approval. (The president subsequently delegated his authority to appoint these judges to the territorial governor.)[82] When the territory entered the second or semi-legislative phase of government in 1805, its voters were allowed to elect representatives to the House of Representatives (lower house) of its bicameral legislature. President Jefferson delegated the task of choosing a five-member Legislative Council (upper house) to the territorial governor, who chose the members from a list of ten candidates provided by the lower house.[83][84] The newly elected territorial legislature met for the first time on July 29, 1805, and gradually became the dominant branch, while the judges continued to focus on judicial matters.[85] Governor Harrison retained veto powers, as well as his general executive and appointment authority. The legislative assembly had the authority to pass laws, subject to the governor's approval before they could be enacted.[83][84]

As the population of the territory grew, so did the people's interest in exercising of their freedoms. In 1809, after the Indiana Territory was divided to create the Illinois Territory, Congress further altered the makeup of the territorial legislature. Voters in the Indiana Territory would continue to elect members to its House of Representatives; however, they were also granted permission for the first time to elect members to its Legislative Council (upper house).[86][87]

The major political issue in Indiana's territorial history was slavery; however, there were others, including Indian affairs, the formation of northern and western territories from portions of the Indiana Territory, concerns about the lack of territorial self-government and representation in Congress, and ongoing criticisms of Harrison's actions at territorial governor.[88][89] Most of these issues were resolved before Indiana achieved statehood, including the debate over the issue of allowing slavery in the territory, which was settled in 1810; however, criticism of Governor Harrison continued.[88]

In December 1802, delegates from Indiana Territory's four counties passed a resolution in favor of a ten-year suspension of Article 6 of the Northwest Ordinance of 1787. The ordinance prohibited slavery in the original Northwest Territory, although it had existed in the region since French rule. The resolution was made to legalize slavery in the territory and to make the region more appealing to slave-holding settlers from the Upper South who occupied areas along the Ohio River and wanted to bring their slaves into the territory. However, Congress failed to take action on the resolution, leaving Harrison and the territorial judges to pursue other options.[90][91]

In 1809 Harrison found himself at odds with the new legislature when the anti-slavery party won a strong majority in the 1809 elections. In 1810 the territorial legislature repealed the indenturing and pro-slavery laws Harrison and the judicial council had enacted in 1803.[92][93] Slavery remained the defining issue in the state for the decades to follow.[94][95]

The first major event in the territory's history was the resumption of hostilities with Native Americans. Unhappy with their treatment since the peace treaty of 1795, native tribes led by the Shawnee Chief Tecumseh and his brother Tenskwatawa formed a coalition against the Americans. Tecumseh's War started in 1811, when General Harrison led an army to rebuff the aggressive movements of Tecumseh's pan-Indian confederation.[96] The Battle of Tippecanoe (1811), which caused a setback for the Native Americans,[97] earned Harrison national fame and the nickname of "Old Tippecanoe".[98]

The war between Tecumseh and Harrison merged with the War of 1812 after the remnants of the pan-Indian confederation allied with the British in Canada. The siege of Fort Harrison is considered to be the Americans' first land victory in the war.[99] Other battles that occurred within the boundaries of the present-day state of Indiana include the siege of Fort Wayne, the Pigeon Roost Massacre and the Battle of the Mississinewa. The Treaty of Ghent (1814) ended the war and relieved American settlers from their fears of attack by the nearby British and their Indian allies.[100] This treaty marked the end of hostilities with the Native Americans in Indiana. During the 19th century, Native Americans were victorious in 43 of the 58 recorded incidents between Native Americans and white settlers in Indiana. In the 37 battles between Native American warriors and U.S. Army troops, victories were nearly evenly split between the two parties. Despite the Native American victories, most of the native population was eventually removed from Indiana, a process that continued after the territory attained statehood.[101]

In 1812, Jonathan Jennings defeated Harrison's chosen candidate and became the territory's representative to Congress. Jennings immediately introduced legislation to grant Indiana statehood, even though the population of the entire territory was under 25,000, but no action was taken on the legislation because of the outbreak of the War of 1812.[102]

Posey had created a rift in the politics of the territory by supporting slavery, much to the chagrin of opponents like Jennings, Dennis Pennington, and others who dominated the Territorial Legislature and who sought to use the bid for statehood to permanently end slavery in the territory.[102][103]

In early 1816, the Territory approved a census and Pennington was named to be the census enumerator. The population of the territory was found to be 63,897,[104] above the cutoff required for statehood. A constitutional convention met on June 10, 1816, in Corydon. Because of the heat of the season, the delegation moved outdoors on many days and wrote the constitution beneath the shade of a giant elm tree. The state's first constitution was completed on June 29, and elections were held in August to fill the offices of the new state government. In November, Congress approved statehood.[105][106]

Jennings and his supporters had control of the convention and Jennings was elected its president. Other notable delegates at the convention included Dennis Pennington, Davis Floyd, and William Hendricks.[107] Pennington and Jennings were at the forefront of the effort to prevent slavery from entering Indiana and sought to create a constitutional ban on it. Pennington was quoted as saying "Let us be on our guard when our convention men are chosen that they be men opposed to slavery".[108] They succeeded in their goal and a ban was placed in the new constitution.[109] But, persons already held in bondage stayed in that status for some time. That same year Indiana statehood was approved by Congress. And, while the Indiana constitution banned slavery in the state, Indiana and its white residents also excluded free Black citizens, and established barriers to their immigration to the state.[110]

Jonathan Jennings, whose motto was "No slavery in Indiana", was elected governor of the state, defeating Thomas Posey 5,211 to 3,934 votes.[111] Jennings served two terms as governor and then went on to represent the state in congress for another 18 years. Upon election, Jennings declared Indiana a free state.[111] The abolitionists won a key victory in the 1820 Indiana Supreme Court case of Polly v. Lasselle, which stated that even those enslaved before Indiana statehood were now free. In the case of Mary Clark, an African American woman born into slavery and then indentured as a servant in Vincennes, Indiana, the Indiana Supreme Court in 1821 decided that indentured servitude was merely a ruse for slavery and was therefore prohibited. All forms of slavery in Indiana were finally banned by 1830.[112][113]

As the northern tribal lands gradually opened to white settlement, Indiana's population rapidly increased and the center of population shifted continually northward.[114] One of the most significant post-frontier events in Indiana occurred in 1818 with the signing of the Treaty of St. Mary's at St. Mary's, Ohio to acquire Indian lands south of the Wabash from the Delaware and others. The area comprised about 1/3 of the present day area of Indiana, the central portion, and was called the "New Purchase". Eventually, 35 new counties were carved out of the New Purchase. An area like a large bite in the middle of the northern boundary[115] was reserved to the Miami, called the Big Miami Reserve, which was the largest Indian reservation ever to exist in Indiana. Indianapolis was selected to be the site of the new state capital in 1820 because of its central position within the state and assumed good water transportation. However the founders were disappointed to discover the White River was too sandy for navigation.[116] In 1825, Indianapolis replaced Corydon as the seat of government. The government became established in the Marion County Courthouse as the second state capital building.[114]

The National Road reached Indianapolis in 1829, connecting Indiana to the Eastern United States.[118] In the early 1830s citizens of Indiana began to be known as Hoosiers, although the origin of the word has been subject considerable debate,[119] and the state took on the motto of "Crossroads of America". In 1832, construction began on the Wabash and Erie Canal, a project connecting the waterways of the Great Lakes to the Ohio River. Railroads soon made the canal system obsolete. These developments in transportation served to economically connect Indiana to the Northern East Coast, rather than relying solely on the natural waterways which connected Indiana to the Mississippi River and Gulf Coast states.[120][note 5]

In 1831, construction on the third state capitol building began. This building, designed by the firm of Ithiel Town and Alexander Jackson Davis, had a design inspired by the Greek Parthenon and opened in 1841. It was the first statehouse that was built and used exclusively by the state government.[121]

The state suffered from financial difficulties during its first three decades. Jonathan Jennings attempted to begin a period of internal improvements. Among his projects, the Indiana Canal Company was reestablished to build a canal around the Falls of the Ohio. The Panic of 1819 caused the state's only two banks to fold. This hurt Indiana's credit, halted the projects, and hampered the start of new projects until the 1830s, after the repair of the state's finances during the terms of William Hendricks and Noah Noble. Beginning in 1831, large scale plans for statewide improvements were set into motion. Overspending on the internal improvements led to a large deficit that had to be funded by state bonds through the newly created Bank of Indiana and sale of over nine million acres (36,000 km2) of public land. By 1841, the debt had become unmanageable.[122] Having borrowed over $13 million, the equivalent to the state's first fifteen years of tax revenue, the government could not even pay interest on the debt.[123] The state narrowly avoided bankruptcy by negotiating the liquidation of the public works, transferring them to the state's creditors in exchange for a 50 percent reduction in the state's debt.[124][note 6] The internal improvements began under Jennings paid off as the state began to experience rapid population growth that slowly remedied the state's funding problems. The improvements led to a fourfold increase in land value, and an even larger increase in farm produce.[125]

During the 1840s, Indiana completed the removal of the Native American tribes. The majority of the Potawatomi voluntarily relocated to Kansas in 1838. Those who did not leave were forced to travel to Kansas in what came to be called the Potawatomi Trail of Death, leaving only the Pokagon Band of Potawatomi Indians in the Indiana area.[126] The majority of the Miami tribe left in 1846, although many members of the tribe were permitted to remain in the state on lands they held privately under the terms of the 1818 Treaty of St. Mary's.[127] The other tribes were also convinced to leave the state voluntarily through the payment of subsidies and land grants further west. The Shawnee migrated westward to settle in Missouri, and the Lenape migrated into Canada. The other minor tribes in the state, including the Wea, moved westward, mostly to Kansas.[128]

By the 1850s, Indiana had undergone major changes: what was once a frontier with sparse population had become a developing state with several cities. In 1816, Indiana's population was around 65,000, and in less than 50 years, it had increased to more than 1,000,000 inhabitants.[129]

Because of the rapidly changing state, the constitution of 1816 began to be criticized.[130][note 7] Opponents claimed the constitution had too many appointed positions, the terms established were inadequate, and some of the clauses were too easily manipulated by the political parties that did not exist when then constitution was written.[131] The first constitution had not been put to a vote by the general public, and following the great population growth in the state, it was seen as inadequate. A constitutional convention was called in January 1851 to create a new one. The new constitution was approved by the convention on February 10, 1851, and submitted for a vote to the electorate that year. It was approved and has since been the official constitution.[132]

African Americans migrated to Indiana before its official statehood in 1816.[135] The first recorded were five enslaved people in Vincennes, Indiana in 1746.[136] In the 1820 federal census, 1,230 reported themselves as residents of Indiana.[137] Most Black migrants to Indiana arrived from South Carolina, Ohio, Virginia, and Kentucky.[135] African Americans pioneered rural settlements in the state throughout the first half of the nineteenth-century and accounted for 1.1% of the total population by 1850.[138] Although Black Hoosiers settled in urban areas, many rural antebellum communities were found throughout the state, including Lyles Station, Roberts Settlement, and Beech Settlement.[139]

Although Indiana entered the Union in 1816 as a free state, it gave only a tepid welcome to African Americans. Throughout the first half of the nineteenth century, Indiana attempted to keep Black Hoosiers from attending public school, voting, testifying in court, and endeavored to set other limits on African American citizenship and inclusion.[140] Black individuals were denied the right to testify in court in 1818.[141] In 1829, the Indiana Colonization Society was founded to help repatriate African Americans to Liberia which reflected a desire to rid the state of its Black residents.[142] The 1830 census recorded three slaves in the state. The earliest days of the territory and of statehood witnessed intense debates over whether to allow slavery in Indiana. Laws in the 1830s sought to prevent free Black individuals from entering the state without certificates of freedom under threat of fines and expulsion.[143] The Black Law of 1831 required Black citizens to register within their county and pay a $500 bond.[144]

While the 1830 law was only sporadically enforced it reflected hostility towards African Americans and their settlement in the state. Throughout the early nineteenth century, Black Hoosiers struggled to enjoy basic civil rights in the state, including the right to educate their children. In 1837 and 1841, the state formally excluded African Americans from public education. In 1837, the state legislature moved to recognize "The white inhabitants of each congressional district" as the citizens qualified to vote in school board elections. Four years later, they followed with an effort to preclude Black households from school board assessments. This helped to establish Hoosier schools as de facto segregated white populations. Efforts in 1842 to formally exclude African American children from public education were rebuffed, however. The State Committee on Education responded to the matter acknowledging that Black students "Are here, unfortunately, for us and them, and we have duties to perform in reference to their well-being."[145] While the state did not have legal segregation, Black children were also excluded public schools as a matter of custom.[141]

Indiana passed laws against interracial marriage in 1818 and 1821.[146] Under 1840 state laws to ban miscegenation, Indiana became the first state to make interracial marriage a felony.[147] Article II of the new constitution of 1851 expanded suffrage for white males, but excluded Black Hoosiers from suffrage. Article XIII of the Indiana Constitution of 1851 sought to exclude African Americans from settling in the state, declaring "No negro or mulatto shall come into or settle in the State."[148] This was the only provision of the new constitution submitted to a special election. Indiana constitutional convention delegates voted 93 to 40 in favor of the article. The popular vote was even more enthusiastic in its support for exclusion with a vote of 113,828 in favor and only 21,873 against excluding African Americans. This ban stood until 1866 when the Indiana Supreme Court ruled that it was unconstitutional.[149][150]

Racial hostility and discrimination co-existed alongside abolition sentiments and efforts. Levi Coffin, unofficially known as "President of the Underground Railroad", and one of the most prominent abolitionists in the United States operated out of Richmond. The Underground Railroad in Indiana sought to help runaway slaves escape to northern states and Canada. White Quakers, Baptists, and others worked to secure safe passage for runaway slaves. Abolition efforts conflicted with a growing antipathy towards free Black Hoosiers in the state. A large influx occurred in 1814 when Paul and Susannah Mitchem immigrated to Indiana from Virginia with over 100 of their slaves. Later that year they emancipated all of their slaves, most of whom formed a large part of the population of the first state capital in Corydon.[151]

Bounty hunters (slavecatchers), mostly operating in the southern part of the state, offered their services and knowledge of the area to southerners searching for runaways.[152] In addition, free Black individuals could become victims when slavecatchers could not find runaway slaves. Bounty hunters and slavecatchers might seize free Black individuals, claiming them to be runaways, and bring them to the Southern United States to be sold into bondage. In one incident in the early 1850s, for example, slavecatchers seized two free Black citizens working on the Wabash and Erie Canal. Although local abolitionists quickly organized and petitioned the sheriff to release the two men, the slavecatchers had documents that described the men and claimed they were runaways. Evidence suggested the documents were false, but there was no way to refute the claim. The slavecatchers were allowed to take the two men as their prisoners, but before they left Indiana a group of abolitionists overtook the party and freed the two Black laborers.[153]

Article 13 of the 1851 Indiana Constitution was deemed 'unconstitutional' in 1866, but was not amended until 1881. Indiana's Black population increased after the Civil War mostly along the Ohio River, such as Spencer County, Indiana, which included 947 Black citizens by 1870.[citation needed]

As Reconstruction ended in the South, former enslaved peoples wanted to move north, which included the migration of Black people from North Carolina to Indiana. Black people who migrated from the South after the Civil War were known as Exodusters, who were in search of access to good schools, Black community-centered churches, and job opportunities. Many migrants during this time who arrived in Indiana were met with anti-Black violence and forced to relocate due to Indiana's numerous sundown towns. Black communities around Indianapolis tried to help those who had migrated, but many of the Exodusters became discouraged and went back to North Carolina. Those who stayed often settled in Indianapolis, contributing to the city's Black population growth.[citation needed]

The Black population in 1880 was 39,228 and by 1900 it was 57,960.[citation needed]

During the Great Migration, Black individuals who came to Indiana between 1910 and 1920, often settled in central or northern parts of the states. New opportunities were available due to industrialization and the war economy, and rumors of new opportunities were appealing.

Frontier Indiana was prime ground missionary for the Second Great Awakening, with a never-ending parade of camp meetings and revivals.[154] Baptist church records show an intense interest in private moral behavior at the weekly meetings, including drinking and proper child-rearing practices. The most contentious issue was the antimission controversy, in which the more traditional elements denounced missionary societies as unbiblical. Daniel Parker, of Vincennes, was a key leader of the antimission movement[155]

Eastern Presbyterian and Congregational denominations funded an aggressive missionary program, 1826–55, through the American Home Missionary Society (AHMS). It sought to bring sinners to Christ and also to modernize society promoted middle class values, mutual trust among the members, and tried to minimize violence and drinking.[156] The frontierspeople were the reformees and they displayed their annoyance at the new morality being imposed on society. The political crisis came in 1854–55 over a pietistic campaign to enact "dry" prohibition of liquor sales. They were strongly opposed by the "wets", especially non-churched, the Catholics, Episcopalians, the antimissionary elements, and the German recent arrivals. Prohibition failed in 1855 and the moralistic pietistic Protestants switched to a new, equally moralistic cause, the anti-slavery crusade led by the new Republican Party.[157][158]

In 1836 Black Hoosiers founded the Bethel AME Church in Indianapolis.[144]

For a list of institutions, see Category:Universities and colleges in Indiana.

The earliest institutions of education in Indiana were missions, established by French Jesuit priests to convert local Native American nations. The Jefferson Academy was founded in 1801 as a public university for the Indiana Territory, and was reincorporated as Vincennes University in 1806, the first in the state.[159]

The 1816 constitution required that Indiana's state legislature create a "general system of education, ascending in a regular gradation, from township schools to a state university, wherein tuition shall be gratis, and equally open to all".[160] It took several years for the legislature to fulfill its promise, partly because of a debate about whether a new public university should be founded to replace the territorial university.[161] The 1820s saw the start of free public township schools. During the administration of William Hendricks, a plot of ground was set aside in each township for the construction of a schoolhouse.[162]

The state government chartered Indiana University in Bloomington in 1820 as the State Seminary. Construction began in 1822, the first professor was hired in 1823, and classes were offered in 1824.

Other state colleges were established for specialized needs. They included Indiana State University, established in Terre Haute in 1865 as the state normal school for training teachers. Purdue University was founded in 1869 as the state's land-grant university, a school of science and agriculture. Ball State University was founded as a normal school in the early 20th century and given to the state in 1918.[163]

Public colleges lagged behind the private religious colleges in both size and educational standards until the 1890s.[164] In 1855, North Western Christian University [now Butler University] was chartered by Ovid Butler after a split with the Christian Church Disciples of Christ over slavery. Significantly the university was founded on the basis of anti-slavery and co-education. It was one of the first to admit African Americans and one of the first to have a named chair for female professors, the Demia Butler Chair in English.[165] Asbury College (now Depauw University) was Methodist. Wabash College was Presbyterian; they led the Protestant schools.[166] The University of Notre Dame, founded by Rev Edward Sorin in 1842, proclaims itself as a prominent Catholic college.[167] Indiana lagged the rest of the Midwest with the lowest literacy and education rates into the early 20th century.[164]

In the early 19th century, most transportation of goods in Indiana was done by river. Most of the state's estuaries drained into the Wabash River or the Ohio River, ultimately meeting up with the Mississippi River, where goods were transported to and sold in St. Louis or New Orleans.[168][169]

The first road in the region was the Buffalo Trace, an old bison trail that ran from the Falls of the Ohio to Vincennes.[170] After the capitol was relocated to Corydon, several local roads were created to connect the new capitol to the Ohio River at Mauckport and to New Albany. The first major road in the state was the National Road, a project funded by the federal government. The road entered Indiana in 1829, connecting Richmond, Indianapolis, and Terre Haute with the eastern states and eventually Illinois and Missouri in the west.[171] The state adopted the advanced methods used to build the national road on a statewide basis and began to build a new road network that was usable year-round. The north–south Michigan Road was built in the 1830s, connecting Michigan and Kentucky and passing through Indianapolis in the middle.[171] These two new roads were roughly perpendicular within the state and served as the foundation for a road system to encompass all of Indiana.

Indiana was flat enough with plenty or rivers to spend heavily on a canal mania in the 1830s. Planning in the lightly populated state began in 1827 as New York had scored a major success with its Erie Canal.[172] In 1836 the legislature allocated $10 million for an elaborate network of internal improvements, promoting canals, turnpikes, and railroads. The goal was to encourage settlement by providing easy, cheap access to the remotest corners of the state, linking every area to the Great Lakes and Ohio River, and thence to the Atlantic seaports and New Orleans. Every region joined in enthusiastically, but the scheme was a financial disaster because the legislature required that work must begin on all parts of all the projects simultaneously; very few were finished. The state was unable to pay the bonds it issued and was blackballed in Eastern and European financial circles for decades.[173][174]

The first major railroad line was completed in 1847, connecting Madison with Indianapolis. By the 1850s, the railroad began to become popular in Indiana. Indianapolis as the focal point, Indiana had 212 miles of railroad in operation in 1852, soaring to 1,278 miles in 1854. They were operated by 18 companies; construction plans were underway to double the totals.[175] The successful railroad network brought major changes to Indiana and enhanced the state's economic growth.[118] Although Indiana's natural waterways connected it to the South via cities such as St. Louis and New Orleans, the new rail lines ran east–west, and connected Indiana with the economies of the northern states.[176] As late as mid-1859, no rail line yet bridged the Ohio or Mississippi rivers.[177] Because of an increased demand on the state's resources and the embargo against the Confederacy, the rail system was mostly completed by 1865.

Temperance became a part of the evangelical Protestant initiative during Indiana's pioneer era and early statehood. Many Hoosiers freely indulged in drinking locally distilled whiskey on a daily basis, with binges on election days and holidays, and during community celebrations[158] Reformers announced that the devil was at work and must be repudiated.[178][179] A state temperance society formed in 1829 and local temperance societies soon organized in Indianapolis, Fort Wayne, and Logansport. By the 1830s pietistic (evangelical) Protestants and community leaders had joined forces to curb consumption of alcohol. In 1847, the Indiana General Assembly passed a local option bill that allowed a vote on whether to prohibit alcohol sales in a township.

By the 1850s Indiana's Republican party, whose adherents tended to favor the temperance movement, began challenging the state's Democrats, who supported personal freedom and a limited federal government, for political power.[180] Early temperance legislation in Indiana earned only limited and temporary success. In 1853, Republicans persuaded the state legislature to pass a local option law that allowed a township voter to declare it dry, but it was later deemed unconstitutional. In 1855, a statewide prohibition law was passed, but it met the same fate as the local option.[181] In the decades to come Protestant churches, especially the Methodists, Baptists, Presbyterians, Disciples of Christ, Quakers, and women's groups continued to support temperance efforts and gave strong support to the mostly dry Republican Party. The Catholics, Episcopalians and Lutherans stood opposed and gave strong support to the wet Democratic Party.[182]

Abolition in Indiana reflected a mix of anti-Black sentiment, religiously oriented social reforms, and pro-Black sentiments.[183] Several groups and notable individuals stood in opposition to slavery and in support of African Americans in the state. The North Western Christian University [later Butler University] was founded by Ovid Butler in 1855 after a schism with the Christian Church (Disciples of Christ) over slavery.

Indiana has a long history of women's activism in social movements including the women's suffrage movement.

The Indiana Woman's Suffrage Association was founded in 1851 by important suffrage leaders such as Agnes Cook, Mary B. Birdsall, Amanda M. Way, and Mary F. Thomas.[184] With the exception of Way, all these women were the first to address the Indiana State Legislature on January 19, 1859, with petitions calling for women's suffrage, temperance, and equal rights.[185] In 1854, Birdsall had purchased The Lily, the first U.S. newspaper edited by and for women, from its founder, Amelia Bloomer, and moved it to Richmond, Indiana. The newspaper had begun as a temperance newspaper but was later used to campaign for women's suffrage and rights.[186]

Indiana, a free state and the boyhood home of Abraham Lincoln, remained a member of the Union during the American Civil War. Indiana regiments were involved in all the major engagements of the war and almost all the engagements in the western theater. Hoosiers were present in the first and last battles of the war. During the war, Indiana provided 126 infantry regiments, 26 batteries of artillery, and 13 regiments of cavalry to the cause of the Union.[187]

In the initial call to arms issued in 1861, Indiana was assigned a quota of 7,500 men—a tenth of the amount called—to join the Union Army in putting down the rebellion.[188] So many volunteered in the first call that thousands had to be turned away. Before the war ended, Indiana contributed 208,367 men to fight and serve in the war.[189] Casualties were over 35% among these men: 24,416 lost their lives in the conflict and over 50,000 more were wounded.[189]

At the outbreak of the war, Indiana was run by a Democratic and southern sympathetic majority in the state legislature. It was by the actions of Governor Oliver Morton, who illegally borrowed millions of dollars to finance the army, that Indiana could contribute so greatly to the war effort.[190] Morton suppressed the state legislature with the help of the Republican minority to prevent it from assembling during 1861 and 1862. This prevented any chance the Democrats might have had to interfere with the war effort or to attempt to secede from the Union.[191]

In March 1862, Governor Oliver Morton also assembled a committee known as the Indiana Sanitary Commission to raise funds and gather supplies for troops in the field. It was not until January 1863 that the commission began recruiting women as nurses for wounded soldiers.[192] Notable women members of the included Mary F. Thomas, a Hoosier suffragist, and Eliza Hamilton-George, also known as "Mother George".[193] Although the exact number of women volunteers is unknown, William Hannaman, president of the Indiana Sanitary Commission, reported to Morton in 1866 that "about two hundred and fifty" women had volunteered as nurses between 1863 and 1865.[192]

Two raids on Indiana soil during the war caused a brief panic in Indianapolis and southern Indiana. The Newburgh Raid on July 18, 1862, occurred when Confederate officer Adam Johnson briefly captured Newburgh by convincing the Union troops garrisoning the town that he had cannon on the surrounding hills, when in fact they were merely camouflaged stovepipes. The raid convinced the federal government that it was necessary to supply Indiana with a permanent force of regular Union Army soldiers to counter future raids.[194]

The most significant Civil War battle fought in Indiana was a small skirmish during Morgan's Raid. On the morning of July 9, 1863, Morgan attempted to cross the Ohio River into Indiana with his force of 2,400 Confederate cavalry. After his crossing was briefly contested, he marched north to Corydon where he fought the Indiana Legion in the short Battle of Corydon. Morgan took command of the heights south of Corydon and shot two shells from his batteries into the town, which promptly surrendered. The battle left 15 dead and 40 wounded. Morgan's main body of troopers briefly raided New Salisbury, Crandall, Palmyra, and Salem. Fear gripped the capitol, and the militia began to form there to contest Morgan's advance. After Salem, however, Morgan turned east, raiding and skirmishing along this path and leaving Indiana through West Harrison on July 13 into Ohio, where he was captured.[195]

The Civil War had a major effect on the development of Indiana. Before the war, the population was generally in the south of the state, where many had entered via the Ohio River, which provided a cheap and convenient means to export products and agriculture to New Orleans to be sold. The war closed the Mississippi River to traffic for nearly four years, forcing Indiana to find other means to export its produce. This led to a population shift to the north where the state came to rely more on the Great Lakes and the railroad for exports.[196][197]

Before the war, New Albany was the largest city in the state, mainly because of its river contacts and extensive trade with the South.[198] Over half of Hoosiers with over $100,000 lived in New Albany.[199] During the war, the trade with the South came to a halt, and many residents considered those of New Albany as too friendly to the South. The city never regained its stature. It was stilled as a city of 40,000 with its early Victorian Mansion-Row buildings remaining from the boom period.[200]

Ohio River ports had been stifled by an embargo on the Confederate South and never fully recovered their economic prominence, leading the south into an economic decline.[196] By contrast, northern Indiana experienced an economic boom when natural gas was discovered in the 1880s, which directly contributed to the rapid growth of cities such as Gas City, Hartford City, and Muncie where a glass industry developed to use the cheap fuel. The Indiana gas field was then the largest known in the world.[201] The boom lasted until the early 20th century, when the gas supplies ran low. This began northern Indiana's industrialization.

The development of heavy industry attracted thousands of European immigrants in the late 19th and early 20th centuries, as well as internal migrants, both Black and white, from the rural and small town South. These developments dramatically altered the demographics of the state. Indiana industrial cities were among the destinations of the Great Migration. After World War II, industrial restructuring and the shifts in heavy industry resulted in Indiana's becoming part of the Rust Belt.[202][203]

In 1876, chemist Eli Lilly, a Union colonel during the Civil War, founded Eli Lilly and Company, a pharmaceutical company. His initial innovation of gelatin-coating for pills led to a rapid growth of the company that eventually developed as Indiana's largest corporation, and one of the largest corporations in the world.[204][205][note 8] Over the years, the corporation developed many widely used drugs, including insulin, and it became the first company to mass-produce penicillin. The company's many advances made Indiana the leading state in the production and development of medicines.[206]

Charles Conn returned to Elkhart after the Civil War and established C.G. Conn Ltd., a manufacturer of musical instruments.[207] The company's innovation in band instruments made Elkhart an important center of the music world, and it became a base of Elkhart's economy for decades. Nearby South Bend experienced continued growth following the Civil War, and became a large manufacturing city centered around the Oliver Farm Equipment Company, the nation's leading plow producer. Gary was founded in 1906 by the United States Steel Corporation as the home for its new plant.[208]

The administration of Governor James D. Williams proposed the construction of the fourth state capitol building in 1878. The third state capitol building was razed and the new one was constructed on the same site. Two million dollars was appropriated for construction and the new building was completed in 1888. The building was still in use in 2008.[209]

The Panic of 1893 had a severely negative effect on the Hoosier economy when many factories closed and several railroads declared bankruptcy. The Pullman Strike of 1894 hurt the Chicago area and coal miners in southern Indiana participated in a national strike. Hard times were not limited to industry; farmers also felt a financial pinch from falling prices. The economy began to recover when World War I broke out in Europe, creating a higher demand for American goods.[210] Despite economic setbacks, advances in industrial technology continued throughout the last years of the 19th and into the 20th century. On July 4, 1894, Elwood Haynes successfully road tested his first automobile, and opened the Haynes-Apperson auto company in 1896.[211] In 1895, William Johnson invented a process for casting aluminum.[212][213]

During the postwar era, Indiana became a critical swing state that often helped decide which party controlled the presidency. Elections were very close, and became the center of frenzied attention with many parades, speeches and rallies as election day approached; voter turnout ranging over 90% to near 100% in such elections as 1888 and 1896. In remote areas, both sides paid their supporters to vote, and occasionally paid supporters of the opposition not to vote. Despite allegations, historians have found very little fraud in national elections.[214]

To win the electoral vote, both national parties looked for Indiana candidates for the national tickets; a Hoosier was included in all but one presidential election between 1880 and 1924.[215][216]

In 1888, Indiana Senator Benjamin Harrison, grandson of territorial Governor William Henry Harrison, was elected president after an intense battle that attracted more than 300,000 partisans to Indianapolis to hear him speak from his famous front porch.[217] Fort Benjamin Harrison was named in his honor. Six Hoosiers have been elected as vice-president. The most recent was Mike Pence, elected in 2016.[218]

Due to rising white supremacist laws and culture in the southern United States, many Black Americans migrated north. Between November 1878 and February 1879 more than 1100 Black people arrived in Indianapolis, with many more settling across the state. By the end of the century, Indiana's Black resident population numbered 57,505.[144]

Article XIII of the Indiana Constitution of 1851, which sought to exclude African Americans from settling in the state, was invalidated when the Indiana Supreme Court ruled in 1866 that it violated the newly passed Thirteenth Amendment to the U.S. Constitution.[149][150] Nevertheless, numerous communities and counties implemented practices to exclude African Americans. These jurisdictions, known as "sundown towns", were prevalent during the 1890s.[219][220] Sundown towns sought to maintain an all-white population by intentionally expelling Black residents and preventing African American settlement.[219][221] As a result of sundown policies, the number of counties that had African American residents dropped significantly between 1890 and 1930.[219] By the 1990s, sundown town policies became less common in Indiana.[110][219]

Black Hoosiers were selectively allowed to hold leadership roles at the state level. James Sidney Hinton was the first Black person to serve as a legislator in the Indiana General Assembly in 1880. Lillian Thomas Fox was the first Black woman to write for the Indianapolis News, a historically white newspaper.[222]

Indiana housed the Senate Avenue YMCA, one of the largest African American YMCA's in the United States. Through activities hosted by the Senate Avenue YMCA, Indianapolis became an influential cultural center for African Americans.[223]

The last decades of the 19th century began what is known as the "golden age of Indiana literature", a period that lasted until the 1920s.[164] Edward Eggleston wrote The Hoosier Schoolmaster (1871), the first best seller to originate in the state. Many other followed, including Maurice Thompson's Hoosier Mosaics (1875), and Lew Wallace's Ben-Hur (1880). Indiana developed a reputation as the "American heartland" following several widely read novels beginning with Booth Tarkington's The Gentleman from Indiana (1899), Meredith Nicholson's The Hoosiers (1900), and Thompson's second famous novel, Alice of Old Vincennes (1900).[164] James Whitcomb Riley, known as the "Hoosier Poet" and the most popular poet of his age, wrote hundreds of poems celebrating Hoosier themes, including Little Orphant Annie. A unique art culture also began developing in the late 19th century, beginning the Hoosier School of landscape painting and the Richmond Group of impressionist painters. The painters were known for their use of vivid colors and artists including T. C. Steele, whose work was influenced by the colorful hills of southern Indiana.[164] Prominent musicians and composers from Indiana also reached national acclaim during the time, including Paul Dresser whose most popular song, "On the Banks of the Wabash, Far Away", was later adopted as the official state song.[224]

By the late nineteenth and early twentieth century, prohibition and women's suffrage had become the major reform issues in the state. Although supporters and their opponents closely linked the two movements, temperance received a broader hearing than the efforts toward equal suffrage. While many Protestant churches in Indiana supported temperance, few provided a forum for discussions on women's voting rights.[225]

The drive for women's suffrage was reinvigorated in the 1870s, and was sponsored by the leaders of the prohibition movement, especially the Woman's Christian Temperance Union (WCTU). The Indiana branch of the American Woman Suffrage Association was re-established in 1869.[226] In 1878, May Wright Sewall founded the Indianapolis Equal Suffrage Society, which fought for world peace before the nation plunged into World War I.[227] Several Indiana women also became temperance leaders and took an active role in the movement.[225][228] The Indiana chapter of the WCTU was formed in 1874 with Zerelda G. Wallace as its first president.[229] Like many other suffrage leaders, Wallace was radicalized for woman's suffrage through her temperance reform work. During her 1875 speech before the Indiana General Assembly in support of prohibition, legislators demonstrated an open contempt for women involved in politics and speaking in public. Afterward, Wallace credited the experience with her embrace of suffrage.[230]

The first major effort to give women the right to vote in all non-federal elections attempted to amend the state constitution. It passed by both houses of the state legislature in 1881;[227] however, the bill failed to pass in the next legislative session in 1883 as state law required. Temperance efforts fared little better. In 1881, the Indiana chapter of the WCTU, along with organizations participating in the Indiana Grand Council of Temperance, successfully lobbied the Indiana General Assembly to pass an amendment to the state constitution to prohibit the manufacture and sale of alcoholic beverages in the state, but the Indiana Liquor League and a Democratic majority in the state legislature killed the bill in the legislative session in 1883.[229] Following these legislative defeats women's suffrage and prohibition became sensitive issues in local politics as the Democrats rallied the opposition.[227] In German strongholds such as Fort Wayne, opposition to prohibition and women's suffrage was strong until World War I. As one historian notes, "within German working-class family traditions, women in particular were sharply defined in terms of family responsibilities. Suffrage and women's rights ran counter to deep social and religious traditions that placed women in a subservient relationship to men."[231] Renewed interest in women's suffrage did not occur until the end of the century,[232] while prohibition crusaders continued to press for legislative action.

To gain political power in favor of prohibition legislation, a state Prohibition Party was formed in 1884; however, it was never able to effectively mobilize a significant force of voters within the state.[233] Many temperance advocates continued to work within the more established political parties. The liquor issue pitted wets and drys in stable uncompromising coalitions that formed a main theme of Hoosier politics into the 1930s.[234] One legislative success occurred in 1895, when the state legislature passed the Nicholson law, a local option law authored by S. E. Nicholson, a Quaker minister who served in the state legislature and was a leader of the national Anti-Saloon League.[235] The League became a political powerhouse, mobilizing pietistic Protestant voters (that is, members of the major denominations except Lutherans and Episcopalians) to support dry legislation. The Nicholson law allowed voters in a city or township to file a remonstrance that would prevent an individual saloon owner from acquiring a liquor license.[229] Additional legislative efforts to extend the Nicholson law and achieve statewide prohibition in Indiana would not occur until the early twentieth century. One of the leading supporters for the temperance movement in Indiana was Emma Barrett Molloy, who was an active member of the WCTU and lectured across the country to promote the ban of alcohol.[236] Through her vocal activism in temperance and prohibition, Molloy also entered into the women's suffrage sphere as a strong supported for women's rights, particularly freedom of speech.[236]

In May 1906, in Kokomo, a meeting was called to try to revive the defunct Indiana suffragist movement. An Indiana Auxiliary of the National American Woman Suffrage Association was formed and officers were elected. The officers included: Sarah Davis, President; Laura Schofield, first vice-president; Anna Dunn Noland, second vice-president; Mrs. E. M. Wood, secretary; Marion Harvie Barnard, treasurer; and Jane Pond and Judge Samuel Artman, auditors.[237]

In 1911, a suffrage group was formed after the Indianapolis Franchise Society and the Legislation Council of Indiana Women merged to form the Women's Franchise League of Indiana (WFL).[238] The WFL was a member of the national suffrage organization, the National American Woman Suffrage Association. The league was influential in obtaining the vote for women at the state level and formed 1,205 memberships in thirteen districts.[239] After the Nineteenth Amendment to the United States Constitution was adopted, the Women's Franchise League of Indiana organized the League of Women Voters of Indiana.[240]

Hoosiers were fascinated with crime and criminals. Some historians have argued that the popularity of bandits and their exploits in robbing banks and getting away with murder derived from working class resentment against the excesses of the Gilded Age.[241] A group of brothers from Seymour, who had served in the Civil War, formed the Reno Gang, the first outlaw gang in the United States.[242] The Reno Gang, named for the brothers, terrorized Indiana and the region for several years. They were responsible for the first train robbery in the United States which occurred near Seymour in 1866. Their actions inspired a host of other outlaw gangs who copied their work, beginning several decades of high-profile train robberies. Pursued by detectives from the Pinkerton Detective Agency, most of the gang was captured in 1868 and lynched by vigilantes.[242] Other notorious Hoosiers also flourished in the post-war years, including Belle Gunness, an infamous "black widow" serial killer. She killed more than twenty people, most of them men, between 1881 and her own murder in 1908.[243]

In response to the Reno Gang and other criminals, several white cap groups began operating in the state, primarily in the southern counties. They began carrying out lynchings against suspected criminals, leading the state to attempt to crack down on their practices. By the turn of the 20th century, they had become so notorious that anti-lynching laws were passed and in one incident the governor called out the militia to protect a prisoner. When the white caps showed up to lynch him, the militia opened fire, killing one and wounding eleven.[244] Vigilante activity decreased following the incident, and remained low until the rise of the Ku Klux Klan in the 1920s.

Crime stories grabbed the headlines in the 1920s and 1930s. After Prohibition took effect in 1920 until its demise in 1933, it opened up a financial bonanza for criminal activity, especially underground bootlegging and the smuggling of liquor into Chicago, Gary, South Bend, Fort Wayne, Indianapolis, Evansville and other thirsty cities. Enforcement was haphazard; the Anti-Saloon League was more of a lobbying agency and never rallied community support for enforcement.[245] The KKK called for punishment of bootleggers and set up the "Horse Thief Detective Association" (HTDA) to make extra-legal raids on speakeasies and gambling joints. It seldom cooperated with law enforcement or the state or federal courts. Instead gave enforcement a bad name. Arthur Gillom, a Republican elected state attorney general over Klan opposition in 1924, did not tolerate its extra-legal operations. Instead, "He stressed the dangers of citizens relinquishing their constitutional rights and personal freedoms, and emphasized the importance of representative government (at all levels), states' rights, and the concept of separation of church and state." When Rev. Shumaker proposed that "personal liberty had to be sacrificed in order to save people," Gilliom replied that surrendering power and individual freedoms was a slippery slope to centralized government and tyranny.[246]

John Dillinger, a native of Indianapolis, began his streak of bank robberies in Indiana and the Midwest during the 1920s. He was in prison 1924 to 1933. After a return to crime, Dillinger was returned to prison the same year, but escaped with the help of his gang. His gang was responsible for multiple murders and the theft of over $300,000. Dillinger was killed by the FBI in a shootout in Chicago in 1934.[247]

Although industry was rapidly expanding throughout the northern part of the state, Indiana remained largely rural at the turn of the 20th century with a growing population of 2.5 million. Like much of the rest of the American Midwest, Indiana's exports and job providers remained largely agricultural until after World War I. Indiana's developing industry, backed by inexpensive natural gas from the large Trenton Gas Field, an educated population, low taxes, easy access to transportation, and business-friendly government, led Indiana to grow into one of the leading manufacturing states by the mid-1920s.[248]

The state's central location gave it a dense network of railroads. The line most identified with the state was the Monon Line. It provided passenger service for students en route to Purdue, Indiana U. and numerous small colleges, painted its cars in school colors, and was especially popular on football weekends. The Monon was merged into larger lines in 1971, closed its passenger service, and lost its identity.[249] Entrepreneurs built an elaborate "interurban" network of light rails to connect rural areas to shopping opportunities in the cities. They began operation in 1892, and by 1908 there were 2,300 miles of track in 62 counties. The automobile made the lines unprofitable unless the destination was Chicago. By 2001, the "South Shore" was the last one; it still operating from South Bend to Chicago.[250][251]

In 1907, Indiana became the first state to adopt eugenics legislation, that allowed the involuntary sterilization of dangerous male criminals and the mentally defectives. It was never put in effect and in 1921 Indiana became the first state to rule such legislation unconstitutional when the Indiana Supreme Court acted.[252] A revised eugenics law was passed in 1927, and it remained in effect until 1974.[253]

The Indianapolis Motor Speedway complex was built in 1909, inaugurating a new era in history. Most Indiana cities within 200 miles of Detroit became part of the giant automobile industry after 1910. The Indianapolis speedway was a venue for auto companies to show off their products.[254] The Indianapolis 500 quickly became the standard in auto racing as European and American companies competed to build the fastest automobile and win at the track.[255] Industrial and technological industries thrived during this era, George Kingston developed an early carburetor in 1902; in 1912, Elwood Haynes received a patent for stainless steel.[211][212]

In the first two decades of the twentieth century the Indiana Anti-Saloon League (IASL), formed in 1898 as a state auxiliary of the national Anti-Saloon League, and the Woman's Christian Temperance Union[233] successfully organized pressure on Indiana politicians, especially members of the Republican party, to support the dry cause.[181] The IASL, although not the first organization to take up the dry crusade in Indiana, became a key force behind efforts at attaining passage of statewide prohibition in early 1917, and rallied state support for ratification of the Eighteenth Amendment to the U.S. Constitution in 1919.[256] The IASL's success, under the leadership of Edward S. Shumaker, an ordained Methodist minister, made it a model for the League's other state organizations.[257] Shumaker made clear to politicians he did not care whether they drank, but insisted they vote for dry laws or face defeated in the next election by dry voters.[258]

In 1905, passage of the Moore amendment expanded the state's Nicholson local option law to apply to all liquor license applicants within a local township or city ward.[229] The next step was to seek countywide prohibition. The IASL appealed to the general public, holding large rallies in Indianapolis and elsewhere, to support a county option law that would provide a more restrictive ban on alcohol.[259] In September 1908 Indiana governor J. Frank Hanly, a Methodist, Republican, and teetotaler, called for a special legislative session to establish a county option that would allow county voters to prohibit alcohol sales throughout their county.[260][261] The state legislature passed the bill with only a narrow margin.[261] By November 1909, 70 of Indiana's 92 counties were dry. In 1911, a Democratic legislative majority replaced the county option with the Proctor law, a less-geographically restrictive local option, and the number of dry counties was reduced to twenty-six.[233][262] Despite the setback prohibition advocates continued to lobby legislators for support. In December 1917, several temperance organizations formed the Indiana Dry Federation to fight the politically powerful liquor interests,[263] with the IASL joining the group a short time later.[264] The Federation and the League vigorously campaigned for statewide prohibition, which the Indiana General Assembly adopted in February 1917.[260][263] Subsequent legal challenges delayed implementation of statewide prohibition until 1918, when a court ruled in June that Indiana's prohibition law was constitutionally valid.[265]

On January 14, 1919, Indiana became the twenty-fifth state to ratify the Eighteenth Amendment, which mandated nationwide prohibition.[260][266][267] Three days later Nebraska became the thirty-sixth state to ratify the amendment, providing the two-thirds majority of states required to amend the U.S. Constitution.[267] With the beginning of nationwide Prohibition on January 17, 1920, after formal ratification of the Eighteenth Amendment the previous day, efforts turned to enforcement of the new law. Protestant support for Prohibition remained intense in Indiana in the 1920s. Shumaker and the IASL lead a statewide grassroots campaign that successfully passed a new prohibition law for the state. Sponsored by Indiana representative Frank Wright and known as the Wright bone-dry law, it was enacted in 1925. The Wright law was part of a national trend toward stricter prohibition legislation and imposed severe penalties for alcohol possession.[268][269]

The Great Depression and the election of Democratic party candidates in 1932 ended widespread national support for Prohibition. Franklin D. Roosevelt, who included repeal of the Eighteenth Amendment as a major issue of his presidential campaign in 1932, made good on his promise to American voters.[270] On December 5, 1933, the Twenty-first Amendment repealed the Eighteenth Amendment and ended nationwide Prohibition. However, Indiana's legislature continued to regulate alcohol within the state through allocation of state liquor licenses and prohibition of sales on Sunday.[268]

White middle-class Indiana women learned organizational skills through the suffrage and temperance movements. By the 1890s they were applying their new skills to the needs of their home communities, by organizing women's clubs, the combined literary activity with social activism focused on such needs as public health, sanitation, and good schools. Hoosier women worked at both the state and local level to materialize Progressive Era reforms. In Lafayette, for example, the suffragists concentrated in the Lafayette Franchise League, while those oriented toward social concerns worked through the Lafayette Charity Organization Society (LCOS), the Free Kindergarten and Industrial School Association (FKISA), and the Martha Home.[271] Albion Fellows Bacon led statewide and national efforts at housing reform. A native of Evansville, Bacon worked to pass tenement and housing legislation in Indiana in 1909, 1913, and 1917. She also held leadership roles in Indiana Child Welfare Association; the Child Welfare Committee, a part of the Women's Section of the Indiana State Council of Defense; the Indiana Conference of Charities and Corrections, and the Juvenile Advisory Commission of Indiana's Probation Department. Women were given the right to vote in 1920 when the Nineteenth Amendment to the United States Constitution was ratified.

 Middle-class Black women activists were organized through African American Baptist and Methodist churches, and under the leadership of Hallie Quinn Brown who formed a statewide umbrella group, the Indiana State Federation of Colored Women's Clubs. Racism prevented the organization from association with its white counterpart, the Indiana State Federation of Women's Clubs.[272] White Hoosier suffragist May Wright Sewall spoke at the founding convention in a show of solidarity with Black Hoosier women.[272] The Indiana Association of Colored Women's Clubs sponsored 56 clubs in 46 cities in the state, with 2000 members by 1933, and a budget of over $20,000. Most members were public school teachers or hairdressers, as well as women active and local business in the Black community, and in government positions. They affiliated with the National Federation of Afro-American Women, headed by Mrs. Booker T. Washington, and became part of her husband's powerful network of Black activists. One of the most prominent members in Indiana was Madame C. J. Walker of Indianapolis, who owned a nationally successful business selling beauty and hair products for Black women. Club meetings focused on home-making classes, research, and statistics regarding the status of African Americans in Indiana and nationwide, suffrage, and anti-lynching activism. The local clubs operated rescue missions, nursery schools, and educational programs.[273]

Between March 23 and March 27, 1913, Indiana and more than a dozen other states experienced major flooding during the Great Flood of 1913; it was Indiana's worst flood disaster up to that time.[274][275][276] The weather system that created the unprecedented flooding arrived in Indiana on Sunday, March 23, with a major tornado at Terre Haute.[277][note 9] In four days, rainfall topped nine inches in southern Indiana, more than half of it falling within a twenty-four-hour period on March 25.[278] Heavy rains, runoff, and rising rivers resulted in extensive flooding in northeast, central, and southern Indiana.[279][note 10] Indiana's flood-related deaths were estimated at 100 to 200,[280][281] with flood damage estimated at $25 million (in 1913 dollars).[279] State and local communities handled their own disaster response and relief.[282] The American Red Cross, still a small organization at that time, established a temporary headquarters in Indianapolis and served the six hardest-hit Indiana counties. Indiana governor Samuel M. Ralston appealed to Indiana cities and other states for relief assistance and appointed a trustee to receive relief funds and arrange for distribution of supplies. Independent organizations, such as the Rotary Club of Indianapolis and others, helped with local relief efforts.[283]

Hoosiers were divided about entering World War I. Before Germany resumed unrestricted submarine warfare and tried to enlist Mexico as a military ally in 1917, most Hoosiers wanted the U.S. to be neutral in the war. Support for Britain came from professions and businessmen. Opposition came from churchmen, women, farmers and Irish Catholics and German-American elements. They called for neutrality and strongly opposed going to war to rescue the British Empire.[284] Influential Hoosiers who opposed involvement in the war included Democratic Senator John W. Kern, and Vice President Thomas R. Marshall.[285] Supporters of military preparedness included James Whitcomb Riley and George Ade. Most of the opposition dissipated when the United States officially declared war against Germany in April 1917, but some teachers lost their jobs on suspicion of disloyalty,[286] and public schools could no longer teach in German.[287][note 11] Socialist leader Eugene V. Debs, from Terre Haute, went to federal prison for encouraging young men to evade the draft.

The Indiana National Guard was federalized during WWI; many units were sent to Europe. A separate organization, the Liberty Guard, was formed in 1910, primarily for social purposes: members marched in parades and at patriotic events. Governor Samuel Ralston had to call out the Liberty Guard in November 1913 to put down a growing workers strike in Indianapolis. By 1920, the state decided to formalize this group, renaming it the Indiana Civil Defense Force and supplying it with equipment and training.[288] In 1941, the unit was named the Indiana Guard Reserve; it effectively became a state militia. During World War II, it was again federalized and members were called up by the federal government.

Indiana provided 130,670 troops during the war; a majority of them were drafted.[289] Over 3,000 men died, many from influenza and pneumonia.[289] To honor the Hoosier veterans of the war, the state began construction of the Indiana World War Memorial.[290]

The war-time economy provided a boom to Indiana's industry and agriculture, which led to more urbanization throughout the 1920s.[291] By 1925, more workers were employed in industry than in agriculture in Indiana. Indiana's greatest industries were steel production, iron, automobiles, and railroad cars.[292]

The Indiana Ku Klux Klan very rapidly in the early 1920s from a cross section of Protestant men. The KKK was operated for the benefit of its well-paid organizers. There was little leadership or coordination among the hundreds of local chapters. During the 1925 General Assembly session, the state Klan leader Grand Dragon D. C. Stephenson boasted, "I am the law in Indiana." The Klan demanded an anti-Catholic legislative agenda, but failed to win any significant legislation. Stephenson was convicted for the murder of Madge Oberholtzer in 1925 and sentenced to life in prison. After Governor Edward L. Jackson, whom Stephenson helped elect, refused to pardon him, Stephenson began to name many of his co-conspirators. This led the state's making a string of arrests and indictments against political leaders, including the governor, mayor of Indianapolis, the attorney general, and many others. The crackdown effectively rendered the Klan powerless and a great majority of members quit.[293][294]

During the 1930s, Indiana, like the rest of the nation, was affected by the Great Depression. The economic downturn had a wide-ranging negative impact on Indiana. Urbanization declined. Governor Paul V. McNutt's administration struggled to build from scratch a state-funded welfare system to help the overwhelmed private charities. During his administration, spending and taxes were cut drastically in response to the Depression. The state government was completely reorganized. McNutt also enacted the state's first income tax. On several occasions, he declared martial law to put an end to worker strikes.[295]

During the Great Depression, unemployment exceeded 25% statewide. Southern Indiana was hard hit, and unemployment in the coal mining districts reached 50% during the worst years, 1931–1933.[292] The federal Works Progress Administration (WPA) began operations in Indiana in July 1935. By October of that year, the agency had put 74,708 Hoosiers to work. In 1940, there were still 64,700 people working for agency.[292] The majority of these workers were employed to improve the state's infrastructure: roads, bridges, flood control projects, and water treatment plants. Some helped index collections of libraries, and artists were employed to create murals for post offices and libraries. Nearly every community had a project to work on.[292][296]

During the 1930s, many local businesses collapsed, several railroads went bankrupt, and numerous small rural banks folded.[297][298] Manufacturing came to an abrupt halt or was severely cut back due to the dwindling demand for products. The Depression continued to negatively affect Indiana until the buildup for World War II. The effects continued to be felt for many years thereafter. After 1935, labor unions grew much stronger, especially in coal, steel and rubber industries.[299]

The economy began to recover in 1933, but unemployment remained high among youth and older workers until 1940, when the federal government built up supplies and armaments going into World War II.[300]

Indiana's factories went into overdrive during World War II to support the war effort, and mass employment and prosperity returned. Indiana manufactured 4.5 percent of total United States military armaments produced during World War II, ranking eighth among the 48 states.

The state produced munitions in many plants, such as an army plant near Sellersburg. The P-47 fighter-plane was manufactured in Evansville at Republic Aviation.[301] The steel produced in northern Indiana was used in tanks, battleships, and submarines. Other war-related materials were produced throughout the state. Indiana's military bases were activated, with areas such as Camp Atterbury reaching historical peaks in activity.[302]

The population was highly supportive of the war efforts.[303] The political left supported the war (unlike World War I, which Socialists opposed.) The churches showed much less pacifism than in 1914. The Church of God, based in Anderson, had a strong pacifist element, reaching a high point in the late 1930s. The Church regarded World War II as a just war because America was attacked. Anti-Communist sentiment has since kept strong pacifism from developing in the Church of God.[304] Likewise the Quakers, with a strong base near Richmond, generally regarded World War II as a just war and about 90% served, although there were some conscientious objectors.[305] The Mennonites and Brethren continued their pacifism, but the federal government was much less hostile than before. The churches helped their young men to both become conscientious objectors and to provide valuable service to the nation. Goshen College set up a training program for unpaid Civilian Public Service jobs. Although the young women pacifists were not liable to the draft, they volunteered for unpaid Civilian Public Service jobs to demonstrate their patriotism; many worked in mental hospitals.[306]

The state sent nearly 400,000 Hoosiers who enlisted or were drafted.[307] More than 11,783 Hoosiers died in the conflict and another 17,000 were wounded. Hoosiers served in all the major theaters of the war.[308][309] Their sacrifice was honored by additions to the World War Memorial in Indianapolis, which was not finished until 1965.[310]

Tens of thousands of women volunteered for war service, through agencies such as the Red Cross. Representative was Elizabeth Richardson of Mishawaka. She served coffee and doughnuts to combat soldiers in England and France from a Red Cross clubmobile. She died in a plane crash in 1945 in France.[311]

During the post-World War II boom from 1945 to 1973, Indiana's economy prospered and Indiana was ranked 20th out of 50 states plus Washington, D.C. in the late-1960s for personal income. However, Indiana's economy began to struggle after the recession of 1969–1970 as the manufacturing sector began to decline. Foreign competition, corporate mergers, automation, and new management strategies lead to downsizing, mass layoffs, diversification, and chronic unemployment. Cities such as Muncie, Anderson, Indianapolis, Kokomo, Gary, East Chicago, Hammond, Michigan City, Fort Wayne, South Bend, Elkhart, and Evansville all witnessed population declines and rising unemployment and poverty during the 1970s and 1980s. Northwest Indiana was hit especially by the steel crisis of 1974 – 1983.

Redlining, or the discriminatory and exclusionary housing practice meant to separate affluent white populations from low-income racial groups, was a form of forced migration and relocation that many Black communities experienced during the twentieth-century.[312] For example, the Indiana Avenue community on the west side of downtown Indianapolis was displaced by the building of Indiana University Purdue University Indianapolis (IUPUI) in the sixties.[313]

Central Indiana was struck by a major flood in 2008, leading to widespread damage and the evacuations of hundreds of thousands of residents. It was the costliest disaster in the history of the state, with early damage estimates topping $1 billion.[314]

Since the early-1990s, Indiana has diversified its economy away from heavy industry and towards service (such as banking, insurance, healthcare, education, financial services, information technology) and high-tech manufacturing. In 2016, 516,000 workers were employed in manufacturing, down from 696,000 in 2000 and nearly 750,000 in 1969, but up from 424,000 in 2009 at the depths of the Great Recession. Heavy industries such as oil, autos, and steel still comprise a significant portion of the states' GDP, but other industries such as electrical goods, medical equipment, and pharmaceuticals have grown recently as well. However, Indiana's wage growth has lagged behind other states, and Indiana has fallen from 20th in personal income during the 1960s to 39th in 2017.

In 2012, Indiana's exports totaled $34.4 billion, a record high for the state. The rate of export growth in 2012 was faster in Indiana than it was for the nation.[315]

In 2021, Indiana was the third-largest auto-producing state in the U.S. The state also became a hub for advanced manufacturing, especially of electrical vehicles and batteries, as well as aerospace and defense products. Like the rest of the country, Indiana was hit hard by the Great Recession of 2008–2009. The state saw high unemployment rates and a drop in manufacturing output during this time. Since the recession, Indiana has focused on diversifying its economy to reduce reliance on manufacturing.[316]