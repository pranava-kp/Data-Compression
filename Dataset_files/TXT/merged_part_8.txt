Rock is a broad genre of popular music that originated in the United States as "rock and roll" in the late 1940s and early 1950s, developing into a range of different styles from the mid-1960s, primarily in the United States and the United Kingdom. It has its roots in rock and roll, a style that drew directly from the black musical genres of blues and rhythm and blues, as well as from country music. Rock also drew strongly from genres such as electric blues and folk, and incorporated influences from jazz and other musical styles. For instrumentation, rock is typically centered on the electric guitar, usually as part of a rock group with electric bass guitar, drums, and one or more singers. Usually, rock is song-based music with a 44 time signature and using a verse–chorus form, but the genre has become extremely diverse. Like pop music, lyrics often stress romantic love but also address a wide variety of other themes that are frequently social or political. Rock was the most popular genre of music in the U.S. and much of the Western world from the 1950s until its decline in the 2010s.

Rock musicians in the mid-1960s began to advance the album ahead of the single as the dominant form of recorded music expression and consumption, with the Beatles at the forefront of this development. Their contributions lent the genre a cultural legitimacy in the mainstream and initiated a rock-informed album era in the music industry for the next several decades. By the late 1960s "classic rock"[3] period, a few distinct rock music subgenres had emerged, including hybrids like blues rock, folk rock, country rock, Southern rock, raga rock, and jazz rock, which contributed to the development of psychedelic rock, influenced by the countercultural psychedelic and hippie scene. New genres that emerged included progressive rock, which extended artistic elements, heavy metal, which emphasized an aggressive thick sound, and glam rock, which highlighted showmanship and visual style. In the second half of the 1970s, punk rock reacted by producing stripped-down, energetic social and political critiques. Punk was an influence in the 1980s on new wave, post-punk and eventually alternative rock.

From the 1990s, alternative rock began to dominate rock music and break into the mainstream in the form of grunge, Britpop, and indie rock. Further fusion subgenres have since emerged, including pop-punk, electronic rock, rap rock, and rap metal. Some movements were conscious attempts to revisit rock's history, including the garage rock/post-punk revival in the 2000s. Since the 2010s, rock has lost its position as the pre-eminent popular music genre in world culture, but remains commercially successful. The increased influence of hip-hop and electronic dance music can be seen in rock music, notably in the techno-pop scene of the early 2010s and the pop-punk-hip-hop revival of the 2020s.


Rock has also embodied and served as the vehicle for cultural and social movements, leading to major subcultures including mods and rockers in the U.K., the hippie movement and the wider Western counterculture movement that spread out from San Francisco in the U.S. in the 1960s, the latter of which continues to this day. Similarly, 1970s punk culture spawned the goth, punk, and emo subcultures. Inheriting the folk tradition of the protest song, rock music has been associated with political activism, as well as changes in social attitudes to race, sex, and drug use, and is often seen as an expression of youth revolt against adult consumerism and conformity. At the same time, it has been commercially highly successful, leading to accusations of selling out.
A good definition of rock, in fact, is that it's popular music that to a certain degree doesn't care if it's popular.

The sound of rock is traditionally centered on the amplified electric guitar, which emerged in its modern form in the 1950s with the popularity of rock and roll.[5] It was also greatly influenced by the sounds of electric blues guitarists.[6] The sound of an electric guitar in rock music is typically supported by an electric bass guitar, which pioneered jazz music in the same era,[7] and by percussion produced from a drum kit that combines drums and cymbals.[8] This trio of instruments has often been complemented by the inclusion of other instruments, particularly keyboards such as the piano, the Hammond organ, and the synthesizer.[9] The basic rock instrumentation was derived from the basic blues band instrumentation (prominent lead guitar, second chordal instrument, bass, and drums).[6] A group of musicians performing rock music is termed as a rock band or a rock group. Furthermore, it typically consists of between three (the power trio) and five members. Classically, a rock band takes the form of a quartet whose members cover one or more roles, including vocalist, lead guitarist, rhythm guitarist, bass guitarist, drummer, and often keyboard player or another instrumentalist.[10]

Rock music is traditionally built on a foundation of simple syncopated rhythms in a 44 meter, with a repetitive snare drum back beat on beats two and four.[11] Melodies often originate from older musical modes such as the Dorian and Mixolydian, as well as major and minor modes. Harmonies range from the common triad to parallel perfect fourths and fifths and dissonant harmonic progressions.[11] Since the late 1950s,[12] and particularly from the mid-1960s onwards, rock music often used the verse–chorus structure derived from blues and folk music, but there has been considerable variation from this model.[13] Critics have stressed the eclecticism and stylistic diversity of rock.[14] Because of its complex history and its tendency to borrow from other musical and cultural forms, it has been argued that "it is impossible to bind rock music to a rigidly delineated musical definition."[15] In 1981, music journalist Robert Christgau said, "the best rock jolts folk-art virtues—directness, utility, natural audience—into the present with shots of modern technology and modernist dissociation".[16]

Rock and roll was conceived as an outlet for adolescent yearnings ... To make rock and roll is also an ideal way to explore intersections of sex, love, violence, and fun, to broadcast the delights and limitations of the regional, and to deal with the depredations and benefits of mass culture itself.

Unlike many earlier styles of popular music, rock lyrics have dealt with a wide range of themes, including romantic love, sex, rebellion against "The Establishment", social concerns, and life styles.[11] These themes were inherited from a variety of sources such as the Tin Pan Alley pop tradition, folk music, and rhythm and blues.[18] Christgau characterizes rock lyrics as a "cool medium" with simple diction and repeated refrains, and asserts that rock's primary "function" "pertains to music, or, more generally, noise."[19] The predominance of white, male, and often middle class musicians in rock music has often been noted,[20] and rock has been seen as an appropriation of Black musical forms for a young, white and largely male audience.[21] As a result, it has also been seen to articulate the concerns of this group in both style and lyrics.[22] Christgau, writing in 1972, said in spite of some exceptions, "rock and roll usually implies an identification of male sexuality and aggression".[23]

Since the term "rock" started being used in preference to "rock and roll" from the late 1960s, it has usually been contrasted with pop music, with which it has shared many characteristics; however, rock is often distanced from pop; the former has an emphasis on musicianship, live performance, and a focus on serious and progressive themes as part of an ideology of authenticity that is frequently combined with an awareness of the genre's history and development.[24] According to Simon Frith, rock was "something more than pop, something more than rock and roll" and "[r]ock musicians combined an emphasis on skill and technique with the romantic concept of art as artistic expression, original and sincere".[24]

In the new millennium, the term rock has occasionally been used as a blanket term including forms like pop music, reggae music, soul music, and even hip hop, which it has been influenced with but often contrasted through much of the latter's history.[25] Christgau has used the term broadly to refer to popular and semipopular music that caters to his sensibility as "a rock-and-roller", including a fondness for a good beat, a meaningful lyric with some wit, and the theme of youth, which holds an "eternal attraction" so objective "that all youth music partakes of sociology and the field report." Writing in Christgau's Record Guide: The '80s (1990), he said this sensibility is evident in the music of folk singer-songwriter Michelle Shocked, rapper LL Cool J, and synth-pop duo Pet Shop Boys—"all kids working out their identities"—as much as it is in the music of Chuck Berry, the Ramones, and the Replacements.[26]

The foundations of rock music are in rock and roll, which originated in the United States during the late 1940s and early 1950s; the genre quickly spread to much of the rest of the world. Its immediate origins lay in a melding of various black musical genres of the time, including rhythm and blues and gospel music, with country and western.[27]

Debate surrounds the many recordings which have been suggested as "the first rock and roll record". Contenders include "Strange Things Happening Every Day" by Sister Rosetta Tharpe (1944);[28] "That's All Right" by Arthur Crudup (1946),[29] which was later covered by Elvis Presley in 1954; "The House of Blue Lights" by Ella Mae Morse and Freddie Slack (1946);[30] Wynonie Harris' "Good Rocking Tonight" (1948);[31] Goree Carter's "Rock Awhile" (1949);[32] Jimmy Preston's "Rock the Joint" (1949), also covered by Bill Haley & His Comets in 1952;[33] and "Rocket 88" by Jackie Brenston and his Delta Cats (in fact, Ike Turner and his band the Kings of Rhythm), recorded by Sam Phillips for Chess Records in 1951.[34]

In 1951, Cleveland, Ohio disc jockey Alan Freed began playing rhythm and blues music (then termed "race music") for a multi-racial audience, and is credited with first using the phrase "rock and roll" to describe the music.[35] Four years later, Bill Haley's "Rock Around the Clock" (1954) became the first rock and roll song to top Billboard magazine's main sales and airplay charts, and opened the door worldwide for this new wave of popular culture.[36][37] Other artists with early rock and roll hits included Chuck Berry, Bo Diddley, Fats Domino, Little Richard, Jerry Lee Lewis, and Gene Vincent.[34] Soon rock and roll was the major force in American record sales and crooners, such as Eddie Fisher, Perry Como, and Patti Page, who had dominated the previous decade of popular music, found their access to the pop charts significantly curtailed.[38]

Rock and roll has been seen as leading to a number of distinct subgenres, including rockabilly, combining rock and roll with "hillbilly" country music, which was usually played and recorded in the mid-1950s by white singers such as Carl Perkins, Jerry Lee Lewis, Roy Orbison, Buddy Holly and with the greatest commercial success, Elvis Presley.[39] Hispanic and Latino American movements in rock and roll, which would eventually lead to the success of Latin rock and Chicano rock within the US, began to rise in the Southwest; with rock and roll standard musician Ritchie Valens and even those within other heritage genres, such as Al Hurricane along with his brothers Tiny Morrie and Baby Gaby as they began combining rock and roll with country-western within traditional New Mexico music.[40] In addition, the 1950s saw the growth in popularity of the electric guitar, and the development of a specifically rock and roll style of playing through such exponents as Chuck Berry, Link Wray, and Scotty Moore.[41] The use of distortion, pioneered by Western swing guitarists such as Junior Barnard[42] and Eldon Shamblin was popularized by Chuck Berry in the mid-1950s.[43] The use of power chords, pioneered by Francisco Tárrega and Heitor Villa-Lobos in the 19th century and later on by Willie Johnson and Pat Hare in the early 1950s, was popularized by Link Wray in the late 1950s.[44]

Commentators have traditionally perceived a decline of rock and roll in the late 1950s and early 1960s. By 1959, the death of Buddy Holly, the Big Bopper and Ritchie Valens in a plane crash, the departure of Elvis for the army, the retirement of Little Richard to become a preacher, prosecutions of Jerry Lee Lewis and Chuck Berry and the breaking of the payola scandal (which implicated major figures, including Alan Freed, in bribery and corruption in promoting individual acts or songs), gave a sense that the rock and roll era established at that point had come to an end.[45]

Rock quickly spread out from its origins in the US, associated with the rapid Americanization that was taking place globally in the aftermath of the Second World War.[46] Cliff Richard is credited with one of the first rock and roll hits outside of North America with "Move It" (1959), effectively ushering in the sound of British rock.[47] Several artists, most prominently Tommy Steele from the UK, found success with covers of major American rock and roll hits before the recordings could spread internationally, often translating them into local languages where appropriate.[48][49] Steele in particular toured Britain, Scandinavia, Australia, the USSR and South Africa from 1955 to 1957, influencing the globalisation of rock.[48] Johnny O'Keefe's 1958 record "Wild One" was one of the earliest Australian rock and roll hits.[50] By the late 1950s, as well as in the American-influenced Western world, rock was popular in communist states such as Yugoslavia,[51] and the USSR,[52] as well as in regions such as South America.[49]

In the late 1950s and early 1960s, U.S. blues music and blues rock artists, who had been surpassed by the rise of rock and roll in the US, found new popularity in the UK, visiting with successful tours.[53] Lonnie Donegan's 1955 hit "Rock Island Line" was a major influence and helped to develop the trend of skiffle music groups throughout the country, many of which, including John Lennon's Quarrymen (later the Beatles), moved on to play rock and roll.[54] While former rock and roll market in the US was becoming dominated by lightweight pop and ballads, British rock groups at clubs and local dances were developing a style more strongly influenced by blues-rock pioneers, and were starting to play with an intensity and drive seldom found in white American acts;[55] this influence would go on to shape the future of rock music through the British Invasion.[53]

The first four years of the 1960s has traditionally been seen as an era of hiatus for rock and roll.[56] More recently, some authors[weasel words] have emphasised important innovations and trends in this period without which future developments would not have been possible.[57][58] While early rock and roll, particularly through the advent of rockabilly, saw the greatest commercial success for male and white performers, in this era, the genre was dominated by black and female artists. Rock and roll had not disappeared entirely from music at the end of the 1950s and some of its energy can be seen in the various dance crazes of the early 1960s, started by Chubby Checker's record "The Twist" (1960).[58][nb 1] Some music historians have also pointed to important and innovative technical developments that built on rock and roll in this period, including the electronic treatment of sound by such innovators as Joe Meek, and the elaborate production methods of the Wall of Sound pursued by Phil Spector.[58]

The instrumental rock and roll of performers such as Duane Eddy, Link Wray and the Ventures was further developed by Dick Dale, who added distinctive "wet" reverb, rapid alternate picking, and Middle Eastern and Mexican influences. He produced the regional hit "Let's Go Trippin'" in 1961 and launched the surf music craze, following up with songs like "Misirlou" (1962).[62] Like Dale and his Del-Tones, most early surf bands were formed in Southern California, including the Bel-Airs, the Challengers, and Eddie & the Showmen.[62] The Chantays scored a top ten national hit with "Pipeline" in 1963 and probably the best-known surf tune was 1963's "Wipe Out", by the Surfaris, which hit number 2 and number 10 on the Billboard charts in 1965.[63] Surf rock was also popular in Europe during this time, with the British group the Shadows scoring hits in the early 1960s with instrumentals such as "Apache" (1960) and "Kon-Tiki" (1961), while Swedish surf group the Spotnicks saw success in both Sweden and Britain.

Surf music achieved its greatest commercial success as vocal pop music, particularly the work of the Beach Boys, formed in 1961 in Southern California. Their early albums included both instrumental surf rock, including covers of music by Dick Dale and vocal songs, drawing on rock and roll and doo wop and the close harmonies of vocal pop acts like the Four Freshmen.[64] The Beach Boys first chart hit, "Surfin'" in 1961 reached the Billboard top 100 and helped make the surf music craze a national phenomenon.[65] It is often argued that the surf music craze and the careers of almost all surf acts was effectively ended by the arrival of the British Invasion from 1964, because most surf music hits were recorded and released between 1960 and 1965.[66][nb 2]

By the end of 1962, what would become the British rock scene had started with beat groups like the Beatles, Gerry & the Pacemakers and the Searchers from Liverpool and Freddie and the Dreamers, Herman's Hermits and the Hollies from Manchester. They drew on a wide range of American influences including 1950s rock and roll, soul, rhythm and blues, and surf music,[67] initially reinterpreting standard American tunes and playing for dancers. Bands like the Animals from Newcastle and Them from Belfast,[68] and particularly those from London like the Rolling Stones and the Yardbirds, were much more directly influenced by rhythm and blues and later blues music.[69] Soon these groups were composing their own material, combining US forms of music and infusing it with a high energy beat. Beat bands tended towards "bouncy, irresistible melodies", while early British blues acts tended towards less sexually innocent, more aggressive songs, often adopting an anti-establishment stance. There was, however, particularly in the early stages, considerable musical crossover between the two tendencies.[70] By 1963, led by the Beatles, beat groups had begun to achieve national success in Britain, soon to be followed into the charts by the more rhythm and blues focused acts.[71]

"I Want to Hold Your Hand" was the Beatles' first number one hit on the Billboard Hot 100,[72] spending seven weeks at the top and a total of 15 weeks on the chart.[73][74] Their first appearance on The Ed Sullivan Show on 9 February 1964, drawing an estimated 73 million viewers (at the time a record for an American television program) is considered a milestone in American pop culture. During the week of 4 April 1964, the Beatles held 12 positions on the Billboard Hot 100 singles chart, including the entire top five. The Beatles went on to become the biggest selling rock band of all time and they were followed into the US charts by numerous British bands.[70] During the next two years, British acts dominated their own and the US charts with Peter and Gordon, the Animals,[75] Manfred Mann, Petula Clark,[75] Freddie and the Dreamers, Wayne Fontana and the Mindbenders, Herman's Hermits, the Rolling Stones,[76] the Troggs, and Donovan[77] all having one or more number one singles.[73] Other major acts that were part of the invasion included the Kinks, the Who, and the Dave Clark Five.[78][79][80]

The British Invasion helped internationalize the production of rock and roll, opening the door for subsequent British (and Irish) performers to achieve international success.[81] In America it arguably spelled the end of instrumental surf music, vocal girl groups and (for a time) the teen idols, that had dominated the American charts in the late 1950s and 1960s.[82] It dented the careers of established R&B acts like Fats Domino and Chubby Checker and even temporarily derailed the chart success of surviving rock and roll acts, including Elvis.[83] The British Invasion also played a major part in the rise of a distinct genre of rock music, and cemented the primacy of the rock group, based on guitars and drums and producing their own material as singer-songwriters.[84] Following the example set by the Beatles' 1965 LP Rubber Soul in particular, other British rock acts released rock albums intended as artistic statements in 1966, including the Rolling Stones' Aftermath, the Beatles' own Revolver, and the Who's A Quick One, as well as American acts in the Beach Boys (Pet Sounds) and Bob Dylan (Blonde on Blonde).[85]

Although the first impact of the British Invasion on American popular music was through beat and R&B based acts, the impetus was soon taken up by a second wave of bands that drew their inspiration more directly from American blues, including the Rolling Stones and the Yardbirds.[86] British blues musicians of the late 1950s and early 1960s had been inspired by the acoustic playing of figures such as Lead Belly, who was a major influence on the Skiffle craze, and Robert Johnson.[87] Increasingly they adopted a loud amplified sound, often centered on the electric guitar, based on the Chicago blues, particularly after the tour of Britain by Muddy Waters in 1958, which prompted Cyril Davies and guitarist Alexis Korner to form the band Blues Incorporated.[88] The band involved and inspired many of the figures of the subsequent British blues boom, including members of the Rolling Stones and Cream, combining blues standards and forms with rock instrumentation and emphasis.[55]

The other key focus for British blues was John Mayall; his band, the Bluesbreakers, included Eric Clapton (after Clapton's departure from the Yardbirds) and later Peter Green. Particularly significant was the release of Blues Breakers with Eric Clapton (Beano) album (1966), considered one of the seminal British blues recordings and the sound of which was much emulated in both Britain and the United States.[89] Eric Clapton went on to form supergroups Cream, Blind Faith, and Derek and the Dominos, followed by an extensive solo career that helped bring blues rock into the mainstream.[88] Green, along with the Bluesbreaker's rhythm section Mick Fleetwood and John McVie, formed Peter Green's Fleetwood Mac, who enjoyed some of the greatest commercial success in the genre.[88] In the late 1960s Jeff Beck, also an alumnus of the Yardbirds, moved blues rock in the direction of heavy rock with his band, the Jeff Beck Group.[88] The last Yardbirds guitarist was Jimmy Page, who went on to form The New Yardbirds which rapidly became Led Zeppelin. Many of the songs on their first three albums, and occasionally later in their careers, were expansions on traditional blues songs.[88]

In the United States, blues rock had been pioneered in the early 1960s by guitarist Lonnie Mack;[90] however, the genre began to take off in the mid-1960s as acts developed a sound similar to British blues musicians. Key acts included Paul Butterfield (whose band acted like Mayall's Bluesbreakers in Britain as a starting point for many successful musicians), Canned Heat, the early Jefferson Airplane, Janis Joplin, Johnny Winter, the J. Geils Band, and Jimi Hendrix with his power trios, the Jimi Hendrix Experience (which included two British members, and was founded in Britain), and Band of Gypsys, whose guitar virtuosity and showmanship would be among the most emulated of the decade.[88] Blues rock bands from the southern states, like the Allman Brothers Band, Lynyrd Skynyrd, and ZZ Top, incorporated country elements into their style to produce the distinctive genre Southern rock.[91]

Early blues rock bands often emulated jazz, playing long, involved improvisations, which would later be a major element of progressive rock. From about 1967 bands like Cream and the Jimi Hendrix Experience had moved away from purely blues-based music into psychedelia.[92] By the 1970s, blues rock had become heavier and more riff-based, exemplified by the work of Led Zeppelin and Deep Purple, and the lines between blues rock and hard rock "were barely visible",[92] as bands began recording rock-style albums.[92] The genre was continued in the 1970s by figures such as George Thorogood and Pat Travers,[88] but, particularly on the British scene (except perhaps for the advent of groups such as Status Quo and Foghat who moved towards a form of high energy and repetitive boogie rock), bands became focused on heavy metal innovation, and blues rock began to slip out of the mainstream.[93]

Garage rock was a raw form of rock music, particularly prevalent in North America in the mid-1960s and so called because of the perception that it was rehearsed in the suburban family garage.[94][95] Garage rock songs often revolved around the traumas of high school life, with songs about "lying girls" and unfair social circumstances being particularly common.[96] The lyrics and delivery tended to be more aggressive than was common at the time, often with growled or shouted vocals that dissolved into incoherent screaming.[94] They ranged from crude one-chord music (like the Seeds) to near-studio musician quality (including the Knickerbockers, the Remains, and the Fifth Estate). There were also regional variations in many parts of the country with flourishing scenes particularly in California and Texas.[96] The Pacific Northwest states of Washington and Oregon had perhaps[according to whom?] the most defined regional sound.[97]

The style had been evolving from regional scenes as early as 1958. "Tall Cool One" (1959) by the Wailers and "Louie Louie" by the Kingsmen (1963) are mainstream examples of the genre in its formative stages.[98] By 1963, garage band singles were creeping into the national charts in greater numbers, including Paul Revere and the Raiders (Boise),[99] the Trashmen (Minneapolis)[100] and the Rivieras (South Bend, Indiana).[101] Other influential garage bands, such as the Sonics (Tacoma, Washington), never reached the Billboard Hot 100.[102]

The British Invasion greatly influenced garage bands, providing them with a national audience, leading many (often surf or hot rod groups) to adopt a British influence, and encouraging many more groups to form.[96] Thousands of garage bands were extant in the United States and Canada during the era and hundreds produced regional hits.[96] Despite scores of bands being signed to major or large regional labels, most were commercial failures. It is generally agreed that garage rock peaked both commercially and artistically around 1966.[96] By 1968, the style largely disappeared from the national charts and at the local level as amateur musicians faced college, work or the draft.[96] New styles had evolved to replace garage rock.[96][nb 3]

By the 1960s, the scene that had developed out of the American folk music revival had grown to a major movement, using traditional music and new compositions in a traditional style, usually on acoustic instruments.[104] In America the genre was pioneered by figures such as Woody Guthrie and Pete Seeger and often identified with progressive or labor politics.[104] In the early sixties figures such as Joan Baez and Bob Dylan had come to the fore in this movement as singer-songwriters.[105] Dylan had begun to reach a mainstream audience with hits including "Blowin' in the Wind" (1963) and "Masters of War" (1963), which brought "protest songs" to a wider public,[106] but, although beginning to influence each other, rock and folk music had remained largely separate genres, often with mutually exclusive audiences.[107]

Early attempts to combine elements of folk and rock included the Animals' "House of the Rising Sun" (1964), which was the first commercially successful folk song to be recorded with rock and roll instrumentation[108] and the Beatles "I'm a Loser" (1964), arguably the first Beatles song to be influenced directly by Dylan.[109] The folk rock movement is usually thought to have taken off with the Byrds' recording of Dylan's "Mr. Tambourine Man" which topped the charts in 1965.[107] With members who had been part of the café-based folk scene in Los Angeles, the Byrds adopted rock instrumentation, including drums and 12-string Rickenbacker guitars, which became a major element in the sound of the genre.[107] Later that year Dylan adopted electric instruments, much to the outrage of many folk purists, with his "Like a Rolling Stone" becoming a US hit single.[107] According to Ritchie Unterberger, Dylan (even before his adoption of electric instruments) influenced rock musicians like the Beatles, demonstrating "to the rock generation in general that an album could be a major standalone statement without hit singles", such as on The Freewheelin' Bob Dylan (1963).[110]

Folk rock particularly took off in California, where it led acts like the Mamas & the Papas and Crosby, Stills, and Nash to move to electric instrumentation, and in New York, where it spawned performers including the Lovin' Spoonful and Simon and Garfunkel, with the latter's acoustic "The Sounds of Silence" (1965) being remixed with rock instruments to be the first of many hits.[107] These acts directly influenced British performers like Donovan and Fairport Convention.[107] In 1969 Fairport Convention abandoned their mixture of American covers and Dylan-influenced songs to play traditional English folk music on electric instruments.[111] This British folk-rock was taken up by bands including Pentangle, Steeleye Span and the Albion Band, which in turn prompted Irish groups like Horslips and Scottish acts like the JSD Band, Spencer's Feat and later Five Hand Reel, to use their traditional music to create a brand of Celtic rock in the early 1970s.[112]

Folk-rock reached its peak of commercial popularity in the period 1967–68, before many acts moved off in a variety of directions, including Dylan and the Byrds, who began to develop country rock.[113] However, the hybridization of folk and rock has been seen as having a major influence on the development of rock music, bringing in elements of psychedelia, and helping to develop the ideas of the singer-songwriter, the protest song, and concepts of "authenticity".[107][114]

Psychedelic music's LSD-inspired vibe began in the folk scene.[115] The first group to advertise themselves as psychedelic rock were the 13th Floor Elevators from Texas.[115] The Beatles introduced many of the major elements of the psychedelic sound to audiences in this period, such as guitar feedback, the Indian sitar and backmasking sound effects.[116] Psychedelic rock particularly took off in California's emerging music scene as groups followed the Byrds' shift from folk to folk rock from 1965.[116] The psychedelic lifestyle, which revolved around hallucinogenic drugs, had already developed in San Francisco and particularly prominent products of the scene were Big Brother and the Holding Company, the Grateful Dead and Jefferson Airplane.[116][117] The Jimi Hendrix Experience's lead guitarist, Jimi Hendrix did
extended distorted, feedback-filled jams which became a key feature of psychedelia.[116] Psychedelic rock reached its apogee in the last years of the decade. 1967 saw the Beatles release their definitive psychedelic statement in Sgt. Pepper's Lonely Hearts Club Band, including the controversial track "Lucy in the Sky with Diamonds", the Rolling Stones responded later that year with Their Satanic Majesties Request,[116] and Pink Floyd debuted with The Piper at the Gates of Dawn. Key recordings included Jefferson Airplane's Surrealistic Pillow and the Doors' self-titled debut album. These trends peaked in the 1969 Woodstock festival, which saw performances by most of the major psychedelic acts.[116]

Sgt. Pepper was later regarded as the greatest album of all time and a starting point for the album era, during which rock music transitioned from the singles format to albums and achieved cultural legitimacy in the mainstream.[118] Led by the Beatles in the mid-1960s,[119] rock musicians advanced the LP as the dominant form of recorded music expression and consumption, initiating a rock-informed album era in the music industry for the next several decades.[120]

Progressive rock, a term sometimes used interchangeably with art rock, moved beyond established musical formulas by experimenting with different instruments, song types, and forms.[121] From the mid-1960s, the Left Banke, the Beatles, the Rolling Stones and the Beach Boys, had pioneered the inclusion of harpsichords, wind, and string sections on their recordings to produce a form of Baroque rock and can be heard in singles like Procol Harum's "A Whiter Shade of Pale" (1967), with its Bach-inspired introduction.[122] The Moody Blues used a full orchestra on their album Days of Future Passed (1967) and subsequently created orchestral sounds with synthesizers.[121] Classical orchestration, keyboards, and synthesizers were a frequent addition to the established rock format of guitars, bass, and drums in subsequent progressive rock.[123]

Instrumentals were common, while songs with lyrics were sometimes conceptual, abstract, or based in fantasy and science fiction.[124] The Pretty Things' SF Sorrow (1968), the Kinks' Arthur (Or the Decline and Fall of the British Empire) (1969), and the Who's Tommy (1969) introduced the format of rock operas and opened the door to concept albums, often telling an epic story or tackling a grand overarching theme.[125] King Crimson's 1969 début album, In the Court of the Crimson King, which mixed powerful guitar riffs and mellotron, with jazz and symphonic music, is often taken as the key recording in progressive rock, helping the widespread adoption of the genre in the early 1970s among existing blues-rock and psychedelic bands, as well as newly formed acts.[121] The vibrant Canterbury scene saw acts following Soft Machine from psychedelia, through jazz influences, toward more expansive hard rock, including Caravan, Hatfield and the North, Gong, and National Health.[126] The French group Magma around drummer Christian Vander almost single-handedly created the new music genre zeuhl with their first albums in the early 1970s.[127]

Greater commercial success was enjoyed by Pink Floyd, who also moved away from psychedelia after the departure of Syd Barrett in 1968, with The Dark Side of the Moon (1973), seen as a masterpiece of the genre, becoming one of the best-selling albums of all time.[128] There was an emphasis on instrumental virtuosity, with Yes showcasing the skills of both guitarist Steve Howe and keyboard player Rick Wakeman, while Emerson, Lake & Palmer were a supergroup who produced some of the genre's most technically demanding work.[121] Jethro Tull and Genesis both pursued very different, but distinctly English, brands of music.[129] Renaissance, formed in 1969 by ex-Yardbirds Jim McCarty and Keith Relf, evolved into a high-concept band featuring the three-octave voice of Annie Haslam.[130] Most British bands depended on a relatively small cult following, but a handful, including Pink Floyd, Genesis, and Jethro Tull, managed to produce top ten singles at home and break the American market.[131] The American brand of progressive rock varied from the eclectic and innovative Frank Zappa, Captain Beefheart and Blood, Sweat & Tears,[132] to more pop rock orientated bands like Boston, Foreigner, Kansas, Journey, and Styx.[121] These, beside British bands Supertramp and ELO, all demonstrated a prog rock influence and while ranking among the most commercially successful acts of the 1970s, heralding the era of pomp or arena rock, which would last until the costs of complex shows (often with theatrical staging and special effects), would be replaced by more economical rock festivals as major live venues in the 1990s.[citation needed]

The instrumental strand of the genre resulted in albums like Mike Oldfield's Tubular Bells (1973), the first record, and worldwide hit, for the Virgin Records label, which became a mainstay of the genre.[121] Instrumental rock was particularly significant in continental Europe, allowing bands like Kraftwerk, Tangerine Dream, Can, Focus (band) and Faust to circumvent the language barrier.[133] Their synthesiser-heavy "krautrock", along with the work of Brian Eno (for a time the keyboard player with Roxy Music), would be a major influence on subsequent electronic rock.[121] With the advent of punk rock and technological changes in the late 1970s, progressive rock was increasingly dismissed as pretentious and overblown.[134][135] Many bands broke up, but some, including Genesis, ELP, Yes, and Pink Floyd, regularly scored top ten albums with successful accompanying worldwide tours.[103] Some bands which emerged in the aftermath of punk, such as Siouxsie and the Banshees, Ultravox, and Simple Minds, showed the influence of progressive rock, as well as their more usually recognized punk influences.[136]

In the late 1960s, jazz-rock emerged as a distinct subgenre out of the blues-rock, psychedelic, and progressive rock scenes, mixing the power of rock with the musical complexity and improvisational elements of jazz. AllMusic states that the term jazz-rock "may refer to the loudest, wildest, most electrified fusion bands from the jazz camp, but most often it describes performers coming from the rock side of the equation." Jazz-rock "...generally grew out of the most artistically ambitious rock subgenres of the late '60s and early '70s", including the singer-songwriter movement.[137] Many early US rock and roll musicians had begun in jazz and carried some of these elements into the new music. In Britain, the subgenre of blues rock, and many of its leading figures, like Ginger Baker and Jack Bruce of the Eric Clapton-fronted band Cream, had emerged from the British jazz scene. Often highlighted as the first true jazz-rock recording is the only album by the relatively obscure New York–based the Free Spirits with Out of Sight and Sound (1966). The first group of bands to self-consciously use the label were R&B oriented white rock bands that made use of jazzy horn sections, like Electric Flag, Blood, Sweat & Tears and Chicago, to become some of the most commercially successful acts of the later 1960s and the early 1970s.[138]

British acts to emerge in the same period from the blues scene, to make use of the tonal and improvisational aspects of jazz, included Nucleus[139] and the Graham Bond and John Mayall spin-off Colosseum. From the psychedelic rock and the Canterbury scenes came Soft Machine, who, it has been suggested, produced one of the artistically successfully fusions of the two genres. Perhaps the most critically acclaimed fusion came from the jazz side of the equation, with Miles Davis, particularly influenced by the work of Hendrix, incorporating rock instrumentation into his sound for the album Bitches Brew (1970). It was a major influence on subsequent rock-influenced jazz artists, including Herbie Hancock, Chick Corea and Weather Report.[138] The genre began to fade in the late 1970s, as a mellower form of fusion began to take its audience,[137] but acts like Steely Dan,[137] Frank Zappa and Joni Mitchell recorded significant jazz-influenced albums in this period, and it has continued to be a major influence on rock music.[138]

Reflecting on developments that occurred in rock music in the early 1970s, Robert Christgau wrote in Christgau's Record Guide: Rock Albums of the Seventies (1981):[17]

The decade is, of course, an arbitrary schema itself—time doesn't just execute a neat turn toward the future every ten years. But like a lot of artificial concepts—money, say—the category does take on a reality of its own once people figure out how to put it to work. "The '60s are over," a slogan one only began to hear in 1972 or so, mobilized all those eager to believe that idealism had become passe, and once they were mobilized, it had. In popular music, embracing the '70s meant both an elitist withdrawal from the messy concert and counterculture scene and a profiteering pursuit of the lowest common denominator in FM radio and album rock.
Rock saw greater commodification during this decade, turning into a multibillion-dollar industry and doubling its market while, as Christgau noted, suffering a significant "loss of cultural prestige". "Maybe the Bee Gees became more popular than the Beatles, but they were never more popular than Jesus", he said. "Insofar as the music retained any mythic power, the myth was self-referential – there were lots of songs about the rock and roll life but very few about how rock could change the world, except as a new brand of painkiller ... In the '70s the powerful took over, as rock industrialists capitalized on the national mood to reduce potent music to an often reactionary species of entertainment—and to transmute rock's popular base from the audience to market."[17]

Roots rock is the term now used to describe a move away from what some saw as the excesses of the psychedelic scene, to a more basic form of rock and roll that incorporated its original influences, particularly blues, country and folk music, leading to the creation of country rock and Southern rock.[140] In 1966, Bob Dylan went to Nashville to record the album Blonde on Blonde.[141] This, and subsequent more clearly country-influenced albums, such as Nashville Skyline, have been seen as creating the genre of country folk, a route pursued by a number of largely acoustic folk musicians.[141] Other acts that followed the back-to-basics trend were the Canadian group the Band and the California-based Creedence Clearwater Revival, both of which mixed basic rock and roll with folk, country and blues, to be among the most successful and influential bands of the late 1960s.[142] The same movement saw the beginning of the recording careers of Californian solo artists like Ry Cooder, Bonnie Raitt and Lowell George,[143] and influenced the work of established performers such as the Rolling Stones' Beggar's Banquet (1968) and the Beatles' Let It Be (1970).[116] Reflecting on this change of trends in rock music over the past few years, Christgau wrote in his June 1970 "Consumer Guide" column that this "new orthodoxy" and "cultural lag" abandoned improvisatory, studio-ornamented productions in favor of an emphasis on "tight, spare instrumentation" and song composition: "Its referents are '50s rock, country music, and rhythm-and-blues, and its key inspiration is the Band."[144]

In 1968, Gram Parsons recorded Safe at Home with the International Submarine Band, arguably the first true country rock album.[145] Later that year he joined the Byrds for Sweetheart of the Rodeo (1968), generally considered one of the most influential recordings in the genre.[145] The Byrds continued in the same vein, but Parsons left to be joined by another ex-Byrds member Chris Hillman in forming the Flying Burrito Brothers who helped establish the respectability and parameters of the genre, before Parsons departed to pursue a solo career.[145] Bands in California that adopted country rock included Hearts and Flowers, Poco, New Riders of the Purple Sage,[145] the Beau Brummels,[145] and the Nitty Gritty Dirt Band.[146] Some performers also enjoyed a renaissance by adopting country sounds, including: the Everly Brothers; one-time teen idol Rick Nelson who became the frontman for the Stone Canyon Band; former Monkee Mike Nesmith who formed the First National Band; and Neil Young.[145] The Dillards were, unusually, a country act, who moved towards rock music.[145] The greatest commercial success for country rock came in the 1970s, with artists including the Doobie Brothers, Emmylou Harris, Linda Ronstadt and the Eagles (made up of members of the Burritos, Poco, and Stone Canyon Band), who emerged as one of the most successful rock acts of all time, producing albums that included Hotel California (1976).[147]

The founders of Southern rock are usually thought to be the Allman Brothers Band, who developed a distinctive sound, largely derived from blues rock, but incorporating elements of boogie, soul, and country in the early 1970s.[91] The most successful act to follow them were Lynyrd Skynyrd, who helped establish the "Good ol' boy" image of the subgenre and the general shape of 1970s' guitar rock.[91] Their successors included the fusion/progressive instrumentalists Dixie Dregs, the more country-influenced Outlaws, funk/R&B-leaning Wet Willie and (incorporating elements of R&B and gospel) the Ozark Mountain Daredevils.[91] After the loss of original members of the Allmans and Lynyrd Skynyrd, the genre began to fade in popularity in the late 1970s, but was sustained the 1980s with acts like .38 Special, Molly Hatchet and the Marshall Tucker Band.[91]

Glam rock emerged from the English psychedelic and art rock scenes of the late 1960s; it can be seen as both an extension of (and reaction against) those trends.[148] Musically diverse, varying between the simple rock and roll revivalism of figures like Alvin Stardust to the complex art rock of Roxy Music, and can be seen as much as a fashion as a musical subgenre.[148] Visually, it was a mesh of various styles, ranging from 1930s Hollywood glamor, through 1950s pin-up sex appeal, pre-war Cabaret theatrics, Victorian literary and symbolist styles, science fiction, to ancient and occult mysticism and mythology; manifesting itself in outrageous clothes, makeup, hairstyles, and platform-soled boots.[149] Glam is most noted for its sexual and gender ambiguity and representations of androgyny, beside extensive use of theatrics.[150] It was prefigured by the showmanship and gender-identity manipulation of American acts such as the Cockettes and Alice Cooper.[151]

The origins of glam rock are associated with Marc Bolan, who had renamed his folk duo to T. Rex and taken up electric instruments by the end of the 1960s. Often cited as the moment of inception is his appearance on the BBC music show Top of the Pops in March 1971 wearing glitter and satins, to perform what would be his second UK Top 10 hit (and first UK Number 1 hit), "Hot Love".[152] From 1971, already a minor star, David Bowie developed his Ziggy Stardust persona, incorporating elements of professional make up, mime and performance into his act.[153] These performers were soon followed in the style by acts including Roxy Music, Sweet, Slade, Mott the Hoople, Mud and Alvin Stardust.[153] While highly successful in the single charts in the United Kingdom, very few of these musicians were able to make a serious impact in the United States; Bowie was the major exception becoming an international superstar and prompting the adoption of glam styles among acts like Lou Reed, Iggy Pop, New York Dolls and Jobriath, often known as "glitter rock" and with a darker lyrical content than their British counterparts.[154] In the UK the term glitter rock was most often used to refer to the extreme version of glam pursued by Gary Glitter and his support musicians the Glitter Band, who between them achieved eighteen top ten singles in the UK between 1972 and 1976.[155] A second wave of glam rock acts, including Suzi Quatro, Roy Wood's Wizzard and Sparks, dominated the British single charts from about 1974 to 1976.[153] Existing acts, some not usually considered central to the genre, also adopted glam styles, including Rod Stewart, Elton John, Queen and, for a time, even the Rolling Stones.[153] It was also a direct influence on acts that rose to prominence later, including Kiss and Adam Ant, and less directly on the formation of gothic rock and glam metal as well as on punk rock, which helped end the fashion for glam from about 1976.[154] Glam has since enjoyed sporadic modest revivals through bands such as Chainsaw Kittens, the Darkness[156] and in R&B crossover act Prince.[157]

After the early successes of Latin rock in the 1960s, Chicano musicians like Carlos Santana and Al Hurricane continued to have successful careers throughout the 1970s. Santana opened the decade with success in his 1970 single "Black Magic Woman" on the Abraxas album.[citation needed] His third album Santana III yielded the single "No One to Depend On", and his fourth album Caravanserai experimented with his sound to mixed reception.[158][159] He later released a series of four albums that all achieved gold status: Welcome, Borboletta, Amigos, and Festivál. Al Hurricane continued to mix his rock music with New Mexico music, though he was also experimenting more heavily with Jazz music, which led to several successful singles, especially on his Vestido Mojado album, including the eponymous "Vestido Mojado", as well as "Por Una Mujer Casada" and "Puño de Tierra"; his brothers had successful New Mexico music singles in "La Del Moño Colorado" by Tiny Morrie and "La Cumbia De San Antone" by Baby Gaby.[160] Al Hurricane Jr. also began his successful rock-infused New Mexico music recording career in the 1970s, with his 1976 rendition of "Flor De Las Flores".[161][162] Los Lobos gained popularity at this time, with their first album Los Lobos del Este de Los Angeles in 1977.

A strange time, 1971—although rock's balkanization into genres was well underway, it was often hard to tell one catch-phrase from the next. "Art-rock" could mean anything from the Velvets to the Moody Blues, and although Led Zeppelin was launched and Black Sabbath celebrated, "heavy metal" remained an amorphous concept.

From the late -1960s, it became common to divide mainstream rock music into soft and hard rock. Soft rock was often derived from folk rock, using acoustic instruments and putting more emphasis on melody and harmonies.[164] Major artists included Carole King, Cat Stevens and James Taylor.[164] It reached its commercial peak in the mid- to late 1970s with acts like Billy Joel, America and the reformed Fleetwood Mac, whose Rumours (1977) was the best-selling album of the decade.[165] In contrast, hard rock was more often derived from blues-rock and was played louder and with more intensity.[166] It often emphasised the electric guitar, both as a rhythm instrument using simple repetitive riffs and as a solo lead instrument, and was more likely to be used with distortion and other effects.[166] Key acts included British Invasion bands like the Kinks, as well as psychedelic era performers like Cream, Jimi Hendrix and the Jeff Beck Group.[166] Hard rock-influenced bands that enjoyed international success in the later 1970s included Queen,[167] Thin Lizzy,[168] Aerosmith, AC/DC,[166] and Van Halen.

Also from the late 1960s, the term "heavy metal" began to be used to describe some hard rock played with even more volume and intensity, first as an adjective and by the early 1970s as a noun.[169] The term was first used in music in Steppenwolf's "Born to Be Wild" (1967); the term began to be associated with pioneer bands like San Francisco's Blue Cheer, Cleveland's James Gang and Michigan's Grand Funk Railroad.[170] By 1970, three key British bands had developed the characteristic sounds and styles which would help shape the subgenre. Led Zeppelin added elements of fantasy to their riff laden blues-rock, Deep Purple brought in symphonic and medieval interests from their progressive rock phase and Black Sabbath introduced facets of the gothic and modal harmony, helping to produce a "darker" sound.[171] These elements were taken up by a "second generation" of hard rock and heavy metal bands into the late 1970s, including: Judas Priest, UFO, Motörhead and Rainbow from Britain; Kiss, Ted Nugent, and Blue Öyster Cult from the US; Rush from Canada and Scorpions from Germany, all marking the expansion in popularity of the subgenre.[171] Despite a lack of airplay and very little presence on the singles charts, late 1970s heavy metal built a considerable following, particularly among adolescent working-class males in North America and Europe.[172]

In the 1980s, bands such as Bon Jovi, Guns N' Roses, Metallica, Mötley Crüe and Def Leppard saw mainstream success, with hard rock and a fusion of hard rock and heavy metal with pop. During the 1990s, hard rock saw a slight decline in popularity, save for some major hits like Guns N' Roses' "November Rain", and Metallica's "Enter Sandman".[173] However, in the early 2000s, Bon Jovi's "It's My Life" saw a huge increase in popularity of rock and pop rock and helped introduce the genres to a newer fanbase.

Rock music, mostly the heavy metal genre, has sometimes been criticized by some Christian leaders, who have condemned it as immoral, anti-Christian, and even satanic.[174] However, Christian rock began to develop in the late 1960s, particularly out of the Jesus movement beginning in Southern California; however, it emerged as a subgenre in the 1970s with artists like Larry Norman, usually seen as the first major "star" of Christian rock.[175] The genre was mostly a phenomenon in the United States.[176] Many Christian rock performers have ties to the contemporary Christian music scene. Starting in the 1980s, Christian pop performers have had some mainstream success. While these artists were largely acceptable in Christian communities, the adoption of heavy rock and glam metal styles by bands like Stryper, who achieved considerable mainstream success in the 1980s, was more controversial.[177][178] Starting in the 1990s, there were increasing numbers of acts who attempted to avoid the Christian band label, preferring to be seen as groups who were also Christians, including P.O.D.[179]

American working-class oriented heartland rock, characterized by a straightforward musical style, and a concern with the lives of ordinary, blue-collar American people, developed in the second half of the 1970s. The term heartland rock was first used to describe Midwestern arena rock groups like Kansas, REO Speedwagon and Styx, but which came to be associated with a more socially concerned form of roots rock more directly influenced by folk, country and rock and roll.[180] It has been seen as an American Midwest and Rust Belt counterpart to West Coast country rock and the Southern rock of the American South.[181] Led by figures who had initially been identified with punk and New Wave, it was most strongly influenced by acts such as Bob Dylan, the Byrds, Creedence Clearwater Revival and Van Morrison, and the basic rock of 1960s garage and the Rolling Stones.[182]

Exemplified by the commercial success of singer songwriters Bruce Springsteen, Bob Seger, and Tom Petty, along with less widely known acts such as Southside Johnny and the Asbury Jukes and Joe Grushecky and the Houserockers, it was partly a reaction to post-industrial urban decline in the East and Mid-West, often dwelling on issues of social disintegration and isolation, beside a form of good-time rock and roll revivalism.[182] The genre reached its commercial, artistic and influential peak in the mid-1980s, with Springsteen's Born in the USA (1984), topping the charts worldwide and spawning a series of top ten singles, together with the arrival of artists including John Mellencamp, Steve Earle and more gentle singer-songwriters such as Bruce Hornsby.[182] It can also be heard as an influence on artists as diverse as Billy Joel,[183] Kid Rock[184] and the Killers.[185]

Heartland rock faded away as a recognized genre by the early 1990s, as rock music in general, and blue-collar and white working class themes in particular, lost influence with younger audiences, and as heartland's artists turned to more personal works.[182] Many heartland rock artists continued to record with critical and commercial success, most notably Bruce Springsteen, Tom Petty, and John Mellencamp, although their output became more personal and experimental, no longer fitting a specific genre.[186]

Punk rock was developed between 1974 and 1976 in the United States and the United Kingdom. Rooted in garage rock and other forms of what is now known as protopunk music, punk rock bands eschewed the perceived excesses of mainstream 1970s rock.[187] They created fast, hard-edged music, typically with short songs, stripped-down instrumentation, and often political, anti-establishment lyrics. Punk embraces a DIY (do it yourself) ethic, with many bands self-producing their recordings and distributing them through informal channels.[188]

By late 1976, acts such as the Ramones and Patti Smith, in New York City, and the Sex Pistols and the Clash, in London, were recognized as the vanguard of a new musical movement.[187] The following year saw punk rock spreading around the world. Punk quickly became a major cultural phenomenon in the UK. The Sex Pistols' live TV skirmish with Bill Grundy on 1 December 1976, was the watershed moment in British punk's transformation into a major media phenomenon, even as some stores refused to stock the records and radio airplay was hard to come by.[189] In May 1977, the Sex Pistols achieved new heights of controversy (and number two on the singles chart) with a song that referenced Queen Elizabeth II, "God Save the Queen", during her Silver Jubilee.[190] For the most part, punk took root in local scenes that tended to reject association with the mainstream. An associated punk subculture emerged, expressing youthful rebellion and characterized by distinctive clothing styles and a variety of anti-authoritarian ideologies.[191]

By the beginning of the 1980s, faster, more aggressive styles such as hardcore and Oi! had become the predominant mode of punk rock.[192] This has resulted in several evolved strains of hardcore punk, such as D-beat (a distortion-heavy subgenre influenced by the UK band Discharge), anarcho-punk (such as Crass), grindcore (such as Napalm Death), and crust punk.[193] Musicians identifying with or inspired by punk also pursued a broad range of other variations, giving rise to New wave, post-punk and the alternative rock movement.[187]

Although punk rock was a significant social and musical phenomenon, it achieved less in the way of record sales (being distributed by small specialty labels such as Stiff Records),[194] or American radio airplay (as the radio scene continued to be dominated by mainstream formats such as disco and album-oriented rock).[195] Punk rock had attracted devotees from the art and collegiate world and soon bands sporting a more literate, arty approach, such as Talking Heads and Devo began to infiltrate the punk scene; in some quarters the description "new wave" began to be used to differentiate these less overtly punk bands.[196] Record executives, who had been mostly mystified by the punk movement, recognized the potential of the more accessible new wave acts and began aggressively signing and marketing any band that could claim a remote connection to punk or new wave.[197] Many of these bands, such as the Cars and the Go-Go's can be seen as pop bands marketed as new wave;[198] other existing acts, including the Police, the Pretenders and Elvis Costello, used the new wave movement as the springboard for relatively long and critically successful careers,[199] while "skinny tie" bands exemplified by the Knack,[200] or the photogenic Blondie, began as punk acts and moved into more commercial territory.[201]

Between 1979 and 1985, influenced by Kraftwerk, Yellow Magic Orchestra, David Bowie and Gary Numan, British new wave went in the direction of such New Romantics as Spandau Ballet, Ultravox, Japan, Duran Duran, A Flock of Seagulls, Culture Club, Talk Talk and the Eurythmics, sometimes using the synthesizer to replace all other instruments.[202] This period coincided with the rise of MTV and led to a great deal of exposure for this brand of synth-pop, creating what has been characterised as a second British Invasion.[203] Some more traditional rock bands adapted to the video age and profited from MTV's airplay, most obviously Dire Straits, whose "Money for Nothing" gently poked fun at the station, despite the fact that it had helped make them international stars,[204] but in general, guitar-oriented rock was commercially eclipsed.[205]

If hardcore most directly pursued the stripped down aesthetic of punk, and new wave came to represent its commercial wing, post-punk emerged in the late 1970s and early 1980s as its more artistic and challenging side. In addition to punk bands, major influences included the Velvet Underground, Frank Zappa, Captain Beefheart, and the New York-based no wave scene, including James Chance and the Contortions, DNA, and Sonic Youth.[206] Early contributors to the genre included U.S. bands Pere Ubu, Devo, the Residents, and Talking Heads.[206]

The first wave of British post-punk included Gang of Four, Siouxsie and the Banshees and Joy Division, who placed less emphasis on art than their US counterparts and more on the dark emotional qualities of their music.[206] Bands like Siouxsie and the Banshees, Bauhaus, the Cure, and the Sisters of Mercy, moved increasingly in this direction to found Gothic rock, which had become the basis of a major sub-culture by the early 1980s.[207] Similar emotional territory was pursued by Australian acts like the Birthday Party and Nick Cave.[206] Members of Bauhaus and Joy Division explored new stylistic territory as Love and Rockets and New Order respectively.[206] Another early post-punk movement was the industrial music[208] developed by British bands Throbbing Gristle and Cabaret Voltaire, and New York-based Suicide, using a variety of electronic and sampling techniques that emulated the sound of industrial production and which would develop into a variety of forms of post-industrial music in the 1980s.[209]

The second generation of British post-punk bands that broke through in the early 1980s, including the Fall, the Pop Group, the Mekons, Echo and the Bunnymen and the Teardrop Explodes, tended to move away from dark sonic landscapes.[206] Arguably the most successful band to emerge from post-punk was Ireland's U2, who incorporated elements of religious imagery together with political commentary into their often anthemic music, and by the late 1980s had become one of the biggest bands in the world.[210] Although many post-punk bands continued to record and perform, it declined as a movement in the mid-1980s as acts disbanded or moved off to explore other musical areas, but it has continued to influence the development of rock music and has been seen as a major element in the creation of the alternative rock movement.[211]

The term alternative rock was coined in the early 1980s to describe rock artists who did not fit into the mainstream genres of the time. Bands dubbed "alternative" had no unified style, but were all seen as distinct from mainstream music. Alternative bands were linked by their collective debt to punk rock, through hardcore, New Wave or the post-punk movements.[212] Important alternative rock bands of the 1980s in the US included R.E.M., Hüsker Dü, Jane's Addiction, Sonic Youth, and the Pixies;[212] in the UK, popular bands  the Cure, New Order, the Jesus and Mary Chain, and the Smiths.[213] Artists were largely confined to independent record labels, building an extensive underground music scene based on college radio, fanzines, touring, and word-of-mouth.[214] They rejected the dominant synth-pop of the early 1980s, marking a return to group-based guitar rock.[215][216][217]

Few of these early bands achieved mainstream success, although exceptions to this rule include R.E.M., the Smiths, and the Cure. Despite a general lack of spectacular album sales, the original alternative rock bands exerted a considerable influence on the generation of musicians who came of age in the 1980s and ended up breaking through to mainstream success in the 1990s. Styles of alternative rock in the US during the 1980s included jangle pop, associated with the early recordings of R.E.M., which incorporated the ringing guitars of mid-1960s pop and rock, and college rock, used to describe alternative bands that began in the college circuit and college radio, including acts such as 10,000 Maniacs and the Feelies.[212] In the UK, Gothic rock was dominant in the early 1980s; however, by the end of the decade, indie or dream pop[218] like Primal Scream, Bogshed, Half Man Half Biscuit and the Wedding Present, and what were dubbed shoegaze bands like My Bloody Valentine, Slowdive, Ride and Lush entered.[219] Particularly vibrant was the Madchester scene, producing such bands as Happy Mondays, Inspiral Carpets and the Stone Roses.[213][220] The next decade would see the success of grunge in the US and Britpop in the UK, bringing alternative rock into the mainstream.

Disaffected by commercialized and highly produced pop and rock in the mid-1980s, bands in Washington state (particularly in the Seattle area) formed a new style of rock which sharply contrasted with the mainstream music of the time.[221] The developing genre came to be known as "grunge", a term descriptive of the dirty sound of the music and the unkempt appearance of most musicians, who actively rebelled against the over-groomed images of other artists.[221] Grunge fused elements of hardcore punk and heavy metal into a single sound, and made heavy use of guitar distortion, fuzz, and feedback.[221] The lyrics were typically apathetic and angst-filled, and often concerned themes such as social alienation and entrapment, although it was also known for its dark humor and parodies of commercial rock.[221]

Bands such as Green River, Soundgarden, Melvins, and Skin Yard pioneered the genre, with Mudhoney becoming the most successful by the end of the decade. Grunge remained largely a local phenomenon until 1991, when Nirvana's album Nevermind became a huge success, containing the anthemic song "Smells Like Teen Spirit".[222] Nevermind was more melodic than its predecessors, by signing to Geffen Records the band was one of the first to employ traditional corporate promotion and marketing mechanisms such as an MTV video, in store displays and the use of radio "consultants" who promoted airplay at major mainstream rock stations. During 1991 and 1992, other grunge albums such as Pearl Jam's Ten, Soundgarden's Badmotorfinger, and Alice in Chains' Dirt, along with the Temple of the Dog album featuring members of Pearl Jam and Soundgarden, became among the 100 top-selling albums.[223] Major record labels signed most of the remaining grunge bands in Seattle, while a second influx of acts moved to the city in the hope of success.[224] However, with the death of Kurt Cobain and the subsequent break-up of Nirvana in 1994, touring problems for Pearl Jam and the departure of Alice in Chains' lead singer Layne Staley in 1998, the genre began to decline, partly to be overshadowed by Britpop and more commercial sounding post-grunge.[225]

Britpop emerged from the British alternative rock scene of the early 1990s and was characterised by bands particularly influenced by British guitar music of the 1960s and 1970s.[213] The Smiths were a major influence, as were bands of the Madchester scene, which had dissolved in the early 1990s.[81] The movement has been seen partly as a reaction against various US-based, musical and cultural trends in the late 1980s and early 1990s, particularly the grunge phenomenon and as a reassertion of a British rock identity.[213] Britpop was varied in style, but often used catchy tunes and hooks, beside lyrics with particularly British concerns and the adoption of the iconography of the 1960s British Invasion, including the symbols of British identity previously used by the mods.[226] It was launched around 1993 with releases by groups such as Suede and Blur, who were soon joined by others including Oasis, Pulp, Supergrass, and Elastica, who produced a series of successful albums and singles.[213] For a while the contest between Blur and Oasis was built by the popular press into the "Battle of Britpop", initially won by Blur, but with Oasis achieving greater long-term and international success, directly influencing later Britpop bands, such as Ocean Colour Scene and Kula Shaker.[227] Britpop groups brought British alternative rock into the mainstream and formed the backbone of a larger British cultural movement known as Cool Britannia.[228] Although its more popular bands, particularly Blur and Oasis, were able to spread their commercial success overseas, especially to the United States, the movement had largely fallen apart by the end of the decade.[213]

The term post-grunge was coined for the generation of bands that followed the emergence into the mainstream and subsequent hiatus of the Seattle grunge bands. Post-grunge bands emulated their attitudes and music, but with a more radio-friendly commercially oriented sound.[225] Often they worked through the major labels and came to incorporate diverse influences from jangle pop, pop-punk, alternative metal or hard rock.[225] The term post-grunge originally was meant to be pejorative, suggesting that they were simply musically derivative, or a cynical response to an "authentic" rock movement.[229] Originally, grunge bands that emerged when grunge was mainstream and were suspected of emulating the grunge sound were pejoratively labelled as post-grunge.[229] From 1994, former Nirvana drummer Dave Grohl's new band, the Foo Fighters, helped popularize the genre and define its parameters.[230]

Some post-grunge bands, like Candlebox, were from Seattle, but the subgenre was marked by a broadening of the geographical base of grunge, with bands like Los Angeles' Audioslave, and Georgia's Collective Soul and beyond the US to Australia's Silverchair and Britain's Bush, who all cemented post-grunge as one of the most commercially viable subgenres of the late 1990s.[212][225] Although male bands predominated post-grunge, female solo artist Alanis Morissette's 1995 album Jagged Little Pill, labelled as post-grunge, also became a multi-platinum hit.[231] Post-grunge morphed during the late 1990s as post-grunge bands like Creed and Nickelback emerged.[229] Bands like Creed and Nickelback took post-grunge into the 21st century with considerable commercial success, abandoning most of the angst and anger of the original movement for more conventional anthems, narratives and romantic songs, and were followed in this vein by newer acts including Shinedown, Seether, Three Days Grace,[232] 3 Doors Down, Breaking Benjamin[233] and Puddle of Mudd.[229]

The origins of 1990s pop-punk can be seen in the more song-oriented bands of the 1970s punk movement like Buzzcocks and the Clash, commercially successful new wave acts such as the Jam and the Undertones, and the more hardcore-influenced elements of alternative rock in the 1980s.[234] Pop-punk tends to use power-pop melodies and chord changes with speedy punk tempos and loud guitars.[235] Punk music provided the inspiration for some California-based bands on independent labels in the early 1990s, including Rancid and Green Day.[234] In 1994, Green Day moved to a major label and produced the album Dookie, which found a new, largely teenage, audience and proved a surprise diamond-selling success, leading to a series of hit singles, including two number ones in the US.[212] They were soon followed by the eponymous debut from Weezer, which spawned three top ten singles in the US.[236] This success opened the door for the multi-platinum sales of metallic punk band the Offspring with Smash (1994).[212] This first wave of pop punk reached its commercial peak with Green Day's Nimrod (1997) and the Offspring's Americana (1998).[237]

A second wave of pop-punk was spearheaded by Blink-182, with their breakthrough album Enema of the State (1999), followed by bands such as Good Charlotte, Simple Plan and Sum 41, who made use of humour in their videos and had a more radio-friendly tone to their music, while retaining the speed, some of the attitude and even the look of 1970s punk.[234] Later pop-punk bands, including All Time Low, the All-American Rejects and Fall Out Boy, had a sound that has been described as closer to 1980s hardcore, while still achieving commercial success.[234]

In the 1980s the terms indie rock and alternative rock were used interchangeably.[238] By the mid-1990s, as elements of the movement began to attract mainstream interest, particularly grunge and then Britpop, post-grunge and pop-punk, the term alternative began to lose its meaning.[238] Those bands following the less commercial contours of the scene were increasingly referred to by the label indie.[238] They characteristically attempted to retain control of their careers by releasing albums on their own or small independent labels, while relying on touring, word-of-mouth, and airplay on independent or college radio stations for promotion.[238] Linked by an ethos more than a musical approach, the indie rock movement encompassed a wide range of styles, from hard-edged, grunge-influenced bands like the Cranberries and Superchunk, through do-it-yourself experimental bands like Pavement, to punk-folk singers such as Ani DiFranco.[212][213] It has been noted that indie rock has a relatively high proportion of female artists compared with preceding rock genres, a tendency exemplified by the development of feminist-informed Riot grrrl music.[239] Many countries have developed an extensive local indie scene, flourishing with bands with enough popularity to survive inside the respective country, but virtually unknown outside them.[240]

By the end of the 1990s many recognisable subgenres, most with their origins in the late 1980s alternative movement, were included under the umbrella of indie. Lo-fi eschewed polished recording techniques for a D.I.Y. ethos and was spearheaded by Beck, Sebadoh and Pavement.[212] The work of Talk Talk and Slint helped inspire both post rock, an experimental style influenced by jazz and electronic music, pioneered by Bark Psychosis and taken up by acts such as Tortoise, Stereolab, and Laika,[241][242] as well as leading to more dense and complex, guitar-based math rock, developed by acts like Polvo and Chavez.[243] Space rock looked back to progressive roots, with drone heavy and minimalist acts like Spacemen 3, the two bands created out of its split, Spectrum and Spiritualized, and later groups including Flying Saucer Attack, Godspeed You! Black Emperor and Quickspace.[244] In contrast, Sadcore emphasised pain and suffering through melodic use of acoustic and electronic instrumentation in the music of bands like American Music Club and Red House Painters,[245] while the revival of baroque pop reacted against lo-fi and experimental music by placing an emphasis on melody and classical instrumentation, with artists like Arcade Fire, Belle and Sebastian and Rufus Wainwright.[246]

Alternative metal emerged from the hardcore scene of alternative rock in the US in the later 1980s, but gained a wider audience after grunge broke into the mainstream in the early 1990s.[247] Early alternative metal bands mixed a wide variety of genres with hardcore and heavy metal sensibilities, with acts like Jane's Addiction and Primus using progressive rock, Soundgarden and Corrosion of Conformity using garage punk, the Jesus Lizard and Helmet mixing noise rock, Ministry and Nine Inch Nails influenced by industrial music, Monster Magnet moving into psychedelia, Pantera, Sepultura and White Zombie creating groove metal, while Biohazard, Limp Bizkit and Faith No More turned to hip hop and rap.[247]

Hip hop had gained attention from rock acts in the early 1980s, including the Clash with "The Magnificent Seven" (1980) and Blondie with "Rapture" (1980).[248][249] Early crossover acts included Run DMC and the Beastie Boys.[250] Detroit rapper Esham became known for his "acid rap" style, which fused rapping with a sound that was often based in rock and heavy metal.[251][252] Rappers who sampled rock songs included Ice-T, the Fat Boys, LL Cool J, Public Enemy and Whodini.[253] The mixing of thrash metal and rap was pioneered by Anthrax on their 1987 comedy-influenced single "I'm the Man".[253]

In 1990, Faith No More broke into the mainstream with their single "Epic", often seen as the first truly successful combination of heavy metal with rap.[254] This paved the way for the success of existing bands like 24-7 Spyz and Living Colour, and new acts including Rage Against the Machine and Red Hot Chili Peppers, who all fused rock and hip hop among other influences.[253][255] Among the first wave of performers to gain mainstream success as rap rock were 311,[256] Bloodhound Gang,[257] and Kid Rock.[258] A more metallic sound – nu metal – was pursued by bands including Limp Bizkit, Korn and Slipknot.[253] Later in the decade this style, which contained a mix of grunge, punk, metal, rap and turntable scratching, spawned a wave of successful bands like Linkin Park, P.O.D. and Staind, who were often classified as rap metal or nu metal, the first of which are the best-selling band of the genre.[259]

In 2001, nu metal reached its peak with albums like Staind's Break the Cycle, P.O.D's Satellite, Slipknot's Iowa and Linkin Park's Hybrid Theory. New bands also emerged like Disturbed, Godsmack and Papa Roach, whose major label début Infest became a platinum hit.[260] Korn's long-awaited fifth album Untouchables, and Papa Roach's second album Lovehatetragedy, did not sell as well as their previous releases, while nu metal bands were played more infrequently on rock radio stations and MTV began focusing on pop punk and emo.[261] Since then, many bands have changed to a more conventional hard rock, heavy metal, or electronic music sound.[261]

From about 1997, as dissatisfaction grew with the concept of Cool Britannia, and Britpop as a movement began to dissolve, emerging bands began to avoid the Britpop label while still producing music derived from it.[262][263] Many of these bands tended to mix elements of British traditional rock (or British trad rock),[264] particularly the Beatles, Rolling Stones and Small Faces,[265] with American influences, including post-grunge.[266][267] Drawn from across the United Kingdom (with several important bands emerging from the north of England, Scotland, Wales and Northern Ireland), the themes of their music tended to be less parochially centered on British, English and London life and more introspective than had been the case with Britpop at its height.[268][269] This, beside a greater willingness to engage with the American press and fans, may have helped some of them in achieving international success.[270] Several alternative bands that had enjoyed some success during the mid-1990s, but did not find major commercial success until the late 1990s included the Verve and Radiohead. After the decline of Britpop they began to gain more critical and popular attention. The Verve's album Urban Hymns (1997) was a worldwide hit, and Radiohead achieved near-universal critical acclaim with their experimental third album OK Computer (1997), as well as its follow-up Kid A (2000).

Post-Britpop bands have been seen as presenting the image of the rock star as an ordinary person and their increasingly melodic music was criticised for being bland or derivative.[271] Post-Britpop bands like Travis from The Man Who (1999), Stereophonics from Performance and Cocktails (1999), Feeder from Echo Park (2001), and particularly Snow Patrol from Final Straw (2003), Keane from their debut album Hopes and Fears (2004), and Coldplay from their debut album Parachutes (2000), achieved much wider international success than most of the Britpop groups that had preceded them, and were some of the most commercially successful acts of the late 1990s and early 2000s, arguably providing a launchpad for the subsequent garage rock revival and post-punk revival, which has also been seen as a reaction to their introspective brand of rock.[267][272][273][274][275]

Post-hardcore developed in the US, particularly in the Chicago and Washington, DC areas, in the early to mid-1980s, with bands that were inspired by the do-it-yourself ethics and guitar-heavy music of hardcore punk, but influenced by post-punk, adopting longer song formats, more complex musical structures and sometimes more melodic vocal styles.[276]

Emo also emerged from the hardcore scene in 1980s Washington, D.C., initially as "emocore", used as a term to describe bands who favored expressive vocals over the more common abrasive, barking style.[277] The early emo scene operated as an underground, with short-lived bands releasing small-run vinyl records on tiny independent labels.[277] Emo broke into mainstream culture in the early 2000s with the platinum-selling success of Jimmy Eat World's Bleed American (2001) and Dashboard Confessional's The Places You Have Come to Fear the Most (2003).[278] The new emo had a much more mainstream sound than in the 1990s and a far greater appeal among adolescents than its earlier incarnations.[278] At the same time, use of the term emo expanded beyond the musical genre, becoming associated with fashion, a hairstyle and any music that expressed emotion.[279] By 2003 post-hardcore bands had also caught the attention of major labels and began to enjoy mainstream success in the album charts.[citation needed] A number of these bands were seen as a more aggressive offshoot of emo and given the often vague label of screamo.[280]

In the early 2000s, a new group of bands that played a stripped down and back-to-basics version of guitar rock, emerged into the mainstream. They were variously characterised as part of a garage rock, post-punk or New Wave revival.[281][282][283][284] Because the bands came from across the globe, cited diverse influences (from traditional blues, through New Wave to grunge), and adopted differing styles of dress, their unity as a genre has been disputed.[285] There had been attempts to revive garage rock and elements of punk in the 1980s and 1990s; by 2000, scenes had grown up in several countries.[286]

The commercial breakthrough from these scenes was led by four bands: the Strokes, who emerged from the New York club scene with their debut album Is This It (2001); the White Stripes, from Detroit, with their third album White Blood Cells (2001); the Hives from Sweden after their compilation album Your New Favourite Band (2001); and the Vines from Australia with Highly Evolved (2002).[287] They were christened by the media as the "The" bands, and dubbed "The saviours of rock 'n' roll", leading to accusations of hype.[288] A second wave of bands that gained international recognition due to the movement included Black Rebel Motorcycle Club, Yeah Yeah Yeahs, the Killers, Interpol and Kings of Leon from the US,[289] the Libertines, Arctic Monkeys, Bloc Party, Editors, Kaiser Chiefs and Franz Ferdinand from the UK,[290] Jet and Wolfmother from Australia,[291] and the Datsuns and the D4 from New Zealand.[292]

In the 2000s, as computer technology became more accessible and music software advanced, it became possible to create high quality music using little more than a single laptop computer.[293] This resulted in a massive increase in the amount of home-produced electronic music available to the general public via the expanding internet,[294] and new forms of performance such as laptronica[293] and live coding.[295] These techniques also began to be used by existing bands and by developing genres that mixed rock with digital techniques and sounds, including indie electronic, electroclash, dance-punk and new rave.[citation needed]

Metalcore is a broadly defined[296] fusion genre combining elements of extreme metal and hardcore punk, that originated in the late 1980s. Metalcore is noted for its use of breakdowns, which are slow, intense passages conducive to moshing, while other defining instrumentation includes heavy and percussive pedal point guitar riffs and double bass drumming. In the late 1980s to early 1990s, pioneering bands such as Integrity, Earth Crisis and Converge, whose hardcore punk-leaning style is sometimes referred to as metallic hardcore,[297] were founded. These bands took influence from a range of styles and genres such as hardcore punk, thrash metal and death metal. During the decade, the genre diversified, with Converge, the Dillinger Escape Plan, Botch and Coalesce pioneering mathcore, while Overcast, Shadows Fall and Darkest Hour merged the genre with melodic death metal to create melodic metalcore. During the early 2000s, melodic metalcore bands such as  Killswitch Engage, All That Remains, Trivium, As I Lay Dying, Atreyu, Bullet for My Valentine and Parkway Drive found mainstream popularity. In the subsequent years, the genre saw increased success through social networking on Myspace and internet memes such as crabcore. During this time, artists began to draw influence from a wide variety of sources, which led to genre cultivating a plethora of fusion genres including electronicore, deathcore, Nintendocore, progressive metalcore and nu metalcore. 

In the 2010s and through to the 2020s, the genre saw even greater commercial success, with albums by Bring Me the Horizon, Architects, Asking Alexandria, the Devil Wears Prada and Of Mice & Men penetrating the top 10 of international albums charts. Formed in 2015, Bad Omens' third album The Death of Peace of Mind (2022) was the band's commercial breakthrough after viral success of the album's second single "Just Pretend" on TikTok[298] which then topped the Billboard Mainstream Rock chart.[299] Bring Me the Horizon's Post Human: Survival Horror (2020)[300] and Architects' For Those That Wish to Exist (2021) both also reached number one in the UK album charts.[301]

During the 2010s, rock music declined from its position as the major popular music genre, now sharing with electronic dance and hip hop, the latter of which had surpassed it as the most consumed musical genre in the United States by 2017.[302][303][304] The rise of streaming and the advent of technology, which changed approaches toward music creation, were cited as major factors.[305] Ken Partridge of Genius suggested that hip-hop became more popular because it is a more transformative genre and does not need to rely on past sounds, and that there is a direct connection to the stagnation of rock music and changing social attitudes during the 2010s.[303] Bill Flanagan, in a 2016 opinion piece for The New York Times, compared the state of rock during this period to the state of jazz in the early 1980s, "slowing down and looking back."[306]

The rock bands which had chart success in the 2010s were mostly associated with the trends that had been popular in the 2000s and earlier decades rather than reflecting new scenes and sounds.[307][308] Some pop rock and hard rock bands continued to see commercial success during this period, including Ghost, Maroon 5, Twenty One Pilots, Fall Out Boy, Imagine Dragons, Halestorm, Panic! at the Disco, Black Veil Brides, Greta Van Fleet, and The Black Keys.[309][310][311] Outside of the charts, the commercialisation of rock festivals was a major theme of the decade, with both global megafestivals such as Coachella, Glastonbury and Roskilde, and smaller-scale local festivals expanding.[312]

In 2020, the COVID-19 pandemic brought extreme changes to the rock scene worldwide. Restrictions, such as quarantine rules, caused widespread cancellations and postponements of concerts, tours, festivals, album releases, award ceremonies, and competitions.[313][314][315][316][317] Some artists resorted to giving online performances to keep their careers active.[318] Another scheme to circumvent the quarantine limitations was used at a concert of Danish rock musician Mads Langer: the audience watched the performance from inside their cars, much like in a drive-in theater.[319] Musically, the pandemic led to a surge in new releases from the slower, less energetic, and more acoustic subgenres of rock music.[320][321] The industry raised funds to help itself through efforts such as Crew Nation, a relief fund for live music crews organised by Livenation.[322]

Psychedelic and progressive styles in rock would see a major resurgence in popularity during the 2010s and 2020s. Some of the most notable acts in neo-psychedelia originated in Australia; Kevin Parker's Tame Impala released the single "Elephant" in 2012, which became a hit on alternative radio in various countries, and would be followed by the release of critically acclaimed albums by Parker such as Lonerism (2012) and Currents (2015).[323][324][325] This new style of Australian psychedelic music not only built on the psychedelic and progressive rock acts of the '60s and '70s, but also incorporated new and unique musical influences from various subgenres of rock, heavy metal, EDM, and world music.[326] A 2014 article in The Guardian described Australia as a place where "independently minded rock bands are free to develop at their own pace".[327] Other Australian psychedelic and progressive revival acts of the 2010s and 2020s include King Gizzard & the Lizard Wizard, Psychedelic Porn Crumpets, Rolling Blackouts Coastal Fever, Bananagun, Jay Watson, The Murlocs, Stonefield, and Tropical Fuck Storm.[328][329]

Psychedelic trends in rock have also seen a revival in Europe, with European and American stoner rock groups such as Uncle Acid & the Deadbeats, Graveyard, Kadavar, All Them Witches, and True Widow performing a heavier, more riff-based version of neo-psychedelia containing stronger blues and metal influences.[330] Europe has been described as "really good" for new psychedelic music, with many American stoner rock bands choosing to tour in Europe as opposed to North America.[331]

At the start of the 2020s, recording artists in both pop and rap music released popular pop-punk-influenced recordings, many of them produced or assisted by Blink-182 drummer Travis Barker. Representing a commercial resurgence for the genre, these acts included Machine Gun Kelly, Willow Smith, Trippie Redd, Halsey, Yungblud, and Olivia Rodrigo. The popularity of the social media platform TikTok helped spark nostalgia for the angst-driven musical style among young listeners during the pandemic. Among the most successful of these releases have been Machine Gun Kelly's 2020 album Tickets to My Downfall, which topped the Billboard 200, and Rodrigo's number-one hit single "Good 4 U" (2021).[332]

In the late 2010s and early 2020s, a new wave of post-punk bands from Britain and Ireland emerged. The groups in this scene have been described with the term "Crank Wave" by NME and The Quietus in 2019, and as "Post-Brexit New Wave" by NPR writer Matthew Perpetua in 2021.[333][334][335] Artists that have been identified as part of the style include Black Midi, Wet Leg, Squid, Black Country, New Road, Dry Cleaning, Shame, Sleaford Mods, Fontaines D.C., The Murder Capital, Idles and Yard Act.[333][334][335][336] Post-punk artists that attained prominence in the 2010s and early 2020s from other countries besides the UK included Parquet Courts, Protomartyr and Geese (United States), Preoccupations (Canada), Iceage (Denmark), Kælan Mikla (Iceland), and Viagra Boys (Sweden),[337][338][339][340] as well as the so-called "Russian doomer music" scene consisting of post-punk, coldwave and darkwave bands from post-Soviet countries like Russia and Belarus, most prominently Molchat Doma (Belarus) and Ploho (Russia).[341][342]

During the mid-to-late 2010s, some mainstream rock bands began to gain notoriety for performing in a back-to-basics style of rock music meant to emulate the sound of legacy acts popular on classic rock radio. The release of albums such as the Struts' Everybody Wants (2014) and Greta Van Fleet's Anthem of the Peaceful Army (2018) saw a renewed mainstream interest in earlier rock styles of the late 1960s and 1970s, with Revolver describing this classic rock revival sound as "hard-hitting, swaggering, riff-driven rock 'n' roll built around a core vocal-guitar-bass-drum configuration".[343] Other groups considered to be a part of this trend include Rival Sons, Dirty Honey, Crown Lands, Larkin Poe, and White Reaper.[344]

Different subgenres of rock were adopted by, and became central to, the identity of a large number of sub-cultures. In the 1950s and 1960s, respectively, British youths adopted the Teddy Boy and Rocker subcultures, which revolved around US rock and roll.[345] The counterculture of the 1960s was closely associated with psychedelic rock.[345] The mid- to late 1970s, punk subculture began in the US, but it was given a distinctive look by British designer Vivienne Westwood, a look which spread worldwide.[346] Out of the punk scene, the Goth and Emo subcultures grew, both of which presented distinctive visual styles.[347]

When an international rock culture developed, it supplanted cinema as the major sources of fashion influence.[348] Paradoxically, followers of rock music have often mistrusted the world of fashion, which has been seen as elevating image above substance.[348] Rock fashions have been seen as combining elements of different cultures and periods, as well as expressing divergent views on sexuality and gender, and rock music in general has been noted and criticised for facilitating greater sexual freedom.[348][349] Rock has also been associated with various forms of drug use, including the amphetamines taken by mods in the early to mid-1960s, through the LSD, mescaline, hashish and other hallucinogenic drugs linked with psychedelic rock in the mid- to late 1960s and early 1970s; and sometimes to cannabis, cocaine and heroin, all of which have been eulogised in song.[350][351]

Rock has been credited with changing attitudes to race by opening up African-American culture to white audiences; but at the same time, rock has been accused of appropriating and exploiting that culture.[352][353] While rock music has absorbed many influences and introduced Western audiences to different musical traditions,[354] the global spread of rock music has been interpreted as a form of cultural imperialism.[355] Rock music inherited the folk tradition of protest song, making political statements on subjects such as war, religion, poverty, civil rights, justice and the environment.[356] Political activism reached a mainstream peak with the "Do They Know It's Christmas?" single (1984) and Live Aid concert for Ethiopia in 1985, which, while raising awareness of world poverty and funds for aid, have also been criticised (along with similar events), for providing a stage for self-aggrandisement and increased profits for the rock stars involved.[357]

Since its early development, rock music has been associated with rebellion against social and political norms, most in early rock and roll's rejection of an adult-dominated culture, the counterculture's rejection of consumerism and conformity and punk's rejection of all forms of social convention;[358] however, it can also be seen as providing a means of commercial exploitation of such ideas and of diverting youth away from political action.[359][360]

Professional women instrumentalists are uncommon in rock genres such as heavy metal although bands such as Within Temptation have featured women as lead singers with men playing instruments. According to Schaap and Berkers, "playing in a band is a male homosocial activity, that is, learning to play in a band is a peer-based ... experience, shaped by existing sex-segregated friendship networks.[361] They note that rock music "is often defined as a form of male rebellion vis-à-vis female bedroom culture."[362] (The theory of "bedroom culture" argues that society influences girls to not engage in crime and deviance by virtually trapping them in their bedroom; it was identified by a sociologist named Angela McRobbie.) In popular music, there has been a gendered "distinction between public (male) and private (female) participation" in music.[362] "Several scholars have argued that men exclude women from bands or from the bands' rehearsals, recordings, performances, and other social activities".[363] "Women are regarded as passive and private consumers of slick, prefabricated – hence, inferior – pop music ..., excluding them from participating as high status rock musicians".[363] One of the reasons that there are mixed gender bands is that "bands operate as tight-knit units in which homosocial solidarity – social bonds between people of the same sex ...  – plays a crucial role".[363] In the 1960s rock music scene, "singing was sometimes an acceptable pastime for a girl, but playing an instrument ... simply wasn't done".[364]

"The rebellion of rock music was a male rebellion; the women – often, in the 1950s and '60s, girls in their teens – in rock sang songs as personæ dependent on their macho boyfriends ...". Philip Auslander says that "Although there were many women in rock by the late 1960s, most performed only as singers, a feminine position in popular music". Though some women played instruments in American all-female garage rock bands, none of these bands achieved more than regional success. So they "did not provide viable templates for women's on-going participation in rock".[365] In relation to the gender composition of heavy metal bands, it has been said that "[h]eavy metal performers are almost exclusively male"[366] "...at least until the mid-1980s"[367] apart from "...exceptions such as Girlschool".[366] However, "...now [in the 2010s] maybe more than ever–strong metal women have put up their dukes and got down to it",[368] "carv[ing] out a considerable place for [them]selves."[369] When Suzi Quatro emerged in 1973, "no other prominent female musician worked in rock simultaneously as a singer, instrumentalist, songwriter, and bandleader".[365] According to Auslander, she was "kicking down the male door in rock and roll and proving that a female musician ... and this is a point I am extremely concerned about ... could play as well if not better than the boys".[365]

An all-female band is a musical group in genres such as rock and blues which is composed of female musicians. This is distinct from a girl group, in which the female members are vocalists, though this terminology is not universally followed.[370]The Roman Republic (Latin: Res publica Romana [ˈreːs ˈpuːblɪka roːˈmaːna]) was the era of classical Roman civilisation beginning with the overthrow of the Roman Kingdom (traditionally dated to 509 BC) and ending in 27 BC with the establishment of the Roman Empire following the War of Actium. During this period, Rome's control expanded from the city's immediate surroundings to hegemony over the entire Mediterranean world.

Roman society at the time was primarily a cultural mix of Latin and Etruscan societies, as well as of Sabine, Oscan, and Greek cultural elements, which is especially visible in the Ancient Roman religion and its pantheon. Its political organisation developed at around the same time as direct democracy in Ancient Greece, with collective and annual magistracies, overseen by a senate.[4] There were annual elections, but the republican system was an elective oligarchy, not a democracy; a small number of powerful families largely monopolised the magistracies. Roman institutions underwent considerable changes throughout the Republic to adapt to the difficulties it faced, such as the creation of promagistracies to rule its conquered provinces, and differences in the composition of the senate.

Unlike the Pax Romana of the Roman Empire, throughout the republican era Rome was in a state of near-perpetual war. Its first enemies were its Latin and Etruscan neighbours, as well as the Gauls, who sacked Rome in 387 BC. After the Gallic sack, Rome conquered the whole Italian Peninsula in a century and thus became a major power in the Mediterranean. Its greatest strategic rival was Carthage, against which it waged three wars. Rome defeated Carthage at the Battle of Zama in 202 BC, becoming the dominant power of the ancient Mediterranean world. It then embarked on a long series of difficult conquests, defeating Philip V and Perseus of Macedon, Antiochus III of the Seleucid Empire, the Lusitanian Viriathus, the Numidian Jugurtha, the Pontic king Mithridates VI, Vercingetorix of the Arverni tribe of Gaul, and the Egyptian queen Cleopatra.

At home, during the Conflict of the Orders, the patricians, the closed oligarchic elite, came into conflict with the more numerous plebs; this was resolved peacefully, with the plebs achieving political equality by the 4th century BC. The late Republic, from 133 BC onward, saw substantial domestic strife, often anachronistically seen as a conflict between optimates and populares, referring to conservative and reformist politicians, respectively. The Social War between Rome and its Italian allies over citizenship and Roman hegemony in Italy greatly expanded the scope of civil violence. Mass slavery also contributed to three Servile Wars. Tensions at home coupled with ambitions abroad led to further civil wars. The first involved Marius and Sulla. After a generation, the Republic fell into civil war again in 49 BC between Julius Caesar and Pompey. Despite his victory and appointment as dictator for life, Caesar was assassinated in 44 BC. Caesar's heir Octavian and lieutenant Mark Antony defeated Caesar's assassins in 42 BC, but they eventually split. Antony's defeat alongside his ally and lover Cleopatra at the Battle of Actium in 31 BC, and the Senate's grant of extraordinary powers to Octavian as Augustus in 27 BC—which effectively made him the first Roman emperor—marked the end of the Republic.

Rome had been ruled by monarchs since its foundation. These monarchs were elected, for life, by the men of the Roman Senate. The last Roman monarch was called Tarquin the Proud, who in traditional histories was expelled from Rome in 509 BC because his son, Sextus Tarquinius, raped a noblewoman, Lucretia.[5][6][7] The tradition asserted that the monarchy was abolished in a revolution led by the semi-mythical Lucius Junius Brutus and the king's powers were then transferred to two separate consuls elected to office for a term of one year; each was capable of checking his colleague by veto.[8] Most modern scholarship describes these accounts as the quasi-mythological detailing of an aristocratic coup within Tarquin's own family[9] or a consequence of an Etruscan occupation of Rome rather than a popular revolution.[10]

According to Rome's traditional histories, Tarquin made several attempts to retake the throne, including the Tarquinian conspiracy, which involved Brutus's own sons, the war with Veii and Tarquinii, and finally the war between Rome and Clusium. The attempts to restore the monarchy did not succeed.[11]

The first Roman republican wars were wars of expansion. One by one, Rome defeated both the persistent Sabines and the local cities. Rome defeated its rival Latin cities in the Battle of Lake Regillus in 496 BC, the Battle of Ariccia in 495 BC, the Battle of Mount Algidus in 458 BC, and the Battle of Corbio in 446 BC. But it suffered a significant defeat at the Battle of the Cremera in 477 BC, wherein it fought against the most important Etruscan city, Veii; this defeat was later avenged at the Battle of Veii in 396 BC, wherein Rome destroyed the city.[12][13] By the end of this period, Rome had effectively completed the conquest of its immediate Etruscan and Latin neighbours and secured its position against the immediate threat posed by the nearby Apennine hill tribes.

Beginning with their revolt against Tarquin, and continuing through the early years of the Republic, Rome's patrician aristocrats were the dominant force in politics and society. They initially formed a closed group of about 50 large families, called gentes, who monopolised Rome's magistracies, state priesthoods, and senior military posts. The most prominent of these families were the Cornelii, Aemilii, Claudii, Fabii, and Valerii. The leading families' power, privilege and influence derived from their wealth, in particular from their landholdings, their position as patrons, and their numerous clients.[14]

The vast majority of Roman citizens were commoners of various social degrees. They formed the backbone of Rome's economy, as smallholding farmers, managers, artisans, traders, and tenants. In wartime, they could be summoned for military service. Most had little direct political influence. During the early Republic, the plebs (or plebeians) emerged as a self-organised, culturally distinct group of commoners, with its own internal hierarchy, laws, customs, and interests.[15] Plebeians had no access to high religious and civil office.[a] For the poorest, one of the few effective political tools was their withdrawal of labour and services, in a secessio plebis; the first such secession occurred in 494 BC, in protest at the abusive treatment of plebeian debtors by the wealthy during a famine.[b] The patrician Senate was compelled to give them direct access to the written civil and religious laws and to the electoral and political process. To represent their interests, the plebs elected tribunes, who were personally sacrosanct, immune to arbitrary arrest by any magistrate, and had veto power over legislation.[c]

By 390 BC, several Gallic tribes were invading Italy from the north. The Romans met the Gauls in pitched battle at the Battle of Allia River around 390–387 BC. The battle was fought at the confluence of the Tiber and Allia rivers, 11 Roman miles (10 mi or 16 km) north of Rome. The Romans were routed and subsequently Rome was sacked by the Senones.[16] There is no destruction layer at Rome around this time, indicating that if a sack occurred, it was largely superficial.[17][better source needed]

Second Samnite War

Third Samnite War

From 343 to 341 BC, Rome won two battles against its Samnite neighbours, but was unable to consolidate its gains, due to the outbreak of war with former Latin allies. In the Latin War (340–338 BC), Rome defeated a coalition of Latins at the battles of Vesuvius and the Trifanum. The Latins submitted to Roman rule.[18]

A Second Samnite War began in 327 BC.[19] The war ended with Samnite defeat at the Battle of Bovianum in 305 BC. By 304 BC, Rome had annexed most Samnite territory and begun to establish colonies there, but in 298 BC the Samnites rebelled, and defeated a Roman army, in a Third Samnite War. After this success, it built a coalition of several previous enemies of Rome.[20] The war ended with Roman victory in 290 BC.

At the Battle of Populonia, in 282 BC, Rome finished off the last vestiges of Etruscan power in the region.



In the 4th century, plebeians gradually obtained political equality with patricians. The first plebeian consular tribunes were elected in 400. The reason behind this sudden gain is unknown,[d] but it was limited as patrician tribunes retained preeminence over their plebeian colleagues.[22] In 385 BC, the former consul and saviour of the besieged capital, Marcus Manlius Capitolinus, is said to have sided with the plebeians, ruined by the sack and largely indebted to patricians. According to Livy, Capitolinus sold his estate to repay the debt of many of them, and even went over to the plebs, the first patrician to do so. Nevertheless, the growing unrest he had caused led to his trial for seeking kingly power; he was sentenced to death and thrown from the Tarpeian Rock.[23][24]

Between 376 and 367 BC, the tribunes of the plebs Gaius Licinius Stolo and Lucius Sextius Lateranus continued the plebeian agitation and pushed for an ambitious legislation, known as the Leges Liciniae Sextiae. The most important bill opened the consulship to plebeians.[25] Other tribunes controlled by the patricians vetoed the bills, but Stolo and Lateranus retaliated by vetoing the elections for five years while being continuously reelected by the plebs, resulting in a stalemate.[e] In 367 BC, they carried a bill creating the Decemviri sacris faciundis, a college of ten priests, of whom five had to be plebeians, thereby breaking patricians' monopoly on priesthoods. The resolution of the crisis came from the dictator Camillus, who made a compromise with the tribunes: he agreed to their bills, and they in return consented to the creation of the offices of praetor and curule aediles, both reserved to patricians. Lateranus became the first plebeian consul in 366 BC; Stolo followed in 361 BC.[26][27][28]

Soon after, plebeians were able to hold both the dictatorship and the censorship. The four-time consul Gaius Marcius Rutilus became the first plebeian dictator in 356 BC and censor in 351 BC. In 342 BC, the tribune of the plebs Lucius Genucius passed his leges Genuciae, which abolished interest on loans, in a renewed effort to tackle indebtedness; required the election of at least one plebeian consul each year; and prohibited magistrates from holding the same magistracy for the next ten years or two magistracies in the same year.[29][30][31] In 339 BC, the plebeian consul and dictator Quintus Publilius Philo passed three laws extending the plebeians' powers. His first law followed the lex Genucia by reserving one censorship to plebeians, the second made plebiscites binding on all citizens (including patricians), and the third required the Senate to give its prior approval to plebiscites before they became binding on all citizens.[32]

During the early Republic, consuls chose senators from among their supporters. Shortly before 312 BC, the lex Ovinia transferred this power to the censors, who could only remove senators for misconduct, thus appointing them for life. This law strongly increased the power of the Senate, which was by now protected from the influence of the consuls and became the central organ of government.[33][34][f] In 312 BC, following this law, the patrician censor Appius Claudius Caecus appointed many more senators to fill the new limit of 300, including descendants of freedmen, which was deemed scandalous. Caecus also launched a vast construction programmee, building the first aqueduct, the Aqua Appia, and the first Roman road, the Via Appia.[35]

In 300 BC, the two tribunes of the plebs Gnaeus and Quintus Ogulnius passed the lex Ogulnia, which created four plebeian pontiffs, equalling the number of patrician pontiffs, and five plebeian augurs, outnumbering the four patricians in the college.[36] The Conflict of the Orders ended with the last secession of the plebs around 287. The dictator Quintus Hortensius passed the lex Hortensia, which reenacted the law of 339 BC, making plebiscites binding on all citizens, while also removing the requirement for prior Senate approval.[37] These events were a political victory of the wealthy plebeian elite, who exploited the economic difficulties of the plebs for their own gain: Stolo, Lateranus, and Genucius bound their bills attacking patricians' political supremacy with debt-relief measures.[38] As a result of the end of the patrician monopoly on senior magistracies, many small patrician gentes faded into history during the 4th and 3rd centuries BC due to the lack of available positions. About a dozen remaining patrician gentes and 20 plebeian ones thus formed a new elite, called the nobiles, or Nobilitas.[39]

By the early 3rd century BC, Rome had established itself as the major power in Italy, but had not yet come into conflict with the dominant military powers of the Mediterranean: Carthage and the Greek kingdoms.[40] In 282, several Roman warships entered the harbour of Tarentum, triggering a violent reaction from the Tarentine democrats, who sank some. The Roman embassy sent to investigate the affair was insulted and war was promptly declared.[41] Facing a hopeless situation, the Tarentines (together with the Lucanians and Samnites) appealed to Pyrrhus, king of Epirus, for military aid. A cousin of Alexander the Great, he was eager to build an empire for himself in the western Mediterranean and saw Tarentum's plea as a perfect opportunity.[42]

Pyrrhus and his army of 25,500 men (with 20 war elephants) landed in the Italian peninsula in 280 BC. The Romans were defeated at Heraclea, as their cavalry were afraid of Pyrrhus's elephants. Pyrrhus then marched on Rome, but the Romans concluded a peace in the north and moved south with reinforcements, placing Pyrrhus in danger of being flanked by two consular armies; Pyrrhus withdrew to Tarentum. In 279 BC, Pyrrhus met the consuls Publius Decius Mus and Publius Sulpicius Saverrio at the Battle of Asculum, which remained undecided for two days. Finally, Pyrrhus personally charged into the melee and won the battle but at the cost of an important part of his troops; he allegedly said, "if we are victorious in one more battle with the Romans, we shall be utterly ruined."[43][g]

He escaped the Italian deadlock by answering a call for help from Syracuse, where tyrant Thoenon was desperately fighting an invasion from Carthage. Pyrrhus could not let them take the whole island, as it would have compromised his ambitions in the western Mediterranean, and so declared war. The Carthaginians lifted the siege of Syracuse before his arrival, but he could not entirely oust them from the island as he failed to take their fortress of Lilybaeum.[44] His harsh rule soon led to widespread antipathy among the Sicilians; some cities even defected to Carthage. In 275 BC, Pyrrhus left the island before he had to face a full-scale rebellion.[45] He returned to Italy, where his Samnite allies were on the verge of losing the war. Pyrrhus again met the Romans at the Battle of Beneventum. This time, the consul Manius Dentatus was victorious and even captured eight elephants. Pyrrhus then withdrew from Italy, but left a garrison in Tarentum, to wage a new campaign in Greece against Antigonus II Gonatas of Macedonia. His death in battle at Argos in 272 BC forced Tarentum to surrender to Rome.

Rome and Carthage were initially on friendly terms, lastly in an alliance against Pyrrhus,[46][47][48] but tensions rapidly rose after the departure of the Epirote king. Between 288 and 283 BC, Messina in Sicily was taken by the Mamertines, a band of mercenaries formerly employed by Agathocles. They plundered the surroundings until Hiero II, the new tyrant of Syracuse, defeated them (in either 269 or 265 BC). In effect under a Carthaginian protectorate, the remaining Mamertines appealed to Rome to regain their independence. Senators were divided on whether to help. A supporter of war, the consul Appius Claudius Caudex, turned to one of the popular assemblies to get a favourable vote by promising plunder to the voters.[h] After the assembly ratified an alliance with the Mamertines, Caudex was dispatched to cross the strait and lend aid.[49]

Messina fell under Roman control quickly.[50] Syracuse and Carthage, at war for centuries, responded with an alliance to counter the invasion and blockaded Messina, but Caudex defeated Hiero and Carthage separately.[51][52] His successor, Manius Valerius Maximus, landed with an army of 40,000 men and conquered eastern Sicily, which prompted Hiero to shift his allegiance and forge a long-lasting alliance with Rome. In 262 BC, the Romans moved to the southern coast and besieged Akragas. In order to raise the siege, Carthage sent reinforcements, including 60 elephants—the first time they used them—but still lost the battle.[53] Nevertheless, Rome could not take all of Sicily because Carthage's naval superiority prevented it from effectively besieging coastal cities. Using a captured Carthaginian ship as blueprint, Rome therefore launched a massive construction programme and built 100 quinqueremes in only two months. It also invented a new device, the corvus, a grappling engine that enabled a crew to board an enemy ship.[54] The consul for 260 BC, Gnaeus Cornelius Scipio Asina, lost the first naval skirmish of the war against Hannibal Gisco at Lipara, but his colleague Gaius Duilius won a great victory at Mylae. He destroyed or captured 44 ships and was the first Roman to receive a naval triumph, which also included captive Carthaginians for the first time.[55] Although Carthage was victorious on land at Thermae in Sicily, the corvus gave a strong advantage to Rome on the waters. The consul Lucius Cornelius Scipio (Asina's brother) captured Corsica in 259 BC; his successors won the naval battles of Sulci in 258, Tyndaris in 257 BC, and Cape Ecnomus in 256.[56]

To hasten the end of the war, the consuls for 256 BC decided to carry the operations to Africa, on Carthage's homeland. The consul Marcus Atilius Regulus landed on the Cap Bon peninsula with about 18,000 soldiers. He captured the city of Aspis, repulsed Carthage's counterattack at Adys, and took Tunis. The Carthaginians hired Spartan mercenaries, led by Xanthippus, to command their troops.[58] In 255, the Spartan general marched on Regulus, crushing the Roman infantry on the Bagradas plain; only 2,000 soldiers escaped, and Regulus was captured. The consuls for 255 nonetheless won a naval victory at Cape Hermaeum, where they captured 114 warships. This success was spoilt by a storm that annihilated the victorious navy: 184 ships of 264 sank, 25,000 soldiers and 75,000 rowers drowned. The corvus considerably hindered ships' navigation and made them vulnerable during tempest. It was abandoned after another similar catastrophe in 253 BC. These disasters prevented any significant campaign between 254 and 252 BC.[59]

Hostilities in Sicily resumed in 252  BC, with Rome's taking of Thermae. The next year, Carthage besieged Lucius Caecilius Metellus, who held Panormos (now Palermo). The consul had dug trenches to counter the elephants, which once hurt by missiles turned back on their own army, resulting in a great victory for Metellus. Rome then besieged the last Carthaginian strongholds in Sicily, Lilybaeum and Drepana, but these cities were impregnable by land. Publius Claudius Pulcher, the consul of 249, recklessly tried to take the latter from the sea, but suffered a terrible defeat; his colleague Lucius Junius Pullus likewise lost his fleet off Lilybaeum. Without the corvus, Roman warships had lost their advantage. By now, both sides were drained and could not undertake large-scale operations. The only military activity during this period was the landing in Sicily of Hamilcar Barca in 247 BC, who harassed the Romans with a mercenary army from a citadel he built on Mt. Eryx.[61]

Unable to take the Punic fortresses in Sicily, Rome tried to decide the war at sea and built a new navy, thanks to a forced borrowing from the rich. In 242 BC, 200 quinqueremes under consul Gaius Lutatius Catulus blockaded Drepana. The rescue fleet from Carthage was soundly defeated by Catulus. Exhausted and unable to bring supplies to Sicily, Carthage sued for peace. Carthage had to pay 1,000 talents immediately and 2,200 over ten years and evacuate Sicily. The fine was so high that Carthage could not pay Hamilcar's mercenaries, who had been shipped back to Africa. They revolted during the Mercenary War, which Carthage suppressed with enormous difficulty. Meanwhile, Rome took advantage of a similar revolt in Sardinia to seize the island from Carthage, in violation of the peace treaty. This led to permanent bitterness in Carthage.[62]

After its victory, the Republic shifted its attention to its northern border as the Insubres and Boii were threatening Italy.[63] Meanwhile, Carthage compensated the loss of Sicily and Sardinia with the conquest of Southern Hispania (up to Salamanca), and its rich silver mines.[64] This rapid expansion worried Rome, which concluded a treaty with Hasdrubal in 226, stating that Carthage could not cross the Ebro river.[65][66] But the city of Saguntum, south of the Ebro, appealed to Rome in 220 to act as arbitrator during a period of internal strife. Hannibal took the city in 219,[67] triggering the Second Punic War.[68]

Initially, the Republic's plan was to carry war outside Italy, sending the consuls P. Cornelius Scipio to Hispania and Ti. Sempronius Longus to Africa, while their naval superiority prevented Carthage from attacking from the sea.[69] This plan was thwarted by Hannibal's bold move to Italy. In May 218, he crossed the Ebro with a large army of about 100,000 soldiers and 37 elephants.[70] He passed in Gaul, crossed the Rhone, then the Alps, possibly through the Col de Clapier.[71] This exploit cost him almost half of his troops,[72] but he could now rely on the Boii and Insubres, still at war with Rome.[73] Publius Scipio, who had failed to block Hannibal on the Rhone, sent his elder brother Gnaeus with the main part of his army in Hispania according to the initial plan, and went back to Italy with the rest to resist Hannibal in Italy, but he was defeated and wounded near the Ticino river.[74]

Hannibal then marched south and won three outstanding victories. The first one was on the banks of the Trebia in December 218, where he defeated the other consul Ti. Sempronius Longus. More than half the Roman army was lost. Hannibal then ravaged the country around Arretium to lure the new consul C. Flaminius into a trap at Lake Trasimene. This clever ambush resulted in the death of the consul and the complete destruction of his army of 30,000 men. In 216, the new consuls L. Aemilius Paullus and C. Terentius Varro mustered the biggest army possible, with eight legions—some 80,000 soldiers, twice as many as the Punic army—and confronted Hannibal, who was encamped at Cannae, in Apulia. Despite his numerical disadvantage, Hannibal used his heavier cavalry to rout the Roman wings and envelop their infantry, which he annihilated. In terms of casualties, the Battle of Cannae was the worst defeat in Roman history: only 14,500 soldiers escaped, and Paullus was killed as well as 80 senators.[76][i] Soon after, the Boii ambushed the army of the consul-elect for 215, L. Postumius Albinus, who died with all his army of 25,000 men in the Battle of Silva Litana.[77]

These disasters triggered a wave of defection among Roman allies, with the rebellions of the Samnites, Oscans, Lucanians, and Greek cities of Southern Italy.[78] In Macedonia, Philip V also made an alliance with Hannibal in order to take Illyria and the area around Epidamnus, occupied by Rome. His attack on Apollonia started the First Macedonian War. In 215, Hiero II of Syracuse died of old age, and his young grandson Hieronymus broke the long alliance with Rome to side with Carthage. At this desperate point, the aggressive strategy against Hannibal the Scipiones advocated was abandoned in favour of a slow reconquest of the lost territories, since Hannibal could not be everywhere to defend them.[79] Although he remained invincible on the battlefield, defeating all the Roman armies on his way, he could not prevent Claudius Marcellus from taking Syracuse in 212 after a long siege, nor the fall of his bases of Capua and Tarentum in 211 and 209.

In Hispania, Publius and Gnaeus Scipio won the battles of Cissa in 218, soon after Hannibal's departure, and Dertosa against his brother Hasdrubal in 215, which enabled them to conquer the eastern coast of Hispania. But in 211, Hasdrubal and Mago Barca successfully turned the Celtiberian tribes that supported the Scipiones, and attacked them simultaneously at the Battle of the Upper Baetis, in which the Scipiones died.[80] Publius's son, the future Scipio Africanus, was then elected with a special proconsulship to lead the Hispanic campaign, winning a series of battles with ingenious tactics. In 209, he took Carthago Nova, the main Punic base in Hispania. The next year, he defeated Hasdrubal at the Battle of Baecula.[80] After his defeat, Carthage ordered Hasdrubal to reinforce his brother in Italy. Since he could not use ships, he followed the same route as his brother through the Alps, but the consuls M. Livius Salinator and C. Claudius Nero were awaiting him and defeated him in the Battle of the Metaurus, where Hasdrubal died.[81] It was the turning point of the war. The campaign of attrition had worked well: Hannibal's troops were now depleted; he only had one elephant left (Surus) and retreated to Bruttium, on the defensive. In Greece, Rome contained Philip V without devoting too many forces by allying with the Aetolian League, Sparta, and Pergamon, which also prevented Philip from aiding Hannibal. The war with Macedon resulted in a stalemate, with the Treaty of Phoenice signed in 205.


In Hispania, Scipio continued his successful campaign at the battles of Carmona in 207, and Ilipa (now Seville) in 206, which ended the Punic threat on the peninsula.[82] Elected consul in 205, he convinced the Senate to invade Africa with the support of the Numidian king Masinissa, who had defected to Rome. Scipio landed in Africa in 204. He took Utica and then won the Battle of the Great Plains, which prompted Carthage to open peace negotiations. The talks failed because Scipio wanted to impose harsher terms on Carthage to prevent it from rising again as a threat. Hannibal was therefore sent to face Scipio at Zama. Scipio could now use the heavy Numidian cavalry of Massinissa—which had hitherto been so successful against Rome—to rout the Punic wings, then flank the infantry, as Hannibal had done at Cannae. Defeated for the first time, Hannibal convinced the Carthaginian Senate to pay the war indemnity, which was even harsher than that of 241: 10,000 talents in 50 instalments. Carthage also had to give up all its elephants, all its fleet but ten triremes, and all its possessions outside its core territory in Africa (what is now Tunisia), and it could not declare war without Roman authorisation. In effect, Carthage was condemned to be a minor power, while Rome recovered from a desperate situation to dominate the western Mediterranean.

Rome's preoccupation with its war with Carthage provided an opportunity for Philip V of Macedonia, in the north of the Greek peninsula, to attempt to extend his power westward. He sent ambassadors to Hannibal's camp in Italy, to negotiate an alliance as common enemies of Rome.[83] But Rome discovered the agreement when Philip's emissaries were captured by a Roman fleet.[83] The First Macedonian War saw the Romans involved directly in only limited land operations, but they achieved their objective of occupying Philip and preventing him from aiding Hannibal.

The past century had seen the Greek world dominated by the three primary successor kingdoms of Alexander the Great's empire: Ptolemaic Egypt, Macedonia and the Seleucid Empire. In 202, internal problems led to a weakening of Egypt's position, disrupting the power balance among the successor states. Macedonia and the Seleucid Empire agreed to an alliance to conquer and divide Egypt.[84] Fearing this increasingly unstable situation, several small Greek kingdoms sent delegations to Rome to seek an alliance.[85] Rome gave Philip an ultimatum to cease his campaigns against Rome's new Greek allies. Doubting Rome's strength, Philip ignored the request, and Rome sent an army of Romans and Greek allies, beginning the Second Macedonian War.[86] In 197, the Romans decisively defeated Philip at the Battle of Cynoscephalae, and Philip was forced to give up his recent Greek conquests.[87] The Romans declared the "Peace of the Greeks", believing that Philip's defeat now meant that Greece would be stable, and pulled out of Greece entirely.[88]

With Egypt and Macedonia weakened, the Seleucid Empire made increasingly aggressive and successful attempts to conquer the entire Greek world.[89] Now not only Rome's allies against Philip, but even Philip himself, sought a Roman alliance against the Seleucids.[90] The situation was exacerbated by the fact that Hannibal was now a chief military advisor to the Seleucid emperor, and the two were believed to be planning outright conquest not just of Greece, but also of Rome.[91] The Seleucids were much stronger than the Macedonians had ever been, because they controlled much of the former Persian Empire and had almost entirely reassembled Alexander the Great's former empire.[91]

Fearing the worst, the Romans began a major mobilisation, all but pulling out of recently conquered Spain and Gaul.[91] This fear was shared by Rome's Greek allies, who now followed Rome again for the first time since that war.[91] A major Roman-Greek force was mobilised under the command of the great hero of the Second Punic War, Scipio Africanus, and set out for Greece, beginning the Roman–Seleucid War. After initial fighting that revealed serious Seleucid weaknesses, the Seleucids tried to turn the Roman strength against them at the Battle of Thermopylae, but were forced to evacuate Greece.[90] The Romans pursued the Seleucids by crossing the Hellespont, the first time a Roman army had ever entered Asia.[90] The decisive engagement was fought at the Battle of Magnesia, resulting in complete Roman victory.[90][92] The Seleucids sued for peace, and Rome forced them to give up their recent Greek conquests. Rome again withdrew from Greece, assuming (or hoping) that the lack of a major Greek power would ensure a stable peace. In fact, it did the opposite.[93]

In 179, Philip died.[95] His talented and ambitious son, Perseus, took the throne and showed a renewed interest in conquering Greece.[96] With its Greek allies facing a major new threat, Rome declared war on Macedonia again, starting the Third Macedonian War. Perseus initially had some success against the Romans, but Rome responded by sending a stronger army which decisively defeated the Macedonians at the Battle of Pydna in 168.[97] The Macedonians capitulated, ending the war.[98]

Convinced now that the Greeks (and therefore the rest of the region) would not have peace if left alone, Rome decided to establish its first permanent foothold in the Greek world, and divided Macedonia into four client republics.[99] Yet Macedonian agitation continued. The Fourth Macedonian War, 150 to 148 BC, was fought against a Macedonian pretender to the throne who was again destabilising Greece by trying to reestablish the old kingdom. The Romans swiftly defeated the Macedonians at the second battle of Pydna.

The Achaean League, seeing the direction of Roman policy trending towards direct administration, met at Corinth and declared war "nominally against Sparta but in reality, against Rome".[100] It was swiftly defeated: in 146, the same year as the destruction of Carthage, Corinth was besieged and destroyed, forcing the league's surrender. Rome decided to divide the Greek territories into two new, directly administered Roman provinces, Achaea and Macedonia.[101]

For Carthage, the Third Punic War was a simple punitive mission after the neighbouring Numidians allied to Rome robbed and attacked Carthaginian merchants. Treaties had forbidden any war with Roman allies; viewing defence against banditry as "war action", Rome decided to annihilate Carthage.[102] Carthage was almost defenceless, and submitted when besieged.[103] But the Romans demanded complete surrender and removal of the city into the desert hinterland, far from any coastal or harbour region; the Carthaginians refused. The city was besieged and completely destroyed.[104] Rome acquired all of Carthage's North African and Iberian territories. The Romans rebuilt Carthage 100 years later as a Roman colony, by order of Julius Caesar. It flourished, becoming one of the most important cities in the Roman Empire.[105]



Views on the structural causes of the Republic's collapse differ. One enduring thesis is that Rome's expansion destabilised its social organisation between conflicting interests; the Senate's policymaking, blinded by its own short-term self-interest, alienated large portions of society, who then joined powerful generals who sought to overthrow the system.[106] Two other theses have challenged this view. The first blames the Romans' inability to conceive of plausible alternatives to the traditional republican system in a "crisis without alternative".[107] The second instead stresses the continuity of the republic: until its disruption by Caesar's civil war and the following two decades of civil war created conditions for autocratic rule and made return to republican politics impossible: and, per Erich S. Gruen, "civil war caused the fall of the republic, not vice versa".[108]

A core cause of the Republic's eventual demise was the loss of elite's cohesion from c. 133 BC: the ancient sources called this moral decay from wealth and the hubris of Rome's domination of the Mediterranean.[109] Modern sources have proposed multiple reasons why the elite lost cohesion, including wealth inequality and a growing willingness by aristocrats to transgress political norms, especially in the aftermath of the Social War.[110]

In the winter of 138–137 BC, a first slave uprising, known as the First Servile War, broke out in Sicily. After initial successes, the slaves led by Eunus and Cleon were defeated by Marcus Perperna and Publius Rupilius in 132 BC.[111]

In this context, Tiberius Gracchus was elected plebeian tribune in 133 BC. He attempted to enact a law to limit the amount of land anyone could own and establish a commission to distribute public lands to poor rural plebs.[112] The aristocrats, who stood to lose an enormous amount of money, bitterly opposed this proposal. Tiberius submitted this law to the Plebeian Council, but it was vetoed by fellow tribune Marcus Octavius.[113] Tiberius induced the plebs to depose Octavius from his office on the grounds that Octavius acted contrary to the manifest will of the people, a position that was unprecedented and constitutionally dubious.[114][115] His law was enacted and took effect,[j] but, when Tiberius ostentatiously stood for reelection to the tribunate, he was murdered by his enemies.[118]

Tiberius's brother Gaius was elected tribune ten years later in 123 and reelected for 122. He induced the plebs to reinforce rights of appeal to the people against capital extrajudicial punishments and institute reforms to improve the people's welfare. While ancient sources tend to "conceive Gracchus' legislation as an elaborate plot against the authority of the Senate... he showed no sign of wanting to replace the Senate in its normal functions".[119] Amid wide-ranging and popular reforms to create grain subsidies, change jury pools, establish and require the Senate to assign provinces before elections, Gaius proposed a law that would grant citizenship rights to Rome's Italian allies.[120] He stood for election to a third term in 121 but was defeated. During violent protests over repeal of an ally's colonisation bill, the Senate moved the first senatus consultum ultimum against him, resulting in his death, with many others, on the Aventine.[121] His legislation (like that of his brother) survived; the Roman aristocracy disliked the Gracchan agitation but accepted their policies.[122]

In 121, the province of Gallia Narbonensis was established after the victory of Quintus Fabius Maximus over a coalition of Arverni and Allobroges in southern Gaul in 123. Lucius Licinius Crassus founded the city of Narbo there in 118.[123]

Rome fought the Jugurthine War from 111 to 104 BC against the North African kingdom of Numidia (in what is now Algeria and Tunisia). In 118, its king, Micipsa, died, and an illegitimate son, Jugurtha, usurped the throne.[125] Numidia had been a loyal ally of Rome since the Punic Wars.[126] Initially, Rome mediated a division of the country. But Jugurtha renewed his offensive, leading to a long and inconclusive war with Rome.[127] Gaius Marius was a legate under the consul directing the war and was elected consul in 107 BC over the objections of the aristocratic senators, relying on support from the businessmen and poor. Marius had the Numidian command reassigned to himself through the popular assembly and, with the capture of Jugurtha at the end of a long campaign, ended the war; in the aftermath, the Romans largely withdrew from the province after installing a client king.[128] Marius's victory played on existing themes of senatorial corruption and incompetence, contrasted especially against the military failure of senatorial leadership in the Cimbrian War.[129]

The Cimbrian War (113–101) was a far more serious affair than the earlier Gallic clashes in 121. The Germanic tribes of the Cimbri and the Teutons[130] migrated from northern Europe into Rome's northern territories,[131] and clashed with Rome and its allies. The defeat of various aristocrats in the conflict, along with Marius's reputation for military victory, led to his holding five successive consulships with little to enable him to lead armies against the threat.[132] At the Battle of Aquae Sextiae and the Battle of Vercellae, Marius led the Roman armies, which virtually annihilated both tribes, ending the threat.[133]

During the Cimbrian War, further conflicts embroiled the Republic: A Second Servile War waged in Sicily from 104 to 101;[111] a campaign was waged against pirates in Cilicia; Rome campaigned in Thrace, adding lands to the province of Macedonia; and Lycaonia was annexed to Rome.[134]

In 91, the Social War broke out between Rome and its former allies in Italy: the main causes of the war were Roman encroachment on allied lands due to the Republic's land redistribution programmes, harsh Roman treatment of the non-citizen allies, and Roman unwillingness to share in the spoils of the empire.[135] After the assassination, in Rome, of a conservative tribune who sought to grant the Italians citizenship, the allies took up arms:[136] most ancient writers explain the conflict in terms of demands for full citizenship, but contemporary rebel propaganda coins indicate it may have been a primarily anti-Roman secessionist movement.[137] The Romans were able to stave off military defeat by conceding the main point almost immediately, tripling the number of citizens.[138] More recent scholarship also has stressed the importance of the war on the allies in destabilising Roman military affairs by blurring the distinction between Romans and foreign enemies.[139]

Further civil conflict emerged, starting in 88. One of the consuls that year, L. Cornelius Sulla, was assigned to take an army against the Pontic king Mithridates. The local governor there was defeated, but C. Marius induced a tribune to promulgate legislation reassigning Sulla's command to Marius. Sulla responded by suborning his army, marching on Rome (the city was undefended but politically outraged), and declaring Marius and 11 of his allies outlaws before departing east to war with Mithridates.[140] Marius, who had escaped into exile, returned, and with L. Cornelius Cinna, took control of the city.[141]

After the Marians took control of the city, they started to purge their political enemies.[142] They elected, in irregular fashion, Marius and Cinna to the consulship of 86 BC. Marius died a fortnight after assuming office. Cinna took control of the state: his policies are unclear and the record is muddled by Sulla's eventual victory.[143] The Cinnan regime declared Sulla a public enemy and ostensibly replaced him in command in the east. Instead of cooperating with his replacement, which Sulla viewed as illegitimate, he made peace with Mithridates and prepared to return to Italy.[144] By 85 BC, the Cinnans in Rome started preparations to defend the peninsula from invasion.[144]

In 83, he returned from the east with a small but experienced army.[145] Initial reactions were negative across the peninsula, but after winning a number of victories he was able to overcome resistance and capture the city. In the Battle of the Colline Gate, just outside Rome,[146] Sulla's army defeated the Marian defenders and then proceeded to "run riot... killing for profit, pleasure, or personal vengeance anyone they pleased".[147] He then instituted procedures to centralise the killing, creating lists of proscribed persons who could be killed for their property without punishment.[148] After establishing political control, Sulla had himself made dictator and passed a series of constitutional reforms intended to strengthen the position of the magistrates and the senate in the state and replace custom with new rigid statute laws enforced by new permanent courts.[149][150] Sulla resigned the dictatorship in 81 after election as consul for 80. He then retired, and died in 78 BC.[151]

Cn. Pompey Magnus served the Sullan regime during a short conflict triggered by the republic's own consul, M. Aemilius Lepidus, in 77 BC[152] and afterwards led troops successfully against the remaining anti-Sullan forces in the Sertorian War; he brought the war successfully to a close in 72 BC. While Pompey was in Spain, the Republic faced agitation both foreign and domestic. The main domestic political struggle was the restoration of tribunician powers stripped during Sulla's dictatorship.[153] After rumours of a pact between Sertorius's ostensible republic-in-exile,[154] Mithridates, and various Mediterranean pirate groups, the Sullan regime feared encirclement and stepped up efforts against the threats: they reinforced Pompey in Spain and fortified Bithynia. In spring 73 BC, Mithridates did so, invading Bithynia.[155]

In 73, a slave uprising started in southern Italy under Spartacus, a gladiator, who defeated the local Roman garrisons and four legions under the consuls of 72.[156] At the head of some 70,000 men, Spartacus led them in a Third Servile War—they sought freedom by escape from Italy—before being defeated by troops raised by M. Licinius Crassus.[157] Although Pompey and Crassus were rivals, they were elected to a joint consulship in 70. During their consulship, they brought—with little opposition—legislation to dismantle the tribunician disabilities imposed by Sulla's constitutional reforms.[158] They also shepherded legislation to settle the contentious matter of jury reform.[159]

L. Licinius Lucullus, one of Sulla's ablest lieutenants, had fought against Mithridates during the first Mithridatic war before Sulla's civil war. Mithridates also had fought Rome in a second Mithridatic war (83–82 BC).[160] Rome for its part seemed equally eager for war and the spoils and prestige that it might bring.[161] After his invasion of Bithynia in 73, Lucullus was assigned against Mithridates and his Armenian ally Tigranes the Great in Asia Minor.[162] Fighting a war of manoeuvre against Mithridates' supply lines, Lucullus was able force Mithridates from an attempted siege of Cyzicus and pursue him into Pontus and thence into Armenia.[163] After defeat forced the Romans from large parts of Armenia and Pontus in 67, Lucullus was replaced in command by Pompey.[164] Pompey moved against Mithridates in 66.[165] Defeating him in battle and securing the submission of Tigranes,[166] Mithridates fled to Crimea, where he was betrayed and killed by his son Pharnaces in 63.[167] Pompey remained in the East to pacify and settle Roman conquests in the region, also extending Roman control south to Judaea.[168]

People

Events

Places

Pompey returned from the Third Mithridatic War at the end of 62 BC. In the interim, before his return to Italy, the senate had successfully suppressed a conspiracy and insurrection led by a senator, Lucius Sergius Catilina, to overthrow that year's consuls.[169] In the aftermath of the conspiracy, which was abetted by popular discontent, the Senate moved legislation to temper unrest in Italy: expanding the grain dole and implementing other reforms.[170] Pompey, landing in Brundisium, publicly dismissed his troops, indicating that he had no desire to follow Sulla's example and dominate the republic by force, as some conservative senators had feared.[171] He attempted to have his eastern settlements passed by the Senate; ratification was not forthcoming, due to the opposition of Lucullus, Crassus, and Cato the Younger.[172]

After Julius Caesar's election as one of the consuls of 59 BC, Pompey, Caesar, and Crassus engaged in a political alliance (traditionally dubbed by scholars as the First Triumvirate).[173] The alliance greatly benefited the three men: Caesar passed legislation to distribute state lands as poor relief while also providing land for Pompey's veterans; he also had Pompey's eastern settlements ratified; for Crassus, he secured relief for tax farmers and a place on agrarian commission.[174] Caesar won for himself the political support needed to acquire a profitable provincial command in Gaul and secure his political future.[175]

Attempting first to pass portions of his programme through the Senate, Caesar found the curia obstinate. He thus unveiled his alliance with Pompey and Crassus and moved his legislation before the people instead.[176] Political opposition to the allies was immense.[177]

Caesar also facilitated the election of the former patrician Publius Clodius Pulcher to the tribunate for 58. Clodius set about depriving Caesar's senatorial enemies of two of their more obstinate leaders in Cato and Cicero. Clodius attempted to try Cicero for executing citizens without a trial during the Catiline conspiracy, resulting in Cicero going into self-imposed exile. Clodius also passed a bill that forced Cato to lead the invasion of Cyprus, which would keep him away from Rome for some years. Clodius also passed a law to expand the previous partial grain subsidy to a fully free grain dole for citizens.[178]

After his term as consul in 59, Caesar was appointed to a five-year term as the proconsular Governor of Cisalpine Gaul (part of current northern Italy), Transalpine Gaul (current southern France) and Illyria (part of the modern Balkans).[179] Caesar sought cause to invade Gaul (modern France and Belgium), which would give him the dramatic military success he sought. When two local tribes began to migrate on a route that would take them near (not into) the Roman province of Transalpine Gaul, Caesar had the barely sufficient excuse he needed for his Gallic Wars, fought between 58 and 49.

Caesar defeated large armies at major battles 58 and 57. In 55 and 54 he made two expeditions into Britain, the first Roman to do so. Caesar then defeated a union of Gauls at the Battle of Alesia,[180][181] completing the Roman conquest of Transalpine Gaul. By 50, all of Gaul lay in Roman hands.

Clodius formed armed gangs that terrorised the city and eventually began to attack Pompey's followers, who in response funded counter-gangs formed by Titus Annius Milo. The political alliance of the triumvirate was crumbling. Domitius Ahenobarbus ran for the consulship in 55, promising to take Caesar's command from him. Eventually, the triumvirate was renewed at Lucca. Pompey and Crassus were promised the consulship in 55, and Caesar's term as governor was extended for five years. Beginning in the summer of 54, a wave of political corruption and violence swept Rome.[182] This chaos reached a climax in January of 52, when Milo murdered Clodius in a gang war.

In 53, Crassus launched a Roman invasion of the Parthian Empire (modern Iraq and Iran). After initial successes,[183] his army was cut off deep in enemy territory, surrounded and slaughtered at the Battle of Carrhae, in which Crassus himself perished. Crassus's death destabilised the Triumvirate. While Caesar was fighting in Gaul, Pompey proceeded with a legislative agenda for Rome that revealed that he was at best ambivalent towards Caesar.[184] Pompey's wife, Julia, who was Caesar's daughter, died in childbirth. This event severed the last remaining bond between Pompey and Caesar. In 51, some Roman senators demanded that Caesar not be permitted to stand for consul unless he turned over control of his armies to the state. Caesar chose civil war over laying down his command and facing trial.

On 1 January 49, an agent of Caesar presented an ultimatum to the senate. The ultimatum was rejected, and the senate then passed a resolution declaring that if Caesar did not lay down his arms by July of that year, he would be considered an enemy of the Republic.[185] Meanwhile, the senators adopted Pompey as their new champion against Caesar, passing a senatus consultum ultimum that vested Pompey with dictatorial powers. On 10 January, Caesar with his veteran army crossed the river Rubicon, the legal boundary of Roman Italy beyond which no commander might bring his army, in violation of Roman laws, and by the spring of 49 swept down the Italian peninsula towards Rome. His rapid advance forced Pompey, the consuls and the senate to abandon Rome for Greece. Caesar entered the city unopposed. Afterwards Caesar turned his attention to the Pompeian stronghold of Hispania (modern Spain)[186] but decided to tackle Pompey himself in Greece.[187] Pompey initially defeated Caesar, but failed to follow up on the victory, and was decisively defeated at the Battle of Pharsalus in 48.[188][189] Pompey fled again, this time to Egypt, where he was murdered.

Pompey's death did not end the civil war. In 46 Caesar lost perhaps as much as a third of his army, but ultimately came back to defeat the Pompeian army of Metellus Scipio in the Battle of Thapsus, after which the Pompeians retreated yet again to Hispania. Caesar then defeated the combined Pompeian forces at the Battle of Munda.

With Pompey defeated and order restored, Caesar wanted to achieve undisputed control over the government. The powers he gave himself were later assumed by his imperial successors.[190] Caesar held both the dictatorship and the tribunate, and alternated between the consulship and the proconsulship.[190] In 48, he was given permanent tribunician powers. This made his person sacrosanct, gave him the power to veto the senate, and allowed him to dominate the Plebeian Council. In 46, Caesar was given censorial powers,[191] which he used to fill the senate with his partisans. He then raised the membership of the Senate to 900.[192] This robbed the senatorial aristocracy of its prestige, and made it increasingly subservient to him.[193] Caesar began to prepare for a war against the Parthian Empire. Since his absence from Rome would limit his ability to install consuls, he passed a law that allowed him to appoint all magistrates, and later all consuls and tribunes. This transformed the magistrates from representatives of the people to representatives of the dictator.[192]

Caesar was now the primary figure of the Roman state, enforcing and entrenching his powers. His enemies feared that he had ambitions to become an autocratic ruler. Arguing that the Roman Republic was in danger, a group of senators led by Gaius Cassius and Marcus Brutus hatched a conspiracy and assassinated Caesar at a meeting of the Senate on 15 March 44. Virtually all the conspirators fled the city after Caesar's death in fear of retaliation.

The civil wars that followed destroyed what was left of the Republic.[196]

After the assassination, Caesar's three most important associates, Mark Antony, Caesar's co-consul, Octavian, Caesar's adopted son and great-nephew, and Lepidus, Caesar's magister equitum, formed an alliance known as the Second Triumvirate.[197] The conspirators were defeated at the Battle of Philippi in 42.

Following Philippi, Rome's territories were divided between the triumvirs, but the agreement was fragile. Antony detested Octavian and spent most of his time in the East, while Lepidus favoured Antony but felt himself obscured by his colleagues. Following the defeat of Sextus Pompeius, a dispute between Lepidus and Octavian regarding the allocation of lands broke out and, in 36 BC, Lepidus was forced into exile in Circeii and stripped of all his offices except that of pontifex maximus. His former provinces were awarded to Octavian.

Antony, meanwhile, married Caesar's lover, Cleopatra of Ptolemaic Egypt, intending to use wealthy Egypt as a base to dominate Rome. The ambitious Octavian built a power base of patronage and then launched a campaign against Antony. Another civil war broke out between Octavian on one hand and Antony and Cleopatra on the other. This culminated in the latter's defeat at Actium in 31 BC; Octavian's forces then chased Antony and Cleopatra to Alexandria, where they both committed suicide in 30 BC.

Octavian was granted a series of special powers, including sole imperium within the city of Rome, permanent consular powers, and credit for every Roman military victory. In 27, he was granted the use of the name "Augustus", from which point he is generally considered the first Roman emperor.[198]

The constitutional history of the Roman Republic began with the revolution that overthrew the monarchy in 509 BC and ended with constitutional reforms that transformed the Republic into what would effectively be the Roman Empire, in 27 BC. The Roman Republic's constitution was a constantly evolving, unwritten set of guidelines and principles passed down mainly through precedent, by which the government and its politics operated.[199]

The senate's authority derived from the senators' esteem and prestige.[200] This esteem and prestige were based on both precedent and custom, as well as the senators' calibre and reputation. The senate passed decrees called senatus consulta. These were officially "advice" from the senate to a magistrate, but in practice, the magistrates usually followed them.[201] Through the course of the middle republic and Rome's expansion, the senate became more dominant in the state: the only institution with the expertise to administer the empire effectively, it controlled state finances, assignment of magistrates, external affairs, and deployment of military forces. Also, a powerful religious body, it received reports of omens and directed Roman responses thereto.[202]

When its prerogatives started to be challenged in the 2nd century, the senate lost its customary preapproval for legislation. Moreover, after the precedent set in 121 BC with the killing of Gaius Gracchus, the senate claimed to assume the power to issue a senatus consultum ultimum: such decrees directed magistrates to take whatever actions were necessary to safeguard the state, irrespective of legality, and signalled the senate's willingness to support that magistrate if such actions were later challenged in the courts.[203]

Its members were usually appointed by censors, who ordinarily selected newly elected magistrates for membership in the senate, making the senate a partially elected body. Status was not hereditary and there were always some new men, though sons of former magistrates found it easier to be elected to the qualifying magistracies. During emergencies, a dictator could be appointed for the purpose of appointing senators (as was done after the Battle of Cannae). However, by the end of the republic men such as Caesar and the members of the Second Triumvirate usurped these powers for themselves.[202]

The legal status of Roman citizenship was limited and a vital prerequisite to possessing many important legal rights, such as the right to trial and appeal, marry, vote, hold office, enter binding contracts, and to special tax exemptions. An adult male citizen with the full complement of legal and political rights was called optimo iure (lit. 'having the greatest rights'). Citizens optimo iure could participate in assemblies that elected magistrates, enacted legislation, presided over trials in capital cases, declared war and peace, and forged or dissolved treaties. Assemblies were called comitia, in which all citizens optimo jure could vote, and concilia (sg. concilium), 'councils', for specific groups of citizens optimo jure, e.g., the plebeians.[204]

Citizens optimo jure were organised on the basis of and divided into centuries and tribes. Each century or tribe cast a collective vote. The centuriate assembly (comitia centuriata) was said to be traced from the Roman centuries of soldiers, and was usually presided over by a consul. The centuries voted, one at a time, until a measure received support from a majority. The centuriate assembly elected magistrates who had imperium (consuls and praetors). It also elected censors. Only the centuriate assembly could declare war and ratify the results of a census.[205] It served as the highest court of appeal in certain judicial cases.

The tribal assembly (comitia tributa) was presided over by a consul, and composed of 35 tribes. Once a measure received support from a majority of the tribes, voting ended. While it did not pass many laws, the tribal assembly did elect quaestors, curule aediles, and military tribunes.[206] The plebeian council (concilium plebis) was identical to the tribal assembly, but excluded the patricians. They elected their own officers, plebeian tribunes and plebeian aediles. Usually, a plebeian tribune would preside over the assembly. This assembly passed most laws and could act as a court of appeal.[207]

Each republican magistrate held certain constitutional powers. Each was assigned a provincia by the Senate. This was the scope of that particular office holder's authority. It could apply to a geographic area or to a particular responsibility or task.[208] The powers of a magistrate came from the people of Rome (both plebeians and patricians).[209] Imperium was held by both consuls and praetors. Strictly speaking, it was the authority to command a military force, but in reality, it carried broad authority in other public spheres, such as diplomacy and the justice system. In extreme cases, those with the imperium power could sentence Roman Citizens to death. All magistrates also had the power of coercitio (coercion). Magistrates used this to maintain public order by imposing punishment for crimes.[210] Magistrates also had both the power and the duty to look for omens. This power could also be used to obstruct political opponents.

One check on a magistrate's power was collega (collegiality). Each magisterial office was held concurrently by at least two people. Another such check was provocatio. While in Rome, all citizens were protected from coercion, by provocatio, an early form of due process. It was a precursor to habeas corpus. If any magistrate tried to use the powers of the state against a citizen, that citizen could appeal the magistrate's decision to a tribune. In addition, once a magistrate's one-year term of office expired, he would have to wait ten years before serving in that office again. This created problems for some consuls and praetors, and these magistrates occasionally had their imperium extended. In effect, they retained the powers of the office (as a promagistrate) without officially holding that office.[211]

In times of military emergency, a dictator was appointed for a term of six months.[212] Constitutional government was dissolved, and the dictator was the absolute master of the state. When the dictator's term ended, constitutional government was restored.

The censor was a magistrate in ancient Rome who was responsible for maintaining the census, supervising public morality, and overseeing certain aspects of the government's finances.[213] The power of the censor was absolute: no magistrate could oppose his decisions, and only another censor who succeeded him could cancel those decisions. The censor's regulation of public morality is the origin of the modern meaning of the words censor and censorship.[214] During the census, they could enroll citizens in the senate or purge them from the senate.[215]

The consuls of the Roman Republic were the highest-ranking ordinary magistrates. Each served for one year.[216] Consular powers included the kings' former imperium and appointment of new senators. Consuls had supreme power in both civil and military matters. While in the city of Rome, the consuls were the head of the Roman government. They presided over the senate and the assemblies. While abroad, each consul commanded an army.[217] His authority abroad was nearly absolute.

Since the tribunes were considered the embodiment of the plebeians, they were sacrosanct. Their sacrosanctity was enforced by a pledge the plebeians took to kill anyone who harmed or interfered with a tribune during his term of office. It was a capital offence to harm a tribune, disregard his veto, or otherwise interfere with him.[218]

Praetors administered civil law[219] and commanded provincial armies. Aediles were officers elected to conduct domestic affairs in Rome, such as managing public games and shows. The quaestors usually assisted the consuls in Rome, and the governors in the provinces. Their duties were often financial.

Rome's military secured Rome's territory and borders and helped to impose tribute on conquered peoples. Rome's armies had a formidable reputation; but Rome also "produced [its] share of incompetents"[220] and catastrophic defeats. Nevertheless, it was generally the fate of Rome's greatest enemies, such as Pyrrhus and Hannibal,[221] to win early battles but lose the war.

During this period, Roman soldiers seem to have been modelled after those of the Etruscans to the north,[222] who themselves are believed to have copied their style of warfare from the Greeks. Traditionally, the introduction of the phalanx formation into the Roman army is ascribed to the city's penultimate king, Servius Tullius (ruled 578–534).[223] The phalanx was effective in large, open spaces, but not on the hilly terrain of the central Italian peninsula. In the 4th century, the Romans replaced it with the more flexible manipular formation. This change is sometimes attributed to Marcus Furius Camillus and placed shortly after the Gallic invasion of 390; more likely, it was copied from Rome's Samnite enemies to the south.[224]

During this period, an army formation of around 5,000 men (of both heavy and light infantry) was known as a legion. Maniples were units of 120 men each drawn from a single infantry class. They were typically deployed into three discrete lines based on the three heavy infantry types:

The three infantry classes[228] may have retained some slight parallel to social divisions within Roman society, but at least officially the three lines were based upon age and experience rather than social class. Young, unproven men served in the first line, older men with some military experience in the second, and veteran troops of advanced age and experience in the third.

The heavy infantry of the maniples was supported by a number of light infantry and cavalry troops, typically 300 horsemen per manipular legion.[228] The cavalry was drawn primarily from the richest class of equestrians. There was an additional class of troops that followed the army without specific martial roles and was deployed to the rear of the third line. Its role in accompanying the army was primarily to supply any vacancies that might occur in the maniples. The light infantry consisted of 1,200 unarmoured skirmishing troops drawn from the youngest and lower social classes. They were armed with a sword, a small shield, and several light javelins.

Rome's military confederation with the other peoples of the Italian peninsula meant that half of its army was provided by the Socii. According to Polybius, Rome could draw on 770,000 men at the beginning of the Second Punic War, of which 700,000 were infantry and 70,000 met the requirements for cavalry.

A small navy had operated at a fairly low level after about 300, but it was massively upgraded about 40 years later, during the First Punic War. After a period of frenetic construction, the navy mushroomed to more than 400 ships on the Carthaginian ("Punic") pattern. Once completed, it could accommodate up to 100,000 sailors and embarked troops for battle. The navy thereafter declined in size.[229]

In 217, near the beginning of the Second Punic War, Rome was forced to effectively ignore its long-standing principle that its soldiers must be both citizens and property owners. Severe social stresses, population decline, and the greater collapse of the middle classes meant that the Roman state was forced to arm its soldiers at the expense of the state, which it had not had to do before. The distinction between the heavy infantry types began to blur, perhaps because the state was now assuming the responsibility of providing standard-issue equipment. In addition, the shortage of available manpower led to a greater burden upon Rome's allies for the provision of allied troops.[230] Eventually, the Romans were forced to begin hiring mercenaries to fight alongside the legions.[231]

The organisation of the legions evolved throughout the Republican period. In 107, all citizens, regardless of their wealth or social class, were made eligible for entry into the Roman army. The distinction among the three heavy infantry classes, which had already blurred, had collapsed into a single class of heavy legionary infantry. The heavy infantry legionaries were drawn from citizen stock, while non-citizens came to dominate the ranks of the light infantry. The army's higher-level officers and commanders were still drawn exclusively from the Roman aristocracy.[232] Unlike earlier in the Republic, legionaries were no longer fighting on a seasonal basis to protect their land. Instead, they received standard pay and were employed by the state on a fixed-term basis. As a consequence, military duty began to appeal most to the poorest sections of society, to whom a salaried pay was attractive.

The legions of the late Republic were almost entirely heavy infantry. The main legionary sub-unit was a cohort of approximately 480 infantrymen, further divided into six centuries of 80 men each.[233] Each century comprised 10 "tent groups" of eight men. Cavalry were used as scouts and dispatch riders rather than as battlefield forces.[234] Legions also contained a dedicated group of artillery crew of perhaps 60 men. Each legion was normally partnered with an approximately equal number of allied (non-Roman) troops.[235]

The army's most obvious deficiency lay in its shortage of cavalry, especially heavy cavalry.[236] Particularly in the East, Rome's slow-moving infantry legions were often confronted by fast-moving cavalry troops and found themselves at a tactical disadvantage.

After Rome's subjugation of the Mediterranean, its navy declined in size, although it underwent short-term upgrading and revitalisation in the late Republic to meet several new demands. Julius Caesar assembled a fleet to cross the English Channel and invade Britannia. Pompey raised a fleet to deal with the Cilician pirates who threatened Rome's Mediterranean trading routes. During the civil war that followed, as many as 1,000 ships were either constructed or pressed into service from Greek cities.[229]

Citizen families were headed by the family's oldest male, the pater familias, who was lawfully entitled to exercise complete authority (patria potestas) over family property and all family members.[237] Citizenship offered legal protection and rights, but citizens who offended Rome's traditional moral code could be declared infamous and lose certain legal and social privileges.[238] Citizenship was also taxable, and undischarged debt was potentially a capital offence. A form of limited, theoretically voluntary slavery (debt bondage, or nexum) allowed wealthy creditors to negotiate payment of debt through bonded service. Poor, landless citizens of the lowest class (proletarii) might contract their sons to a creditor, patron or third party employer to obtain an income or pay off family debts. Nexum was abolished only when slave labour became more readily available, most notably during the Punic wars.[239][240][241]

Slaves could be bought, sold, acquired through warfare, or born and raised in slavery. There were no legal limits on the slave-owner's power over them. A few slaves were freed by their owners, becoming freedmen and in some circumstances citizens too.[242] This degree of social mobility was unusual in the ancient world but itself limited; for example, freedmen were seen as permanently tainted, and their children could not become magistrates.[243] Freedmen could play notable roles in various crafts and trades, particularly those who had been manumitted by the upper classes.[244] Freed slaves and the master who freed them retained certain legal and moral mutual obligations.

At the other extreme were the senatorial families of the landowning nobility, both patrician and plebeian, bound by shifting allegiances and mutual competition. A plebiscite of 218 forbade senators and their sons to engage in substantial trade or money-lending.[245][246] A wealthy equestrian class emerged, not subject to the same trading constraints as senators.[247]

One of Rome's fundamental social and economic institutions was the client-patron relationship; its obligations were largely moral and social rather than legal, but permeated society, including in politics.

Citizen men and citizen women were expected to marry, produce as many children as possible, and improve—or at worst, conserve—their family's wealth, fortune, and public profile. Marriage offered opportunities for political alliance and social advancement. Patricians usually married in a form known as confarreatio, which transferred the bride from her father's legal control (manus) to that of her husband.[248] Patrician status could be inherited only through birth; an early law, introduced by the reactionary Decemviri but rescinded in 445, sought to prevent marriages between patricians and plebeians.[k] Among ordinary plebeians, different marriage forms offered married women considerable more freedom than their patrician counterparts, until manus marriage was replaced by free marriage, in which the wife remained under her absent father's legal authority, not her husband's.[249] Infant mortality was high. Towards the end of the Republic, the birthrate began to fall among the elite. Some wealthy, childless citizens resorted to adoption to provide male heirs for their estates and to forge political alliances. Adoption was subject to the senate's approval.

The Republic was created during a time of warfare, economic recession, food shortages, and plebeian debt. In wartime, plebeian farmers were liable to conscription. In peacetime, most depended on whatever cereal crops they could produce on small farming plots, allotted to them by the state, or by patrons. Soil fertility varied from place to place, and natural water sources were unevenly distributed. In good years, a small holder might trade a small surplus, to meet his family's needs, or to buy the armatures required for his military service. In other years, crop failure through soil exhaustion, adverse weather, disease or military incursions could lead to poverty, unsupported borrowing, and debt. Nobles invested much of their wealth in ever-larger, more efficient farming units, exploiting a range of soil conditions through mixed farming techniques. As farming was labour-intensive, and military conscription reduced the pool of available manpower, over time the wealthy became ever more reliant on the increasingly plentiful slave labour provided by successful military campaigns.[250][251][252] Large, well-managed agricultural estates helped provide for clients and dependents, support an urban family home, and fund the owner's public and military career, in the form of cash for bribes and security for loans. Later Roman moralists idealised farming as an intrinsically noble occupation.[253][254][255]

In law, land taken by conquest was ager publicus (public land). In practice, much of it was exploited by the nobility, using slaves rather than free labour. Rome's expansion via war and colonisation was at least partly driven by the land-hunger of displaced peasants, who must otherwise join the swelling, dependent population of urban plebs.[256] At the end of the second Punic War, Rome added the fertile ager Campanus, suitable for intense cultivation of vines, olives and cereals. Like the grain-fields of Sicily—seized after the same conflict—it was likely farmed extra-legally by leading landowners, using slave gangs. A portion of Sicily's grain harvest was sent to Rome as tribute, for redistribution by the aediles.[257][258] The urban plebs increasingly relied on firstly subsidised, then free grain.[259]

With the introduction of aqueducts (from 312), suburban market-farms could be supplied with runoff or waste aqueduct water. Perishable commodities such as flowers (for perfumes, and festival garlands), fresh grapes, vegetables and orchard fruits, and small livestock such as pigs and chickens, could be farmed close to municipal and urban markets.[251] Food surpluses, no matter how obtained, kept prices low.[260][261] Faced with increasing competition from provincial and allied grain suppliers, many Roman farmers turned to more profitable crops, especially grapes for wine production. By the late Republican era, Roman wine had been transformed from an indifferent local product for local consumption to a major domestic and export commodity, with some renowned, costly and collectable vintages.[262][263]

Roman writers have little to say about large-scale stock-breeding but make passing references to its profitability. Drummond speculates that this focus on agriculture rather than livestock might reflect elite preoccupations with historical grain famines, or long-standing competition between agriculturalists and pastoralists.[264][265] Though meat and hides were valuable by products of stock-raising, cattle were primarily reared to pull carts and ploughs, and sheep were bred for their wool, the mainstay of the Roman clothing industry. Horses, mules and donkeys were bred as civil and military transport. Pigs bred prolifically and could be raised at little cost by any small farmer with rights to pannage. Their central dietary role is reflected by their use as sacrifices in cults and funerals.[264]

Republican Rome's religious practices harked back to Rome's quasi-mythical history.[267][268] Romulus, a son of Mars, founded Rome after Jupiter granted him favourable bird-signs regarding the site.[269] Numa Pompilius, Rome's second king, had established its basic religious and political institutions after direct instructions from the gods, given through augury, dreams and oracle. Each king thereafter was credited with some form of divinely approved innovation, adaptation or reform.[l] An Imperial-era source claims that the Republic's first consul, Brutus, effectively abolished human sacrifice to the goddess Mania, instituted by the last king, Tarquinius.[m]

Romans acknowledged the existence of innumerable deities who controlled the natural world and human affairs. The Roman state's well-being depended on its state deities, whose opinions and will could be discerned by priests and magistrates, trained in augury, haruspicy, oracles and the interpretation of omens. The gods were thought to communicate their wrath (ira deorum) through prodigies (unnatural or aberrant phenomena).

Individuals, occupations and locations had their own protective tutelary deity, or several. Each was associated with a particular, highly prescriptive form of prayer and sacrifice. Piety (pietas) was the correct, dutiful and timely performance of such actions. The well-being of each Roman household was thought to depend on daily cult to its Lares and Penates (guardian deities, or spirits), ancestors, and the divine generative essence embodied within its pater familias. A family which neglected its religious responsibilities could not expect to prosper.[270]

Roman religious authorities were unconcerned with personal beliefs or privately funded cults unless they offended natural or divine laws or undermined the mos maiorum (roughly, "the way of the ancestors"); the relationship between gods and mortals should be sober, contractual, and of mutual benefit. Undignified grovelling, excessive enthusiasm (superstitio) and secretive practices were "weak-minded" and morally suspect.[271] Magical practices were officially banned, as attempts to subvert the will of the gods for personal gain but were probably common among all classes. Private cult organisations that seemed to threaten Rome's political and priestly hierarchy were investigated by the Senate, with advice from the priestly colleges. The Republic's most notable religious suppression was that of the Bacchanalia, a widespread, unofficial, enthusiastic cult to the Greek wine-god Bacchus. The cult organisation was ferociously suppressed, and its deity was absorbed within the official cult to Rome's own wine god, Liber.[272] The official recognition, adoption and supervision of foreign deities and practices had been an important unitary feature in Rome's territorial expansion and dominance since the days of the kings.[267][268]

With the abolition of monarchy, some of its sacral duties were shared by the consuls, while others passed to a Republican rex sacrorum ("king of the sacred rites"), a patrician "king", elected for life, with great prestige but no executive or kingly powers.[273] Rome had no specifically priestly class or caste. As every family's pater familias was responsible for his family's cult activities, he was effectively the senior priest of his own household. In the early Republic, the patricians, as "fathers" to the Roman people, claimed the right of seniority to lead and control the state's relationship with the divine. Patrician families, in particular the Cornelii, Postumii and Valerii, monopolised the leading state priesthoods. The patrician Flamen Dialis employed the "greater auspices" (auspicia maiora) to consult with Jupiter on significant matters of state.

Twelve "lesser flaminates" (Flamines minores) were open to plebeians or reserved to them. They included a Flamen Cerealis in service of Ceres, goddess of grain and growth, and protector of plebeian laws and tribunes.[274] The priesthoods of local urban and rustic Compitalia street festivals, dedicated to the lares of local communities, were open to freedmen and slaves.[275].

The Lex Ogulnia (300) gave patricians and plebeians more-or-less equal representation in the augural and pontifical colleges;[39] other important priesthoods, such as the Quindecimviri ("The Fifteen"), and the epulones[n] were opened to any member of the senatorial class.[278] To restrain the accumulation and potential abuse of priestly powers, each gens was permitted one priesthood at a time, and the censors monitored the senators' religious activities.[278] Magistrates who held an augurate could claim divine authority for their position and policies.[279] In the late Republic, augury came under the control of the pontifices, whose powers were increasingly woven into the civil and military cursus honorum. Eventually, the office of pontifex maximus became a de facto consular prerogative.[280]

Some cults may have been exclusively female; for example, the rites of the Good Goddess (Bona Dea). Towards the end of the second Punic War, Rome rewarded priestesses of Demeter from Graeca Magna with Roman citizenship for training respectable, leading matrons as sacerdotes of "Greek rites" to Ceres.[281] Every matron of a family (the wife of its pater familias) had a religious duty to maintain the household fire, which was considered an extension of Vesta's sacred fire, tended in perpetuity by the chaste Vestal Virgins. The Vestals also made the sacrificial mola salsa employed in many State rituals, and represent an essential link between domestic and state religion. Rome's survival was thought to depend on their sacred status and ritual purity.[282]

Rome's major public temples were within the city's sacred, augural boundary (pomerium), which had supposedly been marked out by Romulus, with Jupiter's approval. The Temple of Jupiter Optimus Maximus ("Jupiter, Best and Greatest") stood on the Capitoline Hill. Among the settled areas outside the pomerium was the nearby Aventine Hill. It was traditionally associated with Romulus's unfortunate twin, Remus, and in later history with the Latins, and the Roman plebs. The Aventine seems to have functioned as a place for the introduction of "foreign" deities.[274] In 392, Camillus established a temple there to Juno Regina, Etruscan Veii's protective goddess. Later introductions include Summanus, c. 278, Vortumnus c. 264, and at some time before the end of the 3rd century, Minerva.[284][o] While Ceres's Aventine temple was most likely built at patrician expense, to mollify the plebs, the patricians brought the Magna Mater ("Great mother of the Gods") to Rome as their own "Trojan" ancestral goddess, and installed her on the Palatine.[285]

Romulus was said to have pitched his augural tent atop the Palatine. Beneath its southern slopes ran the sacred way, next to the former palace of the kings (Regia), the House of the Vestals and Temple of Vesta. Close by were the Lupercal shrine and the cave where Romulus and Remus were said to have been suckled by the she-wolf. On the flat area between the Aventine and Palatine was the Circus Maximus, which hosted chariot races and religious games. Its several shrines and temples included those to Rome's indigenous sun god, Sol, the moon-goddess Luna, the grain-storage god, Consus, and the obscure goddess Murcia.

Whereas Romans marked the passage of years with the names of their ruling consuls, their calendars marked the anniversaries of religious foundations to particular deities, the days when official business was permitted (fas), and those when it was not (nefas). The Romans observed an eight-day week; law courts were closed and markets were held on the ninth day. Each month was presided over by a particular, usually major deity. The oldest calendars were lunar.

Before any campaign or battle, Roman commanders took auspices, or haruspices, to seek the gods' opinion regarding the likely outcome. Military success was achieved through a combination of personal and collective virtus (roughly, "manly virtue") and divine will. Triumphal generals dressed as Jupiter Capitolinus and laid their victor's laurels at his feet. Religious negligence, or lack of virtus, provoked divine wrath and led to military disaster.[287][288] Military oaths dedicated the oath-takers life to Rome's gods and people; defeated soldiers were expected to take their own lives, rather than survive as captives. Examples of devotio, as performed by the Decii Mures, in which soldiers offered and gave their lives to the Di inferi (gods of the underworld) in exchange for Roman victory were celebrated as the highest good.

Life in the Roman Republic revolved around the city of Rome. The most important governing, administrative and religious institutions were concentrated at its heart, on and around the Capitoline and Palatine Hills. The city rapidly outgrew its original sacred boundary (pomerium), and its first city walls. Rome's first aqueduct (312), built during the Punic wars crisis, provided a plentiful, clean water supply. The building of further aqueducts led to the city's expansion and the establishment of public baths (thermae) as a central feature of Roman culture.[p] The city also had several theatres,[289] gymnasiums, and many taverns and brothels. Living space was at a premium. Some ordinary citizens and freedmen of middling income might live in modest houses but most of the population lived in apartment blocks (insulae, literally "islands"), where the better-off might rent an entire ground floor, and the poorest a single, possibly windowless room at the top, with few or no amenities. Nobles and rich patrons lived in spacious, well-appointed town houses; they were expected to keep "open house" for their peers and clients. A semi-public atrium typically functioned as a meeting-space, and a vehicle for display of wealth, artistic taste, and religious piety. Noble atria were also display areas for ancestor-masks (imagines).[q]

Most Roman towns and cities had a forum and temples, as did the city of Rome itself. Aqueducts brought water to urban centres.[290] Landlords generally resided in cities and left their estates in the care of farm managers.

The basic Roman garment was the Greek-style tunic, worn knee-length and short-sleeved (or sleeveless) for men and boys, and ankle-length and long-sleeved for women and girls. The toga was distinctively Roman and became a mark of male citizenship, a statement of social degree.[292] Convention also dictated the type, colour and style of calcei (ankle-boots) appropriate to each level of male citizenship.

The whitest, most voluminous togas were worn by the senatorial class. High-ranking magistrates, priests, and citizen's children were entitled to a purple-bordered toga praetexta. Triumphal generals wore an all-purple, gold-embroidered toga picta, associated with the image of Jupiter and Rome's former kings – but only for a single day; Republican mores simultaneously fostered competitive display and attempted its containment, to preserve at least a notional equality between peers and reduce the potential threats of class envy.[293] Most Roman citizens, particularly the lower class of plebs, opted for more comfortable and practical garments, such as tunics and cloaks.

Luxurious and highly coloured clothing had always been available to those who could afford it, particularly women of the leisured classes. There is material evidence for cloth-of-gold (lamé) as early as the 7th century.[294] By the 3rd century, significant quantities of raw silk were being imported from Han China.[295] Tyrian purple, a quasi-sacred colour, was officially reserved for the border of the toga praetexta and for the solid purple toga picta.[296][297]

For most Romans, even the cheapest linen or woolen clothing represented a major expense. Worn clothing was passed down the social scale until it fell to rags, and these were used for patchwork.[298] Wool and linen were the mainstays of Roman clothing, idealised by moralists as simple and frugal.[299] For most women, the preparation and weaving of wool were part of daily housekeeping, either for family use or for sale. In traditionalist, wealthy households, the family's spindles and looms were positioned in the semi-public reception area (atrium), so the mater familias and her familia could demonstrate their industry and frugality: a largely symbolic and moral activity for those of their class, rather than practical necessity.[r]

As the Republic wore on, its trade, territories and wealth increased. Roman conservatives deplored the apparent erosion of traditional, class-based dress distinctions, and an increasing Roman appetite for luxurious fabrics and exotic "foreign" styles among all classes, including their own. Towards the end of the Republic, the ultra-traditionalist Cato the Younger publicly protested the self-indulgence of his peers, and the loss of Republican "manly virtues", by wearing a "skimpy" dark woolen toga, without tunic or footwear.[299][s]

Modern study of the dietary habits during the Republic are hampered by various factors. Few writings have survived, and because different components of their diet are more or less likely to be preserved, the archaeological record cannot be relied on.[300]
In the early Republic, the main meal (cena) essentially consisted of a kind of porridge, the puls.[301] The simplest kind would be made from emmer, water, salt and fat. The wealthy commonly ate their puls with eggs, cheese, and honey, and it was also occasionally served with meat or fish. Over the course of the Republican period, the cena developed into two courses: the main course and a dessert with fruit and seafood (e.g. molluscs or shrimp). By the late Republic, it was usual for the meal to be served in three parts: an appetiser (gustatio), main course (primae mensae), and dessert (secundae mensae).

During the mid-to-later Republic, wine was increasingly treated as a necessity rather than a luxury. In Ancient Rome, wine was normally mixed with water immediately before drinking, since the fermentation was not controlled and the alcohol proof was high. Sour wine mixed with water and herbs (posca) was a popular drink for the lower classes and a staple part of the Roman soldier's ration.[302] Beer (cerevisia) was known but considered vulgar, and was associated with barbarians.[303][304]

From 123 BC, a ration of unmilled wheat (as much as 33 kg), known as the frumentatio, was distributed to as many as 200,000 people every month by the Roman state.[305]

Rome's original native language was early Latin, the language of the Italic Latins. Most surviving Latin literature is written in Classical Latin, a highly stylised and polished literary language which developed from early and vernacular spoken Latin, from the 1st century. Most Latin speakers used Vulgar Latin, which significantly differed from Classical Latin in grammar, vocabulary, and eventually pronunciation.[306]

Following various military conquests in the Greek East, Romans adapted a number of Greek educational precepts to their own fledgling system.[307] Strenuous, disciplined physical training helped prepare boys of citizen class for their eventual citizenship and a military career. Girls generally received instruction[308] from their mothers in the art of spinning, weaving, and sewing. Schooling of a more formal sort began around 200. Education began at the age of around six, and in the next six to seven years, boys and girls were expected to learn reading, writing and counting. By the age of twelve, they would be learning Latin, Greek, grammar and literature, followed by training for public speaking. Effective oratory and good Latin were highly valued among the elite, and were essential to a career in law or politics.[309][310]

In the 3rd century, Greek art taken as the spoils of war became popular, and many Roman homes were decorated with landscapes by Greek artists.[311]

Over time, Roman architecture was modified as their urban requirements changed, and the civil engineering and building construction technology became developed and refined. Factors such as wealth and high population densities in cities forced the ancient Romans to discover new architectural solutions of their own. The use of vaults and arches, together with a sound knowledge of building materials, enabled them to achieve unprecedented successes in the construction of imposing infrastructure for public use. These were reproduced at a smaller scale in the most important towns and cities in the Roman Republic. The administrative structure and wealth of the Empire made possible very large projects even in locations remote from the main centres.[312]

Early Roman literature was influenced heavily by Greek authors. From the mid-Republic, Roman authors followed Greek models, to produce free-verse and verse-form plays and other in Latin; for example, Livius Andronicus wrote tragedies and comedies. The earliest Latin works to have survived intact are the comedies of Plautus, written during the mid-Republic. Works of well-known, popular playwrights were sometimes commissioned for performance at religious festivals; many of these were satyr plays, based on Greek models and Greek myths. The poet Naevius may be said to have written the first Roman epic poem, although Ennius was the first Roman poet to write an epic in an adapted Latin hexameter. However, only fragments of Ennius' epic, the Annales, have survived, yet both Naevius and Ennius influenced later Latin epic, especially Virgil's Aeneid. Lucretius, in his On the Nature of Things, explicated the tenets of Epicurean philosophy.

The politician, poet and philosopher Cicero's literary output was remarkably prolific and so influential on contemporary and later literature that the period from 83 to 43 BC has been called the "Age of Cicero". His oratory continues to influence modern speakers, while his philosophical works, particularly Cicero's Latin adaptations of Greek Platonic and Epicurean works, influenced many later philosophers.[313][314] Other prominent writers of this period include the grammarian and historian of religion Varro, the politician, general and military commentator Julius Caesar, the historian Sallust and the love poet Catullus.

The Campus Martius was Rome's track and field playground, where youth assembled to play and exercise, which included jumping, wrestling, boxing and racing.[315] Equestrian sports, throwing, and swimming were also preferred physical activities.[316] In the countryside, pastimes included fishing and hunting.[317] Board games played in Rome included dice (Tesserae or Tali), Roman chess (Latrunculi), Roman checkers (Calculi), Tic-tac-toe (Terni Lapilli), and Ludus duodecim scriptorum and Tabula, predecessors of backgammon.[318] Other activities included chariot races, and musical and theatrical performances.[319]Samuel Babson Fuld (born November 20, 1981) is an American former professional baseball outfielder and current executive for the Philadelphia Phillies organization of Major League Baseball (MLB), where he most recently served as the team's general manager. He played eight seasons in MLB for the Chicago Cubs, Tampa Bay Rays, Oakland Athletics, and Minnesota Twins.

He began his baseball career by twice batting .600 in high school, during which time Baseball America ranked him 19th in the country. Fuld played college baseball at Stanford University. There, he was a two-time All-American, set the school record for career runs scored, and established the College World Series record for career hits.

Fuld was selected by the Chicago Cubs in the 2004 Major League Baseball draft. He was an All Star two years later in the Florida State League. A year after that, Fuld was voted the Most Valuable Player in the Arizona Fall League. In the minor leagues—as a result of his defensive play—he was referred to as "a crash test dummy with a death wish", a "human wrecking ball act", a "wall magnet", and a "manager's dream and a trainer's worst nightmare".

Fuld made his major league debut with the Cubs in 2007. He became a fan favorite for his defense and his tendency to run into outfield walls while making catches. Fuld batted .299 in his longest stint with the Cubs, but appeared only in late-season call-ups over three years. After the 2010 season, he was traded to the Tampa Bay Rays. Fuld made the Rays' 2011 opening day roster, and won the jobs of starting left fielder and leadoff hitter by mid-April. Due to early-season heroics, including a "Superman-esque" catch,  he was dubbed "Superman", "Super Sam", and "The Legendary Sam Fuld". Fuld's catch was put to Superman-theme music in a YouTube video, and tweets about him went viral. In late April, Fuld led the American League (AL) in both batting average and steals. He played with the Rays through 2013. In 2014–2015, Fuld played for the Oakland A's (for two stints) and Minnesota Twins.

Fuld played center field and batted lead-off for Team Israel at the 2017 World Baseball Classic.[1]

Fuld was born in Durham, New Hampshire.[2][3][4] His father is Jewish and his mother is Catholic, and he has said that he was "kind of raised celebrating both" religions' holidays.[5][6][7] He weighed 10 pounds (4.5 kg) at birth, so big he was nicknamed "Cool Papa Sam."[8] He is the son of Kenneth Fuld, former Dean of the College of Liberal Arts and Professor of Psychology at the University of New Hampshire, and Amanda Merrill, a former New Hampshire State Senator.[9][10][11] He is also a second cousin, once removed, of former Lehman Brothers CEO Dick Fuld.[12]

He got his start playing baseball hitting plastic wiffle balls pitched by his grandmother when he was three years old.[13][14][15] As a young child, Fuld carried around a copy of The Complete Baseball Handbook instead of a security blanket.[16][14][15] "He was only 5 or 6 and he was already computing batting averages and ERAs", his father said. "He'd sit in the bathtub, and I'd say 'If a guy goes 17-for-38, what's his batting average?' What struck me is that he'd perform these operations in very creative ways–not just that he got the right answer, but his methodology, adding in a factor and then dividing by 10, etc. I'd watch him and say 'wow,' just like I said 'wow' when he used to hit."[14]

Fuld attended Berwick Academy as an eighth grader, during which time he made the high school varsity baseball team, and the University of New Hampshire's baseball coach said that he had the best batting swing of any player in the state.[8][16] He was the team's MVP, and a league All Star.[8] He then transferred to Phillips Exeter Academy in New Hampshire, where he played baseball and soccer, and ran track.[17] Because of the climate, the league played a short baseball season, and some of the games were played as it snowed.[17][18]  Fuld batted .613 as a freshman, and .489 as a sophomore with 11 steals. He led his club to a league title as a junior in 1999, as he batted .600 with 9 steals. As a senior, he hit .550 with 6 homers, 12 RBIs, and 13 stolen bases.[13] He was a three-time team captain and four-time MVP of the varsity baseball team.[13]

He was named a 2000 Pre-season First Team All-American by Baseball America, Collegiate Baseball, USA Today, and Fox Sports.[13] Fuld was also listed 19th among the 100 Top High School Prospects of 2000 by Baseball America, and selected the New Hampshire 2000 Gatorade High School Player of the Year.[13][19][20] In addition he was a four-time Central New England Prep School Baseball League All-Conference player.[13]

Fuld also played from 1998 to 2000 with the Dover Post 8 American Legion team.  With them, he earned 2000 New Hampshire State Tournament MVP honors. He also led his club to a state championship.[17][13]

"I had pretty much made up my mind, both from my perspective and my parents' perspective, that college would be the best option for me. I really value education and I know if you sign out of high school there is always the opportunity to go back and get your degree, but it's tough to go back for four years when you're done playing ball. And I wanted to get my degree."[21]

Ninety-four colleges approached Fuld after high school, and he chose to attend Stanford University.  He was an economics major there, graduating in 2004 with a 3.15 grade point average.[17][2][22] He was a two-time All-American and a four-year starter in center field for the Cardinal, playing alongside future major leaguers Carlos Quentin and Jed Lowrie.[8][22][23][24]

In 2001, as a freshman, he batted .357 as he established himself as the team's leadoff hitter. Fuld was fifth in the Pacific-10 (Pac-10) conference in runs scored (56), sixth in walks (32), and ninth in hits (81). In the post-season he hit .596. He earned NCBWA Third Team All-American honors, and was named a Baseball America Second Team Freshman All-American, All-College World Series, All-NCAA Regional, All-Pac-10, and a Collegiate Baseball Honorable Mention Freshman All-American.[13]

As a sophomore in 2002, he led the Pac-10 in hits (110), breaking Stanford's single-season record, while batting .375, third-best in the conference.[25] Fuld also led the conference in total bases (162), was third in runs scored (67) and doubles (20), and fifth in triples (4). He was named a First-Team (ABCA, Baseball America, Baseball Weekly) and Third-Team (Collegiate Baseball, NCBWA) All-American.  He was also named a Jewish Sports Review College Baseball First Team All-American, along with future major leaguers Craig Breslow and Adam Greenberg.[26] In addition, Fuld earned the Stanford Jack Shepard Memorial Award and Come Through Award. He batted .421 with two homers in four CWS games, earning a spot on the All-College World Series Team for the second straight year.[13] Fuld also played with Team USA in the summers of 2001 and 2002.[13]

In 2003, Fuld was named a First-Team (NCBWA) and Third-Team (Baseball America) Pre-Season All-American. As a junior, he had 83 regular season runs, tying the school record (# 1 in the Pac-10). He hit .321 with 35 RBIs and 10 steals in 10 attempts, leading the conference in triples (9) while coming in fifth in hits (97), eighth in doubles (18) and total bases (145), and ninth in walks (34). In the post-season his 24 career hits broke the College World Series record of 23 set by Keith Moreland in 1973–75.[27][28] And commenting on his defense, Stanford coach Mark Marquess said: "If it's in the ballpark, he's going to catch it. He's the premier center fielder in college baseball."[29] He earned All-Pac-10 honors for the third straight season. Reflecting on his college career, Fuld said: "I think one of the things you take away from playing under Coach Marquess is whatever you do, be it baseball or anything else, you do it well and you do it with passion. You do it with excitement because, really, that's the way to go about things. Not only in sports, but beyond sports."[30]

Fuld was drafted in the 24th round (703rd overall) of the 2003 draft by the Chicago Cubs, but did not sign.[31] Marquess thought that Fuld was drafted relatively late because of his size. At 5 ft 10 in, he is not as tall as most major league outfielders.[29] But Fuld said of the Cubs' bid: "It was a generous offer. It was a tough choice."[29]  He then led the Cape Cod League with a .450 on base percentage and batted .361 (second in the league) with 14 RBIs and 10 stolen bases in the summer of 2003 for the Hyannis Mets, and was voted a Cape Cod League All Star.[13][32][33]

Before the 2004 season, Fuld was named a First-Team (NCBWA), Second-Team (Louisville Slugger),[34] and Third Team (Baseball America, Collegiate Baseball) Pre-season All-American, and an Honorable Mention Pac-10 All-Academic.[13] While playing the outfield at the end of his last college season, he broke his shoulder, tore his labrum, and partially tore his rotator cuff.[14][35] For the season, he received Honorable Mention to the Pac-10 All-Academic Team and All-Conference Team[36][37]

When he became Stanford's and the Pac-10's all-time leader in runs scored (268), Fuld said: "It means a lot to break the record. Obviously, a record like that is more of a tribute to the teammates I've had over the last four years."[38] When he graduated from Stanford, he ranked among the school's all-time top 10 in hits (353; # 2), triples (16; # 3T), and doubles (58; # 6T).[13]

Fuld said: "I want to try my hand. It's been a lifelong dream, really, to play professional baseball. I just love it too much not to give it a shot."[39] The Cubs drafted him for a second time, in the 10th round (306th overall) of the 2004 draft, and this time Fuld signed, for a $25,000 signing bonus.[13][40] When he made it to the major leagues, he became the 78th former Stanford Cardinal to do so.[41] During the baseball off-season, Fuld returned to Stanford to pursue a master's degree in statistics.[14]

The torn labrum injury he incurred in 2004 required surgery and a year of physiotherapy.[14] During that time he read Michael Lewis's book Moneyball and got an internship position with STATS, Inc. of Chicago.[14][42][43] "I was one of their reporters, which meant that I looked at game video and plotted the 'TVL'–type, velocity, and location–of every pitch", Fuld said. "They have this grid where you click on exactly where the ball crosses the plate. Play the tape, pause, and repeat."[14] He also began seeking out stats that were not already kept. "There's so many statistics out there that I thought 'There's no stats on foul balls,' so I picked a few players and started tracking them, thinking I'd find something", Fuld said.[14] But his bag that contained the notebook with all his stats was stolen.[14]

Fuld's minor league career began in 2005, when he hit .300 with a .377 OBP and 18 stolen bases in 443 at bats for the Single-A Peoria Chiefs of the Midwest League.[8] He also had a 17-game hit streak, and turned in 7 outfield assists from center field.[44]

Fuld batted .300 with a .378 OBP and 22 steals in 353 at bats for the High-A Daytona Cubs of the Florida State League in 2006, but he missed part of the season with a hip injury.[45] He had surgery for a sports hernia in the off-season.[46] He was named to the league All-Star team.[47][48]

In 2007, Fuld batted .291 with a .371 on-base percentage, in 282 at bats, as he began the season with the Tennessee Smokies in the Double-A Southern League.[49] "He knows how to play the game, when to take a pitch, when to work the count, and when to go ahead and swing away and juice the ball", said his Tennessee manager, Pat Listach. "He's strong enough that he can hit the ball out of the yard occasionally. He's a gap, line drive type hitter."[46] He was an efficient leadoff batter for Tennessee, with more walks (41) than strikeouts (38). He was touted by Baseball America as having the best strike zone judgment in the organization.[50][51]  "He's a very intelligent player", added Listach.[46] Fuld was also touted by Listach as having the best outfield skills on the team, as he led the Southern League with 13 assists from the outfield.[46][51][52]  In August, Fuld was promoted to the Iowa Cubs in the Triple-A Pacific Coast League.  There, he batted .269, with a .397 on-base percentage.

Fuld played for the Mesa Solar Sox in the Arizona Fall League in the Fall of 2007. On October 30, he was named AFL Player of the Week, after hitting .526.[53] In 29 games with the Solar Sox, Fuld led the league in batting average (.402), hits (43), doubles (11), extra-base hits (16), total bases (67), obp (.492), slugging percentage (.626), and OPS (1.118), and was 3rd in the league in runs (20) and walks (17), 4th in stolen bases (10), and 5th in triples (2).[54][55][56] He batted .462 with runners in scoring position, and .500 against lefties.[56]  Fuld was named the 2007 Most Valuable Player of the Arizona Fall League, after being the season's dominant player.[50][57][58] He also was named to the AFL Top Prospects Team.[59]

Fuld also won the Dernell Stenson Award for Leadership, becoming the first player to win both awards.[44][50][60]  He donated the money he raised in an auction to the Juvenile Diabetes Research Foundation.[50][61] "This kid plays the game like it's supposed to be played", said Mesa Solar Sox manager Dave Clark.[21]

"That guy will run through anything."[62]

In 2008, Fuld started the season with the Triple-A Iowa Cubs. There, due to his style of defensive play, he was referred to as "a crash test dummy with a death wish", a "human wrecking ball act", a "wall magnet", and a "manager's dream and a trainer's worst nightmare."[62]

Fuld began the 2008 season at Triple-A Iowa, but was demoted to the Double-A Tennessee Smokies in May after struggling at the plate due to a right thumb injury. After hitting no higher than .245 in May, June, and July, he batted .345/.424/.445 in August. Fuld averaged only 1 strikeout per 9.9 at bats, good for fifth-best in the Southern League.[63]

Fuld had a "big winter" playing winter ball in Venezuela on the Tigres de Aragua,[64][65] who he helped lead to a championship. He hit leadoff for Aragua, while batting .322 with 5 triples (leading the league), 36 walks and 43 runs (2nd), 16 doubles (5th), a .425 on-base percentage (7th), and a .938 OPS (8th).[66][67][68]  For his 2008–09 winter performance, including walking nearly twice as much as he struck out, he was elected to the Baseball America All Winter League Team.[69]

Fuld began 2009 at Iowa, and hit .286 with 20 stolen bases and 8 triples in 73 games, including .326 versus left-handed pitchers, primarily playing center field until he was called up on June 30.[70][71][72] He walked 32 times compared to 22 strikeouts in 319 plate appearances, and had a .309 batting average with runners in scoring position.[73]

"Fuld is a great baserunner, excellent defender, a tough kid. He's a run-through-the-wall-for-you guy."[74]

In September 2007, the Chicago Cubs called Fuld up to the major league team.[75] He was the 37th ballplayer from New Hampshire to make it to the major leagues.[8]  He made his debut September 5 against the Los Angeles Dodgers, as a defensive replacement.[76]  "I have so much respect for players like (Cubs first baseman) Derrek Lee", Fuld said. "They're great players. But to meet them and actually play with them and contribute and help them win is kind of a surreal experience. It doesn't seem too long ago that I was going to Fenway (Park in Boston) and idolizing these guys. I feel like a fan at times. I have to remind myself I'm a player."[77]

That month he was "forced" to sing Stanford's fight song on the team bus, as part of his rookie treatment.[78] Later in the month, he was hazed after a game—made to walk from the park in Cincinnati to the team's downtown hotel in a Batgirl getup, accompanied by Félix Pie in pink baby pajamas, Carmen Pignatiello (as Supergirl), and Kevin Hart (as Wonder Woman).[79]  After peeking at his outfit, before he had to don it, Fuld said: "I think it was Batgirl. I was actually looking forward to it, believe it or not. [I didn't try it on] I'm pretty sure I would've looked good in it. Black's my color. I never did the frat thing in college, so this would kind of be like that."[80]  In mid-September, as manager Lou Piniella had still not given Fuld an at bat, someone on the team hung a "Moonlight Graham" sign in Fuld's locker.[81]

Against the Pittsburgh Pirates on September 22, 2007, playing right field as a defensive replacement, Fuld raced back and made a "spectacular", "daredevil", catch just left of the 368-foot-marker in right-center field at Wrigley Field, slamming into the ivy-covered brick wall and robbing the Pirates' Nyjer Morgan of extra bases.[82][83] Fuld then bounced off the bricks and threw the ball to first base, doubling off a Pirates baserunner (see video of Fuld catch and throw to first).[84][85][86][87][88] "That was amazing", said Fuld. "They were going crazy."[89][90] General Manager Jim Hendry said: "That's as good a play as I've seen by a Cubs player at Wrigley Field since I've been here."[91] The play later made ESPN on Baseball Tonight as the # 1 web gem of the day. After the season, Paul Sullivan of the Chicago Tribune cited it as the Cubs' "Play of the Year."[92] Many Cub fans refer to it as: "The Catch."[14]

In 2008, Fuld pushed Pie for the center field spot during spring training.  Piniella said that Pie was only "a head or nose in front" of Fuld.[93][94] Hendry observed:  "[Fuld] can run. He's a tremendous defensive player."[95]  Hendry also indicated that Fuld was among the club's untouchables, saying: "We're not going to trade [Fuld]. It's pretty simple."[57]  In late March, however, Piniella chose Pie to open the season as the starting center fielder over Fuld, and sent Fuld down to the minor leagues.[96][97]

Fuld played for the Cubs in spring training in 2009, but was sent down to their AAA team in late March to work on stealing bases.[98]  He was called back up on June 30.[99]  Hendry said: "Sammy has been really hot. This will let Lou (manager Piniella) mix and match with outfield defense until Reed (Johnson) comes back."[100]

On July 1, 2009, Fuld recorded his first major league hit in his first major league start of the season and the second start of his career.[101] Leading off the game Fuld hit a double against Virgil Vasquez of the Pittsburgh Pirates at PNC Park.[102] In the fourth inning Fuld made a sliding catch of a sinking liner (see video of Fuld's sliding catch robbing Vazquez of a hit), and in the fifth inning he fielded a single and threw out Jack Wilson at home (see video).[103] "It was huge", said Cubs starter Randy Wells. "Sam Fuld, man, he gets to so many balls. He made a [heck] of a play throwing that guy out—the perfect throw."[102]  The following month, he crashed into the left field wall while making a sliding catch in a game against the Dodgers (see video of Fuld making wall-crashing catch).[104]

Fuld hit the first home run of his major league career in the final game of the Cubs' 2009 season, on October 4 at Wrigley Field.[105] He finished the season batting .299, and with the highest on-base percentage (.409) of any Cubs player with 100 or more plate appearances.[106][107]

Carrie Muskat, the Cubs' MLB.com beat reporter, wrote on March 1, 2010, that: "I can see Fuld making the Cubs' Opening Day roster... Fuld has shown he can handle the part-time workload, can play all three positions, and can be used as a defensive sub or pinch-runner."[108] On March 30, however, the Cubs sent him down to AAA Iowa.[109]  Piniella remarked: "The decisions weren't easy."[109]  He was called up to Chicago for the first time in the 2010 season on August 19, and ended the season with only a handful of at bats.[106][110]

Baseball writer Rob Neyer, noting that this followed Fuld batting .299 with a .409 on-base percentage for the Cubs, wrote, "don't you think you could find a place for a fast guy who gets on base and plays great defense?  The Cubs did find a place for him.  No, not Wrigley Field. Not Heaven, either. Iowa. Again. Where Fuld posted a .383 on-base percentage.... I'm telling you, there are worse fourth outfielders on half the teams in the majors right now."[111] A writer for the American Spectator mused:  "what were the Cubs thinking ...?  ... perhaps decision makers ... didn't ... pencil him into lineup cards because they were prejudiced against players of Fuld's stature. But those who don't believe guys of Fuld's size can be solid major league players should be sentenced to sit in the corner under the dunce hat, and read Joe Morgan's statistics over and over."[112]

Fuld's play with the Cubs was limited to late-season call-ups over three years, including just 40 starts.[8]  After the 2010 season, they traded him to the Tampa Bay Rays.[16][113][114][115]  He was traded with minor leaguers Hak-Ju Lee, Brandon Guyer, Robinson Chirinos, and Chris Archer for starting pitcher Matt Garza, outfielder Fernando Perez, and reliever Zac Rosscup.[116][117]


Manager Joe Maddon noted Fuld's outstanding walks-to-strikeouts ratio, which Maddon referred to as "freaky-weird":  325 walks vs. 272 strikeouts in his professional career.[118]  Maddon said: "He's a major-league baseball player right now.... He's been needing opportunity; he's probably going to get the opportunity here right now."[119]  He also observed that Fuld was a:  
very, very good defender. He has really great instincts out there, and a fine arm. He works excellent at bats.... If somebody were to get hurt, this guy could fill in on a regular basis for a while. He's not going to be overwhelmed by anything. Great makeup.... He's a pretty tough kid.[120]
Rays hitting coach Derek Shelton noted that: "[Fuld's] contact ratio is off the charts.  He has probably one of the shortest swings I've ever seen."[118]  General manager Andrew Friedman said: "He's a guy with a very interesting profile. He's a plus defender in all three (outfield) spots, with superior contact skills, and a really good ability to discern balls and strikes ... and an ability to hit line drives with ... high frequency."[106]

Fuld made the team's Opening Day roster, his first in the majors, as its fourth outfielder, and a late-inning weapon off the bench, speedster pinch runner, and defensive replacement.[104][121][122]  Tommy Rancel of ESPN described him as: "Tampa Bay's version of Brett Gardner; albeit in a reduced role".[104] However, with Manny Ramirez sudden retirement, left fielder Johnny Damon became the team's DH, expanding Fuld's role. On April 7, he stole a career-high 3 bases in one game, and Maddon began penciling him into the lineup as the team's everyday leadoff hitter.[123][124][125][126]

On April 9, playing right field against the Chicago White Sox who had the bases loaded, Fuld made what the Tampa Tribune called "one of the best catches in team history".[127][128] Rays center fielder B.J. Upton said:  "Oh my god, oh my. I thought he was going to dive into the [wall].... I was screaming from center field when he caught it. 'Great catch!' Should be play of the year so far."[127][129] His popularity spread on the internet by late April, with the Toronto Sun writing:  "Fuld ranks in the Top 10 in batting average, runs, and triples, and also leads the AL in stolen bases and YouTube hits, especially video hits for a catch in Chicago diving towards the right field corner onto the warning track that has been set to a Superman theme."[3]

Later in the month Maddon compared Fuld's play favorably to that of Gold Glove left fielder Carl Crawford, saying that Fuld had a better arm, and compared his style of play to that of former perennial Gold Glove center fielder Jim Edmonds.[16][113][130]

On April 11, as a reporter for major league baseball described it, Fuld "turned Fenway Park into his personal playground."[8]  Fuld missed hitting for the cycle only because he opted to stretch a single into a double.  He had already hit a double, triple, and home run in a game against the Boston Red Sox when he came up in the 9th inning in a runaway game which the Rays won 16–5. Had he stopped at first base, he would have been the second player in Tampa Bay Rays history to hit for the cycle, joining B.J. Upton.[131]  Maddon observed that Fuld's refusal to stop at first reflected his integrity.[132] He became the fifth player in Tampa Bay history to have four extra-base hits in a game, and the first since Tomás Pérez in 2006.[133]  His four extra-base hits and 11 total bases were also records for a Fenway Park debut.[134]

His "Superman" YouTube video was joked about on Twitter with tweets at the  #LegendofSamFuld hash tag, such as "Sam Fuld was once intentionally walked while in the on deck circle", "The Red Sox check under their bed for Sam Fuld", and Maddon's favorite: "Superman wears a Sam Fuld T-shirt to bed."[8][3][135]  Though not a Twitter user, Fuld caught on quickly:  When reporters asked him the next day why he could not stop the rain that was causing a rainout, Fuld quipped:  "This is me, washing my planet".[134][136]  By mid-April, he had won the starting left fielder position and leadoff hitter slot with the Rays.[137][138]

On April 18, a week after his four-hit Fenway Park debut, Fuld had four hits in four at bats against the White Sox. That raised his batting average to an AL-leading .396, to go along with his league lead in stolen bases.[139][140]  His batting average was the second-highest in team history through April 18 in a season, behind only Fred McGriff (1998).[141]  He became the first Tampa Bay leadoff batter with two four-hit games in a season since Carl Crawford, who did it twice in 2005, and the first major leaguer to have two four-hit games in the 2011 season.[142][143]  He was fourth in the league in hits (21), fifth in on-base percentage (.431) and OPS (1.035), tied for fifth in doubles (6), sixth in slugging percentage (.604), and was third-toughest player to strike out (one strikeout per 14.5 plate appearances).[143]  As of April 25, he was leading the AL with 10 stolen bases.[144]  A reporter for major league baseball, in explaining his rise to fame, wrote:  "you won't find any other 5-foot-9... New Hampshire-born, Jewish, diabetic outfielders in the Baseball Encyclopedia.[8]

He made another diving, face-planting catch of a sinking line drive, leading Rays pitcher David Price to say:  "I heard that the world is covered by 75 percent water, and the other 25 percent is covered by Sam Fuld."[139][145]  As of April 20, Fuld led Baseball Tonight's Web Gem standings with 18 points, which included Web Gems at each outfield position.[130]  A writer for The American Spectator observed:  "he owns the 11 o-clock highlight reel. (My sources inform me that ESPN executives are considering changing the name of "Baseball Tonight" to "The Sam Fuld Show.")"[112]  In late April, he was placed on the American League ballot for the 2011 All-Star Game.[146]

The press and the Rays began to refer to him by the nicknames "Superman", "Super Sam", and "The Legendary Sam Fuld".[147][148][149][150]  The Rays gave away Super Sam Fuld Superhero Capes to children at the team's May 29 game.[148][149][151][152]

In a game against the Brewers on June 20, 2011, he reported to the pitcher's mound in the 8th inning and warmed up in order to give a relief pitcher time to warm up in the bull pen. He did not throw a recorded pitch, or appear as a pitcher in the box score.[153]

Fuld finished the season with a .240 batting average, and tied for second on the team in stolen bases, with 20.[154]

Fuld initially injured ligaments in his right wrist in September 2011, but re-aggravated them on a swing in spring training on March 23, 2012.[155]  On April 3, Dr. Thomas Graham performed surgery to repair his wrist ligaments.[156]  Fuld missed the first 96 games of the season, and made his 2012 debut on July 24, playing in 44 games during the season.[156]

On September 2, 2013, against the Los Angeles Angels, Fuld recorded one out as a pitcher, getting J. B. Shuck to fly out to end the eighth inning. He pitched at speeds topping out at 88 MPH.[157][156]  He was the third position player to pitch in Rays history, joining Wade Boggs (in 1999) and Josh Wilson (in 2007).[156]

On September 30, during the 163rd game of the season and tiebreaker to determine the last American League wildcard playoff spot against the Texas Rangers, Fuld scored the fifth run of the game, stealing third base and coming home on an error by Rangers pitcher Tanner Scheppers.[158]

For the season, he played in 70 games as a substitute, which were the most by any AL player since Brian Anderson played in 70 with the White Sox in 2008, and was one of six AL outfielders to start at least 10 games at all each outfield position.[156]

After the season, Fuld was non-tendered by the Rays, making him a free agent.[159]  Through 2017, Fuld had the highest contact rate in Rays’ history among batters with at least 500 plate appearances.[160]

Fuld signed a minor league deal with the Oakland Athletics on January 4, 2014.[161] On April 12, after he played in seven games, Fuld was designated for assignment to make room on the active roster for Craig Gentry.[162]  Oakland was required to put Fuld on waivers within 10 days, and could only retain him if he was not claimed on waivers by another team.[163]

Fuld was claimed on waivers by the Minnesota Twins on April 20, 2014.[164] He played in 13 games with the Twins, hitting .250 before suffering a concussion on May 2 after he crashed into a wall. He woke up with a headache on May 8, and was placed on the 7-day disabled list that day.[165] He had been starting in center field that week as the replacement for Aaron Hicks, who had also suffered a concussion. After a setback in his recovery on May 14,[166] he was transferred to the 15-day disabled list on May 22.  He batted .274 with a team-leading .370 OBP and 12 stolen bases for the Twins, in 53 games, before being traded.[167]

On July 31, 2014, the Twins traded Fuld to the Oakland Athletics for pitcher Tommy Milone. On September 30, in the Athletics' wildcard game against the Kansas City Royals, Fuld reached base three times.[168]

In the 2014 season, Fuld batted .239 in 113 games overall.[169]  He stole 21 bases in 25 attempts, with his 84% percentage tying for the ninth-best percentage in the American League.[169] He stole third base six times, tying for seventh in the AL.[169] His five assists from center field, where he started 58 games, tied him for fifth among AL center fielders, and his career-high 11 total assists tied him for seventh among AL outfielders.[169]

In 2015, in a career-high 120 games Fuld batted .197 for the A's, playing primarily center field and left field.[170][171] He led the league in double plays turned from left field, with three.[170] His seven assists from left field, where he started 23 games, tied him for fourth among AL left fielders.[170]

In 2016, Fuld suffered a left shoulder rotator cuff injury that necessitated surgery, after he batted .417 in spring training for the A's. He was put on the disabled list, and did not play in the regular season.[172][173] He retired in November 2017.[174]

On November 3, 2017, Fuld retired as a player and was named major league player information coordinator for the Philadelphia Phillies under new manager Gabe Kapler.[175][176] In that position he worked closely with the team's players, coaches, front office, and the research and development department, to "integrate the use of information in all areas of on-field performance and preparation and make recommendations regarding the most effective areas of future research and analysis."[175]

After the 2018 season the Toronto Blue Jays interviewed Fuld to be their next manager, and he reportedly “made a strong impression”, but withdrew his name from consideration.[177] At the same time, Gerry Fraley, writer for The Dallas Morning News, opined that Fuld was an interesting possibility to be hired as the next Texas Rangers manager.[178]

After the 2019 regular season, Fuld's name was mentioned by a number of people as a candidate for the position of manager with the Pittsburgh Pirates.[179] At the same time, NBC in Chicago mentioned him as a possible candidate to be the new manager for the Chicago Cubs, Joel Sherman mentioned him as an "off the beaten road" possibility to manage the New York Mets, and he was also mentioned as a possible managerial candidate for both the San Francisco Giants and the Philadelphia Phillies.[180][181][182] R.J. Anderson, writing for CBS Sports, opined: "He's on the fast track to holding down a managerial post of his own."[183] Jayson Stark also mentioned him as someone teams could consider as a new manager.[184]

After the 2020 season, he was reportedly a finalist for the position of manager of the Boston Red Sox.[185]

On July 14, 2020, Fuld returned to the field, playing Center Field during a Phillies Summer Camp exhibition game.[186]

On December 22, 2020, Fuld was announced as the new general manager of the Philadelphia Phillies.[187]

On December 19, 2022, the Phillies extended Fuld's contract through the 2025 season.[188]

On November 8, 2024, the Phillies announced that Fuld would step down as the team's general manager and begin a transition to a role as the team's president of business operations. Fuld is currently pursuing a Master of Business Administration (MBA) degree at the University of Pennsylvania's Wharton School, and will assume the new role upon his graduation in 2026. Preston Mattingly succeeded Fuld as general manager.

Fuld was the starting center fielder and batted lead-off for Team Israel at the 2017 World Baseball Classic.[189][190][191][192]

In June 2009, Fuld married Sarah Kolodner.[193] He had met her while they were fellow students at Phillips Exeter Academy.[14] She was recruited by Princeton University for their lacrosse team, and won two national lacrosse championships.[194]

"[Diabetes is] definitely a battle every day; something that's always on my mind, and never quite figured out. It's like hitting in that way."[195]

Fuld was diagnosed with Type 1 diabetes when he was 10 years old.  He recalled, "I was losing weight, I was thirsty all the time, just classic symptoms, so my parents knew something was wrong and the doctor diagnosed it right away. It was tough, but when I realized there was no other alternative, I just looked at it as a challenge."[21]

At the age of 12, he met pitcher Bill Gullickson, who also had diabetes, and talked to him for 10 minutes.[15] "That was enough to inspire me", Fuld said.[196] "Any time I can talk to young diabetic kids, I look forward to that opportunity", said Gullickson.[197]

Fuld was comfortable handling the kidding in the clubhouse regarding his treatments. "We dish it out pretty good about his insulin shots", said his AA manager Pat Listach. "We always give him stuff about putting needles in the refrigerator, and shooting up in the clubhouse. He takes it all in stride. He's a good guy."[46]Shakib Al Hasan (Bengali: সাকিব আল হাসান, romanized: Sākib āl hāsān; born 24 March 1987) is a former Bangladeshi international cricketer who played Test, ODI and T20I cricket for the Bangladeshi cricket team.[b] He was a former member of Parliament for Magura-1.[c] He is known for his aggressive left-handed batting style in the middle order and controlled slow left-arm orthodox bowling.[d] He was the captain in all three formats of the game for the Bangladesh national cricket team. He was ranked the 90th most famous athlete in the world by ESPN in 2019. He is widely regarded as the greatest Bangladeshi sportsman, and considered as one of the greatest all-rounders of all time.[e]

Shakib Al Hasan made his Test debut in 2007 against India. His breakthrough came in the Test against New Zealand in Chittagong in 2008, where he picked up 7 wickets for 36 runs, the best figures by a Bangladeshi bowler at that time. He quickly established himself as one of the team's leading performers. He has scored over 4,000 runs and taken more than 200 wickets in this format. He has also captained the team on their first-ever Test match win over England in 2016.[16] In ODIs, Shakib has been more successful. He has scored over 6,000 runs and taken more than 270 wickets in the format, becoming the fastest player in history to achieve the double of 5,000 runs and 250 wickets in ODIs.[17] In the 2012 Asia Cup, Shakib scored 237 runs, including three fifties, and took six wickets. Bangladesh progressed to the tournament's final for the first time but eventually lost to Pakistan by 2 runs, and he was awarded the Player of the tournament award in that season.[18] He is considered one of the most impactful players of the 2019 ICC Cricket World Cup,[19] where he broke the record of maximum runs scored in the group stages of the World Cup which was previously held by Sachin Tendulkar and finished as the third-highest run-scorer, scoring 606 runs in total at the tournament.[20][21]

He has played for various T20 tournaments around the world for many teams, including Kolkata Knight Riders, Sunrisers Hyderabad, Jamaica Tallawahs and Dhaka Dominators, where he won the Indian Premier League (IPL) twice with Kolkata Knight Riders in 2012 and 2014. He also won the Bangladesh Premier League (BPL) title 3 times in 2012, 2013 and 2016 with Dhaka Dominators. He was named the player of the tournament in BPL, a record four times in 2012, 2013, 2018 and 2022.[citation needed] He has won 41 man-of-the-match awards, the highest for Bangladesh[22] and 16 man-of-the-series awards, 3rd highest, in all formats. Between 2009 and 2022, he captained the Bangladesh cricket team in 120 matches across all three formats.

Shakib Al Hasan is an integral asset to the Bangladesh cricket team and has been a consistent performer for his country over the years.[23][15] Shakib's career has been filled with controversies over the years, consistently making it to the top of domestic media headlines.[24] He is considered one of the most influential and followed people in Bangladesh. He was ranked the 90th most famous athlete in the world by ESPN in 2019. He was included in the ICC Men's Team of the Year twice (2009, 2021). As of July 2024, Shakib holds the record for the 3rd most men's T20 International wickets.[25]

Born in Magura, Khulna, Shakib began playing cricket at an early age. According to Prothom Alo sports editor Utpal Shuvro, "Shakib was fairly proficient at cricket and was often hired to play for different villages and teams".[26][27] In one of those matches, Shakib batted aggressively and bowled fast, as he usually did, but also chose to experiment with spin bowling which proved to be not so effective. He was picked to play for Islampur, and took a wicket with his first ball; it was his first delivery with a proper cricket ball, having previously played with a taped tennis ball.[26][27] He spent six months training at Bangladesh Krira Shikkha Protishtan,[26][27] a government-run sports institute.[28]

Shakib played his debut Under-17 match against UAE Under-17s in 2003 in the ACC Under-17 Cup where he recorded bowling figure of 3–18 in 8 overs (with 2 maiden overs).[9]

In May 2004, at the age of 17, Shakib made his first-class debut for Khulna where he got a bowling figure of 0/116 off 30 overs in the 1st innings and 3/92 off 28 overs in the 2nd innings. He also scored 13 off 11 with three 4s in the 1st innings and 16 off 43 in the 2nd innings. His 1st first-class wicket was Rakibul Hasan.[10]

Shakib first represented Bangladesh at the Under-19 level in November 2005 in the 2005 Afro-Asia Under-19 Cup against India Under-19. In his debut match, he scored 24 off 23 balls with four 4s and also achieved a bowling figure of 2/26 in 10 overs with 2 maidens by taking his first wicket of Tanmay Srivastava.[11] In the tournament, Shakib played 5 matches, scoring 138 runs at an average of 38.50 and getting 5 wickets at an average of 25.20.[29] On 30 November 2005, 18-year-old Shakib guided Bangladesh to a four-wicket win over England in the opening match of the tri-nation Under-19 tournament (involving England and Sri Lanka) with his 82 off 62.[30] During the final of a tri-nation tournament, Shakib scored an 86-ball century and took three wickets to lead his team to victory.[31][32] In his 18 youth One Day Internationals, he has scored 563 runs at an average of 35.18 with three 50s and one 100 and a high score of 100 and took 22 wickets at an average of 20.18 with an economy of 3.68 and a best figure of 4/34.[22][33]

On 1 January 2005, Shakib made his first-class debut in the match between Bangladesh Cricket Board President's XI and Zimbabweans where he scored 14 off 14 in the 1st innings and 15 off 66 in the 2nd innings. He also got bowling figures of 0/133 in 32 overs.[34] In February 2005, Shakib got his 1st first-class international wicket by dismissing Vusimuzi Sibanda and a five-wicket haul playing against Zimbabwe A.[35]

Since 2004, Shakib has played for Khulna in the National Cricket League.

In the 2004–05 season of the tournament, Shakib, since his debut in May 2004 at the age of 17, played just 3 matches.[f] He scored 129 runs at an average of 25.80 with a high score of 54 and took 16 wickets at an average of 27.87 with a best bowling figure of 6/79 in an innings and 9/114 in a match.[36]

In the 2006–07 season, he played just 2[g] matches scoring 51 runs at an average of 17.00 with a high score of 23. He bowled only in 1 innings where he bowled 4 overs of 9 runs with 1 maiden over.[37]

Shakib has played 14 matches scoring 930 runs at an average of 40.43 with three 100s and a high score of 129.[38] He also got 31 wickets at an average of 33.16 with one 5-wicket haul.[39]

Having signed a contract in November 2009,[40] Shakib joined up with Worcestershire, playing in the second division of the County Championship, in July 2010. He was the first Bangladeshi to represent a county side.[40] Shakib was forced to delay by the BCB for the beginning of his spell as Worcestershire's overseas player.[41] While playing for Worcestershire, he took his best first-class bowling figures of 7/32 against Middlesex.[42] In eight first-class matches, he scored 358 runs at 25.57 (with one score over 50)[43] and took 35 wickets at 22.37,[44] as Worcestershire secured promotion to the first division at the end of the season.[45]

After the 2011 IPL, Shakib returned to Worcestershire for seven weeks. He played a single County Championship match as his time with the team coincided with the 2011 Friends Life t20,[46][47] but in that match, he took seven wickets and passed 3,000 runs in first-class cricket.[48]

Shakib signed an one-match deal with Surrey for their vital County Championship fixture against Somerset at Taunton.[49] In 1st innings of Somerset, Shakib claimed took 4 wickets of 97 runs.[50]

Shakib played five List-A matches for Worcestershire, scoring 187 at an average of 37.40 (including two half-centuries)[51] and taking 9 wickets at 17.77.[52]

Worcestershire finished fifth out of nine teams in their group for the T20 competition, failing to qualify for the quarter-finals.[53] From 12 matches, Shakib took 19 wickets, finishing as the club's leading wicket-taker, equal with seam bowler Gareth Andrew.[54] He also scored 110 runs at an average of 9.16.[55]

Shakib signed for Leicestershire as their second[h] overseas player for 2013 Friends Life t20 campaign.[56] Shakib played in 10 matches scoring 146 runs at an average of 18.25 with a top score of 43* in the 7-wicket win over Nottinghamshire Outlaws at Trent Bridge.[57][58] He also got 9 wickets at an average of 27.00 with an economy of 6.50 and a best bowling figure of 2/7 in the 10-wicket win over Yorkshire.[57][59]

Although Shakib was part of the auction of players held the following month for the 2009 Indian Premier League, Shakib was not chosen by any of the eight teams and no bids were made for him despite being rated as the world's highest-ranked ODI all-rounder at the time. His teammate Mashrafe Mortaza, who was bought in the auction by the Kolkata Knight Riders, said "I would have been a lot happier if Shakib had got a team because he truly deserved it for his sensational form with the bat and ball".[60] Shakib had no buyers in the 2010 IPL auction as well, which was very surprising considering his stature in international cricket at the time.[61][62]

For the 2011 Indian Premier League, Shakib was contracted by Kolkata Knight Riders (KKR) for US$425,000.[63][64] He made his debut in the tournament on 15 April 2011 in a match against Rajasthan Royals. He claimed two wickets in the match, first that of Amit Paunikar and then Shane Watson, but did not get the chance to bat as his team only lost a wicket, winning by 9 wickets.[65][66] KKR were knocked out in the semi-finals of the competition;[67] Shakib played in seven matches, taking 11 wickets at an average of 15.90, and finished as KKR's third-highest wicket-taker.[68]

In the 15th Match of the 2012 IPL against Rajasthan Royals, Shakib got a bowling figure of 3–17 which is still his best in his IPL career.[69][70] KKR won their maiden title in the final against CSK where Shakib contributed with a crucial cameo.[71]

In the 2014 Indian Premier League, Shakib was retained by KKR for a league fee of INR 2.80 crore. In the competition, he scored 227 runs in 11 innings at a strike rate of 149.34; his best being a 60 off 38 balls. He also achieved 11 wickets in 13 matches at an economy of 6.68 and an average of 30.36. His all-round performance was instrumental in KKR's title win. For his performances in 2014, he was named as the 12th man in the Cricinfo IPL XI.[72]

In the 2015 IPL, Shakib played only four matches scoring 36 runs at an average of 12 with 23 being his highest score and picking up four wickets at an average of 30.75 and gave away 8.78 runs per over with his best bowling figures of 2/22 against Mumbai Indians.[73]

"IPL experience helped me improve my game for Bangladesh"

In the 38th match of the 2016 IPL, Shakib combined with Yusuf Pathan put on an unbroken 134 run partnership against the Gujarat Lions, the highest for the fifth wicket in all IPL's. Shakib scored 66*, his best score so far in his IPL career, went in vain as Gujarat Lions won the match by 5 wickets.[75]

"T20 cricket is a challenge for all-rounders"

In the 2017 Indian Premier League, Shakib played his only home match against Gujarat Lions where he scored 1* and returned with bowling figures of 0/31 in that match. He left the tournament midway to play a tri-series in Ireland for the build-up of the 2017 ICC Champions Trophy.[77] Shakib played just 3 matches batting in just 1 inning scoring 1 and achieved 2 wickets at an average of 47.50.

Shakib was released by KKR and was picked up by Sunrisers Hyderabad (SRH) in the 2018 IPL auction.[78] On 24 May 2018, he became the second T20 player to take 300 wickets and score 4,000 runs in the format with the wicket of Rohit Sharma in the SRH's victory against the Mumbai Indians in the 2018 IPL.[79]

Between 2019 Indian Premier League, BCB wanted to call Shakib back for the 2019 ICC Cricket World Cup preparation camp[80] but he skipped the camp for match practice even after playing only 1 match at that time.[81][82][83] Shakib was hoped for some IPL matches before World Cup 2019 as Jonny Bairstow and David Warner would leave for their respective World Cup preparation camps.[84]

Shakib was released by the Sunrisers Hyderabad ahead of the 2020 IPL auction after playing for 2 seasons.[85]

Shakib was listed at the highest base price of INR 2 crore for the 2021 IPL auction and was subsequently bought by his former team KKR again for a price of ₹3.2 crore (US$380,000).[86][87][88] After the suspension of the 2021 Indian Premier League, Shakib and Mustafizur Rahman returned home on 6 May 2021 via a chartered flight arranged by the BCCI with their respective franchises sharing the cost of the flight.[89][90] Shakib could not take part in the remainder of IPL because of not getting a no-objection certificate (NOC) from the BCB.[91]
In IPL 2023, Shakib Al Hasan got signed by Kolkata Knight Riders for Rs 1.5 crore.[92] But due to international engagements with Bangladesh national cricket team and personal reasons he pulled himself out of IPL 2023.[93]

In the 2010 National Cricket League Twenty20 tournament in Bangladesh, a now-defunct Twenty20 league involving the teams in the National Cricket League (NCL), Shakib played as an icon player and captain for the Kings of Khulna.[94][95]

Shakib played 7 matches where he scored 86 runs at an average of 12.28 and achieved 8 wickets at an average of 20.00 with an economy of 5.92.[96]

In September 2009, Shakib joined Abahani along with Mashrafe for TK 20 lakh each.[97]

In November 2010, Shakib joined Mohammedan for Tk 30 lakh.[98]

In the 2014 season, Shakib played for Legends of Rupganj, formerly named Gazi Group Cricketers in the previous season.[99][100][101] He played 8 matches scoring 222 runs at an average of 31.71 and also achieving 13 wickets at an average of 23.61.[102]

In the 2016 season, Abahani registered their sixth win in that season by five wickets against Prime Bank where Shakib, who returned to List A cricket for Abahani after a break of 6 years got a bowling figure of 4–35 in 10 overs.[103]

Due to upcoming 2021 T20 World Cup, 2021 season was played in T20 format[104][105] where he captained the Mohammedan Sporting Club.[106]

In the 2022 season, Shakib switched teams for Super League phase from Mohammedan Sporting to Legends of Rupganj to prepare for the series against Sri Lanka by taking special permission from the league authorities and Mohammedan, since, he could not play for the initial stage and his contracted team did not qualify.[107] In his first match against Prime Bank, he picked 2 wickets in 7 overs and scored 21 off 34.[108] In the match against
Gazi Group, Shakib scored 59 off 26 balls and took one wicket.[109] He played only four matches scoring 98 runs at an average of 24.5 and at a strike rate of 103.15 and took 4 wickets with 6 maidens with the economy rate of 3.48.[110]

In the 2023 season, Shakib played for Mohammedan, which was confirmed by Club Committee President AGM Sabbir.[111]

The Bangladesh Cricket Board founded the six-team Bangladesh Premier League in 2012, a twenty20 tournament to be held in February that year. The BCB made Shakib the 'icon player' for Khulna Royal Bengals (KRB).[112] Under his captaincy, Shakib's team progressed to the semi-finals of the competition where they were beaten by Dhaka Gladiators despite Shakib's 86* off 41 balls.[113] In ten matches he scored 280 runs and took 15 wickets, which made him KRB's leading wicket-taker, and was named Man of the Tournament.[114][115]

In the auction of the BPL 2, Shakib was brought by defending champions Dhaka Gladiators for $365,000, the most expensive player in the tournament.[116] He led his team to the title, getting 329 runs and 15 wickets in 12 matches, emerging as the Man of the Tournament for the consecutive second time in the BPL.[117]

In the BPL 3, Shakib was picked by Rangpur Riders with the 'Players by Choice' system for icon players.[118][119] In a match against Sylhet Super Stars,
Shakib used abusive language towards opposition batsman Dilshan Munaweera as well as on-field umpire Tanvir Ahmed and as a result he got banned for one match after being found guilty.[120][121][122] He got 18 wickets in 11 matches at an economy rate of 6.39 in that season.[123]

Shakib was selected in the team of the tournament from BPL 1 to BPL 3.[124]

In the BPL 4, Shakib became the highest paid local player getting at least Tk 5.5 million as a player of A-plus category turning out for Dhaka Dynamites.[125][126][127][128][129] Shakib became man of the match consecutively two times in the matches against Comilla Victorians (for scoring 41 off 26 balls and achieving a bowling figure of 1/30 in 4 overs and a catch)[130] and Barisal Bulls (for achieving a bowling figure of 4/31 in 4 overs and scoring 22 off 21 balls).[131] Dhaka won the title[132][133] as Shakib notched his first title win as captain[134] who wanted to win the title this season.[135]

In October 2017, Shakib was named in the squad for the Dhaka Dynamites team, following the draft for the BPL 5.[136][137] In the 43-run win over Rangpur Riders where Dhaka confirmed second-place in the points table at the end of the group stage, Shakib became man of the match for his all-round performance. Shakib added 55 runs for the sixth wicket with Mehedi Maruf who however departed after scoring 33 off 23 balls with 3 fours and a six but Shakib remained unbeaten on 47 off 33 balls. He also bagged two wickets for just 13 runs from his four overs.[138][139] In the final of that season, Shakib dropped a catch as he failed to hold on to a chance from Chris Gayle on 22 who went on to break all sorts of records with his unbeaten 69-ball 146, hitting a world record 18 sixes to single-handedly power Rangpur Riders to the title.[140]

In October 2018, Shakib was named in the squad for the Dhaka Dynamites team, following the draft for the BPL 6.[141] On 22 January 2019, Shakib became the first player in BPL's history to take 100 wickets as he achieved the milestone during the match against Comilla Victorians reaching the milestone in his 69th appearance, an average of 16.85 and an economy rate of 6.64.[142] During the tournament, taking the wicket of Comilla Victorians' Anamul Haque during the final of the tournament at Mirpur, Shakib Al Hasan became the highest wicket-taker in a single season of the BPL[143] as he took 23 wickets from 15 matches with an average of 17.65 in that season.[144] Shakib sustained a left ring finger fracture during the match.[145]

In July 2019, Shakib signed a one-year contract with the Rangpur Riders to play for them in BPL 7.[146][147]

In December 2021, Shakib was signed by Fortune Barishal for 2021-22 Bangladesh Premier League. He led the team to the final, but lost to Comilla Victorians by 1 run in a nail-biting finish.[148] On 24 January 2022, in the league match against Minister Dhaka, he picked up his 400th wicket in Twenty20 cricket.[149] He was awarded 5 consecutive Player of the match award, the most consecutive time by any player in T20.[150] He also received the Player of the tournament award for the 4th time for his allround performances, having scored 284 runs in 11 innings at an average of 28.4 and picking up 16 wickets at an average of 14.56.[151]

For the BPL 10, Shakib was directly signed (brought) by Rangpur Riders for ৳40 crore in the draft, becoming the highest paid player among domestic players of this season.

Shakib is the highest wicket-taker in the BPL with 132 wickets in 100 matches at an average of 18.33 in the Bangladesh Premier League.[152][153]

Shakib was expected to play for Uthura Rudras in the inaugural 2012 Sri Lanka Premier League[154] but could not play any match due to suffering a knee injury.[155]

In the 2013 Caribbean Premier League, Shakib played for Barbados Tridents.[156] On 3 August 2013 against Trinidad & Tobago Red Steel, Shakib recorded the second best bowling figure in T20 cricket dismissing six batsmen to finish with figures of 6 for 6 from his four overs at Kensington Oval.[157]

Shakib was retained by the same team for The 2014 CPL.[158]

Shakib played for Jamaica Tallawahs in both the 2016 and 2017 season.

Shakib returned to Barbados for the 2018 to 2019 season.[159][160][161] Shakib becomes CPL champion with Tridents in the 2019 season, where he scored 111 runs at an average of 18.5 and took only 4 wickets for 150 runs in 6 matches.[162]

In May 2021, Shakib was resigned by Jamaica Tallawahs for the 2021 Caribbean Premier League.[163][164][165] He could not participate in the tournament because of not getting a no-objection certificate (NOC) for CPL because of the national duty.[166]

In 2022, Shakib was selected as a replacement player in the Guyana Amazon Warriors squad for the 2022 CPL season.

Shakib has played 30 matches scoring 354 runs at an average of 16.86 and strike rate of 102.91 and getting 29 wickets at an economy of 6.75 in the CPL.[167]

Shakib joined for Adelaide Strikers in 2014 replacing the injured Johan Botha, thus becoming the first Bangladeshi to play in the Big Bash League.[168][169] In his debut match at the 2013–14 Big Bash League season, Shakib scored 41 runs off 29 balls and took 2 wickets for 21 runs in his 4 overs for Adelaide Strikers, although being unable to prevent his team from losing.[170]

On 24 December 2014, it was announced that Shakib would be joining the Melbourne Renegades for the final 4 games of the 2014–15 Big Bash League season, replacing Andre Russell, who would be leaving to join the West Indies in their tour of South Africa.[171][172][173][174]

In November 2020, Cricket Australia announced that Shakib would not be able to play in future in BBL due to objections from the CA ethical police as part of its zero tolerance against corruption in cricket.[175]

Shakib played 6 matches scoring 87 runs at an average of 14.50 and strike rate of 117.57 and taking 9 wickets at an economy of 6.10.[167]

Shakib was one of the headline stars confirmed by the PCB's commitment at the unveiling its preliminary plans for the Pakistan Super League.[176]

In the 2016 Pakistan Super League, Shakib was named one of the Platinum Players[177] and later he was picked up by Karachi Kings for US$140,000.[178][179][180][181] In his debut match against Lahore Qalanders, he was adjudged man of the match for scoring 51 out of 35 balls and getting a bowling figure of 1/26 which helped his team win by 7 wickets.[182][183]

In the 2017 season, Shakib was picked by Peshawar Zalmi.[184] He was supposed to play for the 2018 season also but was injured at that time.[184][185]

Shakib had a chance to play in the middle of PSL 2020 as a replacement of Mahmudullah Riyad, who tested positive of COVID-19 recently. But, he was not allowed to take part for not being part of the draft list from the beginning.[186]

In April 2021, Shakib was signed by Lahore Qalandars to play in the rescheduled matches in the 2021 Pakistan Super League.[187] But later, in May 2021, he expressed his desire to play in the 2021 Dhaka Premier League instead of playing in the rescheduled matches of the 2021 Pakistan Super League.[188] As a result, he missed the rescheduled matches of the 2021 Pakistan Super League.[189]

Shakib played 13 matches scoring 180 runs at an average of 16.36 and strike rate of 107.14 and getting 8 wickets at an economy of 7.39.[167]

In June 2019, Shakib was selected to play for the Brampton Wolves in the 2019 Global T20 Canada,[190] but missed as the BCB granted his request to give him some time off from the cricket.[191]

Shakib was expected to play in 2020 Lanka Premier League as his ban was expected to end in October[192] but BCB declared that no Bangladeshi players, including Shakib, would feature in the LPL.[193]

Shakib, expected to play the 2021 season, was again granted no NOC as BCB declared no Bangladeshi players will feature in the LPL 2021.

Shakib signed up with Gall Marvels for the 2023 season.[194]
Shakib produced an all-round display for massive 83-run victory over B-Love Kandy with a quickfire 21-ball 30 to help his side to reach 180-5 and 2-10 off 3 overs to help bundle out B-Love Kandy to 97 in 17.1 overs.

In November 2020, Shakib made a return to cricket, following the completion of a one-year ban imposed by the International Cricket Council, being selected to play for Gemcon Khulna in Bangabandhu T20 Cup 2020.[195]

Shakib became the third cricketer to achieve the rare double of 5000 runs and 300 wickets (in 311 games) in T20 cricket[196] in the match against Gazi Group Chattogram in the tournament. Shakib however is the 2nd Bangladeshi batsman to score 5000 runs in T20 cricket after Tamim Iqbal. Overall he is the 65th cricketer in the world to reach the 5000 runs mark.[197]

Shakib was not available in the final of the tournament due to seeing his ailing father-in-law in the US. Before leaving, he made a quickfire 28 and took one wicket conceding 31 runs in Khulna's crucial win against Gazi Group Chattogram in the 1st Qualifier of the tournament.[198]

Shakib played nine matches, failing to impress much by scoring only 110 runs and bagging 6 wickets.[199]

Shakib Al Hasan made his return to cricket with the opening match of 2022 Independence Cup, named changed by BCB.He was picked by Walton Central Zone.Shakib scored 35 off 58 with two fours and then followed it up with 2-24 including two maidens to script the victory by 22-run against Islami Bank East Zone at Sylhet International Cricket Stadium.[200]

Shakib made his One Day International (ODI) debut against Zimbabwe on 6 August 2006 at Harare Sports Club. He played a significant part in Bangladesh's victory, where he scored 30 runs and bowled out Elton Chigumbura to get his first ODI wicket.[201][202]

On 28 November 2006, Shakib made his T20 and T20I debut against Zimbabwe. On his debut, Shakib scored 26 off 28 balls and got a bowling figure of 1/31.His 1st T20 & T20I wicket was of Sean Williams.[203][204]

Shakib made his Test debut on 6 May 2007 against India. On his debut, he got a bowling figure of 0/62 (19 overs) and scored 30 off 47 balls in 1st innings and 15 off 64 balls in 2nd innings.[205][206] Shakib's first test wicket was Craig Cumming in the 2nd test vs New Zealand.[207][208]

On 20 October 2008, Shakib took at that time the best bowling figures by a Bangladesh player in Tests, 7 wickets for 36 runs, against New Zealand in the 1st test of the test series.[209][210]

From January 2009[211] to April 2011[212] and again from March 2012[213] to January 2013,[214][215] Shakib was ranked first amongst ODI all-rounders by the ICC. In December 2011, he became the world's top-ranked Test all-rounder.[216] In December 2014, Shakib became the world's top-ranked Twenty 20 all-rounder.[217] He is currently the only all-rounder to be ranked in the top 3 of ICC Player Rankings across every format of international cricket.

Shakib was appointed Bangladesh's vice-captain in June 2009. During Bangladesh's tour of the West Indies the following month, the captain Mashrafe Mortaza was injured and Shakib took over the captaincy. He was 22 years old at the time. Initially, a temporary position, Shakib's success against the West Indies, securing his side's first overseas series win, ensured his retention of captaincy even after Mashrafe recovered. Shakib was named The Wisden Cricketer's "Test Player of the Year" in October 2009.[218][219] In July 2010, he stepped down from the ODI captaincy to concentrate on his personal performance. Mortaza took over until he became injured again and Shakib was asked to resume leadership. This lasted until he was relieved of captaincy in September 2011 due to a poor World Cup campaign.

Shakib was included in Bangladesh's senior squad to tour Zimbabwe in February 2006. Along with Farhad Reza and Mushfiqur Rahim, Shakib was one of the three uncapped players to be included in the squad.[220] Shakib and Reza were described as "very good cricketers in all departments of the game", and Faruq Ahmed – the chief selector – said that Bangladesh had "high hopes from them and it's time for them to perform at the international level".[221] Shakib made his ODI debut on the tour on 6 August; his maiden wicket was that of all-rounder Elton Chigumbura, and he finished with bowling figures of 1/39. He also scored 30 not out while Shahriar Nafees scored his maiden ODI century to help Bangladesh win by eight wickets. It was the final match in the series, which Zimbabwe won 3–2.[222] In September 2006, Shakib was one of three players to be granted a rookie contract with the Bangladesh Cricket Board (BCB), along with Farhad Reza and Mehrab Hossain Jr. This increased the number of players with central contracts and under the control of the BCB from 20 to 23.[223]

Shakib was included in the 15-man squad led by Habibul Bashar for  the 2007 Cricket World Cup hosted  West Indies in March and April.[224] Bangladesh made it to the second stage of the competition and finished seventh.[225] Along the way the team caused an upset by beating India to help knock them out of the tournament. With Tamim Iqbal in just his fifth ODI and Mushfiqur Rahim, Shakib was one of three Bangladesh batsmen in the match to score a half century to help the team reach its target of 192 to win.[226] Later in the tournament, Shakib scored another half-century although Bangladesh were defeated by England on that very occasion.[227] He scored 202 runs from 9 matches at an average of 28.85 with a high score of 57* and achieved 7 wickets at an average of  43.14 with an economy of 4.96.[228]

Later that year, in May, India toured Bangladesh for two Tests and three ODIs. On 18 May, Shakib made his Test debut against India. He batted once, scoring 27 runs, and bowled 13 overs without taking a wicket as the match ended in a draw.[230] In his 2nd match he batted twice scoring 30 and 15 runs and bowled 19 overs without taking any wickets where also wicket-kept for 8 balls till India's innings declaration, as India won by an innings and 238 runs.[231] India won the Test series 1–0 and the ODI series 3–0. After the tour, Dav Whatmore resigned from his position as Bangladesh coach,[232] and batsman Mohammad Ashraful replaced Habibul Bashar as captain.[233]

In September 2007, South Africa hosted the ICC World Twenty20. Victory against West Indies in the first round was enough to ensure Bangladesh's progression to the second round, although it was the only one of their five matches they won. In the match against West Indies, Shakib took 4/34; it was the first time a Bangladesh player had taken more than three wickets in an International Twenty20 match (T20I).[234] Shakib was part of another piece of T20I history when in a match against Australia in the tournament he became one of three victims of the first T20I hat-trick. Brett Lee took Shakib's wicket, followed by those of Mortaza's and Alok Kapali to help Australia to a nine-wicket win.[235] In October that year, it was announced that Jamie Siddons – Australia's assistant coach – would take over the role of Bangladesh coach;[236] Siddons asserted that the previous set-up had focused on short-term goals and that he was planning to improve Bangladesh over the long term and keep together a core squad of talented players to gain experience at international level.[237]

In December 2007 and January 2008, Bangladesh toured New Zealand for two Tests and three ODIs. Although he did not play the first Test, Shakib was selected over Enamul Haque Jr for the second due to his better batting ability.[238] It was Shakib's fourth Test, and until that point he had gone wicketless. His first wicket was that of New Zealand's Craig Cumming.[239][240] New Zealand won by an innings and 137 runs and took the series 2–0.[238] New Zealand also completed a clean sweep in the ODIs which preceded the Tests, winning 3–0.[241] Shakib played in all three ODIs scoring 31 runs at an average of 10.33,[242] and taking 3 wickets at an average of 42.33.[243] Over February and March 2008 South Africa toured Bangladesh, playing two Tests and three ODIs. South Africa won both Tests.[244] Shakib played in both Bangladesh's defeats, taking just one wicket while conceding 122 runs,[245] and scoring 75 runs.[246] South Africa won the subsequent ODI series 3–0.[247] Shakib passed 1,000 ODI runs in the series; he passed the landmark in his 39th ODI with a batting average of 35.37.[248]

Before New Zealand's tour of Bangladesh in October 2008, Shakib was considered more of a batsman than a bowler, despite being an all-rounder. Though he usually batted down the order at number seven in Tests, he had mostly batted in the top five in ODIs. In a departure from Shakib's usual role Jamie Siddons, the coach stated that Shakib would play the Test series against New Zealand as a specialist bowler. The move immediately paid off, and he took 7/37 in New Zealand's first innings in the opening Test which were the best bowling figures by a Bangladesh player in all their 54 Tests suppressing the previous best innings figures by a Bangladeshi bowler set by another left-armer Enamul Haque with 7–95 against Zimbabwe at Dhaka three years ago.,[26] He scored 71 for his maiden Test half-century to guide the home team to 184–8 in their second innings.[249] Bangladesh lost the series 2–0, but Shakib finished as Bangladesh's leading wicket-taker in the series with 10 wickets at 17.80.[250] His spell was nominated to be the Best Test Bowling Performance of 2008 by ESPNcricinfo.[251]
Bangladesh won the opening match of the ODI series against New Zealand – securing their first ever ODI win over them[252] – although they eventually lost the series 2–1.[253] Shakib finished with five wickets from three matches, making him Bangladesh's second-highest wicket-taker for the series behind Mashrafe Mortaza (7);[254] however Shakib scored just 16 runs in the series.[255]

The following month, Bangladesh toured South Africa for two Tests, three ODIs, and a T20I. While Bangladesh lost all their matches against South Africa except for an abandoned ODI,[256] Shakib continued to build on the good bowling form he had found against New Zealand. On the first day of the opening Test, Shakib went wicketless; on the advice of Mohammad Salauddin, Bangladesh's assistant coach, he gave the ball flight on the second day and went on to take five wickets. He took another five-wicket haul in the second Test, again as Bangladesh lost to South Africa.[26] Along with South Africa's Makhaya Ntini, Shakib was the series' leading wicket-taker with 11 at an average of 20.81.[257] Shakib's performance against South Africa in a losing cause prompted former Australian leg spin bowler Kerry O'Keeffe to describe him as the "world's best finger spinner at the moment".[26] Sri Lanka toured Bangladesh in December 2008 and January 2009 for two Tests and a Tri-nation tournament including Zimbabwe. Sri Lanka won both Tests and the tournament final,[258] although Shakib turned in a man of the match performance, scoring 92 not out, in the second ODI against Sri Lanka helped Bangladesh to their only victory against them on the tour.[259] In the first match of the Test series, Shakib took another five-wicket haul as his team again was defeated.[26]

On 22 January 2009, Shakib was ranked first amongst ODI all-rounders by the ICC.[260]

At the beginning of 2009, there was speculation over Mohammad Ashraful and his  position as captain after a succession of defeats for Bangladesh and continuous poor form for Ashraful. Shakib was considered a possible successor by the Bangladesh Cricket Board (BCB). However, the BCB was cautious of over-burdening the all-rounder and decided against the move. Other candidates were discounted, and Ashraful remained as captain.[261] Later in 2009, Ashraful's captaincy was again under scrutiny after Bangladesh exited the 2009 ICC World Twenty20 in the first round following losses to Ireland and India. When Mashrafe Mortaza replaced Mohammad Ashraful in June 2009, Shakib was appointed vice-captain, filling the position vacated by Mortaza.[262]

In July–August 2009, Bangladesh toured the West Indies. When Mortaza injured his knee in the first Test, he was unable to take to the field on the final day and Shakib took over as captain. He and Mahmudullah led Bangladesh's bowling attack, sharing 13 wickets in the match and securing a historic win for Bangladesh.
It was Bangladesh's first against the West Indies, their first overseas Test victory, and only their second Test win.[263][264] The West Indies side was very inexperienced due to the fallout of a dispute between the West Indies Cricket Board and the West Indies Players' Association over pay. The first XI had made themselves unavailable for selection and a new squad had to be chosen. Seven West Indies players made their Test debut in the match and the side was captained by Floyd Reifer who had played the last of his four Tests ten years earlier.[265] Shakib was fined 10% of his match fee for excessive appealing; bowler Shahadat Hossain was also fined and batsman Imrul Kayes was reprimanded for the same reason.[266]

In Mortaza's absence through injury, Shakib led Bangladesh for the remainder of the tour.[267][268] Aged 22 years and 115 days at the start of the second Test, Shakib became Bangladesh's youngest captain and fifth youngest in the history of Test cricket.[269] Under Shakib's leadership Bangladesh went on to win the second Test, and in the process secured their first overseas series win.[270] Individually Shakib performed well, earning both the player-of-the-match and player-of-the-series awards, scoring 16 and 96 not out with the bat and taking 3/59 and 5/70 with the ball.[271] He scored 159 runs in the series at an average of 53.00 and was Bangladesh's second highest run-scorer for the series;[272] his haul of 13 wickets at an average of 18.76 from both matches meant Shakib was the equal highest wicket-taker for the series along with West Indies fast bowler Kemar Roach.[273] After winning the Test series 2–0, Bangladesh proceeded to whitewash the ODI series, winning 3–0. The West Indies' dispute remained unsolved for the whole of Bangladesh's tour and the West Indies continued to field an inexperienced side.[268] Shakib collected two half centuries in the ODI series, averaging 45.00, and was the third highest run-scorer in the series;[274] he also took 2 wickets at an average of 48.00.[275] For his performance in the ODIs, Shakib was named the man of the series.[276] Brian Lara on his Facebook page quoted him becoming 'the man for all seasons' for Bangladesh after the conclusion of 1st ODI during the tour.[277]

Shakib was appointed captain for Bangladesh's tour of Zimbabwe in August 2009 as Mortaza was still injured.[268] In the second ODI of the five-match series in Zimbabwe in August 2009, Shakib scored 104 off only 64 balls before being run-out – his third ODI century – to help his team to their highest score in ODIs and lead Bangladesh to a 2–0 lead in the series.[278] He finished the series with 170 runs from five matches at an average of 42.50 – fifth highest run-scorer[279] – and 6 wickets an average of 39.66, fourth highest wicket-taker in the series.[280] Following his team's 4–1 victory in the ODI series against Zimbabwe, Shakib travelled to Australia to undergo medical treatment for acute groin pain. He had been experiencing pain since the end of the West Indies tour, but he had decided to play through the pain and lead Bangladesh in Zimbabwe.[281] Although he did not win, Shakib was nominated Cricketer of the Year and Test Player of the Year in the 2009 ICC Awards; he was the only Bangladesh player to be nominated in either category that year.[282][283]

Shakib's success meant that it was not a given that Mortaza would replace him as captain once fit,[268] but it was announced in September 2009 that Mortaza would return as captain for Bangladesh's home ODI series against Zimbabwe the following month and Shakib would return to the role of vice-captain.[284] However, Mortaza failed to return from arthroscopic surgery on both knees in time for the series, and Shakib was again named captain.[285] After losing the opening match – in which Shakib criticised the performance of Bangladesh's batsmen including himself[286] – Bangladesh went on to win the series 4–1.[287] In the second match of the series, Shakib passed 2,000 runs in ODIs.[248]

On 5 November 2009, Shakib became no. 1 ODI bowler in ICC rankings.[288]

In November 2009, Shakib was named The Wisden Cricketer's "Test Player of the Year".[289]

Over February and March 2010 England toured Bangladesh for two Tests and three ODIs. England won all of their matches against Bangladesh.[290] Shakib was Bangladesh's leading wicket-taker in both the Test and ODI series (9 in Tests and 5 in ODIs).[291][292] Both Tests went to the final day and Shakib expressed the opinion that the experience had exhausted his side.[293] The match was also a source of controversy after incorrect umpiring decisions on the third day went against Bangladesh, prompting Shakib to blame the lack of a referral system for some of the team's misfortune.[294] In turn, Bangladesh toured England in May and July the same year. They again lost the Test series 2–0.[295] Shakib finished as his team's leading wicket-taker with eight wickets,[296] although he only scored 52 runs.[297] Before the ODI series, Bangladesh left England for Sri Lanka, which was hosting the 2010 Asia Cup in June. Bangladesh lost all three of their matches.[298] With five wickets, Shakib was Bangladesh's leading wicket-taker with Shafiul Islam (5 each).[299]

As he was struggling to cope with the captaincy and his role as an all-rounder, Shakib gave up the captaincy in July 2010 to focus on his own performance. Mashrafe Mortaza returned to take charge in ODIs. Coach Jamie Siddons explained that "Shakib was the main person behind the decision, he decided it was a bit much for him. His form was down with the bat and he needed a rest."[300] Bangladesh returned to England for the ODI half of their tour. Bangladesh lost the series 2–1,[295] but their victory in the second match was the first time Bangladesh had beaten England in international cricket.[301] While in Europe, Bangladesh were scheduled to play two ODIs against Ireland, and one each against Scotland and Netherlands. The match against Scotland was abandoned and Bangladesh lost one match to each of Ireland and the Netherlands.[295]

For his performances in 2009, he was named in the World Test XI by the ICC.[302]

In October 2010, New Zealand went to Bangladesh for five ODIs. In the first match of the series Mortaza injured his ankle and was forced to leave the field; Shakib took over, and under his leadership, Bangladesh secured a nine-run victory, during which Shakib took four wickets and scored 58. Once it emerged that Mortaza would be unable to play in the rest of the series, Shakib was made captain for the remaining matches.[303] In the fourth match, Shakib scored a century and took three wickets to help his team win by nine runs.[304] Bangladesh went on to win the series 4–0,[305] their first series victory against a full strength ICC Full Member nation.[304] Shakib finished the series as the player with most runs and wickets on either side: 213 runs[306] and 11 wickets.[307]

Although Mortaza returned from injury in time for Zimbabwe's tour of Bangladesh in December, Shakib was retained as captain for the five-match ODI series.[308] Following his side's defeat in the opening ODI, Shakib stated that he "was not prepared to take the job and I am also not satisfied with my role as a captain".[309] Bangladesh went on to win the next three complete matches, with one called off due to rain, to beat Zimbabwe 3–1.[310] Shakib was Bangladesh's second-highest run-scorer and wicket taker for the series with 156 runs, including two half centuries, and nine wickets.[311][312]

In February to April 2011, Bangladesh co-hosted the World Cup with India and Sri Lanka. West Indies dismissed Bangladesh for 58 runs, the team's lowest score in ODIs and a record low for a Full Member at the World Cup.[313] Shakib described this match as the 'worst day' of his career.[314] Shakib's house was stoned by angry fans in the aftermath,[315] as was the West Indies team bus as it left the ground.[316] Bangladesh registered wins against England, Ireland, and the Netherlands, but defeats by West Indies, India, and South Africa meant they did not progress beyond the first round of the tournament.[317] With 8 wickets at an average of 27.87, Shakib was Bangladesh's leading wicket-taker in the tournament,[318] and his 142 runs from 6 innings made him the team's third highest run-scorer.[319]

Shortly after the World Cup, Australia toured Bangladesh for three ODIs. In the first match of the series, Shakib scored 51 to pass 3,000 runs in ODIs.[248] Bangladesh lost the series 3–0,[320] Shakib scored 69 runs in three matches[321] and took just one wicket whilst conceding 117 runs.[322] Shane Watson's performances in the series meant he was ranked first amongst ODI all-rounders, claiming the position from Shakib.[323]

When Bangladesh toured Zimbabwe in August 2011 they were expected to win the one-off Test, which was Zimbabwe's first in five years, and the five-match ODI series. As it transpired Bangladesh lost the ODI series 3–2 and the Test. Shakib and his vice-captain Tamim Iqbal were sacked, with a BCB official citing unsatisfactory leadership.[324]

Bangladesh's first series under new leadership was against the West Indies in October 2011. Relieved of the captaincy, Shakib was Bangladesh's leading wicket-taker in both the ODI and Test series,[325][326] his bowling was backed up by 168 runs in the two Tests (of the Bangladesh players, only Tamim Iqbal scored more),[327] and 79 from two innings in the ODIs.[328] Bangladesh lost both series.[329] Following this, Shakib was Bangladesh's top run-scorer and wicket-taker in the home Test series against Pakistan in December 2011.[330][331] In the second Test of the series, he became Bangladesh's first player to score a century (his highest score, 144) and take five wickets in an innings in the same Test.[332] After the series he moved to first place in the ICC's ranking of Test all-rounders.[333]

In the home series against West Indies in late 2012, he became the 2nd Bangladeshi to both take 100 Test wickets, making him the leading wicket-taker in Tests for Bangladesh, and to complete the 1000 run / 100 wicket double.[334] However, he got ruled out of the ODI and T20 series due to a shin injury.[335]

Shakib got ruled out of the Sri Lanka tour in March due to a calf muscle injury. He had a successful operation on the right leg at a private hospital in Sydney. He would be on rest for one month since his operation.[336]

In 2nd Test match of Zimbabwe tour of Bangladesh 2014 Shakib became only the fourth player in Test history after Alan Davidson (1960), Ian Botham (1980) and Imran Khan (1983) to score a hundred and take 10 wickets in the same match.[337]

On 7 December 2014, Shakib became no. 1 T20 all-rounder.[338]

In 2015, Shakib helped Bangladesh to their most successful year in cricket. Following their success in the 2015 Cricket World Cup (where after batting in the first match at the 2015 Cricket World Cup, he had a total of 4,040 runs in ODI matches and became the first Bangladeshi cricketer to score 4,000 runs in ODIs),[339] he also helped the team to a whitewash of Pakistan and series win against India and South Africa. On 12 June, he captured his 100th Test wicket at home on the third day of the one-off Test match against India.[340] On 15 July, he took his 200th wicket of his ODI career by taking the wicket of Hashim Amla in the final ODI of the ODI series becoming the fastest, youngest and the only seventh cricketer to 4,000 runs and 200 wickets double in ODIs (in 156 matches).[i][341]

On 26 September 2016, with the wicket of Shabir Noori during the first ODI against Afghanistan, Shakib became Bangladesh's leading wicket-taker in ODIs as well as in all formats at that time.[342][343]

On 22 October 2016, with the wicket of Joe Root, Shakib became the first Bangladeshi to have bagged 150 Test scalps.[344]

After the retirement from T20I of Mashrafe Mortaza in 2017, Shakib was appointed as the T20I captain of the team for the second time.[345][346] Later Mushfiqur Rahim was also removed from Test captaincy and Hasan was again appointed as Test captain.[347][348] However, during 2017–18 Bangladesh Tri-Nation Series, he injured his finger and was rested for few days. In his absence, Mahmudullah captained the side in the Home series against Sri Lanka and 2018 Nidahas Trophy. His first tour was against Pakistan in July 2017.

On 28 August 2017, Al Hasan became the 4th bowler in Test history to record a 5-wicket haul against all Test Playing nations (except Afghanistan and Ireland, who were only granted Test status earlier that year) when he grabbed 5 wickets against Australia in his first encounter with the team.[349] He also scored a half-century in the first innings. In the second innings, he only scored 8 runs with the bat, but he grabbed another 5 wickets to secure his first 10-wicket haul and the win.[350][351] However, after the series, Al Hasan opted to take a break from Test to focus on limited-overs games.[352]

In April 2018, he was one of ten cricketers to be awarded a central contract by the Bangladesh Cricket Board (BCB) ahead of the 2018 season.[353] Later the same month, he was named in the Rest of the World XI squad for the one-off T20I against the West Indies, played at Lord's on 31 May 2018,[354] but later withdrew from the squad due to personal reason.[355]

In November 2018, in the series against the West Indies, he became the first bowler for Bangladesh to take 200 wickets in Tests.[356] In the same match, he also became the fastest cricketer, in terms of matches, to score 3,000 runs and take 200 wickets in Tests, doing so in his 54th match.[357]

In April 2019, he was named as the vice-captain of Bangladesh's squad for the 2019 Cricket World Cup.[358][359] In the first innings of Bangladesh's opening match in the World Cup, against South Africa, he and Mushfiqur Rahim scored 142 runs for the third wicket, the highest partnership for any wicket for Bangladesh in a World Cup match.[360] Bangladesh went on to score their then highest total in an ODI match at that time, finishing on 330/6 from their 50 overs.[361] In the second innings, he took the wicket of Aiden Markram as his 250th wicket in ODIs, to become the fastest cricketer to take 250 wickets and score 5,000 runs in ODIs, in terms of a number of matches (199).[362] Bangladesh went on to win the match by 21 runs with Shakib named as player of the match.[363] In that match he also became the first cricketer to score a fifty in the first match of Bangladesh in each World Cup since 2007.[364]

In Bangladesh's next match in the tournament, against New Zealand, Shakib played in his 200th ODI.[365] On 17 June 2019, in the match against the West Indies, Shakib became the second batsman for Bangladesh to score 6,000 runs in ODIs.[366] For his unbeaten 124, he was awarded man of the match.[367]

In the match against Australia, Shakib Al Hasan got out for 41 which was his first score of under fifty runs in this World Cup. He also missed out on becoming the first Bangladeshi player to score 6 consecutive 50+ scores in ODIs being now joint with Tamim Iqbal who had 5 50+ scores in 2012.[368] In the second innings, Bangladesh went on to score their then highest total in an ODI match in a losing cause, finishing on 333/8 in 50 overs.[369]

On 24 June 2019, in the match against Afghanistan, where he was again named man of the match, Shakib became the first batsman for Bangladesh to score 1,000 runs in the Cricket World Cup,[370] and the first bowler for Bangladesh to take a five-wicket haul in a World Cup match.[371] He also became the second cricketer, after Yuvraj Singh, to score 50 runs and take five wickets in the same match at a World Cup.[372]

On 2 July 2019, in the match against India, Shakib became the first cricketer in the World Cup to score 600 runs and take 10 wickets in a single tournament.[373] He finished the tournament as the leading run-scorer for Bangladesh, with 606 runs in eight matches[374] and surpassed Sachin Tendulkar's record for the most runs in the group stage of a World Cup.[375] Shakib had a mind blowing average of 86.57 with the bat in the World Cup. He appeared in 8 matches in the World Cup and took 11 wickets.[376] He was named in the 'Team of the Tournament' by the ICC[377] and ESPNcricinfo.[378]

In September 2019, during the 2019–20 Bangladesh Tri-Nation Series, Shakib took his 350th wicket in international cricket, in the final group match against Afghanistan.[379]

Between 1 September 2011 and 28 December 2020, Shakib played 131 matches scoring 2,011 runs at an average of 50.27 with 4 hundreds and also taking 97 wickets at an average of 22.63 with two 5-wicket hauls in ODIs. Thus he was named in the ICC Men's ODI team of the Decade.[380][381]

After serving a one-year ban, he returned to the national squad for their Test and ODI series against West Indies.[382] In the first ODI, he picked up his 150th wicket at home.[383] Shakib also set another record as the left-arm spinner became the first Bangladeshi cricketer to bowl in 100 innings on home.[384]
In the third ODI, he became the only cricketer to register the double of 6,000 runs and 300 wickets across all formats in a single country.[385] Bangladesh went on to win the three-match series by 3–0 and he became the Player of the Series by scoring 113 runs and picking up 6 wickets at an average of 8.33.[386]

In the third ODI, Shakib suffered a groin injury.[387][388] However, he recovered from the injury and thus he was selected in the squad of the Test series.[389][390] But in the first test, he suffered an injury in another region of the same thigh that forced him to leave the field late in the second day.[391] It seemed that Shakib's old groin injury had resurfaced but match officials stated that it was a new injury.[392] He scored 68 runs and bowled 6 overs giving 16 runs in the 1st innings as he was unable to bat and bowl in the 2nd innings of the 1st Test.[393] He was ruled out of the 2nd test.[394]

Shakib Al Hasan missed the Bangladesh's tour of New Zealand as the BCB granted him paternity leave for the duration of the entire tour.[395]

Shakib opted to sit out the test series against Sri Lanka in April to play in the 2021 IPL which was granted by the BCB.[396][397]

Shakib and Fizz were selected in Bangladesh's 23-member preliminary squad for the ODI series against Sri Lanka during their playing in 2021 Indian Premier League.[398] Shakib joined the national camp along with Mustafizur on 19 May 2021 since returning from India on 6 May via a chartered flight following the suspension of the 2021 IPL.[399][400]

He returned to international cricket in the ODI series against Sri Lanka after withdrawing himself from New Zealand tour and test series in Sri Lanka.[401] Though he could not perform well in the series scoring only 19 runs and picked up only 3 wickets at an average of 43.33 in 3 matches.[402] He was included in the squad for all three formats for the series against Zimbabwe. In the one-off test, he failed with bat scoring only 3 runs in one innings and picked up 5 wickets in the match. In the opening match of the ODI series he became the leading wicket-taker for Bangladesh in ODIs, taking his 270th dismissal and consequently ended up picking a five-fer in the match.[403] In the second ODI, he picked up 2 wickets for 42 runs and scored unbeaten 96 runs to help Bangladesh to take an unassailable lead of 2–0 in the series.[404] In the third ODI he also contributed with both bat and ball. He was named Player of the Series for his all-round performance, scoring 145 runs at an average of 72.5 and picking up 8 wickets at an average of 14.75 in 3 matches. In the T20I series, he scored only 37 runs and picked up 3 wickets in 3 matches.

On 8 August 2021, he was nominated for ICC Player of the Month along with Mitchell Marsh and Hayden Walsh Jr. July 2021.[405][406] On 11 August, he was announced as the winner.[407] The following month, he was named in Bangladesh's squad for the 2021 ICC Men's T20 World Cup.[408]

On 30 December 2021, Shakib was nominated for ICC Men's ODI Cricketer of the Year along with Babar Azam, Janneman Malan and Paul Stirling.[409]
On 20 January 2022, ICC announced ICC Team of the Year 2022 for all formats.[410] Shakib was included in the ICC Men's ODI Team of the Year with his teammates Mushfuq and Fizz for his performance where in 9 matches, he managed to score 277 runs at an average of 39.57 with two 50s and he also scalped 17 wickets at an average of 17.52.[411][412][413]

In February 2022, he was included in both ODI and T20I squad against Afghanistan. In the 3-match ODI series, he managed to score only 60 runs and picked up 5 wickets. His poor form continued in 2-match T20I series also, as he scored only 14 runs and picked up 2 wickets at an average of 25.

In the 2nd ODI against England Shakib scored 58 runs from 69 balls while chasing a tough target of 327 runs set by England. In 3rd ODI, he become the first Bangladeshi bowler and 14th overall to take 300 wickets in One day international. In April 2023, he surpassed Southee to become the leading wicket-taker in T20Is with 136 wickets in 114 matches.[414] He was then awarded the player of the month for March.[3] He became Bangladesh's ODI Captain for Asia Cup and World Cup after Tamim Iqbal decided to step down on 3 August 2023.[415]

In May 2024, he was named in Bangladesh's squad for the 2024 ICC Men's T20 World Cup tournament.[416] On 26 September 2024, Shakib announced his retirement from T20Is and stated that the 2nd Test against India could be his last Test. He also confirmed that he will retire from international cricket after the 2025 ICC Champions Trophy.[417][418][419]

Shakib's tenure as MP from the parliamentary seat of Magura-1, was cut short in August 2024, following the 2024 Bangladesh quota reform movement and the Non-cooperation movement (2024).[453] He was criticised by protesters and former Bangladesh Cricket Board member Mohammad Rafiqul Islam Khan for staying silent during these events which is now being referred to as July massacre.[454][455] Despite calls for him and other players perceived to be loyal to the Awami League government of ousted prime minister Sheikh Hasina to be excluded from the national team, the Yunus interim government allowed him to continue playing for Bangladesh.[456]
A murder case was filed against him for ordering the killing of a garment worker, Rubel, during movement.[457][458] Shakib later apologised for his silence throughout the protests and expressed his wish to be able to return to Bangladesh for his final Test match.[459][460] In the end Shakib did not return to Bangladesh to play his final test match and decided to travel to the United States instead.[461][462]

In April 2025, the Bangladesh Financial Intelligence Unit seized all of his bank accounts.[463]

In September 2024, Bangladesh Securities and Exchange Commission (BSEC) fined him Tk 50 lakh on charges of manipulation in share transactions.[464][465]

In 2017, Shakib's company ‘‘Shakib Al Hasan Agro Farm’’, took a loan of total 2.5 crore taka from IFIC Bank. Two checks were issued by the company to repay the loan. However, there were insufficient funds in the bank account to cover the checks, resulting in the checks being dishonored.On December 15 2024, the bank authorities filed a case against Shakib and three others for check fraud.[466][467] On January 19 2025, an arrest warrant was issued against Shakib for failing to appear in court in connection with the case.[468][469] On March 24, 2025, the court ordered the seizure of Hasan's assets.[470][471]

Shakib completed secondary and higher secondary education at the Bangladesh Krira Shikkha Protishtan (BKSP) and obtained graduation in BBA from American International University-Bangladesh (AIUB).[472]

Shakib's father is Khondoker Masroor Reza and his mother is Shirin Reza.[473]

He married Umme Ahmed Shishir, a Bangladeshi American on 12 December 2012.[474][475][476] The couple met in 2010 while Shakib was playing county cricket for Worcestershire in England.[476] They had their first daughter Alayna Aubrey Hasan[477] on 8 November 2015,[478][479] their second daughter Errum Hasan[480] on 24 April 2020[481] and their first son Eyzah Al Hasan[482][483] on 16 March 2021.[484]

In August 2018, he became a green card holder which allows him to live and work in the US. [485]

Shakib is the chairman of Monarch Holdings[486][487] was the goodwill ambassador of UNICEF for Bangladesh,[488][489][490][491][492][493] Huawei[494] and Anti-Corruption Commission (Bangladesh).[495][496] Shakib announced a new company named Burak Commodities Exchange Co in August 2021 to enter into the gold business.[497]

Shakib officially joined the ruling Awami League party in 2023 and subsequently contested the 2024 Bangladeshi general election as the party's candidate for the Magura-1 parliamentary constituency,[498] where he secured a victory.[499][500] However, in August 2024, his tenure as a Member of Parliament ended following the dissolution of the Jatiya Sangsad by the President.[501][502]

Shakib runs a charity organization named SAHF (Shakib Al Hasan Foundation) to carry on his philanthropic works since 2020.[503] The foundation started a project named 'Mission Save Bangladesh' to help 2000 families in March 2020.[504]
In April 2020, Shakib auctioned his 2019 Cricket World Cup bat for COVID-19 relief.[505]

He was the standing captain in the first test against West Indies in 2009 when regular captain Mashrafe Mortaza left the field on the first day due to injury. Bangladesh went on to win the test match which was the first away test win for Bangladesh. He also captained the team for the second test as well, leading the team to win the match and consequently the series, which was also their first away test series win. He was named Player of the Series for his all-round performance.

He captained Bangladesh against West Indies in 2018–19 when Bangladesh defeated them 2–0, which was their second test series win against them. He was named Player of the Series for his all-round performances. During his stint as test captain, Bangladesh won only these 4 tests out of  19 matches.

Bangladesh played 55 ODI matches under his captaincy from 2009 to 2015 and 2023 and won 27 matches. In World Cups, he captained in 2011 and 2023 Cricket World Cup, including 1 match in 2015.

Bangladesh also played 39 T20I matches under his captaincy, winning 16 matches. In World Cups, he captained Bangladesh in 2021 and 2022 editions.

He captained Bangladesh against England in 2022–23, when Bangladesh defeated them 3–0, which was their first T20I series win against them.1800s: Martineau · Tocqueville · Marx · Spencer · Le Bon  · Ward · Pareto · Tönnies · Veblen · Simmel · Durkheim · Addams · Mead · Weber · Du Bois · Mannheim · Elias

Sociology is the scientific study of human society that focuses on society, human social behavior, patterns of social relationships, social interaction, and aspects of culture associated with everyday life.[1][2][3] Regarded as a part of both the social sciences and humanities, sociology uses various methods of empirical investigation and critical analysis[4]: 3–5  to develop a body of knowledge about social order and social change.[4]: 32–40  Sociological subject matter ranges from micro-level analyses of individual interaction and agency to macro-level analyses of social systems and social structure. Applied sociological research may be applied directly to social policy and welfare, whereas theoretical approaches may focus on the understanding of social processes and phenomenological method.[5]

Traditional focuses of sociology include social stratification, social class, social mobility, religion, secularization, law, sexuality, gender, and deviance. Recent studies have added socio-technical aspects of the digital divide as a new focus.[6] As all spheres of human activity are affected by the interplay between social structure and individual agency, sociology has gradually expanded its focus to other subjects and institutions, such as health and the institution of medicine; economy; military; punishment and systems of control; the Internet; sociology of education; social capital; and the role of social activity in the development of scientific knowledge.

The range of social scientific methods has also expanded, as social researchers draw upon a variety of qualitative and quantitative techniques. The linguistic and cultural turns of the mid-20th century, especially, have led to increasingly interpretative, hermeneutic, and philosophical approaches towards the analysis of society. Conversely, the turn of the 21st century has seen the rise of new analytically, mathematically, and computationally rigorous techniques, such as agent-based modelling and social network analysis.[7][8]

Social research has influence throughout various industries and sectors of life, such as among politicians, policy makers, and legislators; educators; planners; administrators; developers; business magnates and managers; social workers; non-governmental organizations; and non-profit organizations, as well as individuals interested in resolving social issues in general.

Sociological reasoning predates the foundation of the discipline itself. Social analysis has origins in the common stock of universal, global knowledge and philosophy, having been carried out as far back as the time of old comic poetry which features social and political criticism,[9] and ancient Greek philosophers Socrates, Plato, and Aristotle. For instance, the origin of the survey can be traced back to at least the Domesday Book in 1086,[10][11] while ancient philosophers such as Confucius wrote about the importance of social roles.[12][13]

Medieval Arabic writings encompass a rich tradition that unveils early insights into the field of sociology. Some sources consider Ibn Khaldun, a 14th-century Muslim scholar from Tunisia,[note 1] to have been the father of sociology, although there is no reference to his work in the writings of European contributors to modern sociology.[14][15][16][17] Khaldun's Muqaddimah was considered to be amongst the first works to advance social-scientific reasoning on social cohesion and social conflict.[18][19][20][21][22][23]

The word sociology derives part of its name from the Latin word socius ('companion' or 'fellowship'[24]). The suffix -logy ('the study of') comes from that of the Greek -λογία, derived from λόγος (lógos, 'word' or 'knowledge').[citation needed]

The term sociology was first coined in 1780 by the French essayist Emmanuel-Joseph Sieyès in an unpublished manuscript.[25][note 2] Sociology was later defined independently by French philosopher of science Auguste Comte (1798–1857) in 1838[26] as a new way of looking at society.[27]: 10  Comte had earlier used the term social physics, but it had been subsequently appropriated by others, most notably the Belgian statistician Adolphe Quetelet.[28] Comte endeavored to unify history, psychology, and economics through the scientific understanding of social life. Writing shortly after the malaise of the French Revolution, he proposed that social ills could be remedied through sociological positivism, an epistemological approach outlined in the Course in Positive Philosophy (1830–1842), later included in A General View of Positivism (1848). Comte believed a positivist stage would mark the final era in the progression of human understanding, after conjectural theological and metaphysical phases.[29] In observing the circular dependence of theory and observation in science, and having classified the sciences, Comte may be regarded as the first philosopher of science in the modern sense of the term.[30][31]

Comte gave a powerful impetus to the development of sociology, an impetus that bore fruit in the later decades of the nineteenth century. To say this is certainly not to claim that French sociologists such as Durkheim were devoted disciples of the high priest of positivism. But by insisting on the irreducibility of each of his basic sciences to the particular science of sciences which it presupposed in the hierarchy and by emphasizing the nature of sociology as the scientific study of social phenomena Comte put sociology on the map. To be sure, [its] beginnings can be traced back well beyond Montesquieu, for example, and to Condorcet, not to speak of Saint-Simon, Comte's immediate predecessor. But Comte's clear recognition of sociology as a particular science, with a character of its own, justified Durkheim in regarding him as the father or founder of this science, even though Durkheim did not accept the idea of the three states and criticized Comte's approach to sociology.

Both Comte and Karl Marx set out to develop scientifically justified systems in the wake of European industrialization and secularization, informed by various key movements in the philosophies of history and science. Marx rejected Comtean positivism[32] but in attempting to develop a "science of society" nevertheless came to be recognized as a founder of sociology as the word gained wider meaning. For Isaiah Berlin, even though Marx did not consider himself to be a sociologist, he may be regarded as the "true father" of modern sociology, "in so far as anyone can claim the title."[33]: 130 
To have given clear and unified answers in familiar empirical terms to those theoretical questions which most occupied men's minds at the time, and to have deduced from them clear practical directives without creating obviously artificial links between the two, was the principal achievement of Marx's theory. The sociological treatment of historical and moral problems, which Comte and after him, Spencer and Taine, had discussed and mapped, became a precise and concrete study only when the attack of militant Marxism made its conclusions a burning issue, and so made the search for evidence more zealous and the attention to method more intense.[33]: 13–14 
Herbert Spencer was one of the most popular and influential 19th-century sociologists. It is estimated that he sold one million books in his lifetime, far more than any other sociologist at the time.[34] So strong was his influence that many other 19th-century thinkers, including Émile Durkheim, defined their ideas in relation to his. Durkheim's Division of Labour in Society is to a large extent an extended debate with Spencer from whose sociology Durkheim borrowed extensively.[35]

Also a notable biologist, Spencer coined the term survival of the fittest.[36] While Marxian ideas defined one strand of sociology, Spencer was a critic of socialism, as well as a strong advocate for a laissez-faire style of government. His ideas were closely observed by conservative political circles, especially in the United States and England.[37]

The first formal Department of Sociology in the world was established in 1892 by Albion Small—from the invitation of William Rainey Harper—at the University of Chicago. The American Journal of Sociology was founded shortly thereafter in 1895 by Small as well.[38]

The institutionalization of sociology as an academic discipline, however, was chiefly led by Émile Durkheim, who developed positivism as a foundation for practical social research. While Durkheim rejected much of the detail of Comte's philosophy, he retained and refined its method, maintaining that the social sciences are a logical continuation of the natural ones into the realm of human activity, and insisting that they may retain the same objectivity, rationalism, and approach to causality.[39] Durkheim set up the first European department of sociology at the University of Bordeaux in 1895, publishing his Rules of the Sociological Method (1895).[40] For Durkheim, sociology could be described as the "science of institutions, their genesis and their functioning."[41]

Durkheim's monograph Suicide (1897) is considered a seminal work in statistical analysis by contemporary sociologists. Suicide is a case study of variations in suicide rates among Catholic and Protestant populations, and served to distinguish sociological analysis from psychology or philosophy. It also marked a major contribution to the theoretical concept of structural functionalism. By carefully examining suicide statistics in different police districts, he attempted to demonstrate that Catholic communities have a lower suicide rate than that of Protestants, something he attributed to social (as opposed to individual or psychological) causes. He developed the notion of objective social facts to delineate a unique empirical object for the science of sociology to study.[39] Through such studies he posited that sociology would be able to determine whether any given society is healthy or pathological, and seek social reform to negate organic breakdown, or "social anomie".

Sociology quickly evolved as an academic response to the perceived challenges of modernity, such as industrialization, urbanization, secularization, and the process of rationalization.[42] The field predominated in continental Europe, with British anthropology and statistics generally following on a separate trajectory. By the turn of the 20th century, however, many theorists were active in the English-speaking world. Few early sociologists were confined strictly to the subject, interacting also with economics, jurisprudence, psychology and philosophy, with theories being appropriated in a variety of different fields. Since its inception, sociological epistemology, methods, and frames of inquiry, have significantly expanded and diverged.[5]

Durkheim, Marx, and the German theorist Max Weber are typically cited as the three principal architects of sociology.[43] Herbert Spencer, William Graham Sumner, Lester F. Ward, W. E. B. Du Bois, Vilfredo Pareto, Alexis de Tocqueville, Werner Sombart, Thorstein Veblen, Ferdinand Tönnies, Georg Simmel, Jane Addams and Karl Mannheim are often included on academic curricula as founding theorists. Curricula also may include Charlotte Perkins Gilman, Marianne Weber, Harriet Martineau, and Friedrich Engels as founders of the feminist tradition in sociology. Each key figure is associated with a particular theoretical perspective and orientation.[44]

Marx and Engels associated the emergence of modern society above all with the development of capitalism; for Durkheim it was connected in particular with industrialization and the new social division of labor which this brought about; for Weber it had to do with the emergence of a distinctive way of thinking, the rational calculation which he associated with the Protestant Ethic (more or less what Marx and Engels speak of in terms of those 'icy waves of egotistical calculation'). Together the works of these great classical sociologists suggest what Giddens has recently described as 'a multidimensional view of institutions of modernity' and which emphasises not only capitalism and industrialism as key institutions of modernity, but also 'surveillance' (meaning 'control of information and social supervision') and 'military power' (control of the means of violence in the context of the industrialisation of war).[44]
The first college course entitled "Sociology" was taught in the United States at Yale in 1875 by William Graham Sumner.[45] In 1883, Lester F. Ward, who later became the first president of the American Sociological Association (ASA), published Dynamic Sociology—Or Applied social science as based upon statical sociology and the less complex sciences, attacking the laissez-faire sociology of Herbert Spencer and Sumner.[37] Ward's 1,200-page book was used as core material in many early American sociology courses. In 1890, the oldest continuing American course in the modern tradition began at the University of Kansas, lectured by Frank W. Blackmar.[46] The Department of Sociology at the University of Chicago was established in 1892 by Albion Small, who also published the first sociology textbook: An introduction to the study of society.[47] George Herbert Mead and Charles Cooley, who had met at the University of Michigan in 1891 (along with John Dewey), moved to Chicago in 1894.[48] Their influence gave rise to social psychology and the symbolic interactionism of the modern Chicago School.[49] The American Journal of Sociology was founded in 1895, followed by the ASA in 1905.[47]

The sociological canon of classics with Durkheim and Max Weber at the top owes its existence in part to Talcott Parsons, who is largely credited with introducing both to American audiences.[50] Parsons consolidated the sociological tradition and set the agenda for American sociology at the point of its fastest disciplinary growth. Sociology in the United States was less historically influenced by Marxism than its European counterpart, and to this day broadly remains more statistical in its approach.[51]

The first sociology department established in the United Kingdom was at the London School of Economics and Political Science (home of the British Journal of Sociology) in 1904.[52] Leonard Trelawny Hobhouse and Edvard Westermarck became the lecturers in the discipline at the University of London in 1907.[53][54] Harriet Martineau, an English translator of Comte, has been cited as the first female sociologist.[55] In 1909, the German Sociological Association was founded by Ferdinand Tönnies, Max Weber, and Georg Simmel, among others.[56] Weber established the first department in Germany at the Ludwig Maximilian University of Munich in 1919, having presented an influential new antipositivist sociology.[57] In 1920, Florian Znaniecki set up the first department in Poland. The Institute for Social Research at the University of Frankfurt (later to become the Frankfurt School of critical theory) was founded in 1923.[58] International co-operation in sociology began in 1893, when René Worms founded the Institut International de Sociologie, an institution later eclipsed by the much larger International Sociological Association (ISA), founded in 1949.[59]

The overarching methodological principle of positivism is to conduct sociology in broadly the same manner as natural science. An emphasis on empiricism and the scientific method is sought to provide a tested foundation for sociological research based on the assumption that the only authentic knowledge is scientific knowledge, and that such knowledge can only arrive by positive affirmation through scientific methodology.[citation needed]

Our main goal is to extend scientific rationalism to human conduct.... What has been called our positivism is but a consequence of this rationalism.[60]
The term has long since ceased to carry this meaning; there are no fewer than twelve distinct epistemologies that are referred to as positivism.[39][61] Many of these approaches do not self-identify as "positivist", some because they themselves arose in opposition to older forms of positivism, and some because the label has over time become a pejorative term[39] by being mistakenly linked with a theoretical empiricism. The extent of antipositivist criticism has also diverged, with many rejecting the scientific method and others only seeking to amend it to reflect 20th-century developments in the philosophy of science. However, positivism (broadly understood as a scientific approach to the study of society) remains dominant in contemporary sociology, especially in the United States.[39]

Loïc Wacquant distinguishes three major strains of positivism: Durkheimian, Logical, and Instrumental.[39] None of these are the same as that set forth by Comte, who was unique in advocating such a rigid (and perhaps optimistic) version.[62][4]: 94–8, 100–4  While Émile Durkheim rejected much of the detail of Comte's philosophy, he retained and refined its method. Durkheim maintained that the social sciences are a logical continuation of the natural ones into the realm of human activity, and insisted that they should retain the same objectivity, rationalism, and approach to causality.[39] He developed the notion of objective sui generis "social facts" to serve as unique empirical objects for the science of sociology to study.[39]

The variety of positivism that remains dominant today is termed instrumental positivism. This approach eschews epistemological and metaphysical concerns (such as the nature of social facts) in favour of methodological clarity, replicability, reliability and validity.[63] This positivism is more or less synonymous with quantitative research, and so only resembles older positivism in practice. Since it carries no explicit philosophical commitment, its practitioners may not belong to any particular school of thought. Modern sociology of this type is often credited to Paul Lazarsfeld,[39] who pioneered large-scale survey studies and developed statistical techniques for analysing them. This approach lends itself to what Robert K. Merton called middle-range theory: abstract statements that generalize from segregated hypotheses and empirical regularities rather than starting with an abstract idea of a social whole.[64]

The German philosopher Hegel criticised traditional empiricist epistemology, which he rejected as uncritical, and determinism, which he viewed as overly mechanistic.[4]: 169  Karl Marx's methodology borrowed from Hegelian dialecticism but also a rejection of positivism in favour of critical analysis, seeking to supplement the empirical acquisition of "facts" with the elimination of illusions.[4]: 202–3  He maintained that appearances need to be critiqued rather than simply documented. Early hermeneuticians such as Wilhelm Dilthey pioneered the distinction between natural and social science ('Geisteswissenschaft'). Various neo-Kantian philosophers, phenomenologists and human scientists further theorized how the analysis of the social world differs to that of the natural world due to the irreducibly complex aspects of human society, culture, and being.[65][66]

In the Italian context of development of social sciences and of sociology in particular, there are oppositions to the first foundation of the discipline, sustained by speculative philosophy in accordance with the antiscientific tendencies matured by critique of positivism and evolutionism, so a tradition Progressist struggles to establish itself.[67]

At the turn of the 20th century, the first generation of German sociologists formally introduced methodological anti-positivism, proposing that research should concentrate on human cultural norms, values, symbols, and social processes viewed from a resolutely subjective perspective. Max Weber argued that sociology may be loosely described as a science as it is able to identify causal relationships of human "social action"—especially among "ideal types", or hypothetical simplifications of complex social phenomena.[4]: 239–40  As a non-positivist, however, Weber sought relationships that are not as "historical, invariant, or generalisable"[4]: 241  as those pursued by natural scientists. Fellow German sociologist, Ferdinand Tönnies, theorised on two crucial abstract concepts with his work on "gemeinschaft and gesellschaft" (lit. 'community' and 'society'). Tönnies marked a sharp line between the realm of concepts and the reality of social action: the first must be treated axiomatically and in a deductive way ("pure sociology"), whereas the second empirically and inductively ("applied sociology").[68]

[Sociology is] ... the science whose object is to interpret the meaning of social action and thereby give a causal explanation of the way in which the action proceeds and the effects which it produces. By 'action' in this definition is meant the human behaviour when and to the extent that the agent or agents see it as subjectively meaningful ... the meaning to which we refer may be either (a) the meaning actually intended either by an individual agent on a particular historical occasion or by a number of agents on an approximate average in a given set of cases, or (b) the meaning attributed to the agent or agents, as types, in a pure type constructed in the abstract. In neither case is the 'meaning' to be thought of as somehow objectively 'correct' or 'true' by some metaphysical criterion. This is the difference between the empirical sciences of action, such as sociology and history, and any kind of prior discipline, such as jurisprudence, logic, ethics, or aesthetics whose aim is to extract from their subject-matter 'correct' or 'valid' meaning.[69]
Both Weber and Georg Simmel pioneered the "Verstehen" (or 'interpretative') method in social science; a systematic process by which an outside observer attempts to relate to a particular cultural group, or indigenous people, on their own terms and from their own point of view.[70] Through the work of Simmel, in particular, sociology acquired a possible character beyond positivist data-collection or grand, deterministic systems of structural law. Relatively isolated from the sociological academy throughout his lifetime, Simmel presented idiosyncratic analyses of modernity more reminiscent of the phenomenological and existential writers than of Comte or Durkheim, paying particular concern to the forms of, and possibilities for, social individuality.[71] His sociology engaged in a neo-Kantian inquiry into the limits of perception, asking 'What is society?' in a direct allusion to Kant's question 'What is nature?'[72]

The deepest problems of modern life flow from the attempt of the individual to maintain the independence and individuality of his existence against the sovereign powers of society, against the weight of the historical heritage and the external culture and technique of life. The antagonism represents the most modern form of the conflict which primitive man must carry on with nature for his bodily existence. The eighteenth century may have called for liberation from all the ties which grew up historically in politics, in religion, in morality, and in economics to permit the original natural virtue of man, which is equal in everyone, to develop without inhibition; the nineteenth century may have sought to promote, in addition to man's freedom, his individuality (which is connected with the division of labor) and his achievements which make him unique and indispensable but which at the same time make him so much the more dependent on the complementary activity of others; Nietzsche may have seen the relentless struggle of the individual as the prerequisite for his full development, while socialism found the same thing in the suppression of all competition – but in each of these the same fundamental motive was at work, namely the resistance of the individual to being leveled, swallowed up in the social-technological mechanism.[73]
The contemporary discipline of sociology is theoretically multi-paradigmatic[74] in line with the contentions of classical social theory. Randall Collins' well-cited survey of sociological theory[75] retroactively labels various theorists as belonging to four theoretical traditions: Functionalism, Conflict, Symbolic Interactionism, and Utilitarianism.[76]

Accordingly, modern sociological theory predominantly descends from functionalist (Durkheim) and conflict (Marx and Weber) approaches to social structure, as well as from symbolic-interactionist approaches to social interaction, such as micro-level structural (Simmel) and pragmatist (Mead, Cooley) perspectives. Utilitarianism (also known as rational choice or social exchange), although often associated with economics, is an established tradition within sociological theory.[77][78]

Lastly, as argued by Raewyn Connell, a tradition that is often forgotten is that of Social Darwinism, which applies the logic of Darwinian biological evolution to people and societies.[79] This tradition often aligns with classical functionalism, and was once the dominant theoretical stance in American sociology, from c. 1881 – c. 1915,[80] associated with several founders of sociology, primarily Herbert Spencer, Lester F. Ward, and William Graham Sumner.

Contemporary sociological theory retains traces of each of these traditions and they are by no means mutually exclusive.[citation needed]

A broad historical paradigm in both sociology and anthropology, functionalism addresses the social structure—referred to as "social organization" by the classical theorists—with respect to the whole as well as the necessary function of the whole's constituent elements. A common analogy (popularized by Herbert Spencer) is to regard norms and institutions as 'organs' that work towards the proper functioning of the entire 'body' of society.[81] The perspective was implicit in the original sociological positivism of Comte but was theorized in full by Durkheim, again with respect to observable, structural laws.


Functionalism also has an anthropological basis in the work of theorists such as Marcel Mauss, Bronisław Malinowski, and Radcliffe-Brown. It is in the latter's specific usage that the prefix "structural" emerged.[82] Classical functionalist theory is generally united by its tendency towards biological analogy and notions of social evolutionism, in that the basic form of society would increase in complexity and those forms of social organization that promoted solidarity would eventually overcome social disorganization. As Giddens states:[83]
Functionalist thought, from Comte onwards, has looked particularly towards biology as the science providing the closest and most compatible model for social science. Biology has been taken to provide a guide to conceptualizing the structure and the function of social systems and to analyzing processes of evolution via mechanisms of adaptation. Functionalism strongly emphasizes the pre-eminence of the social world over its individual parts (i.e. its constituent actors, human subjects).
Functionalist theories emphasize "cohesive systems" and are often contrasted with "conflict theories", which critique the overarching socio-political system or emphasize the inequality between particular groups. The following quotes from Durkheim[84] and Marx[85] epitomize the political, as well as theoretical, disparities, between functionalist and conflict thought respectively:

To aim for a civilization beyond that made possible by the nexus of the surrounding environment will result in unloosing sickness into the very society we live in. Collective activity cannot be encouraged beyond the point set by the condition of the social organism without undermining health.
The history of all hitherto existing society is the history of class struggles.
Freeman and slave, patrician and plebeian, lord and serf, guild-master and journeyman, in a word, oppressor and oppressed, stood in constant opposition to one another, carried on an uninterrupted, now hidden, now open fight, a fight that each time ended, either in a revolutionary re-constitution of society at large, or in the common ruin of the contending classes.
Symbolic interaction—often associated with interactionism, phenomenology, dramaturgy, interpretivism—is a sociological approach that places emphasis on subjective meanings and the empirical unfolding of social processes, generally accessed through micro-analysis.[86] This tradition emerged in the Chicago School of the 1920s and 1930s, which, prior to World War II, "had been the center of sociological research and graduate study."[87][page needed] The approach focuses on creating a framework for building a theory that sees society as the product of the everyday interactions of individuals. Society is nothing more than the shared reality that people construct as they interact with one another. This approach sees people interacting in countless settings using symbolic communications to accomplish the tasks at hand. Therefore, society is a complex, ever-changing mosaic of subjective meanings.[27]: 19  Some critics of this approach argue that it only looks at what is happening in a particular social situation, and disregards the effects that culture, race or gender (i.e. social-historical structures) may have in that situation.[27] Some important sociologists associated with this approach include Max Weber, George Herbert Mead, Erving Goffman, George Homans, and Peter Blau. It is also in this tradition that the radical-empirical approach of ethnomethodology emerges from the work of Harold Garfinkel.

Utilitarianism is often referred to as exchange theory or rational choice theory in the context of sociology. This tradition tends to privilege the agency of individual rational actors and assumes that within interactions individuals always seek to maximize their own self-interest. As argued by Josh Whitford, rational actors are assumed to have four basic elements:[88]

Exchange theory is specifically attributed to the work of George C. Homans, Peter Blau and Richard Emerson.[89] Organizational sociologists James G. March and Herbert A. Simon noted that an individual's rationality is bounded by the context or organizational setting. The utilitarian perspective in sociology was, most notably, revitalized in the late 20th century by the work of former ASA president James Coleman.[citation needed]

Following the decline of theories of sociocultural evolution in the United States, the interactionist thought of the Chicago School dominated American sociology. As Anselm Strauss describes, "we didn't think symbolic interaction was a perspective in sociology; we thought it was sociology."[87] Moreover, philosophical and psychological pragmatism grounded this tradition.[90] After World War II, mainstream sociology shifted to the survey-research of Paul Lazarsfeld at Columbia University and the general theorizing of Pitirim Sorokin, followed by Talcott Parsons at Harvard University. Ultimately, "the failure of the Chicago, Columbia, and Wisconsin [sociology] departments to produce a significant number of graduate students interested in and committed to general theory in the years 1936–45 was to the advantage of the Harvard department."[91] As Parsons began to dominate general theory, his work primarily referenced European sociology—almost entirely omitting citations of both the American tradition of sociocultural-evolution as well as pragmatism. In addition to Parsons' revision of the sociological canon (which included Marshall, Pareto, Weber and Durkheim), the lack of theoretical challenges from other departments nurtured the rise of the Parsonian structural-functionalist movement, which reached its crescendo in the 1950s, but by the 1960s was in rapid decline.[92]


By the 1980s, most functionalist perspectives in Europe had broadly been replaced by conflict-oriented approaches,[93] and to many in the discipline, functionalism was considered "as dead as a dodo:"[94] According to Giddens:[95]
The orthodox consensus terminated in the late 1960s and 1970s as the middle ground shared by otherwise competing perspectives gave way and was replaced by a baffling variety of competing perspectives. This third 'generation' of social theory includes phenomenologically inspired approaches, critical theory, ethnomethodology, symbolic interactionism, structuralism, post-structuralism, and theories written in the tradition of hermeneutics and ordinary language philosophy.
While some conflict approaches also gained popularity in the United States, the mainstream of the discipline instead shifted to a variety of empirically oriented middle-range theories with no single overarching, or "grand", theoretical orientation. John Levi Martin refers to this "golden age of methodological unity and theoretical calm" as the Pax Wisconsana,[96] as it reflected the composition of the sociology department at the University of Wisconsin–Madison: numerous scholars working on separate projects with little contention.[97] Omar Lizardo describes the pax wisconsana as "a Midwestern flavored, Mertonian resolution of the theory/method wars in which [sociologists] all agreed on at least two working hypotheses: (1) grand theory is a waste of time; [and] (2) good theory has to be good to think with or goes in the trash bin."[98] Despite the aversion to grand theory in the latter half of the 20th century, several new traditions have emerged that propose various syntheses: structuralism, post-structuralism, cultural sociology and systems theory.[citation needed] Some sociologists have called for a return to 'grand theory' to combat the rise of scientific and pragmatist influences within the tradition of sociological thought (see Duane Rousselle).[99]

The structuralist movement originated primarily from the work of Durkheim as interpreted by two European scholars: Anthony Giddens, a sociologist, whose theory of structuration draws on the linguistic theory of Ferdinand de Saussure; and Claude Lévi-Strauss, an anthropologist. In this context, 'structure' does not refer to 'social structure', but to the semiotic understanding of human culture as a system of signs. One may delineate four central tenets of structuralism:[100]

The second tradition of structuralist thought, contemporaneous with Giddens, emerges from the American School of social network analysis in the 1970s and 1980s,[101] spearheaded by the Harvard Department of Social Relations led by Harrison White and his students. This tradition of structuralist thought argues that, rather than semiotics, social structure is networks of patterned social relations. And, rather than Levi-Strauss, this school of thought draws on the notions of structure as theorized by Levi-Strauss' contemporary anthropologist, Radcliffe-Brown.[102] Some[103] refer to this as "network structuralism", and equate it to "British structuralism" as opposed to the "French structuralism" of Levi-Strauss.

Post-structuralist thought has tended to reject 'humanist' assumptions in the construction of social theory.[104] Michel Foucault provides an important critique in his Archaeology of the Human Sciences, though Habermas (1986) and Rorty (1986) have both argued that Foucault merely replaces one such system of thought with another.[105][106] The dialogue between these intellectuals highlights a trend in recent years for certain schools of sociology and philosophy to intersect. The anti-humanist position has been associated with "postmodernism", a term used in specific contexts to describe an era or phenomena, but occasionally construed as a method.[citation needed]

Overall, there is a strong consensus regarding the central problems of sociological theory, which are largely inherited from the classical theoretical traditions. This consensus is: how to link, transcend or cope with the following "big three" dichotomies:[107]

Lastly, sociological theory often grapples with the problem of integrating or transcending the divide between micro, meso, and macro-scale social phenomena, which is a subset of all three central problems.[citation needed]

The problem of subjectivity and objectivity can be divided into two parts: a concern over the general possibilities of social actions, and the specific problem of social scientific knowledge. In the former, the subjective is often equated (though not necessarily) with the individual, and the individual's intentions and interpretations of the objective. The objective is often considered any public or external action or outcome, on up to society writ large. A primary question for social theorists, then, is how knowledge reproduces along the chain of subjective-objective-subjective, that is to say: how is intersubjectivity achieved? While, historically, qualitative methods have attempted to tease out subjective interpretations, quantitative survey methods also attempt to capture individual subjectivities. Qualitative methods take an approach to objective description known as in situ, meaning that descriptions must have appropriate contextual information to understand the information.[108]

The latter concern with scientific knowledge results from the fact that a sociologist is part of the very object they seek to explain, as Bourdieu explains:

 How can the sociologist effect in practice this radical doubting which is indispensable for bracketing all the presuppositions inherent in the fact that she is a social being, that she is therefore socialised and led to feel "like a fish in water" within that social world whose structures she has internalised? How can she prevent the social world itself from carrying out the construction of the object, in a sense, through her, through these unself-conscious operations or operations unaware of themselves of which she is the apparent subject?
Structure and agency, sometimes referred to as determinism versus voluntarism,[109] form an enduring ontological debate in social theory: "Do social structures determine an individual's behaviour or does human agency?" In this context, agency refers to the capacity of individuals to act independently and make free choices, whereas structure relates to factors that limit or affect the choices and actions of individuals (e.g. social class, religion, gender, ethnicity, etc.). Discussions over the primacy of either structure or agency relate to the core of sociological epistemology (i.e. "what is the social world made of?", "what is a cause in the social world, and what is an effect?").[110] A perennial question within this debate is that of "social reproduction": how are structures (specifically, structures producing inequality) reproduced through the choices of individuals?

Synchrony and diachrony (or statics and dynamics) within social theory are terms that refer to a distinction that emerged through the work of Levi-Strauss who inherited it from the linguistics of Ferdinand de Saussure.[102] Synchrony slices moments of time for analysis, thus it is an analysis of static social reality. Diachrony, on the other hand, attempts to analyse dynamic sequences. Following Saussure, synchrony would refer to social phenomena as a static concept like a language, while diachrony would refer to unfolding processes like actual speech. In Anthony Giddens' introduction to Central Problems in Social Theory, he states that, "in order to show the interdependence of action and structure…we must grasp the time space relations inherent in the constitution of all social interaction." And like structure and agency, time is integral to discussion of social reproduction.

In terms of sociology, historical sociology is often better positioned to analyse social life as diachronic, while survey research takes a snapshot of social life and is thus better equipped to understand social life as synchronized. Some argue that the synchrony of social structure is a methodological perspective rather than an ontological claim.[102] Nonetheless, the problem for theory is how to integrate the two manners of recording and thinking about social data.

Sociological research methods may be divided into two broad, though often supplementary, categories:[111]

Sociologists are often divided into camps of support for particular research techniques. These disputes relate to the epistemological debates at the historical core of social theory. While very different in many aspects, both qualitative and quantitative approaches involve a systematic interaction between theory and data.[112] Quantitative methodologies hold the dominant position in sociology, especially in the United States.[39] In the discipline's two most cited journals, quantitative articles have historically outnumbered qualitative ones by a factor of two.[113] (Most articles published in the largest British journal, on the other hand, are qualitative.) Most textbooks on the methodology of social research are written from the quantitative perspective,[114] and the very term "methodology" is often used synonymously with "statistics". Practically all sociology PhD programmes in the United States require training in statistical methods. The work produced by quantitative researchers is also deemed more 'trustworthy' and 'unbiased' by the general public,[115] though this judgment continues to be challenged by antipositivists.[115]

The choice of method often depends largely on what the researcher intends to investigate. For example, a researcher concerned with drawing a statistical generalization across an entire population may administer a survey questionnaire to a representative sample population. By contrast, a researcher who seeks full contextual understanding of an individual's social actions may choose ethnographic participant observation or open-ended interviews. Studies will commonly combine, or 'triangulate', quantitative and qualitative methods as part of a 'multi-strategy' design. For instance, a quantitative study may be performed to obtain statistical patterns on a target sample, and then combined with a qualitative interview to determine the play of agency.[112]

Quantitative methods are often used to ask questions about a population that is very large, making a census or a complete enumeration of all the members in that population infeasible. A 'sample' then forms a manageable subset of a population. In quantitative research, statistics are used to draw inferences from this sample regarding the population as a whole. The process of selecting a sample is referred to as 'sampling'. While it is usually best to sample randomly, concern with differences between specific subpopulations sometimes calls for stratified sampling. Conversely, the impossibility of random sampling sometimes necessitates nonprobability sampling, such as convenience sampling or snowball sampling.[112]

The following list of research methods is neither exclusive nor exhaustive:

Sociologists increasingly draw upon computationally intensive methods to analyse and model social phenomena.[120] Using computer simulations, artificial intelligence, text mining, complex statistical methods, and new analytic approaches like social network analysis and social sequence analysis, computational sociology develops and tests theories of complex social processes through bottom-up modelling of social interactions.[7]

Although the subject matter and methodologies in social science differ from those in natural science or computer science, several of the approaches used in contemporary social simulation originated from fields such as physics and artificial intelligence.[121][122] By the same token, some of the approaches that originated in computational sociology have been imported into the natural sciences, such as measures of network centrality from the fields of social network analysis and network science. In relevant literature, computational sociology is often related to the study of social complexity.[123] Social complexity concepts such as complex systems, non-linear interconnection among macro and micro process, and emergence, have entered the vocabulary of computational sociology.[124] A practical and well-known example is the construction of a computational model in the form of an "artificial society", by which researchers can analyse the structure of a social system.[125][126]

Sociologists' approach to culture can be divided into "sociology of culture" and "cultural sociology"—terms which are similar, though not entirely interchangeable. Sociology of culture is an older term, and considers some topics and objects as more or less "cultural" than others. Conversely, cultural sociology sees all social phenomena as inherently cultural.[127] Sociology of culture often attempts to explain certain cultural phenomena as a product of social processes, while cultural sociology sees culture as a potential explanation of social phenomena.[128]

For Simmel, culture referred to "the cultivation of individuals through the agency of external forms which have been objectified in the course of history."[71] While early theorists such as Durkheim and Mauss were influential in cultural anthropology, sociologists of culture are generally distinguished by their concern for modern (rather than primitive or ancient) society. Cultural sociology often involves the hermeneutic analysis of words, artefacts and symbols, or ethnographic interviews. However, some sociologists employ historical-comparative or quantitative techniques in the analysis of culture, Weber and Bourdieu for instance. The subfield is sometimes allied with critical theory in the vein of Theodor W. Adorno, Walter Benjamin, and other members of the Frankfurt School. Loosely distinct from the sociology of culture is the field of cultural studies. Birmingham School theorists such as Richard Hoggart and Stuart Hall questioned the division between "producers" and "consumers" evident in earlier theory, emphasizing the reciprocity in the production of texts. Cultural Studies aims to examine its subject matter in terms of cultural practices and their relation to power. For example, a study of a subculture (e.g. white working class youth in London) would consider the social practices of the group as they relate to the dominant class. The "cultural turn" of the 1960s ultimately placed culture much higher on the sociological agenda.[citation needed]

Sociology of literature, film, and art is a subset of the sociology of culture. This field studies the social production of artistic objects and its social implications. A notable example is Pierre Bourdieu's Les Règles de L'Art: Genèse et Structure du Champ Littéraire (1992).[129] None of the founding fathers of sociology produced a detailed study of art, but they did develop ideas that were subsequently applied to literature by others. Marx's theory of ideology was directed at literature by Pierre Macherey, Terry Eagleton and Fredric Jameson. Weber's theory of modernity as cultural rationalization, which he applied to music, was later applied to all the arts, literature included, by Frankfurt School writers such as Theodor Adorno and Jürgen Habermas. Durkheim's view of sociology as the study of externally defined social facts was redirected towards literature by Robert Escarpit. Bourdieu's own work is clearly indebted to Marx, Weber and Durkheim.[citation needed]

Criminologists analyse the nature, causes, and control of criminal activity, drawing upon methods across sociology, psychology, and the behavioural sciences. The sociology of deviance focuses on actions or behaviours that violate norms, including both infringements of formally enacted rules (e.g., crime) and informal violations of cultural norms. It is the remit of sociologists to study why these norms exist; how they change over time; and how they are enforced. The concept of social disorganization is when the broader social systems leads to violations of norms. For instance, Robert K. Merton produced a typology of deviance, which includes both individual and system level causal explanations of deviance.[130]

The study of law played a significant role in the formation of classical sociology. Durkheim famously described law as the "visible symbol" of social solidarity.[131] The sociology of law refers to both a sub-discipline of sociology and an approach within the field of legal studies. Sociology of law is a diverse field of study that examines the interaction of law with other aspects of society, such as the development of legal institutions and the effect of laws on social change and vice versa. For example, an influential recent work in the field relies on statistical analyses to argue that the increase in incarceration in the US over the last 30 years is due to changes in law and policing and not to an increase in crime; and that this increase has significantly contributed to the persistence of racial stratification.[132]

The sociology of communications and information technologies includes "the social aspects of computing, the Internet, new media, computer networks, and other communication and information technologies."[133]

The Internet is of interest to sociologists in various ways, most practically as a tool for research and as a discussion platform.[134] The sociology of the Internet in the broad sense concerns the analysis of online communities (e.g. newsgroups, social networking sites) and virtual worlds, meaning that there is often overlap with community sociology. Online communities may be studied statistically through network analysis or interpreted qualitatively through virtual ethnography. Moreover, organizational change is catalysed through new media, thereby influencing social change at-large, perhaps forming the framework for a transformation from an industrial to an informational society. One notable text is Manuel Castells' The Internet Galaxy—the title of which forms an inter-textual reference to Marshall McLuhan's The Gutenberg Galaxy.[135] Closely related to the sociology of the Internet is digital sociology, which expands the scope of study to address not only the internet but also the impact of the other digital media and devices that have emerged since the first decade of the twenty-first century.[citation needed]

As with cultural studies, media study is a distinct discipline that owes to the convergence of sociology and other social sciences and humanities, in particular, literary criticism and critical theory. Though neither the production process nor the critique of aesthetic forms is in the remit of sociologists, analyses of socializing factors, such as ideological effects and audience reception, stem from sociological theory and method. Thus the 'sociology of the media' is not a subdiscipline per se, but the media is a common and often indispensable topic.[citation needed]

The term "economic sociology" was first used by William Stanley Jevons in 1879, later to be coined in the works of Durkheim, Weber, and Simmel between 1890 and 1920.[136] Economic sociology arose as a new approach to the analysis of economic phenomena, emphasizing class relations and modernity as a philosophical concept. The relationship between capitalism and modernity is a salient issue, perhaps best demonstrated in Weber's The Protestant Ethic and the Spirit of Capitalism (1905) and Simmel's The Philosophy of Money (1900). The contemporary period of economic sociology, also known as new economic sociology, was consolidated by the 1985 work of Mark Granovetter titled "Economic Action and Social Structure: The Problem of Embeddedness". This work elaborated the concept of embeddedness, which states that economic relations between individuals or firms take place within existing social relations (and are thus structured by these relations as well as the greater social structures of which those relations are a part). Social network analysis has been the primary methodology for studying this phenomenon. Granovetter's theory of the strength of weak ties and Ronald Burt's concept of structural holes are two of the best known theoretical contributions of this field.[citation needed]

The sociology of work, or industrial sociology, examines "the direction and implications of trends in technological change, globalization, labour markets, work organization, managerial practices and employment relations to the extent to which these trends are intimately related to changing patterns of inequality in modern societies and to the changing experiences of individuals and families the ways in which workers challenge, resist and make their own contributions to the patterning of work and shaping of work institutions."[137]

The sociology of education is the study of how educational institutions determine social structures, experiences, and other outcomes. It is particularly concerned with the schooling systems of modern industrial societies.[138] A classic 1966 study in this field by James Coleman, known as the "Coleman Report", analysed the performance of over 150,000 students and found that student background and socioeconomic status are much more important in determining educational outcomes than are measured differences in school resources (i.e. per pupil spending).[139] The controversy over "school effects" ignited by that study has continued to this day. The study also found that socially disadvantaged black students profited from schooling in racially mixed classrooms, and thus served as a catalyst for desegregation busing in American public schools.[citation needed]

Environmental sociology is the study of human interactions with the natural environment, typically emphasizing human dimensions of environmental problems, social impacts of those problems, and efforts to resolve them. As with other sub-fields of sociology, scholarship in environmental sociology may be at one or multiple levels of analysis, from global (e.g. world-systems) to local, societal to individual. Attention is paid also to the processes by which environmental problems become defined and known to humans. As argued by notable environmental sociologist John Bellamy Foster, the predecessor to modern environmental sociology is Marx's analysis of the metabolic rift, which influenced contemporary thought on sustainability. Environmental sociology is often interdisciplinary and overlaps with the sociology of risk, rural sociology and the sociology of disaster.[citation needed]

Human ecology deals with interdisciplinary study of the relationship between humans and their natural, social, and built environments. In addition to Environmental sociology, this field overlaps with architectural sociology, urban sociology, and to some extent visual sociology. In turn, visual sociology—which is concerned with all visual dimensions of social life—overlaps with media studies in that it uses photography, film and other technologies of media.[citation needed]

Social pre-wiring deals with the study of fetal social behavior and social interactions in a multi-fetal environment. Specifically, social pre-wiring refers to the ontogeny of social interaction. Also informally referred to as, "wired to be social". The theory questions whether there is a propensity to socially oriented action already present before birth. Research in the theory concludes that newborns are born into the world with a unique genetic wiring to be social.[140]

Circumstantial evidence supporting the social pre-wiring hypothesis can be revealed when examining newborns' behavior. Newborns, not even hours after birth, have been found to display a preparedness for social interaction. This preparedness is expressed in ways such as their imitation of facial gestures. This observed behavior cannot be attributed to any current form of socialization or social construction. Rather, newborns most likely inherit to some extent social behavior and identity through genetics.[140]

Principal evidence of this theory is uncovered by examining Twin pregnancies. The main argument is, if there are social behaviors that are inherited and developed before birth, then one should expect twin foetuses to engage in some form of social interaction before they are born. Thus, ten foetuses were analyzed over a period of time using ultrasound techniques. Using kinematic analysis, the results of the experiment were that the twin foetuses would interact with each other for longer periods and more often as the pregnancies went on. Researchers were able to conclude that the performance of movements between the co-twins were not accidental but specifically aimed.[140]


The social pre-wiring hypothesis was proved correct:[140] 
The central advance of this study is the demonstration that 'social actions' are already performed in the second trimester of gestation. Starting from the 14th week of gestation twin foetuses plan and execute movements specifically aimed at the co-twin. These findings force us to predate the emergence of social behavior: when the context enables it, as in the case of twin foetuses, other-directed actions are not only possible but predominant over self-directed actions.
Family, gender and sexuality form a broad area of inquiry studied in many sub-fields of sociology. A family is a group of people who are related by kinship ties :- Relations of blood / marriage / civil partnership or adoption. The family unit is one of the most important social institutions found in some form in nearly all known societies. It is the basic unit of social organization and plays a key role in socializing children into the culture of their society. The sociology of the family examines the family, as an institution and unit of socialization, with special concern for the comparatively modern historical emergence of the nuclear family and its distinct gender roles. The notion of "childhood" is also significant. As one of the more basic institutions to which one may apply sociological perspectives, the sociology of the family is a common component on introductory academic curricula. Feminist sociology, on the other hand, is a normative sub-field that observes and critiques the cultural categories of gender and sexuality, particularly with respect to power and inequality. The primary concern of feminist theory is the patriarchy and the systematic oppression of women apparent in many societies, both at the level of small-scale interaction and in terms of the broader social structure. Feminist sociology also analyses how gender interlocks with race and class to produce and perpetuate social inequalities.[141] "How to account for the differences in definitions of femininity and masculinity and in sex role across different societies and historical periods" is also a concern.[142]

The sociology of health and illness focuses on the social effects of, and public attitudes toward, illnesses, diseases, mental health and disabilities. This sub-field also overlaps with gerontology and the study of the ageing process. Medical sociology, by contrast, focuses on the inner-workings of the medical profession, its organizations, its institutions and how these can shape knowledge and interactions. In Britain, sociology was introduced into the medical curriculum following the Goodenough Report (1944).[143][144]

The sociology of the body and embodiment[145] takes a broad perspective on the idea of "the body" and includes "a wide range of embodied dynamics including human and non-human bodies, morphology, human reproduction, anatomy, body fluids, biotechnology, genetics". This often intersects with health and illness, but also theories of bodies as political, social, cultural, economic and ideological productions.[146] The ISA maintains a Research Committee devoted to "the Body in the Social Sciences".[147]

A subfield of the sociology of health and illness that overlaps with cultural sociology is the study of death, dying and bereavement,[148] sometimes referred to broadly as the sociology of death. This topic is exemplified by the work of Douglas Davies and Michael C. Kearl.[citation needed]

The sociology of knowledge is the study of the relationship between human thought and the social context within which it arises, and of the effects prevailing ideas have on societies. The term first came into widespread use in the 1920s, when a number of German-speaking theorists, most notably Max Scheler, and Karl Mannheim, wrote extensively on it. With the dominance of functionalism through the middle years of the 20th century, the sociology of knowledge tended to remain on the periphery of mainstream sociological thought. It was largely reinvented and applied much more closely to everyday life in the 1960s, particularly by Peter L. Berger and Thomas Luckmann in The Social Construction of Reality (1966) and is still central for methods dealing with qualitative understanding of human society (compare socially constructed reality). The "archaeological" and "genealogical" studies of Michel Foucault are of considerable contemporary influence.

The sociology of science involves the study of science as a social activity, especially dealing "with the social conditions and effects of science, and with the social structures and processes of scientific activity."[149] Important theorists in the sociology of science include Robert K. Merton and Bruno Latour. These branches of sociology have contributed to the formation of science and technology studies. Both the ASA and the BSA have sections devoted to the subfield of Science, Knowledge and Technology.[150][151] The ISA maintains a Research Committee on Science and Technology.[152]

Sociology of leisure is the study of how humans organize their free time. Leisure includes a broad array of activities, such as sport, tourism, and the playing of games. The sociology of leisure is closely tied to the sociology of work, as each explores a different side of the work–leisure relationship. More recent studies in the field move away from the work–leisure relationship and focus on the relation between leisure and culture. This area of sociology began with Thorstein Veblen's Theory of the Leisure Class.[153]

This subfield of sociology studies, broadly, the dynamics of war, conflict resolution, peace movements, war refugees, conflict resolution and military institutions.[154] As a subset of this subfield, military sociology aims towards the systematic study of the military as a social group rather than as an organization. It is a highly specialized sub-field which examines issues related to service personnel as a distinct group with coerced collective action based on shared interests linked to survival in vocation and combat, with purposes and values that are more defined and narrower than within civil society. Military sociology also concerns civilian-military relations and interactions between other groups or governmental agencies. Topics include the dominant assumptions held by those in the military, changes in military members' willingness to fight, military unionization, military professionalism, the increased utilization of women, the military industrial-academic complex, the military's dependence on research, and the institutional and organizational structure of military.[155]

Historically, political sociology concerned the relations between political organization and society. A typical research question in this area might be: "Why do so few American citizens choose to vote?"[156] In this respect questions of political opinion formation brought about some of the pioneering uses of statistical survey research by Paul Lazarsfeld. A major subfield of political sociology developed in relation to such questions, which draws on comparative history to analyse socio-political trends. The field developed from the work of Max Weber and Moisey Ostrogorsky.[157]

Contemporary political sociology includes these areas of research, but it has been opened up to wider questions of power and politics.[158] Today political sociologists are as likely to be concerned with how identities are formed that contribute to structural domination by one group over another; the politics of who knows how and with what authority; and questions of how power is contested in social interactions in such a way as to bring about widespread cultural and social change. Such questions are more likely to be studied qualitatively. The study of social movements and their effects has been especially important in relation to these wider definitions of politics and power.[159]

Political sociology has also moved beyond methodological nationalism and analysed the role of non-governmental organizations, the diffusion of the nation-state throughout the Earth as a social construct, and the role of stateless entities in the modern world society. Contemporary political sociologists also study inter-state interactions and human rights.[citation needed]

Demographers or sociologists of population study the size, composition and change over time of a given population. Demographers study how these characteristics impact, or are impacted by, various social, economic or political systems. The study of population is also closely related to human ecology and environmental sociology, which studies a population's relationship with the surrounding environment and often overlaps with urban or rural sociology. Researchers in this field may study the movement of populations: transportation, migrations, diaspora, etc., which falls into the subfield known as mobilities studies and is closely related to human geography. Demographers may also study spread of disease within a given population or epidemiology.[citation needed]

Public sociology refers to an approach to the discipline which seeks to transcend the academy in order to engage with wider audiences. It is perhaps best understood as a style of sociology rather than a particular method, theory, or set of political values. This approach is primarily associated with Michael Burawoy who contrasted it with professional sociology, a form of academic sociology that is concerned primarily with addressing other professional sociologists. Public sociology is also part of the broader field of science communication or science journalism.[citation needed]

The sociology of race and of ethnic relations is the area of the discipline that studies the social, political, and economic relations between races and ethnicities at all levels of society. This area encompasses the study of racism, residential segregation, and other complex social processes between different racial and ethnic groups. This research frequently interacts with other areas of sociology such as stratification and social psychology, as well as with postcolonial theory. At the level of political policy, ethnic relations are discussed in terms of either assimilationism or multiculturalism. Anti-racism forms another style of policy, particularly popular in the 1960s and 1970s.[citation needed]

The sociology of religion concerns the practices, historical backgrounds, developments, universal themes and roles of religion in society.[160] There is particular emphasis on the recurring role of religion in all societies and throughout recorded history. The sociology of religion is distinguished from the philosophy of religion in that sociologists do not set out to assess the validity of religious truth-claims, instead assuming what Peter L. Berger has described as a position of "methodological atheism".[161] It may be said that the modern formal discipline of sociology began with the analysis of religion in Durkheim's 1897 study of suicide rates among Roman Catholic and Protestant populations. Max Weber published four major texts on religion in a context of economic sociology and social stratification: The Protestant Ethic and the Spirit of Capitalism (1905), The Religion of China: Confucianism and Taoism (1915), The Religion of India: The Sociology of Hinduism and Buddhism (1915), and Ancient Judaism (1920). Contemporary debates often centre on topics such as secularization, civil religion, the intersection of religion and economics and the role of religion in a context of globalization and multiculturalism.[162]

The sociology of change and development attempts to understand how societies develop and how they can be changed. This includes studying many different aspects of society, for example demographic trends,[163] political or technological trends,[164] or changes in culture. Within this field, sociologists often use macrosociological methods or historical-comparative methods. In contemporary studies of social change, there are overlaps with international development or community development. However, most of the founders of sociology had theories of social change based on their study of history. For instance, Marx contended that the material circumstances of society ultimately caused the ideal or cultural aspects of society, while Weber argued that it was in fact the cultural mores of Protestantism that ushered in a transformation of material circumstances. In contrast to both, Durkheim argued that societies moved from simple to complex through a process of sociocultural evolution. Sociologists in this field also study processes of globalization and imperialism. Most notably, Immanuel Wallerstein extends Marx's theoretical frame to include large spans of time and the entire globe in what is known as world systems theory. Development sociology is also heavily influenced by post-colonialism. In recent years, Raewyn Connell issued a critique of the bias in sociological research towards countries in the Global North. She argues that this bias blinds sociologists to the lived experiences of the Global South, specifically, so-called, "Northern Theory" lacks an adequate theory of imperialism and colonialism.[citation needed]

There are many organizations studying social change, including the Fernand Braudel Center for the Study of Economies, Historical Systems, and Civilizations, and the Global Social Change Research Project.[citation needed]

A social network is a social structure composed of individuals (or organizations) called "nodes", which are tied (connected) by one or more specific types of interdependency, such as friendship, kinship, financial exchange, dislike, sexual relationships, or relationships of beliefs, knowledge or prestige. Social networks operate on many levels, from families up to the level of nations, and play a critical role in determining the way problems are solved, organizations are run, and the degree to which individuals succeed in achieving their goals. An underlying theoretical assumption of social network analysis is that groups are not necessarily the building blocks of society: the approach is open to studying less-bounded social systems, from non-local communities to networks of exchange. Drawing theoretically from relational sociology, social network analysis avoids treating individuals (persons, organizations, states) as discrete units of analysis, it focuses instead on how the structure of ties affects and constitutes individuals and their relationships. In contrast to analyses that assume that socialization into norms determines behaviour, network analysis looks to see the extent to which the structure and composition of ties affect norms. On the other hand, recent research by Omar Lizardo also demonstrates that network ties are shaped and created by previously existing cultural tastes.[165] Social network theory is usually defined in formal mathematics and may include integration of geographical data into sociomapping.[citation needed]

Sociological social psychology focuses on micro-scale social actions. This area may be described as adhering to "sociological miniaturism", examining whole societies through the study of individual thoughts and emotions as well as behaviour of small groups.[166] One special concern to psychological sociologists is how to explain a variety of demographic, social, and cultural facts in terms of human social interaction. Some of the major topics in this field are social inequality, group dynamics, prejudice, aggression, social perception, group behaviour, social change, non-verbal behaviour, socialization, conformity, leadership, and social identity. Social psychology may be taught with psychological emphasis.[167] In sociology, researchers in this field are the most prominent users of the experimental method (however, unlike their psychological counterparts, they also frequently employ other methodologies). Social psychology looks at social influences, as well as social perception and social interaction.[167]

Social stratification is the hierarchical arrangement of individuals into social classes, castes, and divisions within a society.[27]: 225  Modern Western societies stratification traditionally relates to cultural and economic classes arranged in three main layers: upper class, middle class, and lower class, but each class may be further subdivided into smaller classes (e.g. occupational).[168] Social stratification is interpreted in radically different ways within sociology. Proponents of structural functionalism suggest that, since the stratification of classes and castes is evident in all societies, hierarchy must be beneficial in stabilizing their existence. Conflict theorists, by contrast, critique the inaccessibility of resources and lack of social mobility in stratified societies.[citation needed]

Karl Marx distinguished social classes by their connection to the means of production in the capitalist system: the bourgeoisie own the means, but this effectively includes the proletariat itself as the workers can only sell their own labour power (forming the material base of the cultural superstructure). Max Weber critiqued Marxist economic determinism, arguing that social stratification is not based purely on economic inequalities, but on other status and power differentials (e.g. patriarchy). According to Weber, stratification may occur among at least three complex variables:

Pierre Bourdieu provides a modern example in the concepts of cultural and symbolic capital. Theorists such as Ralf Dahrendorf have noted the tendency towards an enlarged middle-class in modern Western societies, particularly in relation to the necessity of an educated work force in technological or service-based economies.[170] Perspectives concerning globalization, such as dependency theory, suggest this effect owes to the shift of workers to the developing countries.[171]

Urban sociology involves the analysis of social life and human interaction in metropolitan areas. It is a discipline seeking to provide advice for planning and policy making. After the Industrial Revolution, works such as Georg Simmel's The Metropolis and Mental Life (1903) focused on urbanization and the effect it had on alienation and anonymity. In the 1920s and 1930s The Chicago School produced a major body of theory on the nature of the city, important to both urban sociology and criminology, utilizing symbolic interactionism as a method of field research. Contemporary research is commonly placed in a context of globalization, for instance, in Saskia Sassen's study of the "global city".[172] Rural sociology, by contrast, is the analysis of non-metropolitan areas. As agriculture and wilderness tend to be a more prominent social fact in rural regions, rural sociologists often overlap with environmental sociologists.[citation needed]

Often grouped with urban and rural sociology is that of community sociology or the sociology of community.[173] Taking various communities—including online communities—as the unit of analysis, community sociologists study the origin and effects of different associations of people. For instance, German sociologist Ferdinand Tönnies distinguished between two types of human association: gemeinschaft (usually translated as "community") and gesellschaft ("society" or "association"). In his 1887 work, Gemeinschaft und Gesellschaft, Tönnies argued that Gemeinschaft is perceived to be a tighter and more cohesive social entity, due to the presence of a "unity of will".[174] The 'development' or 'health' of a community is also a central concern of community sociologists also engage in development sociology, exemplified by the literature surrounding the concept of social capital.[citation needed]

Sociology overlaps with a variety of disciplines that study society, in particular social anthropology, political science, economics, social work and social philosophy. Many comparatively new fields such as communication studies, cultural studies, demography and literary theory, draw upon methods that originated in sociology. The terms "social science" and "social research" have both gained a degree of autonomy since their origination in classical sociology. The distinct field of social anthropology or anthroposociology is the dominant constituent of anthropology throughout the United Kingdom and Commonwealth and much of Europe (France in particular),[175] where it is distinguished from cultural anthropology.[176] In the United States, social anthropology is commonly subsumed within cultural anthropology (or under the relatively new designation of sociocultural anthropology).[177]

Sociology and applied sociology are connected to the professional and academic discipline of social work.[178] Both disciplines study social interactions, community and the effect of various systems (i.e. family, school, community, laws, political sphere) on the individual.[179] However, social work is generally more focused on practical strategies to alleviate social dysfunctions; sociology in general provides a thorough examination of the root causes of these problems.[180] For example, a sociologist might study why a community is plagued with poverty. The applied sociologist would be more focused on practical strategies on what needs to be done to alleviate this burden. The social worker would be focused on action; implementing theses strategies "directly" or "indirectly" by means of mental health therapy, counselling, advocacy, community organization or community mobilization.[179]

Social anthropology is the branch of anthropology that studies how contemporary living human beings behave in social groups. Practitioners of social anthropology, like sociologists, investigate various facets of social organization. Traditionally, social anthropologists analyzed non-industrial and non-Western societies, whereas sociologists focused on industrialized societies in the Western world. In recent years, however, social anthropology has expanded its focus to modern Western societies, meaning that the two disciplines increasingly converge.[181][178]

Sociocultural anthropology, which includes linguistic anthropology, is concerned with the problems of difference and similarity within and between human populations. The discipline arose concomitantly with the expansion of European colonial empires, and its practices and theories have been questioned and reformulated along with processes of decolonization. Such issues have re-emerged as transnational processes have challenged the centrality of the nation-state to theorizations about culture and power. New challenges have emerged as public debates about multiculturalism, and the increasing use of the culture concept outside of the academy and among peoples studied by anthropology.[citation needed]

Irving Louis Horowitz, in his The Decomposition of Sociology (1994), has argued that the discipline, while arriving from a "distinguished lineage and tradition", is in decline due to deeply ideological theory and a lack of relevance to policy making: "The decomposition of sociology began when this great tradition became subject to ideological thinking, and an inferior tradition surfaced in the wake of totalitarian triumphs."[182] Furthermore: "A problem yet unmentioned is that sociology's malaise has left all the social sciences vulnerable to pure positivism—to an empiricism lacking any theoretical basis. Talented individuals who might, in an earlier time, have gone into sociology are seeking intellectual stimulation in business, law, the natural sciences, and even creative writing; this drains sociology of much needed potential."[182] Horowitz cites the lack of a 'core discipline' as exacerbating the problem. Randall Collins, the Dorothy Swaine Thomas Professor in Sociology at the University of Pennsylvania and a member of the Advisory Editors Council of the Social Evolution & History journal, has voiced similar sentiments: "we have lost all coherence as a discipline, we are breaking up into a conglomerate of specialities, each going on its own way and with none too high regard for each other."[183]

In 2007, The Times Higher Education Guide published a list of 'The most cited authors of books in the Humanities' (including philosophy and psychology). Seven of the top ten are listed as sociologists: Michel Foucault (1), Pierre Bourdieu (2), Anthony Giddens (5), Erving Goffman (6), Jürgen Habermas (7), Max Weber (8), and Bruno Latour (10).[184]

The most highly ranked general journals which publish original research in the field of sociology are the American Journal of Sociology and the American Sociological Review.[185] The Annual Review of Sociology, which publishes original review essays, is also highly ranked.[185] Many other generalist and specialized journals exist.

[1]Republicans

 Nationalists

100,000–200,000 civilians killed inside the Nationalist zone[7][8][9]

50,000–72,000 civilians killed inside the Republican zone

The Spanish Civil War (Spanish: guerra civil española)[note 2] was a military conflict fought from 1936 to 1939 between the Republicans and the Nationalists. Republicans were loyal to the left-leaning Popular Front government of the Second Spanish Republic.[10] The opposing Nationalists were an alliance of Falangists, monarchists, conservatives, and traditionalists led by a military junta among whom General Francisco Franco quickly achieved a preponderant role. Due to the international political climate at the time, the war was variously viewed as class struggle, a religious struggle, or a struggle between dictatorship and republican democracy, between revolution and counterrevolution, or between fascism and communism.[11] The Nationalists won the war, which ended in early 1939, and ruled Spain until Franco's death in November 1975.

The war began after the partial failure of the coup d'état of July 1936 against the Popular Front government by a group of generals of the Spanish Republican Armed Forces, with General Emilio Mola as the primary planner and leader and General José Sanjurjo as a figurehead.[12][13] The Nationalist faction was supported by several  conservative groups, including CEDA, monarchists, including both the opposing Alfonsists and the religious conservative Carlists, and the Falange Española de las JONS, a fascist political party.[14] The uprising was supported by military units in Morocco, Pamplona, Burgos, Zaragoza, Valladolid, Cádiz, Córdoba, Málaga, and Seville. However, rebelling units in almost all important cities did not gain control. Those cities remained in the hands of the government, leaving Spain militarily and politically divided. The Nationalist forces received munitions, soldiers, and air support from Fascist Italy and Nazi Germany while the Republican side received support from the Soviet Union and Mexico. Other countries, such as the United Kingdom, France, and the United States, continued to recognise the Republican government but followed an official policy of non-intervention. Despite this policy, tens of thousands of citizens from non-interventionist countries directly participated in the conflict, mostly in the pro-Republican International Brigades.

Franco gradually emerged as the primary leader of the Nationalist side. The Nationalists advanced from their strongholds in the south and west, capturing most of Spain's northern coastline in 1937. They besieged Madrid and the area to its south and west. After much of Catalonia was captured in 1938 and 1939, and Madrid cut off from Barcelona, the Republican military position became hopeless. On 5 March 1939, in response to an alleged increasing communist dominance of the Republican government and the deteriorating military situation, Colonel Segismundo Casado led a military coup against the Republican government, intending to seek peace with the Nationalists. These peace overtures, however, were rejected by Franco. Following internal conflict between Republican factions in Madrid in the same month, Franco entered the capital and declared victory on 1 April 1939. Hundreds of thousands of Spaniards fled to refugee camps in southern France.[15] Those associated with the losing Republicans who stayed were persecuted by the victorious Nationalists. Franco established a dictatorship.[14]

The war became notable for the passion and political division it inspired worldwide and for the many atrocities that occurred. Organised purges occurred in territory captured by Franco's forces so they could consolidate their future regime.[16] Mass executions also took place in areas controlled by the Republicans,[17] with the participation of local authorities varying from location to location.[18][19]

In 1868, popular uprisings led to the overthrow of Queen Isabella II of the House of Bourbon. Two distinct factors led to the uprisings: a series of urban riots and a liberal movement within the middle classes and the military (led by General Joan Prim), which was concerned about the ultra-conservatism of the monarchy. In 1873, Isabella's replacement, King Amadeo I of the House of Savoy, abdicated due to increasing political pressure, and the short-lived First Spanish Republic was proclaimed.[20][21] The Republic was marred with political instability and conflicts and was quickly overthrown by a coup d'état by General Arsenio Martínez Campos in December 1874, after which the Bourbons were restored to the throne in the figure of Alfonso XII, Isabella's son.[22]

After the restoration, Carlists and anarchists emerged in opposition to the monarchy.[23][24] Alejandro Lerroux, Spanish politician and leader of the Radical Republican Party, helped to bring republicanism to the fore in Catalonia—a region of Spain with its own cultural and societal identity in which poverty was particularly acute at the time.[25] Conscription was a controversial policy that was eventually implemented by the government of Spain. As evidenced by the Tragic Week in 1909, resentment and resistance were factors that continued well into the 20th century.[26]

Spain was neutral in World War I. Following the war, wide swathes of Spanish society, including the armed forces, united in hopes of removing the corrupt central government of the country in Madrid, but these circles were ultimately unsuccessful.[27] Popular perception of communism as a major threat significantly increased during this period.[28]

In 1923, a military coup brought Miguel Primo de Rivera to power. As a result, Spain transitioned to government by military dictatorship.[29] Support for the Rivera regime gradually faded, and he resigned in January 1930. He was replaced by General Dámaso Berenguer, who was in turn himself replaced by Admiral Juan Bautista Aznar-Cabañas; both men continued a policy of rule by decree.

There was little support for the monarchy in the major cities. Consequently, King Alfonso XIII of Spain relented to popular pressure and called municipal elections for 12 April 1931. Left-wing entities such as the Socialist and Liberal Republicans won almost all the provincial capitals and, following the resignation of Aznar's government, Alfonso XIII fled the country.[30] At this time, the Second Spanish Republic was formed. This republic remained in power until the beginning of the civil war five years later.[31]

The revolutionary committee headed by Niceto Alcalá-Zamora became the provisional government, with Alcalá-Zamora himself as president and head of state.[32] The republic had broad support from all segments of society.[33]

In May 1931, an incident in which a taxi driver was attacked outside a monarchist club sparked anti-clerical violence throughout Madrid and south-west portion of the country. The slow response on the part of the government disillusioned the right and reinforced their view that the Republic was determined to persecute the church. In June and July, the Confederación Nacional del Trabajo (CNT) called several strikes, which led to a violent incident between CNT members and the Civil Guard and a brutal crackdown by the Civil Guard and the army against the CNT in Seville. This led many workers to believe the Second Spanish Republic was just as oppressive as the monarchy, and the CNT announced its intention of overthrowing it via revolution.[34]

Elections in June 1931 returned a large majority of Republicans and Socialists.[22] With the onset of the Great Depression, the government tried to assist rural Spain by instituting an eight-hour day and redistributing land tenure to farm workers.[35][36] The rural workers lived in some of the worst poverty in Europe at the time and the government tried to increase their wages and improve working conditions. This estranged small and medium landholders who used hired labour. The Law of Municipal Boundaries forbade owners from hiring workers outside their locality. When some localities had labour shortages, the law shut out workers seeking extra income as pickers.

Newly established labour arbitration boards regulated salaries, contracts, and working hours, but were more favourable to workers than employers. Class struggle intensified as landowners turned to counterrevolutionary organisations and local oligarchs. Strikes, workplace theft, arson, robbery and assaults on shops, strikebreakers, employers and machines became increasingly common.[37]

Republican Manuel Azaña became prime minister of a minority government in October 1931.[40][41] Fascism remained a reactive threat and it was facilitated by controversial reforms to the military.[42] In December, a new reformist, liberal and democratic constitution was declared. It included strong provisions enforcing a broad secularisation of the Catholic country, which included the abolition of Catholic schools and charities, a move which was met with opposition.[43] At this point, once the constituent assembly had fulfilled its mandate of approving a new constitution, but fearing an increasing popular opposition, the Radical and Socialist majority postponed the regular elections, prolonging their time in power for two more years. Diaz's Republican government initiated numerous reforms to, in their view, modernize the country. In 1932, the Jesuits were banned and their property was confiscated, the army was reduced, landowners were expropriated. Home rule was granted to Catalonia, with a local parliament and a president of its own.[44] In June 1933, Pope Pius XI issued the encyclical Dilectissima Nobis, "On Oppression of the Church of Spain", raising his voice against the persecution of the Catholic Church in Spain.[45]

In November 1933, the right-wing parties won the general election.[46] The causal factors were increased resentment of the incumbent government caused by a controversial decree implementing land reform,[47] by the Casas Viejas incident,[48] and the formation of a right-wing alliance, Spanish Confederation of Autonomous Right-wing Groups (CEDA). Another factor was the recent enfranchisement of women, most of whom voted for centre-right parties.[49] According to Stanley G. Payne and Jesús Palacios Tapias, left Republicans attempted to have Niceto Alcalá Zamora cancel the electoral results but did not succeed. Despite CEDA's electoral victory, President Alcalá-Zamora declined to invite its leader, Gil Robles, to form a government, fearing CEDA's monarchist sympathies and proposed changes to the constitution. Instead, he invited the Radical Republican Party's Alejandro Lerroux to do so.[50]

Events in the period after November 1933, called the "black biennium", seemed to make a civil war more likely.[51] Alejandro Lerroux of the Radical Republican Party (RRP) formed a government, reversing changes made by the previous administration[52] and granting amnesty to the collaborators of the unsuccessful uprising by General José Sanjurjo in August 1932.[53][54] Some monarchists joined with the then fascist-nationalist Falange Española y de las JONS ("Falange") to help achieve their aims.[55] Open violence occurred in the streets of Spanish cities, and militancy continued to increase,[56] reflecting a movement towards radical upheaval, rather than peaceful democratic means as solutions.[57] A small insurrection by anarchists occurred in December 1933 in response to CEDA's victory, in which around 100 people died.[58] After a year of intense pressure, CEDA, the party with the most seats in parliament, finally succeeded in forcing the acceptance of three ministries. The Socialists (PSOE) and Communists reacted with an insurrection for which they had been preparing for nine months.[59]

The rebellion developed into a bloody uprising known as the Revolution of 1934. Fairly well armed revolutionaries managed to take the whole province of Asturias, murdering numerous policemen, clergymen and civilians, destroying religious buildings including churches, convents and part of the university at Oviedo.[60] Rebels in the occupied areas proclaimed revolution for the workers and abolished the existing currency.[61] The rebellion was crushed in two weeks by the Spanish Navy and the Spanish Republican Army, the latter using mainly Moorish colonial troops from Spanish Morocco.[62] Azaña was in Barcelona that day, and the Lerroux-CEDA government tried to implicate him. He was arrested and charged with complicity. In fact, Azaña had no connection with the rebellion and was released from prison in January 1935.[63]

In sparking an uprising, the non-anarchist socialists, like the anarchists, manifested their conviction that the existing political order was illegitimate.[64] The Spanish historian Salvador de Madariaga, an Azaña supporter and an exiled vocal opponent of Francisco Franco, wrote a sharp criticism of the left's participation in the revolt: "The uprising of 1934 is unforgivable. The argument that Mr Gil Robles tried to destroy the Constitution to establish fascism was, at once, hypocritical and false. With the rebellion of 1934, the Spanish left lost even the shadow of moral authority to condemn the rebellion of 1936."[65]

Reversals of land reform resulted in expulsions, firings and arbitrary changes to working conditions in the central and southern countryside in 1935, with landowners' behaviour at times reaching "genuine cruelty", which included violence against farmworkers and socialists, causing several deaths. One historian argued that the behaviour of the right in the southern countryside was one of the main causes of hatred during the Civil War and possibly even the Civil War itself.[66] Landowners taunted workers by saying that if they went hungry, they should "Go eat the Republic!"[67][68] Bosses fired leftist workers and imprisoned trade union and socialist militants; wages were reduced to "salaries of hunger".[69]

In 1935, the government, led by the Radical Republican Party, had now endured a series of crises. After a number of corruption scandals, President Niceto Alcalá-Zamora, who was hostile to this government, called another election.

The Popular Front narrowly won the 1936 general election. The revolutionary left-wing masses took to the streets and freed prisoners. In the thirty-six hours following the election, sixteen people were killed (mostly by police officers attempting to maintain order or to intervene in violent clashes) and thirty-nine were seriously injured. Additionally, fifty churches and seventy conservative political centres were attacked or set ablaze.[70] Manuel Azaña was called to form a government before the electoral process had ended. He shortly replaced Zamora as president, taking advantage of a constitutional loophole. Convinced that the left was no longer willing to follow the rule of law and that its vision of Spain was under threat, the right abandoned the parliamentary option and began planning to overthrow the republic, rather than to control it.[71]

PSOE's left wing socialists started to take action. Julio Álvarez del Vayo talked about "Spain being converted into a socialist Republic in association with the Soviet Union". Francisco Largo Caballero declared that "the organized proletariat will carry everything before it and destroy everything until we reach our goal".[72] The country had rapidly become anarchic. Even the staunch socialist Indalecio Prieto, at a party rally in Cuenca in May 1936, complained: "we have never seen so tragic a panorama or so great a collapse as in Spain at this moment. Abroad, Spain is classified as insolvent. This is not the road to socialism or communism but to desperate anarchism without even the advantage of liberty".[72] The disenchantment with Azaña's ruling was also voiced by Miguel de Unamuno, a republican and one of Spain's most respected intellectuals who, in June 1936, told a reporter who published his statement in El Adelanto that President Manuel Azaña should commit suicide "as a patriotic act".[73]

Laia Balcells observes that polarisation in Spain just before the coup was so intense that physical confrontations between leftists and rightists were a routine occurrence in most localities; six days before the coup occurred, there was a riot between the two in the province of Teruel. Balcells notes that Spanish society was so divided along Left-Right lines that the monk Hilari Raguer stated that in his parish, instead of playing "cops and robbers", children would sometimes play "leftists and rightists".[74] Within the first month of the Popular Front's government, nearly a quarter of the provincial governors had been removed due to their failure to prevent or control strikes, illegal land occupation, political violence and arson. The Popular Front government was more likely to prosecute rightists for violence than leftists who committed similar acts.[75] Between February and July, approximately 300 to 400 deaths occurred from political violence, while hundreds of churches, religious buildings and monuments were destroyed, damaged, or vandalised.[76][77]

Azaña was hesitant to use the army to shoot or stop rioters or protestors as many of them supported his coalition. On the other hand, he was reluctant to disarm the military as he believed he needed them to stop insurrections from the extreme left. Illegal land occupation became widespread—poor tenant farmers knew the government was disinclined to stop them. By April 1936, nearly 100,000 peasants had appropriated 400,000 hectares of land and perhaps as many as 1 million hectares by the start of the civil war; for comparison, the 1931–33 land reform had granted only 6,000 peasants 45,000 hectares.[78] As many strikes occurred between April and July as had occurred in the entirety of 1931. Workers increasingly demanded less work and more pay. "Social crimes"—refusing to pay for goods and rent—became increasingly common. In some cases, this was done in the company of armed militants. Conservatives, the middle classes, businessmen and landowners became convinced that revolution had already begun.[79]

Prime Minister Santiago Casares Quiroga ignored warnings of a military conspiracy involving several generals, who decided that the government had to be replaced to prevent the dissolution of Spain.[80] Both sides had become convinced that, if the other side gained power, it would discriminate against their members and attempt to suppress their political organisations.[81]

Shortly after the Popular Front's victory in the 1936 election, groups of officers, both active and retired, got together to discuss a coup. By the end of April General Emilio Mola emerged as the leader of a national conspiracy network.[82] The Republican government reacted by reshuffling suspect generals from influential posts, Azana however acutely aware that as he did so, the Army still served as a possible buffer to leftist power brokers threatening his government.[83] Franco was sacked as chief of staff and transferred to command of the Canary Islands.[84] Manuel Goded Llopis was removed as inspector general and made general of the Balearic Islands.  Mola was moved from head of the Army of Africa to commander of Pamplona.[85] This latter reassignment, however, allowed Mola to direct the mainland uprising; General José Sanjurjo became the figurehead of the operation and helped reach an agreement with the Carlists,[85] Mola was chief planner and second in command.[71] José Antonio Primo de Rivera was put in prison in mid-March to restrict the Falange.[85] However, government actions were not as thorough as they might have been, and warnings by the Director of Security and other figures were not acted upon.[84]

The revolt was devoid of ideology. The goal was to put an end to anarchical disorder.[86] Mola's plan for the new regime was a "republican dictatorship", modelled after Salazar's Portugal and along the lines of being semi-pluralist authoritarian, rather than fascist totalitarian. The initial government would be an all-military "Directory", which would create a "strong and disciplined state". Sanjurjo would be the head of this new regime, due to being liked and respected within the military, though his position would be symbolic due to his lack of political talent. The 1931 Constitution would be suspended, replaced by a new "constituent parliament" which would be chosen by a new politically purged electorate, who would vote on the issue of republic versus monarchy. Liberal elements would remain, such as separation of church and state as well as freedom of religion. Agrarian issues would be solved by regional commissioners on the basis of smallholdings, but collective cultivation would be permitted in some circumstances. Legislation prior to February 1936 would be respected. Violence would be required to destroy opposition to the coup, though it seems Mola did not envision the mass atrocities and repression that would manifest during the civil war.[87][88] Of particular importance to Mola was ensuring the revolt was an Army affair, not subject to special interests, ensuring the position of the armed forces as the basis for the new state.[89] However, the separation of church and state was forgotten once the conflict assumed the dimension of a war of religion, and military authorities increasingly deferred to the Church and to the expression of Catholic sentiment.[90] Mola's program was vague and only a rough sketch, and there were disagreements among coupists about their vision.[91][92]

On 12 June, Prime Minister Casares Quiroga met General Juan Yagüe, who falsely convinced Casares of his loyalty to the republic.[93] Mola began planning in the spring. Franco was a key player because of his prestige as a former director of the military academy and as the man who suppressed the Asturian miners' strike of 1934.[71] He was respected in the Army of Africa, the Army's toughest troops.[94] He wrote a cryptic letter to Casares on 23 June, suggesting the military was disloyal, but could be restrained if he were put in charge. Casares did nothing, failing to arrest or buy off Franco.[94] With the help of the British intelligence agents, the rebels chartered a Dragon Rapide aircraft[95] to transport Franco from the Canary Islands to Spanish Morocco.[96] Franco arrived in Morocco on 19 July.[97] Franco was offered this position as Mola's planning for the coup had become increasingly complex and it did not look like it would be as swift as he hoped, instead likely turning into a miniature civil war that would last weeks. Mola had concluded troops in Spain were insufficient and it would be necessary to use elite units from North Africa, something Franco had always believed would be necessary.[98]

On 12 July 1936, Falangists in Madrid killed police officer Lieutenant José Castillo of the Guardia de Asalto (Assault Guard). Castillo was a Socialist party member who was giving military training to the UGT youth. Castillo had led the Assault Guards that violently suppressed the riots after the funeral of Guardia Civil lieutenant Anastasio de los Reyes.[97] Assault Guard Captain Fernando Condés was a friend of Castillo. The next day, after getting the approval of the minister of interior to illegally arrest members of parliament, he led his squad to arrest José María Gil-Robles y Quiñones, founder of CEDA, as a reprisal for Castillo's murder, but he was not at home, so they went to the house of José Calvo Sotelo, a Spanish monarchist and prominent parliamentary conservative.[99] Luis Cuenca, a member of the arresting group and a Socialist who was known as the bodyguard of PSOE leader Indalecio Prieto, summarily executed Sotelo.[99][100]

Reprisals followed.[99] The killing of Calvo Sotelo with police involvement aroused suspicions and reactions among the government's opponents on the right.[100] Although the nationalist generals were planning an uprising, the event was a catalyst and a public justification for a coup.[99] Stanley Payne claims that before these events, the idea of rebellion by army officers against the government had weakened; Mola had estimated only 12% of officers reliably supported the coup and Mola considered fleeing for fear he was compromised.  He had to be convinced to remain by his co-conspirators.[101] However, the kidnapping and murder of Sotelo transformed the "limping conspiracy" into a revolt that could trigger a civil war.[102][103]

The arbitrary use of lethal force by the state and lack of action against the attackers led to public disapproval of the government. No effective punitive, judicial or even investigative action was taken; Payne points to a possible veto by socialists within the government who shielded the killers drawn from their ranks.[104] The murder of a parliamentary leader by state police was unprecedented, and the belief the state had ceased to be neutral and effective encouraged important sectors of the right to join the rebellion.[105] Franco changed his mind on rebellion and dispatched a message to Mola to display his firm commitment.[106]

The Socialists and Communists, led by Indalecio Prieto, demanded that arms be distributed to the people before the military took over. The prime minister was hesitant.[99]

The uprising's timing was fixed at 17 July, at 17:01, agreed to by the leader of the Carlists, Manuel Fal Conde.[107] However, the timing was changed—the men in the Morocco protectorate were to rise up at 05:00 on 18 July and those in Spain proper a day later so that control of Spanish Morocco could be achieved and forces sent back to the Iberian Peninsula to coincide with the risings there.[108] The rising was intended to be a swift coup d'état, but the government retained control of most of the country.[109]

Control over Spanish Morocco was all but certain.[110] The plan was discovered in Morocco on 17 July, which prompted the conspirators to enact it immediately. Little resistance was encountered. The rebels shot 189 people.[111] Goded and Franco immediately took control of the islands to which they were assigned.[71] On 18 July, Casares Quiroga refused an offer of help from the CNT and Unión General de Trabajadores (UGT), leading the groups to proclaim a general strike—in effect, mobilising. They opened weapons caches, some buried since the 1934 risings, and formed militias.[112] The paramilitary security forces often waited for the outcome of militia action before either joining or suppressing the rebellion. Quick action by either the rebels or anarchist militias was often enough to decide the fate of a town.[113] General Gonzalo Queipo de Llano secured Seville for the rebels, arresting a number of other officers.[114]

The rebels failed to take any major cities with the critical exception of the July 1936 military uprising in Seville, which provided a landing point for Franco's African troops, and the primarily conservative and Catholic areas of Old Castile, León and Galicia, which fell quickly.[109] They took Cádiz with help from the first troops from Africa.[115] The "air bridge" between Tetouan and Seville with German planes allowed the Army of Africa to advance towards Extremadura and Madrid.[116]

The government retained control of Málaga, Jaén, and Almería. In Madrid, the rebels were hemmed into the Cuartel de la Montaña siege, which fell with considerable bloodshed. Republican leader Casares Quiroga was replaced by José Giral, who ordered the distribution of weapons among the civilian population.[117] This facilitated the defeat of the army insurrection in the main industrial centres, including Madrid, Barcelona, and Valencia, but it allowed anarchists to take control of Barcelona along with large swathes of Aragón and Catalonia.[118] General Goded surrendered in Barcelona and was later condemned to death.[119] The Republican government ended up controlling almost all the east coast and central area around Madrid, as well as most of Asturias, Cantabria and part of the Basque Country in the north.[120]

Hugh Thomas suggested that the civil war could have ended in the favour of either side almost immediately if certain decisions had been taken during the initial coup. Thomas argues that if the government had taken steps to arm the workers, they could probably have crushed the coup very quickly. Conversely, if the coup had risen everywhere in Spain on the 18th rather than be delayed, it could have triumphed by the 22nd.[121] While the militias that rose to meet the rebels were often untrained and poorly armed (possessing only a small number of pistols, shotguns and dynamite), this was offset by the fact that the rebellion was not universal. In addition, the Falangists and Carlists were themselves often not particularly powerful fighters either. However, enough officers and soldiers had joined the coup to prevent it from being crushed swiftly.[102]

The rebels termed themselves Nacionales, normally translated "Nationalists", although the former implies "true Spaniards" rather than a nationalistic cause.[122] The result of the coup was a nationalist area of control containing 11 million of Spain's population of 25 million.[123] The Nationalists had secured the support of around half of Spain's territorial army, some 60,000 men, joined by the Army of Africa, made up of 35,000 men,[124] and just under half of Spain's militaristic police forces, the Assault Guards, the Civil Guards, and the Carabineers.[125] Republicans controlled under half of the rifles and about a third of both machine guns and artillery pieces.[126]

The Spanish Republican Army had just 18 tanks of a sufficiently modern design, and the Nationalists took control of 10.[127] Naval capacity was uneven, with the Republicans retaining a numerical advantage, but with the Navy's top commanders and two of the most modern ships, heavy cruisers Canarias—captured at the Ferrol shipyard—and Baleares, in Nationalist control.[128] The Spanish Republican Navy suffered from the same problems as the army—many officers had defected or been killed after trying to do so.[127] Two-thirds of air capability was retained by the government—however, the whole of the Republican Air Force was very outdated.[129]

The war was cast by Republican sympathisers as a struggle between tyranny and freedom, and by Nationalist supporters as communist and anarchist red hordes versus Christian civilisation.[103] Nationalists also claimed they were bringing security and direction to an ungoverned and lawless country.[103] Spanish politics, especially on the left, was quite fragmented: on the one hand socialists and communists supported the republic but on the other, during the republic, anarchists had mixed opinions, though both major groups opposed the Nationalists during the Civil War; the latter, in contrast, were united by their fervent opposition to the Republican government and presented a more unified front.[130]

The coup divided regular forces fairly evenly. Out of some 66,000 military actually under arms in July 1936 (including the Army of Africa and the navy, excluding soldiers in service but on leave during the coup) some 52% (34,000) were in the Republican zone and 48% (32,000) in the Nationalist one.[131] Out of some 66,000 men in other armed services (Guardia Civil, Guardia de Asalto, Carabineros)[132] some 59% (39,000) joined the loyalists and some 41% (27,000) joined the rebels.[133] In total, out of some 132,000 armed and uniformed men actually in service, some 55% (73,000) seemed available to the loyalists and some 45% (59,000) to the rebels. However, one popular work claims that the loyalists controlled 90,000 men and the rebels controlled some 130,000.[134]

During the first few months, both armies were joined in high numbers by volunteers, Nationalists by some 100,000 men and Republicans by some 120,000.[135] From August, both sides launched their own, similarly scaled conscription schemes, resulting in further massive growth of their armies. Finally, the final months of 1936 saw the arrival of foreign troops, International Brigades joining the Republicans and Italian Corpo Truppe Volontarie (CTV), German Legion Condor and Portuguese Viriatos joining the Nationalists. The result was that in April 1937 there were some 360,000 soldiers in the Republican ranks and some 290,000 in the Nationalist ones.[136]

The armies kept growing. The principal source of manpower was conscription; both sides continued and expanded their schemes, the Nationalists drafting more aggressively, and there was little room left for volunteering. Foreigners contributed little to further growth; on the Nationalist side the Italians scaled down their engagement, while on the Republican side the influx of new interbrigadistas did not cover losses on the front. At the turn of 1937–1938, each army numbered about 700,000.[137]

Throughout 1938, the principal if not exclusive source of new men was a draft; at this stage it was the Republicans who conscripted more aggressively, and only 47% of their combatants were in age corresponding to the Nationalist conscription age limits.[138] Just prior to the Battle of Ebro, Republicans achieved their all-time high, slightly above 800,000; yet Nationalists numbered 880,000.[139] The Battle of Ebro, fall of Catalonia and collapsing discipline caused a great shrinking of Republican troops. In late February 1939, their army was 400,000[140] compared to more than double that number of Nationalists. In the moment of their final victory, Nationalists commanded over 900,000 troops.[141]

During the war, Nationalists also "recycled" former Republican soldiers as an additional source of manpower. As both sides employed conscription, this meant that not all soldiers were willing, committed partisans of either side and could shift their loyalties out of self-interest. Therefore, captured Republicans deemed apolitical or sufficiently sympathetic to the Nationalist cause were released from their concentration camps and then usually conscripted into the Nationalist army. By the end of 1937, out of around 107,000 Republicans taken prisoner, approximately 59,000 were classified as politically reliable enough for release and conscription into the Nationalist army.[142][143] The Republicans were also known to "recycle" Nationalist defectors as well.[144]

The total number of Spaniards serving in the Republican forces was officially stated as 917,000; later scholarly work estimated the number as "well over 1 million men",[145] though other studies claim the Republican total of 1.75 million (including non-Spaniards)[146] and "27 age groups, ranging from 18 to 44 years old".[147] The total number of Spaniards serving in the Nationalist units is estimated between "nearly 1 million men",[145] and 1.26 million (including non-Spaniards),[148] which comprised "15 age groups, ranging from 18 to 32 years old".[149]

Only two countries openly and fully supported the Republic: the Mexican government and the USSR. From them, especially the USSR, the Republic received diplomatic support, volunteers, weapons and vehicles. Other countries remained neutral; this neutrality faced serious opposition from sympathizers in the United States and United Kingdom, and to a lesser extent in other European countries and from Marxists worldwide. This led to formation of the International Brigades, thousands of foreigners of all nationalities who voluntarily went to Spain to aid the Republic in the fight; they meant a great deal to morale but militarily were not very significant.

The Republic's supporters within Spain ranged from centrists who supported a moderately capitalist liberal democracy to revolutionary anarchists who opposed the Republic but sided with it against the coup forces. Their base was primarily secular and urban but also included landless peasants and was particularly strong in industrial regions like Asturias, the Basque country, and Catalonia.[150]

This faction was called variously leales "Loyalists" by supporters, "Republicans", the "Popular Front", or "the government" by all parties; or los rojos "the Reds" by their opponents.[151] Republicans were supported by urban workers, agricultural labourers, and parts of the middle class.[152]

The conservative, strongly Catholic Basque country, along with Catholic Galicia and the more left-leaning Catalonia, sought autonomy or independence from the central government of Madrid. The Republican government allowed for the possibility of self-government for the two regions,[153] whose forces were gathered under the People's Republican Army (Ejército Popular Republicano, or EPR), which was reorganised into mixed brigades after October 1936.[154] 
Historian Stanley Payne argues that the Republicans' diverse combination of movements produced an unusual regime that lacked any exact counterpart elsewhere as it combined libertarian collectivism and regional autonomy with centralisation, state control and economic nationalisation. Payne argues that Republican Spain was not a democracy but also not a strict dictatorship, with the four different major left-wing factions remaining relatively autonomous from one another and operating within a semi-pluralist political framework and limited rule of law.[155]

A few well-known people fought on the Republican side, such as English writer George Orwell (who wrote Homage to Catalonia (1938), an account of his experiences in the war)[156] and Canadian thoracic surgeon Norman Bethune, who developed a mobile blood-transfusion service for front-line operations.[157] Simone Weil briefly fought with the anarchist columns of Buenaventura Durruti.[158]

At the beginning of the war, the Republicans outnumbered the Nationalists ten to one on the front in Aragon, but by January 1937, that advantage had dropped to four to one.[159]

The Nacionales or Nationalists, also called "insurgents", "rebels" or, by opponents, Franquistas or "fascists" —feared national fragmentation and opposed the separatist movements. They were chiefly defined by their anti-communism, which galvanised diverse or opposed movements like Falangists and monarchists. Their leaders had a generally wealthier, more conservative, monarchist, landowning background.[160]

The Nationalist side included the Carlists and Alfonsists, Spanish nationalists, the fascist Falange, and most conservatives and monarchist liberals. Virtually all Nationalist groups had strong Catholic convictions and supported the native Spanish clergy.[151] The Nationals included the majority of the Catholic clergy and practitioners (outside of the Basque region), important elements of the army, most large landowners, and many businessmen.[103] The Nationalist base largely consisted of the middle classes, conservative peasant smallholders in the North and Catholics in general. Catholic support became particularly pronounced as a consequence of the burning of churches and killing of priests in most leftists zones during the first six months of the war. By mid-1937, the Catholic Church gave its official blessing to the Franco regime; religious fervor was a major source of emotional support for the Nationalists during the civil war.[161] Michael Seidmann reports that devout Catholics, such as seminary students, often volunteered to fight and would die in disproportionate numbers in the war. Catholic confession cleared the soldiers of moral doubt and increased fighting ability; Republican newspapers described Nationalist priests as ferocious in battle and Indalecio Prieto remarked that the enemy he feared most was "the requeté who has just received communion".[162]

One of the rightists' principal motives was to confront the anti-clericalism of the Republican regime and to defend the Catholic Church,[160] which had been targeted by opponents, including Republicans, who blamed the institution for the country's ills. The Church opposed many of the Republicans' reforms, which were fortified by the Spanish Constitution of 1931.[163] Articles 24 and 26 of the 1931 constitution had banned the Society of Jesus. This proscription deeply offended many within the conservative fold. The revolution in the Republican zone at the outset of the war, in which 7,000 clergy and thousands of lay people were killed, deepened Catholic support for the Nationalists.[164][165]

Prior to the war, during the Asturian miners' strike of 1934, religious buildings were burnt and at least 100 clergy, religious civilians, and pro-Catholic police were killed by revolutionaries.[161][166] Franco had brought in Spain's colonial Army of Africa (Spanish: Ejército de África or Cuerpo de Ejército Marroquí) and reduced the miners to submission by heavy artillery attacks and bombing raids. The Spanish Legion committed atrocities and the army carried out summary executions of leftists. The repression in the aftermath was brutal and prisoners were tortured.[167] The Moroccan Fuerzas Regulares Indígenas joined the rebellion and played a significant role in the civil war.[168]

While the Nationalists are often assumed to have drawn in the majority of military officers, this is a somewhat simplistic analysis. The Spanish army had its own internal divisions and long-standing rifts. Officers supporting the coup tended to be africanistas (men who fought in North Africa between 1909 and 1923) while those who stayed loyal tended to be peninsulares (men who stayed back in Spain during this period). This was because during Spain's North African campaigns, the traditional promotion by seniority was suspended in favour of promotion by merit through battlefield heroism. This tended to benefit younger officers starting their careers as they could, while older officers had familial commitments that made it harder for them to be deployed in North Africa. Officers in front line combat corps (primarily infantry and cavalry) benefited over those in technical corps (those in artillery, engineering etc.) because they had more chances to demonstrate the requisite battlefield heroism and had also traditionally enjoyed promotion by seniority. The peninsulares resented seeing the africanistas rapidly leapfrog through the ranks, while the africanistas themselves were seen as swaggering and arrogant, further fueling resentment. Thus, when the coup occurred, officers who joined the rebellion, particularly from Franco's rank downwards, were often africanistas, while senior officers and those in non-front line positions tended to oppose it (though a small number of senior africanistas opposed the coup as well).[102] It has also been argued that officers who stayed loyal to the Republic were more likely to have been promoted and to have been favoured by the Republican regime (such as those in the Aviation and Assault Guard units).[169] Thus, while often thought of as a "rebellion of the generals", this is not correct. Of the eighteen division generals, only four rebelled (of the four division generals without postings, two rebelled and two remained loyal). Fourteen of the fifty-six brigade generals rebelled. The rebels tended to draw from less senior officers. Of the approximately 15,301 officers, just over half rebelled.[170]

Catalan and Basque nationalists were divided. Left-wing Catalan nationalists sided with the Republicans, while Conservative Catalan nationalists were far less vocal in supporting the government, due to anti-clericalism and confiscations occurring in areas within its control. Basque nationalists, heralded by the conservative Basque Nationalist Party, were mildly supportive of the Republican government, although some in Navarre sided with the uprising for the same reasons influencing conservative Catalans. Not withstanding religious matters, Basque nationalists, who were for the most part Catholic, generally sided with the Republicans, although the PNV, Basque nationalist party, was reported passing the plans of Bilbao defences to the Nationalists, in an attempt to reduce the duration and casualties of siege.[171]

The Spanish Civil War exposed political divisions across Europe. The right and the Catholics supported the Nationalists to stop the spread of Bolshevism. On the left, including labour unions, students and intellectuals, the war represented a necessary battle to stop the spread of fascism. Anti-war and pacifist sentiment was strong in many countries, leading to warnings that the Civil War could escalate into a second world war.[172] In this respect, the war was an indicator of the growing instability across Europe.[173]

The Spanish Civil War involved large numbers of non-Spanish citizens who participated in combat and advisory positions. Britain and France led a political alliance of 27 nations that pledged non-intervention, including an embargo on all arms exports to Spain. The United States unofficially adopted a position of non-intervention as well, despite abstaining from joining the alliance (due in part to its policy of political isolation). The group from the United States called themselves the "Abraham Lincoln Brigade". Germany, Italy and the Soviet Union signed on officially, but ignored the embargo. The attempted suppression of imported material was largely ineffective, and France was especially accused of allowing large shipments to Republican troops.[174] The clandestine actions of the various European powers were, at the time, considered to be risking another world war, alarming antiwar elements across the world.[175]

The League of Nations' reaction to the war was influenced by a fear of communism,[176] and was insufficient to contain the massive importation of arms and other war resources by the fighting factions. Although a Non-Intervention Committee was formed, its policies accomplished very little and its directives were ineffective.[177]

Benito Mussolini joined the war to secure Fascist control of the Mediterranean, as the conquest of Ethiopia in the Second Italo-Ethiopian War made the Italian government confident in its military power.[178] Italy became the stronger backer of the Nationalists.[179] Italy supplied machine guns, artillery, aircraft, tankettes, the Aviazione Legionaria, and the Corpo Truppe Volontarie (CTV) to the Nationalist cause, and assisted with the Mediterranean blockade.[180] The Italian CTV would supply the Nationalists with 50,000 men.[180] Italian warships took part in breaking the Republican navy's blockade of Nationalist-held Spanish Morocco and took part in naval bombardment of Republican-held Málaga, Valencia, and Barcelona.[181] Italian air raids targeted mainly cities and civilians.[182] These Italian commitments were heavily propagandised in Italy and became a point of fascist pride.[182]

Despite the German signing of a non-intervention agreement in September 1936, Nazi Germany gave various aid and military support for the Nationalists,[183] including the formation of the Condor Legion as a land and air force.[184] Germany successfully flew the Army of Africa to Mainland Spain in the early stages of the war.[185] The bombing of Guernica, on 26 April 1937, would be the most controversial event of German involvement, with perhaps 200 to 300 civilians killed.[186] German involvement also included Operation Ursula, a U-boat undertaking and contributions from the Kriegsmarine.[187] Strategically, Nazi support for Franco provided a distraction from Hitler's central European strategy and created a friendly Spanish state to threaten France.[185] Hitler wanted to help Franco just enough to gain his gratitude and to prevent the side supported by the Soviet Union from winning, but not large enough to give the Caudillo a quick victory.[188]

Germany's Condor Legion spearheaded many Nationalist victories, particularly in the air dominance from 1937 onward.[189] Spain was a proving ground for German tank and aircraft tactics, the latter being only moderately successful.[190] Germany trained 56,000 Nationalist soldiers, who were technically proficient and covered infantry, tanks and anti-tank units; air and anti-aircraft forces; and those trained in naval warfare.[184] About 16,000 German citizens fought in the conflict, mostly as pilots, ground crew, artillery and tank crew and military advisers and instructors. About 10,000 Germans were in Spain at the peak.[191] German aid to the Nationalists amounted to approximately £43,000,000 ($215,000,000) in 1939 prices, mostly for the Condor Legion.[191] No detailed list of German supplies furnished to Spain has been found.[191]

Portugal supplied the Nationalists with critical ammunition and logistical resources.[192] Throughout the war, Portugal ensured that Iberian borders would continue to supply the Nationalists. The Nationalists even referred to Lisbon as "the port of Castile".[193] Portuguese Prime Minister António de Oliveira Salazar semi-officially endorsed the "Viriatos" volunteer force of 8,000 to 12,000.[194] With Franco's victory increasingly certain, Portugal recognised Franco's regime and soon after the war signed a treaty of friendship and non-aggression pact, the Iberian Pact.[195] Portugal played an important diplomatic role in supporting Franco and insisting to the British government that Franco sought to replicate Salazar's Estado Novo, not Mussolini's Fascist Italy or Hitler's Nazi Germany.[196]

Other national groups fought alongside the Nationalists. Despite the Irish government's prohibition against participating in the war, about 600 followers of Eoin O'Duffy and Fine Gael formed the "Irish Brigade" to fight for Franco.[197] Their majority were anti-Communism Catholics.[198][199] Around 150 to 170 White Russians fought for Franco.[200] Romanian volunteers and legionaries of the Iron Guard allied their movement with the Nationalists.[201]

Volunteers from many countries fought in Spain, most of them for the Republicans. About 32,000[202] fought in the International Brigades. Perhaps another 3,000[202] fought as members of the Confederación Nacional del Trabajo (CNT) and the Workers' Party of Marxist Unification (POUM) militias. Those fighting with POUM most famously included George Orwell and the small ILP Contingent. Around 2,000 Portuguese leftists fought for the Republicans and were spread throughout different units.[203] Many of the non-Spaniards were affiliated with radical communist or socialist entities and believed that the Spanish Republic was a front line in the war against fascism. The units represented the largest foreign contingent of those fighting for the Republicans. Roughly 40,000 foreign nationals fought with the Brigades, though no more than 18,000 were in the conflict at any given time. They claimed to represent 53 nations.[204] Significant numbers came from France (10,000), Nazi Germany and Austria (5,000), and Italy (3,350). More than 1,000 each came from the Soviet Union, the United States, the United Kingdom, Poland, Yugoslavia, Czechoslovakia, Hungary and Canada.[204] The Thälmann Battalion, a group of Germans, and the Garibaldi Battalion, a group of Italians, distinguished their units during the siege of Madrid. Americans fought in units such as the XV International Brigade ("Abraham Lincoln Brigade"), while Canadians joined the Mackenzie–Papineau Battalion.[205]

The International Brigades were supported by Communists worldwide, beginning with a Prague conference to raise a brigade soon after the July revolt, followed by a full-scale propaganda campaign for the Popular Front. Communist parties worldwide launched a full-scale propaganda campaign for the Popular Front. Leaders of the Communist International and the Italian Communist Party went to Spain.[206][207]

Soviet Union General Secretary Joseph Stalin broke the Non-Intervention Agreement and League of Nations embargo by providing material assistance to the Republican forces, becoming their only source of major weapons. Unlike Hitler and Mussolini, Stalin tried to do this covertly.[208] Estimates of material provided by the USSR to the Republicans vary between 634 and 806 aircraft, 331 and 362 tanks and 1,034 to 1,895 artillery pieces.[209] Stalin also created Section X of the Soviet Union military to head the weapons shipment operation, called Operation X. Despite Stalin's interest in aiding the Republicans, the quality of arms was inconsistent.[210][211] Many rifles and field guns provided were old or obsolete, but the T-26 and BT-5 tanks were modern and effective.[210] Their supplied aircraft was current with their own forces but the German aircraft for the Nationalists proved superior by the end of the war.[212] From August 1936 onward, over one ship per day arrived at Spain's Mediterranean ports with Russian aid: munitions, rifles, machine guns, hand grenades, artillery, trucks, Soviet agents, technicians, instructors, and propagandists.[213] The movement of arms from Russia to Spain was extremely slow. Many shipments were lost or incomplete.[214] The ships hid arms under false decks. At sea, Soviet captains used deceptive flags and paint schemes to evade Nationalist detection.[215]

The USSR sent 2,000–3,000 military advisers to Spain; while the Soviet commitment of troops was fewer than 500 men at a time, Soviet volunteers often operated Soviet-made tanks and aircraft, particularly at the beginning of the war.[216][217][218][204] The Spanish commander of every military unit on the Republican side was attended by a "Comissar Politico" of equal rank, who represented Moscow.[219] The Soviets also ran the People's Commissariat for Internal Affairs (NKVD) inside the Republican rearguard. Communist figures led operations that included the murders of leftist figures.[220][221][222]

After the USSR, Poland was the second largest arms supplier for the Republic and the 4th largest arms supplier when considering Italy and Germany. The Polish sold arms to Republican Spain throughout the war, motivated exclusively by economic interest, as their government favored the Nationalists. Since Poland was bound by non-intervention obligations, Polish governmental officials and the military disguised sales as commercial transactions. The weapons were obsolete, second-rate, and overpriced. Though sales amounted to $40M and up to 7% of overall Republican military spending, in some categories like machine-guns, they might have accounted for 50% of all arms delivered.[223]

Unlike the United States and major Latin American governments, such as the ABC nations and Peru, the Mexican government supported the Republicans.[224][225] Mexico abstained from following the French-British non-intervention proposals,[224] and provided $2,000,000 in aid and material assistance, which included 20,000 rifles and 20 million cartridges.[224] Mexico provided diplomatic help and arranged sanctuary for some 50,000 Republican refugees including Spanish intellectuals and orphaned children from Republican families.[226]

French Prime Minister Léon Blum was sympathetic to the republic,[227] fearing that the success of Nationalist forces in Spain would create an alliance with Nazi Germany and Fascist Italy, encircling France.[227] After initially declaring that they would not aid the Republicans[228] and signing the Non-Intervention Agreement,[229] France provided the Republicans with aircraft,[230][231] fighter pilots and engineers to help the Republicans.[232][233] Covert support ended by December 1936 but the spectre of French intervention against the Nationalists remained a serious possibility throughout the war.[234][235]

A large air and sealift of Nationalist troops in Spanish Morocco was organised to the southwest of Spain.[236] Coup leader Sanjurjo was killed in a plane crash on 20 July,[237][238] leaving an effective command split between Mola in the North and Franco in the South.[71] This period also saw the worst actions of the so-called "Red" and "White Terrors" in Spain.[239] On 21 July, the fifth day of the rebellion, the Nationalists captured the central Spanish naval base, located in Ferrol, Galicia.[240]

A rebel force under Colonel Alfonso Beorlegui Canet, sent by General Mola and Colonel Esteban García, undertook the Campaign of Gipuzkoa from July to September. The capture of Gipuzkoa isolated the Republican provinces in the north. On 5 September, the Nationalists closed the French border to the Republicans in the battle of Irún.[241] On 15 September San Sebastián, home to a divided Republican force of anarchists and Basque nationalists, was taken by Nationalist soldiers.[242]

The Republic proved ineffective militarily, relying on disorganised revolutionary militias. The Republican government under Giral resigned on 4 September, unable to cope with the situation, and was replaced by a mostly Socialist organisation under Francisco Largo Caballero.[243] The new leadership began to unify central command in the republican zone.[244] The civilian militias were often simply just civilians armed with whatever was available. Thus, they fared poorly in combat, particularly against the professional Army of Africa armed with modern weapons, ultimately contributing to Franco's rapid advance.[245]

On the Nationalist side, Franco was chosen as chief military commander at a meeting of ranking generals at Salamanca on 21 September, now called by the title Generalísimo.[71][248] Franco won another victory on 27 September when his troops relieved the siege of the Alcázar in Toledo,[248] which had been held by a Nationalist garrison under Colonel José Moscardó Ituarte since the beginning of the rebellion, resisting thousands of Republican troops, who completely surrounded the isolated building. Moroccans and elements of the Spanish Legion came to the rescue.[249] Two days after relieving the siege, Franco proclaimed himself Caudillo ("chieftain", the Spanish equivalent of the Italian Duce and the German Führer—meaning: 'director') while forcibly unifying the various and diverse Falangist, Royalist and other elements within the Nationalist cause.[243] The diversion to Toledo gave Madrid time to prepare a defense but was hailed as a major propaganda victory and personal success for Franco.[250] On 1 October 1936, General Franco was confirmed head of state and armies in Burgos. A similar dramatic success for the Nationalists occurred on 17 October, when troops coming from Galicia relieved the besieged town of Oviedo, in Northern Spain.[251][252]

In October, the Francoist troops launched a major offensive toward Madrid,[253] reaching it in early November and launching a major assault on the city on 8 November.[254] The Republican government was forced to shift from Madrid to Valencia, outside the combat zone, on 6 November.[255] However, the Nationalists' attack on the capital was repulsed in fierce fighting between 8 and 23 November. A contributory factor in the successful Republican defense was the effectiveness of the Fifth Regiment[256] and later the arrival of the International Brigades, though only an approximate 3,000 foreign volunteers participated in the battle.[257] Having failed to take the capital, Franco bombarded it from the air and, in the following two years, mounted several offensives to try to encircle Madrid, beginning the three-year siege of Madrid. The Second Battle of the Corunna Road, a Nationalist offensive to the northwest, pushed Republican forces back, but failed to isolate Madrid. The battle lasted into January.[258]

With his ranks swelled by Italian troops and Spanish colonial soldiers from Morocco, Franco made another attempt to capture Madrid in January and February 1937, but was again unsuccessful. The Battle of Málaga started in mid-January, and this Nationalist offensive in Spain's southeast would turn into a disaster for the Republicans, who were poorly organised and armed. The city was taken by Franco on 8 February.[259] The consolidation of various militias into the Republican Army had started in December 1936.[260] The main Nationalist advance to cross the Jarama and cut the supply to Madrid by the Valencia road, termed the Battle of Jarama, led to heavy casualties (6,000–20,000) on both sides. The operation's main objective was not met, though Nationalists gained a modest amount of territory.[261]

A similar Nationalist offensive, the Battle of Guadalajara, was a more significant defeat for Franco and his armies. This was the only publicised Republican victory of the war. Franco used Italian troops and blitzkrieg tactics; while many strategists blamed Franco for the rightists' defeat, the Germans believed it was the former at fault for the Nationalists' 5,000 casualties and loss of valuable equipment.[262] The German strategists successfully argued that the Nationalists needed to concentrate on vulnerable areas first.[263]

The "War in the North" began in mid-March, with the Biscay Campaign. The Basques suffered most from the lack of a suitable air force.[264] On 26 April, the Condor Legion bombed the town of Guernica, killing 200–300 and causing significant damage. The bombing of Guernica had a significant effect on international opinion. The Basques retreated from the area.[265]

April and May saw the May Days, infighting among Republican groups in Catalonia. The dispute was between an ultimately victorious government—Communist forces and the anarchist CNT. The disturbance pleased Nationalist command, but little was done to exploit Republican divisions.[266] After the fall of Guernica, the Republican government began to fight back with increasing effectiveness. In July, it made a move to recapture Segovia, forcing Franco to delay his advance on the Bilbao front, but for only two weeks. The Huesca Offensive failed similarly.[267]

Mola, Franco's second-in-command, was killed on 3 June, in an airplane accident.[268] In early July, despite the earlier loss at the Battle of Bilbao, the government launched a strong counter-offensive to the west of Madrid, focusing on Brunete. The Battle of Brunete, however, was a significant defeat for the Republic, which lost many of its most accomplished troops. The offensive led to an advance of 50 square kilometres (19 sq mi), and left 25,000 Republican casualties.[269]

A Republican offensive against Zaragoza was also a failure. Despite having land and aerial advantages, the Battle of Belchite, a place lacking any military interest, resulted in an advance of only 10 kilometres (6.2 mi) and the loss of much equipment.[270] Franco invaded Aragón and took the city of Santander in Cantabria in August.[271] With the surrender of the Republican army in the Basque territory came the Santoña Agreement.[272] Gijón finally fell in late October in the Asturias Offensive.[273] Franco had effectively won in the north. At November's end, with Franco's troops closing in on Valencia, the government had to move again, this time to Barcelona.[161]

The Battle of Teruel was an important confrontation in 1938, its outcome heralding future progress of the war. The city, which had formerly belonged to the Nationalists, was conquered by Republicans in January. Francoist troops launched a counter-offensive and recovered the city by 22 February, the Nationalists relying heavily on German and Italian air support.[274]

Teruel secured, on 7 March the Nationalists launched the Aragon Offensive; by 14 April they had pushed east through to the Mediterranean, cutting the Republican-held portion of Spain in two. The Republican government attempted to sue for peace in May,[275] but Franco demanded unconditional surrender, and the war raged on.

In July, the Nationalist army pressed southward from Teruel, pushing south along the coast toward the capital of the Republic at Valencia, but was halted in heavy fighting along the XYZ Line, a system of fortifications defending Valencia.[276]
The Republican government then launched an all-out campaign to reconnect their territory in the Battle of the Ebro, from 24 July until 26 November; the scale of the Republican offensive forced Franco to personally take command.[277]

The Republican Ebro campaign was unsuccessful, undermined by the agreement signed in Munich, Germany, between Hitler and Chamberlain. The Munich Agreement effectively caused a collapse in Republican morale by ending hope of an anti-fascist alliance with Western powers.[278] The subsequent Republican retreat from the Ebro all but determined the outcome of the war.[277] Eight days before the new year, Franco threw massive forces into an invasion of Catalonia.[279]

Franco's troops conquered Catalonia in a whirlwind campaign during the first two months of 1939. Tarragona fell on 15 January,[280] followed by Barcelona on 26 January[281] and Girona on 2 February.[282] On 27 February, the United Kingdom and France recognized the Franco regime.[283]

Only Madrid and a few other strongholds remained for the Republican forces. On 5 March 1939 the Republican army, led by the Colonel Segismundo Casado and the politician Julián Besteiro, rose against the prime minister Juan Negrín and formed the National Defence Council (Consejo Nacional de Defensa or CND) to negotiate a peace deal.[284] Negrín fled to France on 6 March,[285] but the Communist troops around Madrid rose against the junta, starting a brief civil war within the civil war.[286] Casado defeated them, and began peace negotiations with the Nationalists, but Franco refused to accept anything less than unconditional surrender.[287]

On 26 March, the Nationalists started a general offensive, on 28 March the Nationalists occupied Madrid and, by 31 March, they controlled all Spanish territory.[288] Franco proclaimed victory in a radio speech aired on 1 April, when the last of the Republican forces surrendered.[289]

After the end of the war, there were harsh reprisals against Franco's former enemies.[290] Thousands of Republicans were imprisoned and at least 30,000 executed.[291] Other estimates of these deaths range from 50,000[292] to 200,000, depending on which deaths are included. Many others were put to forced labour, building railways, draining swamps, and digging canals.[292]

At the end of the war, in what was called La Retirada (withdrawal) Hundreds of thousands of Republicans fled abroad, with some 500,000 fleeing to France.[293] Refugees were confined in internment camps of the French Third Republic, such as Camp Gurs or Camp Vernet, where 12,000 Republicans were housed in squalid conditions. In his capacity as consul in Paris, Chilean poet and politician Pablo Neruda organised the immigration to Chile of 2,200 Republican exiles.[294]

Of the 17,000 refugees housed in Gurs, farmers and others who could not find relations in France were encouraged by the Third Republic, in agreement with the Francoist government, to return to Spain. The great majority did so and were turned over to the Francoist authorities in Irún.[295] From there, they were transferred to the Miranda de Ebro camp for "purification" according to the Law of Political Responsibilities. After the proclamation by Marshal Philippe Pétain of the Vichy regime, the refugees became political prisoners, and the French police attempted to round up those who had been liberated from the camp. Along with other "undesirable" people, the Spaniards were sent to the Drancy internment camp before being deported to Nazi Germany. About 5,000 Spaniards died in the Mauthausen concentration camp.[295]

After the official end of the war, guerrilla warfare was waged on an irregular basis by the Spanish Maquis well into the 1950s, gradually reduced by military defeats and scant support from the exhausted population. In 1944, a group of republican veterans, who also fought in the French resistance against the Nazis, invaded the Val d'Aran in northwest Catalonia, but were defeated after 10 days.[296] According to some scholars, the Spanish Civil War lasted until 1952; until 1939 it was "conventional civil war", but afterwards it turned into an "irregular civil war".[297]

The Republicans oversaw the evacuation of 30,000–35,000 children from their zone,[298] starting with Basque areas, from which 20,000 were evacuated. Their destinations included the United Kingdom[299] and the USSR, and many other countries in Europe, along with Mexico. The policy of evacuating children to foreign countries was initially opposed to by elements in the government as well as private charities, who saw the policy as unnecessary and harmful to the well-being of the evacuated children.[298] On 21 May 1937, around 4,000 Basque children were evacuated to the UK on the aging steamship SS Habana from the Spanish port of Santurtzi. Upon their arrival two days later in Southampton, the children were sent to families all over England, with over 200 children accommodated in Wales.[300] The upper age limit was initially set at 12 but raised to 15.[301] By mid-September, all of los niños vascos, as they became known,[302] had found homes with families.[303] Most were repatriated to Spain after the war, but some 250 were still in Britain by the end of the Second World War in 1945 and some chose to settle there.[304]

During the Civil War the Nationalist and Republican military expenditures combined totalled some $3.89bn, on average $1.44bn annually.[note 4] The overall Nationalist expenditures are calculated at $2.04bn, while the Republican ones reached ca. $1,85bn.[305] In comparison, in 1936–1938 the French military expenditure totalled $0.87bn, the Italian ones reached $2.64bn, and the British ones stood at $4.13bn.[306] As in the mid-1930s the Spanish GDP was much smaller than the Italian, French or British ones,[307] and as in the Second Republic the annual defence and security budget was usually around $0,13bn (total annual governmental spendings were close to $0.65bn),[note 5] wartime military expenditures put huge strain on the Spanish economy. Financing the war posed enormous challenge for both the Nationalists and the Republicans.

The two combatant parties followed similar financial strategies; in both cases money creation, rather than new taxes or issue of debt, was key to financing the war.[305]

Both sides relied mostly on domestic resources; in the case of the Nationalists, they amounted to 63% of the overall spendings ($1.28bn) and in the case of the Republicans they stood at 59% ($1.09bn). In the Nationalist zone money creation was responsible for some 69% of domestic resources, while in the Republican one the corresponding figure stood at 60%; it was accomplished mostly by means of advances, credits, loans and debit balances from respective central banks.[305] However, while in the Nationalist zone the rising stock of money was only marginally above the production growth rate, in the Republican zone it by far exceeded dwindling production figures. The result was that while by the end of the war the Nationalist inflation was 41% compared to 1936, the Republican one was in triple digits. The second component of domestic resource was fiscal revenue. In the Nationalist zone it grew steadily and in the 2nd half of 1938 it was 214% of the figure from the 2nd half of 1936.[308] In the Republican zone fiscal revenues in 1937 dropped to some 25% of revenues recorded in the proportional area in 1935 but recovered slightly in 1938. Neither side re-engineered the pre-war tax system; differences resulted from dramatic problems with tax collection in the Republican zone and from the course of the war, as more and more of the population were governed by the Nationalists. A smaller percentage of domestic resources came from expropriations, donations or internal borrowing.[305]

Foreign resources amounted to 37% in case of the Nationalists ($0,76bn) and 41% in case of the Republicans ($0,77bn).[note 6] For the Nationalists it was mostly the Italian and German credit;[note 7] in case of the Republicans, it was sales of gold reserves, mostly to the USSR and in much smaller amount to France. None of the sides resolved to public borrowing and none floated debt on foreign exchange markets.[305]

Authors of recent studies suggest that given Nationalist and Republican spendings were comparable, earlier theory pointing to Republican mismanagement of resources is no longer tenable.[note 8] Instead, they claim that the Republicans failed to translate their resources into military victory largely because of constraints of the international non-intervention agreement; they were forced to spend in excess of market prices and accept goods of lower quality. Initial turmoil in the Republican zone contributed to problems, while at later stages the course of the war meant that population, territory and resources kept shrinking.[305]

Given the lack of operativeness of the republican army after the Spanish coup of July 1936, the columns of militiamen temporarily played their role. There was also an expedition supported by the Generalitat de Catalunya to recover Mallorca. The lack of support from the Spanish government for the underlying cause of the Generalitat being involved into the operation and the Catalanist propaganda that promoted the enlistment of volunteers forced the withdrawal.[310] Not recapturing Mallorca would be of great importance in the future course of the war.

The Republic, prevented from buying weapons abroad by the international agreement of neutrality, which both Germany and Italy ignored, urgently required war material. In this context, the Generalitat built a network of war industries converting civilian industries. When the republican government moved to Barcelona in 1937, it took control of the war industries from the Generalitat. But under his control, production dropped dramatically, with the consequent impact on supplies to the war fronts.[311]

While this was happening, Prime Minister Negrín treated President Companys with notable disloyalty, to the final point of abandoning him at the French border, after appropriating the Generalitat's reserve funds for exile.[311]


All of above can be illustrated with Negrín's statement collected by Julián Zugazagoitia:[312]
I am not waging war against Franco so that a stupid and sleazy separatism will return to us in Barcelona. I am waging war for Spain and for Spain! For greatness and for greatness! Those who assume otherwise are mistaken. There is only one nation: Spain! Before consenting to nationalist campaigns that lead us to dismemberment that I in no way admit, I would give way to Franco without any other condition than to get aside the Germans and Italians.
The death toll of the Spanish Civil War is far from clear and remains—especially in part related to war and postwar repression—controversial. Many general historiographic works—notably in Spain—refrain from advancing any figures or at best propose general descriptions.[324][325][326][note 54][note 55] Foreign scholars, especially English-speaking historians, are more willing to offer some general estimates, though some have revised their projections, usually downward,[note 56] and the figures vary from 1 million to 250,000.

The totals advanced usually include or exclude various categories. Scholars who focus on killings or "violent deaths" most typically list (1) combat and combat-related deaths; figures in this rubric range from 100,000[327][328] to 700,000;[329] (2) rearguard terror, both judicial and extrajudicial, recorded until the end of the Civil War: 103,000[330] to 235,000;[331] (3) civilian deaths from military action, typically air raids: 10,000[331] to 15,000.[332] These categories combined point to totals from 235,000[333] to 715,000.[334] Many authors opt for a broader view and calculate "death toll" by adding also (4) above-the-norm deaths caused by malnutrition,[335] hygiene shortcomings, cold, illness, etc. recorded until the end of the Civil War: 30,000[336] to 630,000.[337] It is not unusual to encounter war statistics which include (5) postwar terror related to Civil War, at times up until 1961: 23,000[338] to 200,000.[331] Some authors also add (6) foreign combat and combat-related deaths: 3,000[339] to 25,000,[338] (7) Spaniards killed in World War II: 6,000,[338] (8) deaths related to postwar guerrilla, typically the Invasion of Val d'Aran: 4,000,[338] (9) above-the-norm deaths caused by malnutrition, etc., recorded after the Civil War but related to it: 160,000[338] to 300,000.[340]

Demographers instead try to gauge the difference between the total number of deaths recorded during the war and the total that would result from applying annual death averages from the 1926–1935 period; this difference is considered excess death resulting from the war. The figure they arrive at for the 1936–1939 period is 346,000; the figure for 1936–1942, including the years of postwar deaths resulting from terror and war sufferings, is 540,000.[note 57] Some scholars calculate the war's "population loss" or "demographic impact"; in this case they might include also (10) migration abroad: 160,000[note 58] to 730,000[note 59] and (11) decrease in birth rate: 500,000[note 60] to 570,000.[note 61]

Death totals remain debated. British historian Antony Beevor wrote in his history of the Civil War that Franco's ensuing "white terror" resulted in the deaths of 200,000 people and that the "red terror" killed 38,000.[341] Julius Ruiz contends that, "Although the figures remain disputed, a minimum of 37,843 executions were carried out in the Republican zone, with a maximum of 150,000 executions (including 50,000 after the war) in Nationalist Spain".[342] Historian Michael Seidman stated that the Nationalists killed approximately 130,000 people and the Republicans approximately 50,000 people.[343]

In 2008 a Spanish judge, Baltasar Garzón, opened an investigation into the executions and disappearances of 114,266 people between 17 July 1936 and December 1951. Among the executions investigated was that of the poet and dramatist Federico García Lorca, whose body has never been found.[344] Mention of García Lorca's death was forbidden during Franco's regime.[345]

Research since 2016 has started to locate mass graves, using a combination of witness testimony, remote sensing and forensic geophysics techniques.[346]

Historians such as Helen Graham,[347] Paul Preston,[348] Antony Beevor,[17] Gabriel Jackson[349] and Hugh Thomas[350] argue that the mass executions behind the Nationalist lines were organised and approved by the Nationalist rebel authorities, while the executions behind the Republican lines were the result of the breakdown of the Republican state and chaos:

Though there was much wanton killing in rebel Spain, the idea of the limpieza, the "cleaning up", of the country from the evils which had overtaken it, was a disciplined policy of the new authorities and a part of their programme of regeneration. In republican Spain, most of the killing was the consequence of anarchy, the outcome of a national breakdown, and not the work of the state, although some political parties in some cities abetted the enormities, and some of those responsible ultimately rose to positions of authority.
Conversely, historians such as Stanley Payne, Julius Ruiz[352] and José Sánchez[353] argue that the political violence in the Republican zone was in fact organized by the left:

In general, this was not an irrepressible outpouring of hatred, by the man in the street for his "oppressors", as it has sometimes been painted, but a semi-organized activity carried out by sections of nearly all the leftist groups. In the entire leftist zone the only organized political party that eschewed involvement in such activity were the Basque Nationalists.[354]
Nationalist atrocities, which authorities frequently ordered so as to eradicate any trace of "leftism" in Spain, were common. The notion of a limpieza (cleansing) formed an essential part of the rebel strategy, and the process began immediately after an area had been captured.[355] Estimates of the death toll vary; historian Paul Preston estimates the minimum number of those executed by the rebels as 130,000,[356] while Antony Beevor places the figure much higher at an estimated 200,000 dead.[357] The violence was carried out in the rebel zone by the military, the Civil Guard and the Falange in the name of the regime.[358] Julius Ruiz reports that the Nationalists killed 100,000 people during the war and executed at least 28,000 immediately after. The first three months of the war were the bloodiest, with 50 to 70 percent of all executions carried out by Franco's regime, from 1936 to 1975, occurring during this period.[359] The first few months of killings lacked much in the way of centralisation, being largely in the hands of local commanders. According to Stanely Payne and Jesús Palacios, General Mola was taken aback by them, despite his own planning emphasising the need for violence; early in the conflict he had ordered a group of leftist militiamen to be immediately executed, only to change his mind and rescind the order.[360]

Many such acts were committed by reactionary groups during the first weeks of the war.[358] This included the execution of schoolteachers,[361] because the efforts of the Second Spanish Republic to promote laicism and displace the Church from schools by closing religious educational institutions were considered by the Nationalists as an attack on the Roman Catholic Church. Extensive killings of civilians were carried out in the cities captured by the Nationalists,[362] along with the execution of unwanted individuals. These included non-combatants such as trade-unionists, Popular Front politicians, suspected Freemasons, Basque, Catalan, Andalusian, and Galician Nationalists, Republican intellectuals, relatives of known Republicans, and those suspected of voting for the Popular Front.[358][363][364][365][366] The Nationalists also frequently killed military officers who refused to support them in the early days of the coup.[367] Many killings in the first few months were often done by vigilantes and civilian death squads, with the Nationalist leadership often condoning their actions or even assisting them.[368] Post-war executions were conducted by military tribunal, though the accused had limited ways to defend themselves. A large number of the executed were done so for their political activities or positions they held under the Republic during the war, though those who committed their own killings under the Republic were also amongst executed as well.[369] A 2010 analysis of Catalonia argued that Nationalist executions were more likely to occur when they occupied an area that experienced greater prior violence, likely due to pro-Nationalist civilians seeking revenge for earlier actions by denouncing others to the Nationalist forces.[370] Michael Seidman argues that the Nationalists' greater death toll may be partially attributable to their military success resulting in territorial gains and thus more opportunities to enact violence against their enemies.[371] However, during the war, executions declined as the Francoist state began to establish itself.[372]

Nationalist forces massacred civilians in Seville, where some 8,000 people were shot; 10,000 were killed in Cordoba; 6,000–12,000 were killed in Badajoz[373] after more than 1,000 landowners and conservatives were killed by the revolutionaries. In Granada, where working-class neighbourhoods were hit with artillery and right-wing squads were given free rein to kill government sympathizers,[374] at least 2,000 people were murdered.[361] In February 1937, over 7,000 were killed after the capture of Málaga.[375] When Bilbao was conquered, thousands of people were sent to prison. There were fewer executions than usual, however, because of the effect Guernica left on Nationalists' reputations internationally.[376] The numbers killed as the columns of the Army of Africa devastated and pillaged their way between Seville and Madrid are particularly difficult to calculate.[377] Landowners who owned the large estates of Southern Spain rode alongside the Army of Africa to reclaim via force of arms the land given to the landless peasants by the Republican government. Rural workers were executed, and it was mockingly joked that they had received their "land reform" in the form of a burial plot.[378]

Nationalists also murdered Catholic clerics. In one particular incident, following the capture of Bilbao, they took hundreds of people, including 16 priests who had served as chaplains for the Republican forces, to the countryside or graveyards and murdered them.[379][380]

Franco's forces also persecuted Protestants, including murdering 20 Protestant ministers.[381] Franco's forces were determined to remove the "Protestant heresy" from Spain.[382] The Nationalists also persecuted Basques, as they strove to eradicate Basque culture.[271] According to Basque sources, some 22,000 Basques were murdered by Nationalists immediately after the Civil War.[383]

The Nationalist side conducted aerial bombing of cities in Republican territory, carried out mainly by the Luftwaffe volunteers of the Condor Legion and the Italian air force volunteers of the Corpo Truppe Volontarie: Madrid, Barcelona, Valencia, Guernica, Durango, and other cities were attacked. The Bombing of Guernica was the most controversial.[384] The Italian air force conducted a particularly heavy bombing raid on Barcelona in early 1938. While some Nationalist leaders did oppose the bombing of the city—for example, Generals Yagüe and Moscardó, who were noted for being nonconformists, protested against the indiscriminate destruction—other Nationalist leaders, often those of a fascist persuasion, saw bombings as necessary to "cleanse" Barcelona.[385]

Michael Seidman observes that the Nationalist terror was a key part of the Nationalist victory as it allowed them to secure their rear; the Russian Whites, in their civil war, had struggled to suppress peasant rebellions, bandits and warlordism behind their lines, while the inability of the Chinese Nationalists to stop banditry during the Chinese Civil War did severe damage to the regime's legitimacy. The Spanish Nationalists, in contrast, imposed a puritanically terrorist order on the populace in their territory. They never suffered from serious partisan activity behind their lines and the fact that banditry did not develop into a serious problem in Spain, despite how easy it would have been in such mountainous terrain, demands explanation. Seidman argues that severe terror, combined with control of the food supply, explains the general lack of guerrilla warfare in the Nationalist rear.[386] A 2009 analysis of Nationalist violence argues that evidence supports the view that killings were used strategically by the Nationalists to pre-emptively counter potential opposition by targeting individuals and groups deemed most likely to cultivate future rebellions, thus helping the Nationalists win the war.[387]

Scholars have estimated that between 38,000[388] and 70,000[389] civilians were killed in Republican-held territories, with the most common estimate being around 50,000.[390][391][392][393]

Whatever the exact number, the death toll was far exaggerated by both sides, for propaganda reasons, giving birth to the legend of the millón de muertos.[note 62] Franco's government would later give names of 61,000 victims of the red terrors, but which are not considered objectively verifiable.[161] The deaths would form the prevailing outside opinion of the republic up until the bombing of Guernica.[388]

The leftist Revolution of 1936 that preceded the war was accompanied since the first months by an escalation of leftist anticlerical terror that, between 18 and 31 July alone, killed 839 religious, continuing during the month of August with 2055 other victims, including 10 bishops killed, that was 42% of the total number of registered victims in that year.[394] Particularly noteworthy repression was conducted in Madrid during the war.

The Republican government was anticlerical, and, when the war began, supporters attacked and murdered Roman Catholic clergy in reaction to the news of military revolt.[380] In his 1961 book, Spanish archbishop Antonio Montero Moreno, who at the time was director of the journal Ecclesia, wrote that 6,832 were killed during the war, including 4,184 priests, 2,365 monks and friars, and 283 nuns (many were first raped before they died),[395][396] in addition to 13 bishops, a figure accepted by historians, including Beevor.[397][398][399] Some of the killings were carried out with extreme cruelty, some were burned to death, there are reports of castration and disembowelment.[397] Some sources claim that by the conflict's end, 20 percent of the nation's clergy had been killed.[400][note 63] The "Execution" of the Sacred Heart of Jesus by Communist militiamen at Cerro de los Ángeles near Madrid, on 7 August 1936, was the most infamous of widespread desecration of religious property.[401] In dioceses where the Republicans had general control, a large proportion—often a majority—of secular priests were killed.[402] Michael Seidman argues that the hatred of the Republicans for the clergy was in excess of anything else; while local revolutionaries might spare the lives of the rich and right-wingers, they seldom offered the same to priests.[69]

Like clergy, civilians were executed in Republican territories. Some civilians were executed as suspected Falangists.[403] Others died in acts of revenge after Republicans heard of massacres carried out in the Nationalist zone.[404] Even families who simply attended Catholic Mass were hunted down; including children.[405][406] Air raids committed against Republican cities were another driving factor.[407] Shopkeepers and industrialists were shot if they did not sympathise with the Republicans and were usually spared if they did.[408] Fake justice was sought through commissions, named checas after the Soviet secret police organization.[403]

 Many killings were done by paseos, impromptu death squads that emerged as a spontaneous practice amongst revolutionary activists in Republican areas. According to Seidman, the Republican government only made efforts to stop the actions of the paseos late in the war; during the first few months, the government either tolerated it or made no efforts to stop it.[410] The killings often contained a symbolic element, as those killed were seen as embodying an oppressive source of power and authority. This was also why the Republicans would kill priests or employers who were not considered to personally have done anything wrong but were nonetheless seen as representing the old oppressive order that needed to be destroyed.[411]

There was infighting between the Republican factions, and the Communists following Stalinism declared the Workers' Party of Marxist Unification (POUM), an anti-Stalinist communist party, to be an illegal organization, along with the Anarchists. The Stalinists betrayed and committed mass atrocities on the other Republican factions, such as torture and mass executions. George Orwell would record this in his Homage to Catalonia as well as write Nineteen Eighty-Four and Animal Farm to criticize Stalinism.[412][413]
As pressure mounted with the increasing success of the Nationalists, many civilians were executed by councils and tribunals controlled by competing Communist and anarchist groups.[403] Some members of the latter were executed by Soviet-advised communist functionaries in Catalonia,[409] as recounted by George Orwell's description of the purges in Barcelona in 1937 in which followed a period of increasing tension between competing elements of the Catalan political scene. Some individuals fled to friendly embassies, which would house up to 8,500 people during the war.[404]

In the Andalusian town of Ronda, 512 suspected Nationalists were executed in the first month of the war.[409] Communist Santiago Carrillo Solares was accused of the killing of Nationalists in the Paracuellos massacre near Paracuellos de Jarama.[415] Pro-Soviet Communists committed numerous atrocities against fellow Republicans, including other Marxists: André Marty, known as the Butcher of Albacete, was responsible for the deaths of some 500 members of the International Brigades.[416] Andrés Nin, leader of the POUM (Workers' Party of Marxist Unification), and many other prominent POUM members, were murdered by the Communists, with the help of the USSR's NKVD.[417]

The Republicans also conducted their own bombing attacks on cities, such as the bombing of Cabra. According to Stanley Payne and Jesús Palacios, the Republicans conducted more indiscriminate air raids on cities and civilian targets than the Nationalists,[418] although their attacks were often weak and ineffective.[419] Michael Seidman argues that the better trained Nationalist air force was more effective at inflicting casualties, killing an estimated 11,000 civilians compared to approximately 4,000 for the Republican air force.[420]

38,000 people were killed in the Republican zone during the war, 17,000 of whom were killed in Madrid or Catalonia within a month of the coup. While the Communists were forthright in their support of extrajudicial killings, much of the Republican side was appalled by the murders.[421] Azaña came close to resigning.[404] He, alongside other members of Parliament and a great number of other local officials, attempted to prevent Nationalist supporters from being lynched. Some of those in positions of power intervened personally to stop the killings.[421]

In the Republican zone, especially in Aragon and Catalonia which were anarchist strongholds, in addition to the temporary military success, there was a vast social revolution in which the workers and peasants collectivised land and industry and set up councils parallel to the paralyzed Republican government.[422][423] This revolution was opposed by the Soviet-supported communists who campaigned against the loss of civil property rights.[422]

As the war progressed, the government and the communists were able to exploit their access to Soviet arms to restore government control over the war effort, through diplomacy and force.[417] Anarchists and the Workers' Party of Marxist Unification (Partido Obrero de Unificación Marxista, POUM) were integrated into the regular army, albeit with resistance. The anti-Stalinists of POUM were outlawed and denounced by the Soviet-aligned Communists as an instrument of the fascists.[417] In the May Days of 1937, many thousands of anarchist and communist Republican soldiers fought for control of strategic points in Barcelona.[266]

The pre-war Falange was a small party of some 30,000–40,000 members.[424] It also called for a social revolution that would have seen Spanish society transformed by National Syndicalism.[425] Following the execution of its leader, José Antonio Primo de Rivera, by the Republicans, the party swelled in size to several hundred thousand members.[426] The leadership of the Falange suffered 60 percent casualties in the early days of the civil war, and the party was transformed by new members and rising new leaders, called camisas nuevas ("new shirts"), who were less interested in the revolutionary aspects of National Syndicalism.[427] Subsequently, Franco united all fighting groups into the Traditionalist Spanish Falange and the National Syndicalist Offensive Juntas (Spanish: Falange Española Tradicionalista de las Juntas de Ofensiva Nacional-Sindicalista, FET y de las JONS).[428]

The 1930s also saw Spain become a focus for pacifist organisations, including the Fellowship of Reconciliation, the War Resisters League, and the War Resisters' International. Many people including, as they are now called, the insumisos ("defiant ones", conscientious objectors) argued and worked for non-violent strategies.[note 64]

Throughout the course of the Spanish Civil War, people all over the world were exposed to the goings-on and effects of it on its people not only through standard art, but also through propaganda. Motion pictures, posters, books, radio programs, and leaflets are a few examples of this media art that was so influential during the war. Produced by both nationalists and republicans, propaganda allowed Spaniards a way to spread awareness about their war all over the world. A film co-produced by famous early-twentieth century authors such as Ernest Hemingway and Lillian Hellman was used as a way to advertise Spain's need for military and monetary aid. This film, The Spanish Earth, premiered in America in July 1937. In 1938, George Orwell's Homage to Catalonia, a personal account of his experiences and observations in the war, was published in the United Kingdom. In 1939, Jean-Paul Sartre published in France a short story, The Wall in which he describes the last night of prisoners of war sentenced to death by shooting.

Leading works of sculpture include Alberto Sánchez Pérez's El pueblo español tiene un camino que conduce a una estrella ("The Spanish People Have a Path that Leads to a Star"), a 12.5 m monolith constructed out of plaster representing the struggle for a socialist utopia;[429] Julio González's La Montserrat, an anti-war work which shares its title with a mountain near Barcelona, is created from a sheet of iron which has been hammered and welded to create a peasant mother carrying a small child in one arm and a sickle in the other. and Alexander Calder's Fuente de mercurio (Mercury Fountain) a protest work by the American against the Nationalist forced control of Almadén and the mercury mines there.[430]

Salvador Dalí responded to the conflict in his homeland with two powerful oil paintings in 1936: Soft Construction with Boiled Beans: A Premonition of Civil War (Philadelphia Museum of Art) and Autumnal Cannibalism (Tate Modern, London). Of the former, the art historian Robert Hughes stated, "Salvador Dalí appropriated the horizontal thigh of Goya's crouching Saturn for the hybrid monster in the painting Soft Construction with Boiled Beans, Premonition of Civil War, which rather than Picasso's Guernica – is the finest single work of visual art inspired by the Spanish Civil War."[431]: 383 p.  On the later, Dalí commented "These Iberian beings mutually devouring each other correspond to the pathos of civil war considered as a pure phenomenon of natural history as opposed to Picasso who considered it a political phenomenon."[432]: 223 p. 

Pablo Picasso painted Guernica in 1937, inspired by the bombing of Guernica. The work's size (11 ft by 25.6 ft) grabbed much attention and cast the horrors of the mounting Spanish civil unrest into a global spotlight.[433] The painting has since been heralded as an antiwar work and a symbol of peace in the 20th century.[434]

Joan Miró created El Segador (The Reaper) in 1937, formally titled El campesino catalán en rebeldía (Catalan peasant in revolt).[435] It depicted a peasant brandishing a sickle in the air, to which Miró commented that "The sickle is not a communist symbol. It is the reaper's symbol, the tool of his work, and, when his freedom is threatened, his weapon."[436]

The Army of Africa would feature a place in propaganda on both sides, due to the complex history of the Army and Spanish colonialism in North Africa.[437]

Costs for the war on both sides were very high. Monetary resources on the Republican side were completely drained from weapons acquisition. On the Nationalist side, the biggest losses came after the conflict, when they had to let Germany exploit the country's mining resources, so until the beginning of World War II they barely had the chance to make any profit.[438]

The political and emotional repercussions of the War transcended the national scale, becoming a precursor to the Second World War.[439] The war has frequently been described by historians as the "prelude to" or the "opening round of" the Second World War, as part of an international battle against fascism. Historian Stanley Payne suggests that this view is an incorrect summary of the geopolitic position of the interwar period, arguing that the international alliance that was created in December 1941, once the United States entered the Second World War, was politically much broader than the Spanish Popular Front. The Spanish Civil War, Payne argues, was thus a far more clear-cut revolutionary and counter-revolutionary struggle between the left and right wings, while the Second World War initially had fascists and communist powers on the same side with the combined Nazi-Soviet invasion of Poland. Payne suggests that instead the civil war was the last of the revolutionary crises that emerged from the First World War.[440][441]

After the War, Spanish policy leaned heavily towards Germany, Portugal and Italy, since they had been the greatest Nationalist supporters and aligned with Spain ideologically. However, the end of the Civil War and later the Second World War saw the isolation of the country from most other nations until the 1950s.[442]Star Trek is an American science fiction media franchise that started with a television series (simply called Star Trek but now referred to as Star Trek: The Original Series) created by Gene Roddenberry. The series was first broadcast from 1966 to 1969. Since then, the Star Trek canon has expanded to include many other series, a film franchise, and other media.

The film franchise is produced by Paramount Pictures and began with Star Trek: The Motion Picture in 1979. That film and the five that followed all starred the cast of The Original Series. The seventh film, Star Trek Generations (1994), was designed to serve as a transition from the original cast to that of the next series, Star Trek: The Next Generation. The next three films just starred the cast of The Next Generation, and ended with Star Trek: Nemesis (2002), which disappointed at the box office.

After a break of several years, a new film simply titled Star Trek was released in 2009. It serves as a reboot of the franchise, with new actors portraying younger versions of the Original Series characters, but it is technically a narrative continuation set in an alternate timeline called the "Kelvin Timeline". Two sequels have been produced and another is in development. A franchise prequel film is also in development. The first television film, Star Trek: Section 31, was released on the streaming service Paramount+ in 2025 and is set in the original timeline.

Star Trek creator Gene Roddenberry first suggested the idea of a Star Trek feature in 1969.[1] When the original television series was cancelled, he lobbied to continue the franchise through a film. The success of the series in syndication convinced the studio to begin work on a feature film in 1975.[2] A series of writers attempted to craft a suitably epic screenplay, but the attempts did not satisfy Paramount, so the studio scrapped the project in 1977. Paramount instead planned on returning the franchise to its roots with a new television series (Phase II ). The massive worldwide box office success of Star Wars in mid-1977 sent Hollywood studios to their vaults in search of similar sci-fi properties that could be adapted or re-launched to the big screen. Following the huge opening of Columbia's Close Encounters of the Third Kind in late December 1977, production of Phase II was cancelled in favor of making a Star Trek film.[3]

A massive energy cloud from deep space heads toward Earth, leaving destruction in its wake, and the Enterprise must intercept it to determine what lies within, and what its intent might be.

The movie borrows many elements from "The Changeling" of the original series and "One of Our Planets Is Missing" from the animated series. Principal photography commenced on August 7, 1978[4] with director Robert Wise helming the feature. The production encountered difficulties and slipped behind schedule,[5] with effects team Robert Abel and Associates[6] proving unable to handle the film's large amount of effects work. Douglas Trumbull was hired and given a blank check to complete the effects work in time and location;[7] the final cut of the film was completed just in time for the film's premiere. The film introduced an upgrade to the technology and starship designs, making for a dramatic visual departure from the original series. Many of the set elements created for Phase II were adapted and enhanced for use in the first feature films. It received mixed reviews from critics; while it grossed $139 million the price tag had climbed to about $45 million due to costly effects work and delays.

Khan Noonien Singh (Ricardo Montalbán), whom Kirk thwarted in his attempt to seize control of the Enterprise fifteen years earlier ("Space Seed"), seeks his revenge on the Admiral and lays a cunning and sinister trap.

The Motion Picture's gross was considered disappointing, but it was enough for Paramount to back a sequel with a reduced budget. After Roddenberry pitched a film in which the crew of the Enterprise goes back in time to ensure the assassination of John F. Kennedy, he was "kicked upstairs" to a ceremonial role while Paramount brought in television producer Harve Bennett to craft a better—and cheaper—film than the first.[8] After watching all the television episodes, Bennett decided that the character Khan Noonien Singh was the perfect villain for the new film. Director Nicholas Meyer finished a complete screenplay in just twelve days, and did everything possible within budget to give The Wrath of Khan a nautical, swashbuckling feel,[9] which he described as "Horatio Hornblower in outer space".[8] Upon release, the reception of The Wrath of Khan was highly positive;[10] Entertainment Weekly's Mark Bernadin called The Wrath of Khan "the film that, by most accounts, saved Star Trek as we know it".[11]

Both the first and second films have television versions with additional footage and alternate takes that affect the storyline. (Subsequent Star Trek films tended to have shorter television versions.) Especially notable in The Wrath of Khan is the footage establishing that a young crew member who acts courageously and dies during an attack on the Enterprise is Scotty's nephew.

The plot picks up shortly after the conclusion of the previous film. When McCoy begins acting irrationally, Kirk learns that Spock, in his final moments, transferred his katra, his living spirit, to the doctor. To save McCoy from emotional ruin, Kirk and crew steal the Enterprise and violate the quarantine of the Genesis Planet to retrieve Spock, his body regenerated by the rapidly dying planet itself, in the hope that body and soul can be rejoined. However, bent on obtaining the secret of Genesis for themselves, a renegade Klingon (Christopher Lloyd) and his crew interfere, with deadly consequences.

Meyer declined to return for this film, so directing duties were given to cast member Leonard Nimoy. Paramount gave Bennett the green light to write Star Trek III the day after The Wrath of Khan opened.[12] The producer penned a resurrection story for Spock that built on threads from the previous film and the original series episode "Amok Time".[citation needed]

While returning to stand court-martial for their actions in rescuing Spock, Kirk and crew learn that Earth is under siege by a giant probe that is transmitting a destructive signal, attempting to communicate with the now-extinct species of humpback whales. To save the planet, the crew must time-travel back to the late 20th century to obtain a mating pair of these whales.

Nimoy returned as director for this film. Nimoy and Bennett wanted a film with a lighter tone that did not have a classic antagonist. They decided on a time travel story with the Enterprise crew returning to their past to retrieve something to save their present—eventually, humpback whales. After having been dissatisfied with the script written by Daniel Petrie Jr., Paramount hired Meyer to rewrite the screenplay with Bennett's help. Meyer drew upon his own time travel story Time After Time for elements of the screenplay.[citation needed] Star William Shatner was promised his turn as director for Star Trek V, and Nicholas Meyer returned as director/co-writer for Star Trek VI.

Spock's half-brother (Laurence Luckinbill) believes he is summoned by God, and hijacks the brand-new (and problem-ridden) Enterprise-A to take it through the Great Barrier, at the center of the Milky Way, beyond which he believes his maker waits for him. Meanwhile, a young and arrogant Klingon captain (Todd Bryant), seeking glory in what he views as an opportunity to avenge his people of the deaths of their crewmen on Genesis, sets his sights on Kirk.

This is the only Star Trek film directed by William Shatner.

When Qo'noS' moon Praxis (the Klingon Empire's chief energy source) is devastated by an explosion, caused by over-mining, the catastrophe also contaminating Qo'noS' atmosphere, the Klingons make peace overtures to the Federation. While on the way to Earth for a peace summit, the Klingon Chancellor (David Warner) is assassinated by Enterprise crewmen, and Kirk and McCoy are held accountable by the Chancellor's Chief of Staff (Christopher Plummer) and sentenced to life on a prison planet. Spock attempts to prove Kirk's innocence, but in doing so, uncovers a massive conspiracy against the peace process with participants from both sides.

This film is a sendoff to the original series cast. One Next Generation cast member, Michael Dorn, appears as the grandfather of the character he plays on the later television series, Worf. It is the second and last Star Trek film directed by Nicholas Meyer and last screenplay co-authored by Leonard Nimoy.

The seventh film acted as a transition between the films featuring the original cast and those with the Next Generation cast. The Next Generation cast made four films over a period of eight years, with the last two performing only moderately well (Insurrection) and disappointingly (Nemesis) at the box office. Film titles of the North American and UK releases of the films no longer contained the number of the film following the sixth film (the sixth was Star Trek VI: The Undiscovered Country but the seventh was Star Trek Generations). However, European releases continued using numbers in the film titles until Nemesis.

Picard enlists the help of Kirk, who is presumed long dead but flourishes in an extra-dimensional realm, to prevent a deranged scientist (Malcolm McDowell) from destroying a star and its populated planetary system in an attempt to enter that realm. This film also included original crew members Scotty (James Doohan) and Chekov (Walter Koenig).

Following seven seasons of The Next Generation, the next Star Trek film was the first to feature the crew of the Enterprise-D, along with a long prologue sequence featuring three cast members of the original series and the only appearance of the Enterprise-B.

After a failed attempt to assault Earth, the Borg attempt to prevent First Contact between Humans and Vulcans by interfering with Zefram Cochrane's (James Cromwell) warp test in the past. Picard must confront the demons which stem from his assimilation into the Collective ("The Best of Both Worlds") as he leads the new Enterprise-E back through time to ensure the test and subsequent meeting with the Vulcans take place.

The first of two films directed by series actor Jonathan Frakes.

Profoundly disturbed by what he views as a blatant violation of the Prime Directive, Picard deliberately interferes with a Starfleet admiral's (Anthony Zerbe) plan to relocate a relatively small but seemingly immortal population from a mystical planet to gain control of the planet's natural radiation, which has been discovered to have substantial medicinal properties. However, the admiral himself is a pawn in his alien partner's (F. Murray Abraham) mission of vengeance.

Insurrection brought in Deep Space Nine writer Michael Piller instead of Ronald D. Moore and Brannon Braga who had written for Generations and First Contact.[13]

A clone of Picard (Tom Hardy), created by the Romulans, assassinates the Romulan Senate, assumes absolute power, and lures Picard and the Enterprise to Romulus under the false pretext of a peace overture.

Written by John Logan and directed by Stuart Baird, this film was a critical and commercial failure (released December 13, 2002, in direct competition with Die Another Day, Harry Potter and the Chamber of Secrets and The Lord of the Rings: The Two Towers) and was the final Star Trek film to feature the Next Generation cast and to be produced by Rick Berman.


Despite development on an eleventh film beginning after Nemesis was released, the poor reception to that film and a sense of "franchise fatigue" meant Paramount was not in a hurry to make the next film. With the cancellation of the television series Star Trek: Enterprise in 2005, there was no new Star Trek being made for the screen for the first time in nearly 20 years.[14] In 2005, Viacom, which owned Paramount Pictures, separated from CBS Corporation, which retained Paramount's television properties including ownership of the Star Trek brand. Paramount president Gail Berman (no relation to Rick Berman) convinced CBS chief executive Leslie Moonves to allow them eighteen months to develop a new Star Trek film, otherwise Paramount would lose the film rights. Berman approached Mission: Impossible III director J. J. Abrams and writers Roberto Orci and Alex Kurtzman to develop the next film.[15]

Star Trek (2009) introduces a new cast as younger versions of the Original Series characters, and was widely considered to be a reboot of the franchise.[16] However, it is actually a continuation set in an alternate timeline that is created after the events of the previous films by Spock, with Nimoy reprising his role. The writers chose this approach to free the new film from the restrictions of established continuity without completely discarding it.[16][17] Orci said he used the term reboot because "that is what the press calls it", but he did not feel it was accurate.[16] The new reality was informally referred to by several names, including the "Abramsverse", "JJ Trek", and "NuTrek",[18] before it was named the "Kelvin Timeline" (versus the "Prime Timeline" of the original series and films) by Michael and Denise Okuda for use in official Star Trek reference guides and encyclopedias. The name comes from the USS Kelvin, a starship involved in the creation of the new timeline which Abrams named after his grandfather, Henry Kelvin.[19][18] The Kelvin Timeline has since been used as a collective term for the reboot films by Paramount.[20]

In the 24th century, Spock (Leonard Nimoy) fails to stop a supernova from destroying Romulus using an artificial black hole. He is pulled into the black hole with an attacking Romulan mining vessel, captained by Nero (Eric Bana), and they are sent back in time to the 23rd century. This creates a new timeline in which volatile Starfleet cadet James Kirk (Chris Pine) must work with Spock's younger self (Zachary Quinto) to stop Nero.

The Enterprise crew hunt a rogue Starfleet operative (Benedict Cumberbatch) who has committed several terrorist attacks, and learn that he is actually Khan Noonien Singh.

The Enterprise is ambushed and the crew are stranded on an unknown planet, where they find themselves in conflict with a new sociopathic enemy (Idris Elba) who hates the Federation and what it stands for.

Roberto Orci, co-writer of the first two reboot films, was hired to direct the third film,[21] but he was replaced by Justin Lin in December 2014.[22] Doug Jung and co-star Simon Pegg wrote the script.[23] Star Trek Beyond was released on July 22, 2016, close to the franchise's 50th anniversary in September 2016.[24]

In June 2018, after becoming showrunner of Star Trek: Discovery, Alex Kurtzman signed a five-year overall deal with CBS Television Studios to expand the Star Trek franchise beyond Discovery to several new series, miniseries, and animated series.[25] In March 2023, Kurtzman expressed interest in making television films for the franchise as well,[26] as he was concerned about oversaturating the franchise with too many ongoing television series. Kurtzman reportedly planned to release a Star Trek streaming film every two years.[27]

Former emperor Philippa Georgiou from the Mirror Universe rejoins Section 31, a secret division of Starfleet tasked with protecting the United Federation of Planets through unsanctioned tactics, and must face the sins of her past.[29]

Paramount+ announced in April 2023 that Star Trek: Section 31, which had been in development as a spin-off series from Discovery, was moving forward as a streaming "event film" instead. Michelle Yeoh was attached to reprise her Discovery role of Philippa Georgiou in the film, which was written by Craig Sweeny and directed by Discovery executive producer Olatunde Osunsanmi.[29] Filming took place at Pinewood Toronto Studios in Canada, where Discovery was produced,[30] from January to March 2024.[31] It was released on Paramount+ on January 24, 2025.[32]

In January 2024, an "origin story" film was added to Paramount's Star Trek slate. Toby Haynes had been hired to direct it and Seth Grahame-Smith was writing the script, with Abrams producing.[79] By the end of March, the project was further along in development than Star Trek 4 and was expected to begin pre-production by the end of the year.[80] Paramount officially announced the film at CinemaCon in April 2024, and said filming would begin later that year for a planned 2025 release.[81] Simon Kinberg was in talks to join as a producer the next month, with potential to become the "franchise shepherd" for Paramount's Star Trek films. At that time, the film was reported to either be set in the "Prime Timeline" in the aftermath of humanity's first contact with alien life, as depicted in First Contact, or to be another reboot of the franchise that retells the first contact and creation of Starfleet stories.[82][83][84]

There have been several failed attempts to make a fourth film in the reboot series since Beyond was released: 

In January 2024, Star Trek 4 was described as the "final chapter" of the main reboot film series.[79] Steve Yockey was writing a new draft of the script by the end of March.[80]

In March 2021, Paramount set Star Trek: Discovery writer Kalinda Vazquez to write a new Star Trek film based on her own original idea.[104] In March 2024, the film was revealed to still be in development.[80]

As Star Trek: Picard was coming to an end, star Patrick Stewart began pushing for a new film to be made starring himself and the rest of the Next Generation cast.[105][106][107] In January 2024, Stewart said he had just been told that a Star Trek film was being written for him to star in.[108] By the end of March, Kurtzman was considering a follow-up to Picard as one of the next Star Trek television films if Section 31 was successful.[80]Sustainability is a social goal for people to co-exist on Earth over a long period of time. Definitions of this term are disputed and have varied with literature, context, and time.[2][1] Sustainability usually has three dimensions (or pillars): environmental, economic, and social.[1] Many definitions emphasize the environmental dimension.[3][4] This can include addressing key environmental problems, including climate change and biodiversity loss. The idea of sustainability can guide decisions at the global, national, organizational, and individual levels.[5] A related concept is that of sustainable development, and the terms are often used to mean the same thing.[6] UNESCO distinguishes the two like this: "Sustainability is often thought of as a long-term goal (i.e. a more sustainable world), while sustainable development refers to the many processes and pathways to achieve it."[7]

Details around the economic dimension of sustainability are controversial.[1] Scholars have discussed this under the concept of weak and strong sustainability. For example, there will always be tension between the ideas of "welfare and prosperity for all" and environmental conservation,[8][1] so trade-offs are necessary. It would be desirable to find ways that separate economic growth from harming the environment.[9] This means using fewer resources per unit of output even while growing the economy.[10] This decoupling reduces the environmental impact of economic growth, such as pollution. Doing this is difficult.[11][12] Some experts say there is no evidence that such a decoupling is happening at the required scale.[13]

It is challenging to measure sustainability as the concept is complex, contextual, and dynamic.[14] Indicators have been developed to cover the environment, society, or the economy but there is no fixed definition of sustainability indicators.[15] The metrics are evolving and include indicators, benchmarks and audits. They include sustainability standards and certification systems like Fairtrade and Organic. They also involve indices and accounting systems such as corporate sustainability reporting and Triple Bottom Line accounting. 

It is necessary to address many barriers to sustainability to achieve a sustainability transition or sustainability transformation.[5]: 34 [16] Some barriers arise from nature and its complexity while others are extrinsic to the concept of sustainability. For example, they can result from the dominant institutional frameworks in countries.

Global issues of sustainability are difficult to tackle as they need global solutions. Existing global organizations such as the UN and WTO are seen as inefficient in enforcing current global regulations. One reason for this is the lack of suitable sanctioning mechanisms.[5]: 135–145  Governments are not the only sources of action for sustainability. For example, business groups have tried to integrate ecological concerns with economic activity, seeking sustainable business.[17][18] Religious leaders have stressed the need for caring for nature and environmental stability. Individuals can also live more sustainably.[5]

Some people have criticized the idea of sustainability. One point of criticism is that the concept is vague and only a buzzword.[19][1] Another is that sustainability might be an impossible goal.[20] Some experts have pointed out that "no country is delivering what its citizens need without transgressing the biophysical planetary boundaries".[21]: 11 

Sustainability is regarded as a "normative concept".[5][22][23][2] This means it is based on what people value or find desirable: "The quest for sustainability involves connecting what is known through scientific study to applications in pursuit of what people want for the future."[23]

The 1983 UN Commission on Environment and Development (Brundtland Commission) had a big influence on the use of the term sustainability today. The commission's 1987 Brundtland Report provided a definition of sustainable development. The report, Our Common Future, defines it as development that "meets the needs of the present without compromising the ability of future generations to meet their own needs".[24][25] The report helped bring sustainability into the mainstream of policy discussions. It also popularized the concept of sustainable development.[1]

Some other key concepts to illustrate the meaning of sustainability include:[23]

In everyday usage, sustainability often focuses on the environmental dimension.[citation needed]

Scholars say that a single specific definition of sustainability may never be possible. But the concept is still useful.[2][23] There have been attempts to define it, for example:

Some definitions focus on the environmental dimension. The Oxford Dictionary of English defines sustainability as: "the property of being environmentally sustainable; the degree to which a process or enterprise is able to be maintained or continued while avoiding the long-term depletion of natural resources".[27]

The term sustainability is derived from the Latin word sustinere. "To sustain" can mean to maintain, support, uphold, or endure.[28][29] So sustainability is the ability to continue over a long period of time.

In the past, sustainability referred to environmental sustainability. It meant using natural resources so that people in the future could continue to rely on them in the long term.[30][31] The concept of sustainability, or Nachhaltigkeit in German, goes back to Hans Carl von Carlowitz (1645–1714), and applied to forestry. The term for this now would be sustainable forest management.[32] He used this term to mean the long-term responsible use of a natural resource. In his 1713 work Silvicultura oeconomica,[33] he wrote that "the highest art/science/industriousness [...] will consist in such a conservation and replanting of timber that there can be a continuous, ongoing and sustainable use".[34] The shift in use of "sustainability" from preservation of forests (for future wood production) to broader preservation of environmental resources (to sustain the world for future generations) traces to a 1972 book by Ernst Basler, based on a series of lectures at M.I.T.[35]

The idea itself goes back a very long time: Communities have always worried about the capacity of their environment to sustain them in the long term. Many ancient cultures, traditional societies, and indigenous peoples have restricted the use of natural resources.[36]

The terms sustainability and sustainable development are closely related. In fact, they are often used to mean the same thing.[6] Both terms are linked with the "three dimensions of sustainability" concept.[1] One distinction is that sustainability is a general concept, while sustainable development can be a policy or organizing principle. Scholars say sustainability is a broader concept because sustainable development focuses mainly on human well-being.[23]

Sustainable development has two linked goals. It aims to meet human development goals. It also aims to enable natural systems to provide the natural resources and ecosystem services needed for economies and society. The concept of sustainable development has come to focus on economic development, social development and environmental protection for future generations.[37]

Scholars usually distinguish three different areas of sustainability. These are the environmental, the social, and the economic. Several terms are in use for this concept. Authors may speak of three pillars, dimensions, components, aspects,[38] perspectives, factors, or goals. All mean the same thing in this context.[1] The three dimensions paradigm has few theoretical foundations.[1]

The popular three intersecting circles, or Venn diagram, representing sustainability first appeared in a 1987 article by the economist Edward Barbier.[1][39]

Scholars rarely question the distinction itself. The idea of sustainability with three dimensions is a dominant interpretation in the literature.[1]

In the Brundtland Report, the environment and development are inseparable and go together in the search for sustainability. It described sustainable development as a global concept linking environmental and social issues. It added sustainable development is important for both developing countries and industrialized countries:

The 'environment' is where we all live; and 'development' is what we all do in attempting to improve our lot within that abode. The two are inseparable. [...] We came to see that a new development path was required, one that sustained human progress not just in a few pieces for a few years, but for the entire planet into the distant future. Thus 'sustainable development' becomes a goal not just for the 'developing' nations, but for industrial ones as well.
The Rio Declaration from 1992 is seen as "the foundational instrument in the move towards sustainability".[40]: 29  It includes specific references to ecosystem integrity.[40]: 31  The plan associated with carrying out the Rio Declaration also discusses sustainability in this way. The plan, Agenda 21, talks about economic, social, and environmental dimensions:[41]: 8.6 

Countries could develop systems for monitoring and evaluation of progress towards achieving sustainable development by adopting indicators that measure changes across economic, social and environmental dimensions.
Agenda 2030 from 2015 also viewed sustainability in this way. It sees the 17 Sustainable Development Goals (SDGs) with their 169 targets as balancing "the three dimensions of sustainable development, the economic, social and environmental".[42]

Scholars have discussed how to rank the three dimensions of sustainability. Many publications state that the environmental dimension is the most important.[3][4] (Planetary integrity or ecological integrity are other terms for the environmental dimension.) 

Protecting ecological integrity is the core of sustainability according to many experts.[4] If this is the case then its environmental dimension sets limits to economic and social development.[4]

The diagram with three nested ellipses is one way of showing the three dimensions of sustainability together with a hierarchy: It gives the environmental dimension a special status. In this diagram, the environment includes society, and society includes economic conditions. Thus it stresses a hierarchy. 

Another model shows the three dimensions in a similar way: In this SDG wedding cake model, the economy is a smaller subset of the societal system. And the societal system in turn is a smaller subset of the biosphere system.[44]

In 2022 an assessment examined the political impacts of the Sustainable Development Goals. The assessment found that the "integrity of the earth's life-support systems" was essential for sustainability.[3]: 140  The authors said that "the SDGs fail to recognize that planetary, people and prosperity concerns are all part of one earth system, and that the protection of planetary integrity should not be a means to an end, but an end in itself".[3]: 147  The aspect of environmental protection is not an explicit priority for the SDGs. This causes problems as it could encourage countries to give the environment less weight in their developmental plans.[3]: 144  The authors state that "sustainability on a planetary scale is only achievable under an overarching Planetary Integrity Goal that recognizes the biophysical limits of the planet".[3]: 161 

Other frameworks bypass the compartmentalization of sustainability into separate dimensions completely.[1]

The environmental dimension is central to the overall concept of sustainability. People became more and more aware of environmental pollution in the 1960s and 1970s. This led to discussions on sustainability and sustainable development. This process began in the 1970s with concern for environmental issues. These included natural ecosystems or natural resources and the human environment. It later extended to all systems that support life on Earth, including human society.[45]: 31  Reducing these negative impacts on the environment would improve environmental sustainability.[45][46]

Environmental pollution is not a new phenomenon. But it has been only a local or regional concern for most of human history. Awareness of global environmental issues increased in the 20th century.[45]: 5 [47] The harmful effects and global spread of pesticides like DDT came under scrutiny in the 1960s.[48] In the 1970s it emerged that chlorofluorocarbons (CFCs) were depleting the ozone layer. This led to the de facto ban of CFCs with the Montreal Protocol in 1987.[5]: 146 

In the early 20th century, Arrhenius discussed the effect of greenhouse gases on the climate (see also: history of climate change science).[49] Climate change due to human activity became an academic and political topic several decades later. This led to the establishment of the IPCC in 1988 and the UNFCCC in 1992.

In 1972, the UN Conference on the Human Environment took place. It was the first UN conference on environmental issues. It stated it was important to protect and improve the human environment.[50]: 3 It emphasized the need to protect wildlife and natural habitats:[50]: 4 

The natural resources of the earth, including the air, water, land, flora and fauna and [...] natural ecosystems must be safeguarded for the benefit of present and future generations through careful planning or management, as appropriate.
In 2000, the UN launched eight Millennium Development Goals. The aim was for the global community to achieve them by 2015. Goal 7 was to "ensure environmental sustainability". But this goal did not mention the concepts of social or economic sustainability.[1]

Specific problems often dominate public discussion of the environmental dimension of sustainability: In the 21st century these problems have included climate change, biodiversity and pollution. Other global problems are loss of ecosystem services, land degradation, environmental impacts of animal agriculture and air and water pollution, including marine plastic pollution and ocean acidification.[51][52] Many people worry about human impacts on the environment. These include impacts on the atmosphere, land, and water resources.[45]: 21 

Human activities now have an impact on Earth's geology and ecosystems. This led Paul Crutzen to call the current geological epoch the Anthropocene.[53]

The economic dimension of sustainability is controversial.[1] This is because the term development within sustainable development can be interpreted in different ways. Some may take it to mean only economic development and growth. This can promote an economic system that is bad for the environment.[54][55][56] Others focus more on the trade-offs between environmental conservation and achieving welfare goals for basic needs (food, water, health, and shelter).[8]

Economic development can indeed reduce hunger or energy poverty. This is especially the case in the least developed countries. That is why Sustainable Development Goal 8 calls for economic growth to drive social progress and well-being. Its first target is for: "at least 7 per cent GDP growth per annum in the least developed countries".[57] However, the challenge is to expand economic activities while reducing their environmental impact.[10]: 8  In other words, humanity will have to find ways how societal progress (potentially by economic development) can be reached without excess strain on the environment. 

The Brundtland report says poverty causes environmental problems. Poverty also results from them. So addressing environmental problems requires understanding the factors behind world poverty and inequality.[24]: Section I.1.8  The report demands a new development path for sustained human progress. It highlights that this is a goal for both developing and industrialized nations.[24]: Section I.1.10 

UNEP and UNDP launched the Poverty-Environment Initiative in 2005 which has three goals. These are reducing extreme poverty, greenhouse gas emissions, and net natural asset loss. This guide to structural reform will enable countries to achieve the SDGs.[58][59]: 11  It should also show how to address the trade-offs between ecological footprint and economic development.[5]: 82 

The social dimension of sustainability is not well defined.[60][61][62] One definition states that a society is sustainable in social terms if people do not face structural obstacles in key areas. These key areas are health, influence, competence, impartiality and meaning-making.[63]

Some scholars place social issues at the very center of discussions.[64] They suggest that all the domains of sustainability are social. These include ecological, economic, political, and cultural sustainability. These domains all depend on the relationship between the social and the natural. The ecological domain is defined as human embeddedness in the environment. From this perspective, social sustainability encompasses all human activities.[65] It goes beyond the intersection of economics, the environment, and the social.[66]

There are many broad strategies for more sustainable social systems. They include improved education and the political empowerment of women. This is especially the case in developing countries. They include greater regard for social justice. This involves equity between rich and poor both within and between countries. And it includes intergenerational equity.[67] Providing more social safety nets to vulnerable populations would contribute to social sustainability.[68]: 11 

A society with a high degree of social sustainability would lead to livable communities with a good quality of life (being fair, diverse, connected and democratic).[69]

Indigenous communities might have a focus on particular aspects of sustainability, for example spiritual aspects, community-based governance and an emphasis on place and locality.[70]

Some experts have proposed further dimensions. These could cover institutional, cultural, political, and technical dimensions.[1]

Some scholars have argued for a fourth dimension. They say the traditional three dimensions do not reflect the complexity of contemporary society.[71] For example, Agenda 21 for culture and the United Cities and Local Governments argue that sustainable development should include a solid cultural policy. They also advocate for a cultural dimension in all public policies. Another example was the Circles of Sustainability approach, which included cultural sustainability.[72]

People often debate the relationship between the environmental and economic dimensions of sustainability.[73] In academia, this is discussed under the term weak and strong sustainability. In that model, the weak sustainability concept states that capital made by humans could replace most of the natural capital.[74][73] Natural capital is a way of describing environmental resources. People may refer to it as nature. An example for this is the use of environmental technologies to reduce pollution.[75]

The opposite concept in that model is strong sustainability. This assumes that nature provides functions that technology cannot replace.[76] Thus, strong sustainability acknowledges the need to preserve ecological integrity.[5]: 19  The loss of those functions makes it impossible to recover or repair many resources and ecosystem services. Biodiversity, along with pollination and fertile soils, are examples. Others are clean air, clean water, and regulation of climate systems.

Weak sustainability has come under criticism. It may be popular with governments and business but does not ensure the preservation of the earth's ecological integrity.[77] This is why the environmental dimension is so important.[4]

The World Economic Forum illustrated this in 2020. It found that $44 trillion of economic value generation depends on nature. This value, more than half of the world's GDP, is thus vulnerable to nature loss.[78]: 8  Three large economic sectors are highly dependent on nature: construction, agriculture, and food and beverages. Nature loss results from many factors. They include land use change, sea use change and climate change. Other examples are natural resource use, pollution, and invasive alien species.[78]: 11 

Trade-offs between different dimensions of sustainability are a common topic for debate. Balancing the environmental, social, and economic dimensions of sustainability is difficult. This is because there is often disagreement about the relative importance of each. To resolve this, there is a need to integrate, balance, and reconcile the dimensions.[1] For example, humans can choose to make ecological integrity a priority or to compromise it.[4]

Some even argue the Sustainable Development Goals are unrealistic. Their aim of universal human well-being conflicts with the physical limits of Earth and its ecosystems.[21]: 41 

There are several methods to measure or describe human impacts on Earth. They include the ecological footprint, ecological debt, carrying capacity, and sustainable yield. The idea of planetary boundaries is that there are limits to the carrying capacity of the Earth. It is important not to cross these thresholds to prevent irreversible harm to the Earth.[86][87] These planetary boundaries involve several environmental issues. These include climate change and biodiversity loss. They also include types of pollution. These are biogeochemical (nitrogen and phosphorus), ocean acidification, land use, freshwater, ozone depletion, atmospheric aerosols, and chemical pollution.[86][88] (Since 2015 some experts refer to biodiversity loss as change in biosphere integrity. They refer to chemical pollution as introduction of novel entities.)

The IPAT formula measures the environmental impact of humans. It emerged in the 1970s. It states this impact is proportional to human population, affluence and technology.[89] This implies various ways to increase environmental sustainability. One would be human population control. Another would be to reduce consumption and affluence[90] such as energy consumption. Another would be to develop innovative or green technologies such as renewable energy. In other words, there are two broad aims. The first would be to have fewer consumers. The second would be to have less environmental footprint per consumer.

The Millennium Ecosystem Assessment from 2005 measured 24 ecosystem services. It concluded that only four have improved over the last 50 years. It found 15 are in serious decline and five are in a precarious condition.[91]: 6–19 

Experts in environmental economics have calculated the cost of using public natural resources. One project calculated the damage to ecosystems and biodiversity loss. This was the Economics of Ecosystems and Biodiversity project from 2007 to 2011.[92]

An entity that creates environmental and social costs often does not pay for them. The market price also does not reflect those costs. In the end, government policy is usually required to resolve this problem.[93]

Decision-making can take future costs and benefits into account. The tool for this is the social discount rate. The bigger the concern for future generations, the lower the social discount rate should be.[94] Another approach is to put an economic value on ecosystem services. This allows us to assess environmental damage against perceived short-term welfare benefits. One calculation is that, "for every dollar spent on ecosystem restoration, between three and 75 dollars of economic benefits from ecosystem goods and services can be expected".[95]

In recent years, economist Kate Raworth has developed the concept of doughnut economics. This aims to integrate social and environmental sustainability into economic thinking. The social dimension acts as a minimum standard to which a society should aspire. The carrying capacity of the planet acts an outer limit.[96]

There are many reasons why sustainability is so difficult to achieve. These reasons have the name sustainability barriers.[5][16] Before addressing these barriers it is important to analyze and understand them.[5]: 34  Some barriers arise from nature and its complexity ("everything is related").[23] Others arise from the human condition. One example is the value-action gap. This reflects the fact that people often do not act according to their convictions. Experts describe these barriers as intrinsic to the concept of sustainability.[97]: 81 

Other barriers are extrinsic to the concept of sustainability. This means it is possible to overcome them. One way would be to put a price tag on the consumption of public goods.[97]: 84  Some extrinsic barriers relate to the nature of dominant institutional frameworks. Examples would be where market mechanisms fail for public goods. Existing societies, economies, and cultures encourage increased consumption. There is a structural imperative for growth in competitive market economies. This inhibits necessary societal change.[90]

Furthermore, there are several barriers related to the difficulties of implementing sustainability policies. There are trade-offs between the goals of environmental policies and economic development. Environmental goals include nature conservation. Development may focus on poverty reduction.[16][5]: 65  There are also trade-offs between short-term profit and long-term viability.[97]: 65  Political pressures generally favor the short term over the long term. So they form a barrier to actions oriented toward improving sustainability.[97]: 86 

Barriers to sustainability may also reflect current trends. These could include consumerism and short-termism.[97]: 86 

Sustainability transformation (or transition), though not universally defined, refers to a deep, system-wide change affecting technology, economy, society, values, and goals. It is a complex and multi-layered process that must happen at all scales, from local communities to global governance institutions. However, it is often politically debated, as different stakeholders may disagree on both the goals and the methods of change. Additionally, such transformations can challenge existing power structures and resource distribution.[98]

A sustainability transition requires major change in societies. They must change their fundamental values and organizing principles.[45]: 15  These new values would emphasize "the quality of life and material sufficiency, human solidarity and global equity, and affinity with nature and environmental sustainability".[45]: 15  A transition may only work if far-reaching lifestyle changes accompany technological advances.[90]

Scientists have pointed out that: "Sustainability transitions come about in diverse ways, and all require civil-society pressure and evidence-based advocacy, political leadership, and a solid understanding of policy instruments, markets, and other drivers."[52]

There are four possible overlapping processes of transformation. They each have different political dynamics. Technology, markets, government, or citizens can lead these processes.[22]

The European Environment Agency defines a sustainability transition as "a fundamental and wide-ranging transformation of a socio-technical system towards a more sustainable configuration that helps alleviate persistent problems such as climate change, pollution, biodiversity loss or resource scarcities."[99]: 152  The concept of sustainability transitions is similar to the concept of energy transitions.[100]

One expert argues a sustainability transition must be "supported by a new kind of culture, a new kind of collaboration, [and] a new kind of leadership".[101] It requires a large investment in "new and greener capital goods, while simultaneously shifting capital away from unsustainable systems".[21]: 107 

In 2024 an interdisciplinary group of experts including Chip Fletcher, William J. Ripple, Phoebe Barnard, Kamanamaikalani Beamer, Christopher Field, David Karl, David King, Michael E. Mann and Naomi Oreskes advocated for a paradigm shift toward genuine sustainability and resource regeneration. They said that "such a transformation is imperative to reverse the tide of biodiversity loss due to overconsumption and to reinstate the security of food and water supplies, which are foundational for the survival of global populations."[102]

It is possible to divide action principles to make societies more sustainable into four types. These are nature-related, personal, society-related and systems-related principles.[5]: 206 

There are many approaches that people can take to transition to environmental sustainability. These include maintaining ecosystem services, protecting and co-creating common resources, reducing food waste, and promoting dietary shifts towards plant-based foods.[103]  Another is reducing population growth by cutting fertility rates. Others are promoting new green technologies, and adopting renewable energy sources while phasing out subsidies to fossil fuels.[52]

In 2017 scientists published an update to the 1992 World Scientists' Warning to Humanity. It showed how to move towards environmental sustainability. It proposed steps in three areas:[52]

In 2015, the United Nations agreed the Sustainable Development Goals (SDGs). Their official name is Agenda 2030 for the Sustainable Development Goals. The UN described this programme as a very ambitious and transformational vision. It said the SDGs were of unprecedented scope and significance.[42]: 3/35 

The UN said: "We are determined to take the bold and transformative steps which are urgently needed to shift the world on to a sustainable and resilient path."[42]

The 17 goals and targets lay out transformative steps. For example, the SDGs aim to protect the future of planet Earth. The UN pledged to "protect the planet from degradation, including through sustainable consumption and production, sustainably managing its natural resources and taking urgent action on climate change, so that it can support the needs of the present and future generations".[42]

Eco-economic decoupling is an idea to resolve tradeoffs between economic growth and environmental conservation. The idea is to "decouple environmental bads from economic goods as a path towards sustainability".[11] This would mean "using less resources per unit of economic output and reducing the environmental impact of any resources that are used or economic activities that are undertaken".[10]: 8  The intensity of pollutants emitted makes it possible to measure pressure on the environment. This in turn makes it possible to measure decoupling. This involves following changes in the emission intensity associated with economic output.[10] Examples of absolute long-term decoupling are rare. But some industrialized countries have decoupled GDP growth from production- and consumption-based CO2 emissions.[104] Yet, even in this example, decoupling alone is not enough. It is necessary to accompany it with "sufficiency-oriented strategies and strict enforcement of absolute reduction targets".[104]: 1 

One study in 2020 found no evidence of necessary decoupling. This was a meta-analysis of 180 scientific studies. It found that there is "no evidence of the kind of decoupling needed for ecological sustainability" and that "in the absence of robust evidence, the goal of decoupling rests partly on faith".[11] Some experts have questioned the possibilities for decoupling and thus the feasibility of green growth.[12] Some have argued that decoupling on its own will not be enough to reduce environmental pressures. They say it would need to include the issue of economic growth.[12] There are several reasons why adequate decoupling is currently not taking place. These are rising energy expenditure, rebound effects, problem shifting, the underestimated impact of services, the limited potential of recycling, insufficient and inappropriate technological change, and cost-shifting.[12]

The decoupling of economic growth from environmental deterioration is difficult. This is because the entity that causes environmental and social costs does not generally pay for them. So the market price does not express such costs.[93] For example, the cost of packaging into the price of a product. may factor in the cost of packaging. But it may omit the cost of disposing of that packaging. Economics describes such factors as externalities, in this case a negative externality.[105] Usually, it is up to government action or local governance to deal with externalities.[106]

There are various ways to incorporate environmental and social costs and benefits into economic activities. Examples include: taxing the activity (the polluter pays); subsidizing activities with positive effects (rewarding stewardship); and outlawing particular levels of damaging practices (legal limits on pollution).[93]

A textbook on natural resources and environmental economics stated in 2011: "Nobody who has seriously studied the issues believes that the economy's relationship to the natural environment can be left entirely to market forces."[107]: 15  This means natural resources will be over-exploited and destroyed in the long run without government action.

Elinor Ostrom (winner of the 2009Nobel economics prize) expanded on this. She stated that local governance (or self-governance) can be a third option besides the market or the national government.[108] She studied how people in small, local communities manage shared natural resources.[109] She showed that communities using natural resources can establish rules their for use and maintenance. These are resources such as pastures, fishing waters, and forests. This leads to both economic and ecological sustainability.[108] Successful self-governance needs groups with frequent communication among participants. In this case, groups can manage the usage of common goods without overexploitation.[5]: 117  Based on Ostrom's work, some have argued that: "Common-pool resources today are overcultivated because the different agents do not know each other and cannot directly communicate with one another."[5]: 117 

Questions of global concern are difficult to tackle. That is because global issues need global solutions. But existing global organizations (UN, WTO, and others) do not have sufficient means.[5]: 135  For example, they lack sanctioning mechanisms to enforce existing global regulations.[5]: 136  Some institutions do not enjoy universal acceptance. An example is the International Criminal Court. Their agendas are not aligned (for example UNEP, UNDP, and WTO) And some accuse them of nepotism and mismanagement.[5]: 135–145  

Multilateral international agreements, treaties, and intergovernmental organizations (IGOs) face further challenges. These result in barriers to sustainability. Often these arrangements rely on voluntary commitments. An example is Nationally Determined Contributions for climate action. There can be a lack of enforcement of existing national or international regulation. And there can be gaps in regulation for international actors such as multi-national enterprises. Critics of some global organizations say they lack legitimacy and democracy. Institutions facing such criticism include the WTO, IMF, World Bank, UNFCCC, G7, G8 and OECD.[5]: 135 

Sustainable business practices integrate ecological concerns with social and economic ones.[17][18] One accounting framework for this approach uses the phrase "people, planet, and profit". The name of this approach is the triple bottom line. The circular economy is a related concept. Its goal is to decouple environmental pressure from economic growth.[110][111]

Growing attention towards sustainability has led to the formation of many organizations. These include the Sustainability Consortium of the Society for Organizational Learning,[112] the Sustainable Business Institute,[113] and the World Business Council for Sustainable Development.[114] Supply chain sustainability looks at the environmental and human impacts of products in the supply chain. It considers how they move from raw materials sourcing to production, storage, and delivery, and every transportation link on the way.[115]


Religious leaders have stressed the importance of caring for nature and environmental sustainability. In 2015 over 150 leaders from various faiths issued a joint statement to the UN Climate Summit in Paris 2015.[116] They reiterated a statement made in the Interfaith Summit in New York in 2014:
As representatives from different faith and religious traditions, we stand together to express deep concern for the consequences of climate change on the earth and its people, all entrusted, as our faiths reveal, to our common care. Climate change is indeed a threat to life, a precious gift we have received and that we need to care for.[117]
Individuals can also live in a more sustainable way. They can change their lifestyles, practise ethical consumerism, and embrace frugality.[5]: 236   These sustainable living approaches can also make cities more sustainable. They do this by altering the built environment.[118] Such approaches include sustainable transport, sustainable architecture, and zero emission housing. Research can identify the main issues to focus on. These include flying, meat and dairy products, car driving, and household sufficiency. Research can show how to create cultures of sufficiency, care, solidarity, and simplicity.[90]

Some young people are using activism, litigation, and on-the-ground efforts to advance sustainability. This is particularly the case in the area of climate action.[68]: 60 

Scholars have criticized the concepts of sustainability and sustainable development from different angles. One was Dennis Meadows, one of the authors of the first report to the Club of Rome, called "The Limits to Growth". He argued many people deceive themselves by using the Brundtland definition of sustainability.[54] This is because the needs of the present generation are actually not met today. Instead, economic activities to meet present needs will shrink the options of future generations.[119][5]: 27  Another criticism is that the paradigm of sustainability is no longer suitable as a guide for transformation. This is because societies are "socially and ecologically self-destructive consumer societies".[120]

Some scholars have even proclaimed the end of the concept of sustainability. This is because humans now have a significant impact on Earth's climate system and ecosystems.[20] It might become impossible to pursue sustainability because of these complex, radical, and dynamic issues.[20] Others have called sustainability a utopian ideal: "We need to keep sustainability as an ideal; an ideal which we might never reach, which might be utopian, but still a necessary one."[5]: 5 

The term is often hijacked and thus can lose its meaning. People use it for all sorts of things, such as saving the planet to recycling your rubbish.[27] A specific definition may never be possible. This is because sustainability is a concept that provides a normative structure. That describes what human society regards as good or desirable.[2]

But some argue that while sustainability is vague and contested it is not meaningless.[2] Although lacking in a singular definition, this concept is still useful. Scholars have argued that its fuzziness can actually be liberating. This is because it means that "the basic goal of sustainability (maintaining or improving desirable conditions [...]) can be pursued with more flexibility".[23]

Sustainability has a reputation as a buzzword.[1] People may use the terms sustainability and sustainable development in ways that are different to how they are usually understood. This can result in confusion and mistrust. So a clear explanation of how the terms are being used in a particular situation is important.[23]

Greenwashing is a practice of deceptive marketing. It is when a company or organization provides misleading information about the sustainability of a product, policy, or other activity.[68]: 26 [121] Investors are wary of this issue as it exposes them to risk.[122] The reliability of eco-labels is also doubtful in some cases.[123] Ecolabelling is a voluntary method of environmental performance certification and labelling for food and consumer products. The most credible eco-labels are those developed with close participation from all relevant stakeholders.[124]Sir Terence David John Pratchett OBE (28 April 1948 – 12 March 2015) was an English author, humorist, and satirist, best known for the Discworld series of 41 comic fantasy novels published between 1983 and 2015, and for the apocalyptic comedy novel Good Omens (1990), which he co-wrote with Neil Gaiman.

Pratchett's first novel, The Carpet People, was published in 1971. The first Discworld novel, The Colour of Magic, was published in 1983, after which Pratchett wrote an average of two books a year. The final Discworld novel, The Shepherd's Crown, was published in August 2015, five months after his death.

With more than 100 million books sold worldwide in 43 languages, Pratchett was the UK's best-selling author of the 1990s. He was appointed an Officer of the Order of the British Empire (OBE) in 1998 and was knighted for services to literature in the 2009 New Year Honours. In 2001, he won the annual Carnegie Medal for The Amazing Maurice and His Educated Rodents, the first Discworld book marketed for children. He received the World Fantasy Award for Life Achievement in 2010.

In December 2007 Pratchett announced that he had been diagnosed with early-onset Alzheimer's disease. He later made a substantial public donation to the Alzheimer's Research Trust (now Alzheimer's Research UK, ARUK), filmed three television programmes chronicling his experiences with the condition for the BBC, and became a patron of ARUK. Pratchett died on 12 March 2015, at the age of 66.

Pratchett was born on 28 April 1948 in Beaconsfield in Buckinghamshire, England, the only child of David (1921–2006), a mechanic, and Eileen Pratchett (1922–2010), a secretary, of Hay-on-Wye.[1][2] His maternal grandparents came from Ireland.[3] Pratchett attended Holtspur School, where he was bullied for his speech impediments.[4] He was bothered by the head teacher, who, he said, thought "he could tell how successful you were going to be in later life by how well you could read or write at the age of six".[4]

Pratchett's family moved to Bridgwater, Somerset, briefly in 1957.[1] He passed his eleven plus exam in 1958, earning a place at High Wycombe Technical High School,[a][6] where he was a key member of the debating society[7][8] and wrote stories for the school magazine.[7][9] Pratchett described himself as a "non-descript" student and,[10] in his Who's Who entry, credited his education to the Beaconsfield Public Library.[1][11]

Pratchett's early interests included astronomy.[5] He collected Brooke Bond tea cards about space, owned a telescope and wanted to be an astronomer, but lacked the necessary mathematical skills.[5] He developed an interest in science fiction and attended science fiction conventions from about 1963–1964, but stopped a few years later when he got his first job as a trainee journalist at the local paper.[12] His early reading included the works of H. G. Wells, Arthur Conan Doyle, and "every book you really ought to read", which he later regarded as "getting an education".[13]

Pratchett published his first short story, "Business Rivals", in the High Wycombe Technical School's magazine in 1962. It is the tale of a man named Crucible who finds the Devil in his flat in a cloud of sulphurous smoke.[14] "The Hades Business" was published in the school magazine when he was 13, and published commercially when he was 15.[15]

Pratchett earned five O-levels and started A-level courses in Art, English and History.[16] His initial career choice was journalism and he left school at 17, in 1965, to start an apprenticeship with Arthur Church, the editor of the Bucks Free Press. In this position he wrote, among other things, more than 80 stories for the Children's Circle section under the name Uncle Jim. Two of the stories contain characters found in his novel The Carpet People (1971).[17] While on day release from his apprenticeship, Pratchett finished his A-Level in English and took the National Council for the Training of Journalists proficiency course.[7][18]

In 1968 Pratchett interviewed Peter Bander van Duren, co-director of a small publishing company, Colin Smythe Ltd. Pratchett mentioned that he had written a manuscript, The Carpet People.[19][20] Colin Smythe Ltd published the book in 1971, with illustrations by Pratchett.[21] It received strong, although few, reviews and was followed by the science fiction novels The Dark Side of the Sun (1976) and Strata (1981).[22] In the 1970s and 1980s, Pratchett published stories in a regional newspaper under the pseudonym Patrick Kearns.[23]

After various positions in journalism, in 1979 Pratchett became press officer for the South West Region of the Central Electricity Generating Board (CEGB) in an area that contained three nuclear power stations.[b] He later joked that he had demonstrated "impeccable timing" by making this career change so soon after the Three Mile Island nuclear accident in Pennsylvania, US, and said he would "write a book about his experiences if he thought anyone would actually believe them".[25][26]

The first Discworld novel, The Colour of Magic, was published in hardback by Colin Smythe Ltd in 1983. Pratchett gave up working for the CEGB to make his living through writing in 1987, after finishing the fourth Discworld novel, Mort. His sales increased quickly and many of his books occupied top places on bestseller lists; he was the UK's bestselling author of the 1990s.[27] According to The Times, Pratchett was the top-selling and highest earning UK author in 1996.[18] Some of his books have been published by Doubleday, another Transworld imprint.[28] In the United States, where his books are published by HarperCollins, Pratchett had poorer sales, marketing and distribution until 2005, when Thud! reached the New York Times bestseller list.[29]

According to the Bookseller's Pocket Yearbook (2005), in 2003 Pratchett's UK sales amounted to 3.4% of the fiction market by hardback sales and 3.8% by value, putting him in second place behind J. K. Rowling (6% and 5.6%, respectively), while in the paperback sales list Pratchett came 5th with 1.2% and 1.3% by value (behind James Patterson (1.9% and 1.7%), Alexander McCall Smith, John Grisham and J. R. R. Tolkien).[30] He has UK sales of more than 2.5 million copies a year.[31] His 2011 Discworld novel Snuff became the third-fastest-selling hardback adult-readership novel since records began in the UK, selling 55,000 copies in the first three days.[32] As of 2023, Pratchett's works have sold more than 100 million copies in 43 languages.[33]

Pratchett married Lyn Purves at the Congregational Church, Gerrards Cross, on 5 October 1968.[18] They moved to Rowberrow, Somerset, in 1970. Their daughter Rhianna Pratchett, also a writer, was born there in 1976. In 1993 the family moved to Broad Chalke, a village west of Salisbury, Wiltshire.[34]

Pratchett was the patron of the Friends of High Wycombe Library.[35] In 2013 he gave a talk at Beaconsfield Library, which he had visited as a child, and donated the income from the event to it. He also visited his former school to speak to the students.[14]

Pratchett often wore large black hats, in a style described as "more that of urban cowboy than city gent".[36] Concern for the future of civilisation prompted him to install five kilowatts of photovoltaic cells (for solar energy) at his house in 2007.[37]

Pratchett had an observatory built in his back garden[12] and was a keen astronomer from childhood.[5] He made a 2005 appearance on the BBC programme The Sky at Night[38] and appeared on the 50th anniversary of the show in 2007.[39] He travelled on a cruise ship from Taiwan to watch the 2009 solar eclipse.[40]

Pratchett started to use computers for writing as soon as they were available to him. His first computer was a ZX81; the first computer he used properly for writing was an Amstrad CPC 464, later replaced by an IBM PC compatible. Pratchett was one of the first authors to routinely use the Internet to communicate with fans, and was a contributor to the Usenet newsgroup alt.fan.pratchett from 1992.[41] However, he did not consider the Internet a hobby, just another "thing to use".[26] He had many computers in his house,[26] with a bank of six monitors to ease writing.[42][43] When he travelled, he always took a portable computer, originally a 1992 Olivetti Quaderno,[44] with him to write.[26]

In a 1995 interview with Microsoft co-founder Bill Gates, Pratchett expressed concern about the potential spread of misinformation online. He felt that there was a "kind of parity of esteem of information" on the internet, and gave the example of Holocaust denial being presented on the same terms as peer-reviewed research, with no easy way to gauge reliability. Gates disagreed, saying that online authorities would index and check facts and sources in a much more sophisticated way than in print. The interview was rediscovered in 2019, and seen by Pratchett's biographer as prescient of fake news.[45]

Pratchett was an avid video game player, and collaborated in the creation of a number of game adaptations of his books. He favoured games that are "intelligent and have some depth", citing Half-Life 2 (2004) and fan missions for Thief as examples.[46][47] The red army in Interesting Times prompted comparisons to the 1991 puzzle game Lemmings. When asked about this connection, Pratchett said: "Merely because the red army can fight, dig, march and climb and is controlled by little icons? Can't imagine how anyone thought that ... Not only did I wipe Lemmings from my hard disk, I overwrote it so I couldn't get it back."[48] He described The Elder Scrolls IV: Oblivion (2006) as his favourite video game, saying that he used many of its non-combat-oriented fan-made mods,[49] and contributed to the development of at least one popular fan-made mod.[50]

Pratchett had a fascination with natural history that he referred to many times, and he owned a greenhouse full of carnivorous plants.[51] He described them in the biographical notes on the dust jackets of some of his books, and elsewhere,[52] as "not as interesting as people think".[53] By Carpe Jugulum the account had become that "he used to grow carnivorous plants, but now they've taken over the greenhouse and he avoids going in".[54]

In 1995, a fossil of a sea-turtle from the Eocene epoch of New Zealand was named Psephophorus terrypratchetti in his honour by the palaeontologist Richard Köhler.[55]

In 2016, Pratchett fans unsuccessfully petitioned the International Union of Pure and Applied Chemistry (IUPAC) to name chemical element 117, temporarily called ununseptium, as octarine with the proposed symbol Oc (pronounced "ook").[56] The final name chosen for element 117 was tennessine with the symbol Ts.[57]

Pratchett was a trustee for the Orangutan Foundation[58] but was pessimistic about the future of orangutans.[37] His activities included visiting Borneo with a Channel 4 film crew to make an episode of Jungle Quest in 1995, seeing orangutans in their natural habitat.[59] Following Pratchett's lead, fan events such as the Discworld Conventions have adopted Orangutan Foundation as their nominated charity, which has been acknowledged by the foundation.[60] One of Pratchett's most popular fictional characters, the Librarian, is a wizard who was transformed into an orangutan in a magical accident and decides to remain in that condition as it is so convenient for his work.

Pratchett, who was brought up in a Church of England family,[61] described himself as an atheist[62] and a humanist. He was a Distinguished Supporter of Humanists UK (formerly known as the British Humanist Association)[63] and an Honorary Associate of the National Secular Society.[64]

Pratchett wrote that he read the Old Testament as a child and "was horrified", but liked the New Testament and thought that Jesus "had a lot of good things to say ... But I could never see the two testaments as one coherent narrative".[61] He then read On the Origin of Species, which "all made perfect sense ... Evolution was far more thrilling to me than the biblical account."[61] He said that he had never disliked religion and thought it had a purpose in human evolution.[61] In an interview Pratchett cites a quotation from the protagonist in his novel Nation, "It is better to build a seismograph than to worship the volcano", a statement Pratchett said he agreed with.[61]

Pratchett told The Times in 2008: "I believe in the same God that Einstein did ... And it is just possible that once you have got past all the gods that we have created with big beards and many human traits, just beyond all that on the other side of physics, there just may be the ordered structure from which everything flows."[62] In an interview on Front Row he described an experience of hearing his dead father's voice and feeling a sense of peace.[65] Commentators took these statements to mean that Pratchett had become religious, but Pratchett responded in an article published in the Daily Mail in which he denied that he had found God, and clarified that he believed the voice had come from a memory of his father and a sense of personal elation.[61]

In August 2007, Pratchett was misdiagnosed as having had a minor stroke a few years before, which doctors believed had damaged the right side of his brain.[36][66][67] In December 2007, he announced that he had been diagnosed with early-onset Alzheimer's disease, which had been responsible for the "stroke".[67][68] He had a rare form of posterior cortical atrophy (PCA),[36][66] a disease in which areas at the back of the brain begin to shrink and shrivel.[69]

Describing the diagnosis as an "embuggerance" in a radio interview, Pratchett appealed to people to "keep things cheerful" and proclaimed that "we are taking it fairly philosophically down here and possibly with a mild optimism".[70] He stated he felt he had time for "at least a few more books yet", and added that while he understood the impulse to ask "is there anything I can do?", in this case he would only entertain such offers from "very high-end experts in brain chemistry".[70] Discussing his diagnosis at the Bath Literature Festival in early 2008, Pratchett revealed that by then he found it too difficult to write dedications when signing books.[71] In his later years Pratchett wrote by dictating to his assistant, Rob Wilkins, or by using speech-recognition software.[72]

In March 2008, Pratchett announced he was donating $1 million (about £494,000) to the Alzheimer's Research Trust (later called Alzheimer's Research UK), and that he was shocked "to find out that funding for Alzheimer's research is just 3% of that to find cancer cures".[69][73][74] He said: "I am, along with many others, scrabbling to stay ahead long enough to be there when the cure comes along."[69]

In April 2008, Pratchett worked with the BBC to make a two-part documentary series about his illness, Terry Pratchett: Living With Alzheimer's.[75] The first part was broadcast on BBC Two on 4 February 2009, drawing 2.6 million viewers and a 10.4% audience share.[76] The second, broadcast on 11 February 2009, drew 1.72 million viewers and a 6.8% audience share.[77] The documentary won a BAFTA award in the Factual Series category.[78]

On 26 November 2008, Pratchett met Gordon Brown, then the British prime minister, and asked for an increase in dementia-research funding.[79] Pratchett tested a prototype device to address his condition.[80][81] The ability of the device to alter the course of the illness has been met with scepticism from Alzheimer's researchers.[82]

In an article published in 2009 Pratchett stated that he wished to die by assisted suicide (a term he disliked) before his disease progressed to a critical point.[83] He later said that he felt "it should be possible for someone stricken with a serious and ultimately fatal illness to choose to die peacefully with medical help, rather than suffer".[84] Pratchett was selected to give the 2010 BBC Richard Dimbleby Lecture,[85] Shaking Hands With Death, broadcast on 1 February 2010.[86] Pratchett introduced his lecture on the topic of assisted death (he preferred this to the term "assisted suicide"), but the main text was read by his friend Tony Robinson because his condition made it difficult for him to read.[87] In June 2011, Pratchett presented a BBC television documentary, Terry Pratchett: Choosing to Die, about assisted suicide. It won the Best Documentary award at the Scottish BAFTAs in November 2011.[88][89][90]

In September 2012, Pratchett told an interviewer: "I have to tell you that I thought I'd be a lot worse than this by now, and so did my specialist." In the same interview he said that the cognitive part of his mind was "untouched" and his symptoms were physical (normal for PCA).[91] However, in July 2014 he cancelled his appearance at the biennial International Discworld Convention, citing his condition and "other age-related ailments".[92]

Pratchett died at his home from complications of Alzheimer's disease on the morning of 12 March 2015. He was 66 years old.[93][94] The Telegraph reported an unidentified source as saying that, despite his previous discussion of assisted suicide, his death had been natural.[95] After Pratchett's death, his assistant, Rob Wilkins, wrote from the official Terry Pratchett Twitter account:

AT LAST, SIR TERRY, WE MUST WALK TOGETHER.

Terry took Death's arm and followed him through the doors and on to the black desert under the endless night.


The End.[96]
Public figures who paid tribute included the British prime minister David Cameron, the comedian Ricky Gervais,[97] and the authors Ursula K. Le Guin, Terry Brooks, Margaret Atwood, George R. R. Martin, and Neil Gaiman.[98][99] Pratchett was memorialised in graffiti in East London.[100] The video game companies Frontier Developments[101] and Valve added elements to their games named after him.[102] Users of the social news site Reddit organised a tribute by which an HTTP header, "X-Clacks-Overhead: GNU Terry Pratchett", was added to websites' responses, a reference to the Discworld novel Going Postal, in which "the clacks" (a semaphore system, used as Discworld's equivalent to a telegraph) are programmed to repeat the name of its creator's deceased son; the sentiment in the novel is that no one is ever forgotten as long as their name is still spoken.[103] A June 2015 web server survey reported that approximately 84,000 websites had been configured with the header.[104] Pratchett's humanist funeral service was held on 25 March 2015.[105]

In 2015, Pratchett's estate announced an endowment in perpetuity to the University of South Australia.[106] The Sir Terry Pratchett Memorial Scholarship supports a Masters scholarship at the university's Hawke Research Institute.[107]

In 2023, several stories published in a regional newspaper in the 1970s and 1980s under the pen name Patrick Kearns were discovered to have been authored by Pratchett. They were published as A Stroke of the Pen: The Lost Stories in October 2023.[23]

Pratchett received a knighthood for "services to literature" in the 2009 UK New Year Honours list.[108][109][110] He was previously appointed Officer of the Order of the British Empire, also for "services to literature", in 1998. He formally received the accolade at Buckingham Palace on 18 February 2009.[111] Pratchett commented in the Ansible science fiction/fan newsletter, "I suspect the 'services to literature' consisted of refraining from trying to write any", but added, "Still, I cannot help feeling mightily chuffed about it."[112] On 31 December 2008, it was announced that Pratchett would be knighted (as a Knight Bachelor) in the Queen's 2009 New Year Honours.[108][113] Afterwards he said, "You can't ask a fantasy writer not to want a knighthood. You know, for two pins I'd get myself a horse and a sword."[114] In 2010, Pratchett created his own sword from deposits of iron he had found in a field near his home as he believed a knight should have a sword.[115]

Ten honorary doctorates were conferred on Pratchett: from the University of Warwick in 1999,[116] the University of Portsmouth in 2001,[117] the University of Bath in 2003,[118] the University of Bristol in 2004,[119] Buckinghamshire New University in 2008,[120] the University of Dublin in 2008,[121] Bradford University in 2009,[122] University of Winchester in 2009,[123][124] The Open University in 2013[125] for his contribution to Public Service and his last, from the University of South Australia, in May 2014.[126] 
Pratchett was made an adjunct Professor in the School of English at Trinity College Dublin in 2010, with a role in postgraduate education in creative writing and popular literature.[127][128]

Pratchett won the British Book Awards' "Fantasy and Science Fiction Author of the Year" category in 1994,[129] the British Science Fiction Award in 1989 for his novel Pyramids,[130] and a Locus Award for Best Fantasy Novel in 2008 for Making Money.[131] He won the 2001 Carnegie Medal from the British librarians, which recognised The Amazing Maurice and His Educated Rodents as the year's best children's book published in the UK.[132][133] Night Watch won the 2003 Prometheus Award for best libertarian novel.[134] Four of the five Discworld novels that centre on the trainee witch Tiffany Aching won the annual Locus Award for Best Young Adult Book in 2004, 2005, 2007 and 2016.[135] In 2005, Going Postal was shortlisted for the Hugo Award for Best Novel; however, Pratchett recused himself, stating that stress over the award would mar his enjoyment of Worldcon.[136][137] In the same year, A Hat Full of Sky won a Mythopoeic Award.[138] In 2008, Making Money was nominated for the Nebula Award for Best Novel.[139] I Shall Wear Midnight[140] won the 2010 Andre Norton Award, presented by the Science Fiction and Fantasy Writers of America (SFWA) as a part of the Nebula Award ceremony.

In 2016 the SFWA named Pratchett the recipient of Kate Wilhelm Solstice Award, given for "significant impact on the science fiction or fantasy landscape".[141] He received the NESFA Skylark Award in 2009[142] and the World Fantasy Award for Life Achievement in 2010.[143] In 2011 he won Margaret A. Edwards Award from the American Library Association, a lifetime honour for "significant and lasting contribution to young adult literature".[144][145] The librarians cited nine Discworld novels published from 1983 to 2004 and observed that "Pratchett's tales of Discworld have won over generations of teen readers with intelligence, heart, and undeniable wit. Comic adventures that fondly mock the fantasy genre, the Discworld novels expose the hypocrisies of contemporary society in an intricate, ever-expanding universe. With satisfyingly multilayered plots, Pratchett's humor honors the intelligence of the reader. Teens eagerly lose themselves in a universe with no maps."[144] In 2003 the BBC conducted The Big Read to identify the "Nation's Best-loved Novel" and finally published a ranked list of the "Top 200". Pratchett's highest-ranking novel was Mort, number 65, but he and Charles Dickens were the only authors with five in the Top 100 (four of his were from the Discworld series). He also led all authors with fifteen novels in the Top 200.[146]

An asteroid (127005 Pratchett) is named after Pratchett.[147] In 2013 Pratchett was named Humanist of the Year by the British Humanist Association for his campaign to fund research into Alzheimers, his contribution to the right to die public debate and his Humanist values.[148] Pratchett's Discworld novels have led to dedicated conventions, the first in Manchester in 1996,[149] then worldwide,[150] often with the author as guest of honour.[151] Publication of a new novel was sometimes accompanied by an international book-signing tour;[152] queues were known to stretch outside the bookshop as he continued to sign books well after the intended finishing time.[149] His fans were not restricted by age or gender, and he received a large amount of fan mail from them.[149] Pratchett enjoyed meeting fans and hearing what they think about his books, saying that since he was well paid for his novels, his fans were "everything" to him.[153]

In March 2017, Beaconsfield Town Council commissioned a commemorative plaque dedicated to Pratchett for Beaconsfield Library.[154][155]

In 2010, Pratchett was granted his own coat of arms following his knighthood.[115] The arms were designed by Hubert Chesshyre and granted by Letters Patent of Garter and Clarenceux Kings of Arms.[156][157] The owl is a morepork, which taken together with the ankh is a reference to the city of Ankh-Morpork. The image of a morpork holding an ankh appears in the fictional Ankh-Morpork City Arms. The motto "Noli Timere Messorem" is a corrected version of the dog Latin "Non Timetis Messor", the motto of Death's son-in-law and former apprentice, Mort of Sto Helit[158] and his heirs. The phrase is a reference to the song "(Don't Fear) The Reaper" by Blue Öyster Cult.[159]

Pratchett said that to write, one must read extensively, both inside and outside one’s chosen genre[160] and to the point of "overflow".[26] He advised that writing is hard work, and that writers must "make grammar, punctuation and spelling a part of your life".[26] However, Pratchett enjoyed writing, regarding its monetary rewards as "an unavoidable consequence" rather than the reason for writing.[161]

Although during his early career he wrote for the science fiction and horror genres, Pratchett later focused almost entirely on fantasy, and said: "It is easier to bend the universe around the story."[162] In the acceptance speech for his Carnegie Medal, he said: "Fantasy isn't just about wizards and silly wands. It's about seeing the world from new directions", pointing to the Harry Potter novels and The Lord of the Rings. In the same speech, he acknowledged benefits of these works for the genre.[163]

Pratchett believed he owed "a debt to the science fiction/fantasy genre which he grew up out of" and disliked the term "magical realism", which, he said, is "like a polite way of saying you write fantasy and is more acceptable to certain people".[164] He expressed annoyance that fantasy is "unregarded as a literary form", arguing that it "is the oldest form of fiction";[153] he said he was infuriated when novels containing science fiction or fantasy ideas were not regarded as part of those genres.[160] He debated this issue with novelist A. S. Byatt and critic Terry Eagleton, arguing that fantasy is fundamental to the way we understand the world and therefore an integral aspect of all fiction.[165]

Pratchett's earliest Discworld novels were written largely to parody classic sword-and-sorcery fiction (and occasionally science fiction);[166] as the series progressed, Pratchett dispensed with parody almost entirely, and the Discworld series evolved into straightforward (though still comedic) satire.[167]

Pratchett had a tendency to avoid using chapters, arguing in a Book Sense interview that "life does not happen in regular chapters, nor do movies, and Homer did not write in chapters", adding "I'm blessed if I know what function they serve in books for adults".[168] However, there were exceptions; Going Postal and Making Money and several of his books for younger readers are divided into chapters.[169] Pratchett said that he used chapters in the young adult novels because "[his] editor screams until [he] does", but otherwise felt that they were an unnecessary "stopping point" that got in the way of the narrative.[citation needed]

Characters, place names, and titles in Pratchett's books often contain puns, allusions and cultural references.[170][171] Some characters are parodies of well-known characters: for example, Pratchett's character Cohen the Barbarian, also called Ghengiz Cohen, is a parody of Conan the Barbarian and Genghis Khan, and his character Leonard of Quirm is a parody of Leonardo da Vinci.[172][173]

Another feature of his writing is the use of dialogue in small capitals, without quotation marks, for utterances by the character Death.

Pratchett was an only child, and his characters are often without siblings. Pratchett explained, "In fiction only children are the interesting ones."[174]

Discworld novels often included a modern innovation and its introduction to the world's medieval setting, such as a public police force (Guards! Guards!), guns (Men at Arms), cinema (Moving Pictures), investigative journalism (The Truth), the postage stamp (Going Postal), modern banking (Making Money), and the steam engine (Raising Steam). The "clacks", the tower-to-tower semaphore system that sprang up in later novels, is a mechanical optical telegraph (as created by the Chappe brothers and employed during the French Revolution) before wired electric telegraph chains, with all the change and turmoil that such an advancement implies. The resulting social upheaval driven by these changes serves as the setting for the main story.

Pratchett's earliest inspirations were The Wind in the Willows by Kenneth Grahame, and the works of H. G. Wells, Isaac Asimov and Arthur C. Clarke.[27][175][176] His literary influences were P.G. Wodehouse, Tom Sharpe, Jerome K. Jerome, Roy Lewis,[177] Alan Coren,[178] G. K. Chesterton, and Mark Twain.[179]

The Discworld series consists of 41 novels and a variety of supporting material.  Pratchett began writing the Discworld series in order to "have fun with some of the cliches".[12] The Discworld is a large disc resting on the backs of four giant elephants, all supported by the giant turtle Great A'Tuin as it swims its way through space. The books were published essentially in chronological order,[169] and advancements can be seen in the development of the Discworld civilisations, such as the creation of paper money in Ankh-Morpork.[168]

Pratchett wrote four Science of Discworld books in collaboration with professor of mathematics Ian Stewart and reproductive biologist Jack Cohen, both of the University of Warwick: The Science of Discworld (1999), The Science of Discworld II: The Globe (2002), The Science of Discworld III: Darwin's Watch (2005), and The Science of Discworld IV: Judgement Day (2013).

All four books have chapters that alternate between fiction and non-fiction: the fictional chapters are set within the Discworld universe, where characters observe, and experiment on, a universe with the same physics as ours. The non-fiction chapters (written by Stewart and Cohen) explain the science behind the fictional events.

In 1999, Pratchett appointed both Cohen and Stewart as "Honorary Wizards of the Unseen University" at the same ceremony at which the University of Warwick awarded him an honorary degree.[116]

Pratchett collaborated with the folklorist Dr Jacqueline Simpson on The Folklore of Discworld (2008), a study of the relationship between many of the persons, places and events described in the Discworld books and their counterparts in myths, legends, fairy tales and folk customs on Earth.

Pratchett's first two adult novels, The Dark Side of the Sun (1976) and Strata (1981), were both science fiction, the latter taking place partly on a disc-shaped world. Subsequent to these, Pratchett mostly concentrated on his Discworld series and novels for children, with two exceptions: Good Omens (1990), a collaboration with Neil Gaiman (which was nominated for both Locus and World Fantasy Awards in 1991[180]), a humorous story about the Apocalypse set on Earth, and Nation (2008), a book for young adults.

After writing Good Omens Pratchett brainstormed with Larry Niven on a story that became the short novel "Rainbow Mars". Niven eventually completed the story on his own, but he states in the afterword that a number of Pratchett's ideas remained in the finished version.
[181]
Pratchett also collaborated with the British science fiction author Stephen Baxter on a parallel Earth series.[182][183] The first novel, entitled The Long Earth was published on 21 June 2012. A second novel, The Long War, was released on 18 June 2013.[184] The Long Mars was published in 2014. The fourth book in the series, The Long Utopia, was published in June 2015, and the fifth, The Long Cosmos, in June 2016.

In 2012, the first volume of Pratchett's collected short fiction was published under the title A Blink of the Screen. In 2014 a similar collection was published of Pratchett's non-fiction, entitled A Slip of the Keyboard.[185]

Pratchett wrote dialogue for a mod for the game The Elder Scrolls IV: Oblivion (2006), which added a Nord companion named Vilja. He also worked on a similar mod for The Elder Scrolls V: Skyrim (2011), which featured Vilja's great-great-granddaughter.[186][187]

Pratchett's first children's novel was also his first published novel: The Carpet People in 1971, which Pratchett substantially rewrote and re-released in 1992. The next, Truckers (1988), was the first in The Nome Trilogy of novels for young readers (also known as The Bromeliad Trilogy), about small gnome-like creatures called "Nomes", and the trilogy continued in Diggers (1990) and Wings (1990). Subsequently, Pratchett wrote the Johnny Maxwell trilogy, about the adventures of a boy called Johnny Maxwell and his friends, comprising Only You Can Save Mankind (1992), Johnny and the Dead (1993) and Johnny and the Bomb (1996).
Nation (2008) marked his return to the non-Discworld children's novel, and this was followed in 2012 by Dodger, a children's novel set in Victorian London.[188] On 21 November 2013 Doubleday Children's released Pratchett's Jack Dodger's Guide to London.[189]

In 2001, he wrote The Amazing Maurice and His Educated Rodents, his first Discworld book marketed for children.[132]

Pratchett also wrote a five-book children's series featuring a trainee witch, Tiffany Aching, and taking place on Discworld, beginning with The Wee Free Men in 2003.

In September 2014, a collection of children's stories, Dragons at Crumbling Castle, written by Pratchett, and illustrated by Mark Beech, was published.[190] This was followed by another collection, The Witch's Vacuum Cleaner, also illustrated by Mark Beech, in 2016. A third volume, Father Christmas's Fake Beard, was released in 2017. A fourth collection, The Time-travelling Caveman, was released in September 2020.[190] A final collection, A Stroke of the Pen: The Lost Stories, was published in October 2023, collecting 20 stories written by Pratchett for newspapers in the 1970s and 80s under pseudonyms such as "Patrick Kearns" which had not previously been attributed to Pratchett.[191]

Pratchett's daughter, the writer Rhianna Pratchett, is the custodian of the Discworld franchise. She said that she had no plans to publish her father's unfinished work or continue the Discworld series.[195] Pratchett told Neil Gaiman that anything that he had been working on at the time of his death should be destroyed by a steamroller. On 25 August 2017 his former assistant Rob Wilkins fulfilled this wish by arranging for Pratchett's hard drive to be crushed under a steamroller at the Great Dorset Steam Fair.[196]

According to Wilkins, Pratchett left "an awful lot" of unfinished writing, "10 titles I know of and fragments from many other bits and pieces".[197] Pratchett had mentioned two new texts, Scouting for Trolls[198] and a Discworld novel following a new character.[199] The notes left behind outline ideas about "how the old folk of the Twilight Canyons solve the mystery of a missing treasure and defeat the rise of a Dark Lord despite their failing memories"; "the secret of the crystal cave and the carnivorous plants in the Dark Incontinent", about Constable Feeney of the Watch, first introduced in Snuff, involving how he "solves a whodunnit among the congenitally decent and honest goblins"; and a second book about Amazing Maurice from The Amazing Maurice and His Educated Rodents.[200]

A collection of essays about Pratchett's writings is compiled in the book Terry Pratchett: Guilty of Literature, edited by Andrew M. Butler, Edward James and Farah Mendlesohn, published by Science Fiction Foundation in 2000. A second, expanded edition was published by Old Earth Books in 2004. Andrew M. Butler wrote the Pocket Essentials Guide to Terry Pratchett published in 2001. Writers Uncovered: Terry Pratchett is a biography for young readers by Vic Parker, published by Heinemann Library in 2006.

A BBC docudrama based on Pratchett's life, Terry Pratchett: Back In Black, was broadcast in February 2017, starring Paul Kaye as Pratchett. Neil Gaiman was involved with the project which used Pratchett's own words. Pratchett's assistant, Rob Wilkins, said that Pratchett was working on this documentary before he died. According to the BBC, finishing it would "show the author was still having the last laugh".[204]

The English author, critic and performer Marc Burrows wrote an unofficial biography, The Magic of Terry Pratchett, published by Pen & Sword on 6 July 2020.[205] Though it was not endorsed by the Pratchett estate, prior to its publication they did wish Burrows "all the best" regarding the book through the official Pratchett Twitter account.[206] It received generally favourable reviews and won the 2021 Locus Award for Non-Fiction.[207]

In 2022, Wilkins wrote the official biography, Terry Pratchett: A Life with Footnotes.[208] The biography was well received.[c] In The Daily Telegraph, Tristram Fane Saunders wrote that it "spins magic from mundanity in precisely the way Pratchett himself did".[209] However, in a review for the Irish Independent, Kevin Power called it more a collection of fan notes than a serious biography.[212]

In April 2023, "Entering Discworld Population", an episode of the podcast Imaginary Worlds, was released to mark the 75th anniversary of Pratchett's birth.[213] It discussed four of Pratchett's recurring fiction characters as representative of his underlying philosophy.The Daily Show is an American late-night talk and news satire television program. It airs each Monday through Thursday on Comedy Central in the United States, with extended episodes released shortly after on Paramount+. The Daily Show draws its comedy and satire from recent news stories, political figures, and media organizations. It often uses self-referential humor.[1]

The half-hour-long show premiered on July 22, 1996, and was first hosted by Craig Kilborn until December 17, 1998. Jon Stewart then took over as the host from January 11, 1999, until August 6, 2015, making the show more strongly focused on political satire and news satire, in contrast with the pop culture focus during Kilborn's tenure. Stewart was succeeded by Trevor Noah, whose tenure began on September 28, 2015, and ended in December 2022.[2] Under the different hosts, the show has been formally known as The Daily Show with Craig Kilborn from 1996 to 1998, The Daily Show with Jon Stewart from 1999 until 2015, and The Daily Show with Trevor Noah from 2015 to 2022. The Daily Show is the longest-running program on Comedy Central (counting all three tenures), and has won 26 Primetime Emmy Awards.[3][4][5]

The program has been popular among young audiences. The Pew Research Center suggested in 2010 that 74% of regular viewers were between 18 and 49, and that 10% of the audience watched the show for its news headlines, 2% for in-depth reporting, and 43% for entertainment; compared with respectively 64%, 10% and 4%, who said the same of CNN.[6] In 2015, The Daily Show's median age of viewership was 36 years old.[7] Between 2014 and 2023, the show's ratings declined by 75%, and its average viewer age increased to 63. In 2023, the viewership for age range of 25–54 year olds was 158,000 and the age range for 18–34 year olds was 30,000.[8]

Critics chastised Stewart[when?] for not conducting sufficiently hard-hitting interviews with his political guests, some of whom he may have lampooned in previous segments. Stewart and other Daily Show writers responded to such criticism by saying that they do not have any journalistic responsibility and that as comedians, their only duty is to provide entertainment. Stewart's appearance on the CNN show Crossfire picked up this debate, where he chastised the CNN production and hosts for not conducting informative and current interviews on a news network.[9]

As a new permanent host had not been chosen after Noah's tenure ended in 2022, the show featured a rotating cast of guest hosts, with Jon Stewart returning to host Monday night shows starting February 12, 2024, and through the fall elections, with the correspondents rotating hosting duties for other shows.[10][11] Stewart later extended his contract into 2025. 

During Trevor Noah's tenure as host, each episode began with announcer Drew Birns announcing the date and the introduction, "From Comedy Central's World News Headquarters in New York, this is The Daily Show with Trevor Noah".[12][13] Previously, the introduction was "This is The Daily Show, the most important television program, ever."[citation needed] The host then opens the show with a monologue drawing from current news stories and issues. Previously, the show had divided its news commentary into sections known as "Headlines", "Other News", and "This Just In"; these titles were dropped from regular use on October 28, 2002, and were last used on March 6, 2003. Some episodes will begin with a 1–3 minute intro on a small story (or small set of stories) before fully transitioning into the main story of the night. Currently, the segment is simply called "Headlines."

The monologue segment is often followed by a segment featuring an exchange with a correspondent, either at the anchor desk with the host or reporting from a false location in front of a greenscreen showing stock footage. They typically present absurd or humorously exaggerated takes on current events against the host's straight man. Some correspondent segments involve the show's members travelling to different locations to file comedic reports on current news stories and conduct interviews with people related to the featured issue.

Correspondents are typically introduced as the show's "senior" specialist in the story's subject, and can range from relatively general (such as Senior Political Analyst) to absurdly specific (such as Senior Religious Registry Correspondent). The cast of correspondents is quite diverse, and many often sarcastically portray extreme stereotypes of themselves to poke fun at a news story, such as "Senior Latino Correspondent", "Senior Youth Correspondent" or "Senior Black Correspondent".

While correspondents stated to be reporting abroad are usually performing in-studio in front of a greenscreen background, on rare occasions, cast members have recorded pieces on location. For instance, during the week of August 20, 2007, the show aired a series of segments called "Operation Silent Thunder: The Daily Show in Iraq" in which correspondent Rob Riggle reported from Iraq.[14] In August 2008, Riggle traveled to China for a series of segments titled "Rob Riggle: Chasing the Dragon", which focused on the 2008 Beijing Olympics.[15]

Jason Jones traveled to Iran in early June 2009 to report on the Iranian elections, and John Oliver traveled to South Africa for the series of segments "Into Africa" to report on the 2010 FIFA World Cup. In March 2012, Oliver traveled to Gabon, on the west African coast, to report on the Gabonese government's decision to donate $2 million to UNESCO after the United States cut its funding for UNESCO earlier that year. On July 19, 2016, Roy Wood Jr. reported live from the Republican National Convention and talked about Donald Trump's African-American support.[16][17]

Topics have varied widely; during the early years of the show, they tended toward character-driven human interest stories such as Bigfoot enthusiasts. Since Stewart began hosting in 1999, the focus of the show has become more political and the field pieces have come to more closely reflect current issues and debates.[18] Under Kilborn and the early years of Stewart, most interviewees were either unaware or not entirely aware of the comedic nature of The Daily Show. However, as the show began to gain popularity — particularly following its coverage of the 2000 and 2004 presidential elections — most of the subjects now interviewed are aware of the comedic element.[19]

Some segments have recurred periodically throughout different tenures, such as "Back in Black" (segments hosted by comedian Lewis Black) & "Your Moment of Zen". Since the 2003 invasion of Iraq, a common segment of the show has been dubbed "Mess O' Potamia", focusing on the United States' policies in the Middle East, especially Iraq.[20] Elections in the United States were a prominent focus in the show's "Indecision" coverage throughout Stewart & Noah's time as host (the title "InDecision" is a parody of NBC News' "Decision" segment). Since 2000, under Stewart's tenure, the show went on the road to record week-long specials from the cities hosting the Democratic and Republican national conventions.[21] For the 2006 U.S. midterm elections, a week of episodes was recorded in the contested state of Ohio.[22] The "Indecision" & "Democalypse" coverage of the 2000, 2002, 2004, 2006, 2008, 2010, 2012, 2014, 2016 elections all culminated in live Election Night specials.[23]

With Noah as host, one new recurring segment has been "What the Actual Fact", with correspondent Desi Lydic examining statements made by political figures during speeches or events. Under Noah, the continuation of "Democalypse" and "Indecision" also took place with live shows after the Republican National Convention and Democratic National Convention.[24] For the first time, under Noah, the show also went live after all three U.S. presidential debates in 2016.[25]

In the show's third act, the host conducts an interview with a celebrity guest. Guests come from a wide range of cultural sources, and include actors, musicians, authors, athletes, pundits, policy experts and political figures.[26] During Stewart's tenure, the show's guests tended away from celebrities and more towards non-fiction authors and political pundits, as well as many prominent elected officials.[20] In the show's earlier years it struggled to book high-profile politicians. (In 1999, for an Indecision 2000 segment, Steve Carell struggled to talk his way off Republican candidate John McCain's press overflow bus and onto the Straight Talk Express).[citation needed]
However its rise in popularity, particularly following the show's coverage of the 2000 and 2004 elections, made Stewart according to a Rolling Stone (2006) article, "the hot destination for anyone who wants to sell books or seem hip, from presidential candidates to military dictators". Newsweek labeled it "the coolest pit stop on television".[27][28]

Prominent political guests have included U.S. President Joe Biden,[29] former Presidents Jimmy Carter, Bill Clinton and Barack Obama,[30] former British Prime Ministers Tony Blair and Gordon Brown, former Pakistani President Pervez Musharraf, former Liberian President Ellen Johnson Sirleaf, former Bolivian President Evo Morales, Jordanian King Abdullah II, former Estonian Prime Minister Taavi Roivas, Canadian Prime Minister Justin Trudeau and former Mexican President Vicente Fox.[31]

The show has played host to former and current members of the administration and Cabinet as well as members of Congress. Numerous presidential candidates have appeared on the show during their campaigns, including John McCain, John Kerry, Barack Obama and Hillary Clinton.[32]

In a closing segment, there is a brief segue to the closing credits in the form of the host introducing "Your Moment of Zen", a humorous piece of video footage without commentary that has been part of the show's wrap-up since the series began in 1996.[33] The segment often relates to a story covered earlier in the episode, but occasionally is merely a humorous or ridiculous clip. Occasionally, the segment is used as a tribute to someone who has died.[34]

Sometimes, before the "Your Moment of Zen", this segment is used for quick promotions. The host might promote the show that follows right after their broadcast, such as promoting the show @midnight. This time has also been used to promote films, books or stand-up specials that are affiliated with the host.[35][36][37]

In October 2005, following The Colbert Report's premiere, a new feature (sometimes referred to as the toss) was added to the closing segment in which Stewart would have a short exchange with "our good friend, Stephen Colbert at The Colbert Report", which aired immediately after. The two would have a scripted comedic exchange via split-screen from their respective sets. In 2007, the "toss" was cut back to twice per week, and by 2009 was once a week before gradually being phased out. It was used on the 2014 mid-term election night and again just before the final episode of The Colbert Report on December 18, 2014, and returned upon the premiere of The Nightly Show with Larry Wilmore. Stewart then regularly tossed to Wilmore at the end of his Monday night episodes. Under Noah, the "toss" has been used for The Opposition with Jordan Klepper and Lights Out with David Spade.

The host sits at his desk on the elevated island stage in the style of a traditional news show. The show initially used New York PBS station WNET's facilities until late 1998, when it moved a few blocks to NEP Studio 54. The Colbert Report would claim NEP Studio 54 in 2005.[38][39] On July 11, 2005, the show premiered in its new studio, NEP Studio 52, at 733 11th Avenue, a few blocks west of its former location.[40]
The set of the new studio was given a sleeker, more formal look, including a backdrop of three large projection screens. The traditional guests' couch, which had been a part of the set since the show's premiere, was done away with in favor of simple upright chairs. The change was initially not well-received, spawning a backlash among some fans and prompting a "Bring Back the Couch" campaign. The campaign was mentioned on subsequent shows by Stewart and supported by Daily Show contributor Bob Wiltfong.[41][42] The couch was eventually featured in a sweepstakes in which the winner received the couch, round-trip tickets to New York, tickets to the show, and a small sum of money.[43]

On April 9, 2007, the show debuted a new set. The projection screens were revamped (with one large screen behind Stewart, while the smaller one behind the interview subject remained the same), a large, global map directly behind Stewart, a more open studio floor, and a J-shaped desk supported at one end by a globe. The intro was also updated; the graphics, display names, dates, and logos were all changed.[44]

On September 28, 2015, the show debuted a new set alongside the debut of Trevor Noah's tenure. According to Larry Hartman, Noah took a lot of inspiration from Stewart's set.[45] A second on-stage 'jumbo-tron' was added and the colours of the set were made lighter. The graphics, intro, theme music, lower thirds, logo, etc. were also all revamped.[46] On July 19, 2016, the set and graphics were given another change to reflect Democalypse 2016 and denote The Daily Show's RNC and DNC coverage (which was taped in the conventions' respective cities).[47] The new temporary sets had a Washington theme, and was meant to show that Washington is "a little broke" and needs "repair".[48] Though the studio was reverted to its former self after the election week in 2016, the changes to the graphics were kept.

After a stretch of episodes filmed from Trevor Noah's apartment due to the COVID-19 pandemic, the show returned to a smaller studio at One Astor Plaza, the corporate headquarters of ViacomCBS in Times Square. The new studio had no audience, and a smaller, more intimate atmosphere with muted colors. In April 2022, The Daily Show returned to NEP Studio 52 with a revamped set, combining elements of the Times Square studio with a revamped version of its previous layout.[49]

The show's writers begin each day with a morning meeting where they review material that researchers have gathered from major newspapers, the Associated Press, cable news television channels and websites, and discuss headline material for the lead news segment. Throughout the morning they work on writing deadline pieces inspired by recent news, as well as longer-term projects. By lunchtime, Noah — who describes his role as that of the captain of a team[50] — has begun to review headline jokes. The script is submitted by 3 pm, and at 4:15 there is a rehearsal. An hour is left for rewrites before a 6 pm taping in front of a live studio audience.[12][28]

The Daily Show typically tapes four new episodes a week, Monday through Thursday, forty-two weeks a year.[51] The show is broadcast at 11 PM Eastern/10 PM Central, a time when local television stations show their news reports and about half an hour before most other late-night comedy programs begin to go on the air. The program used to be rerun several times the next day, including a 7:30 PM Eastern/6:30 PM Central prime time broadcast.[52]

From 2007 to 2024, full archive clips from the show under Jon Stewart's tenure were available on the Comedy Central website.[53][54] In June 2024, the Comedy Central website was shut down in favor of the Paramount+ streaming service, where full episodes going back to December 2023 are available.[54][55] Clips dating from the beginning of Trevor Noah's tenure to the present are available on the show's YouTube channel.[56]

The Daily Show was created by Lizz Winstead and Madeleine Smithberg[57] and premiered on Comedy Central on July 22, 1996, having been marketed as a replacement for Politically Incorrect (a successful Comedy Central program that had moved to ABC earlier that year).[58] Madeleine Smithberg was co-creator of The Daily Show as well as the former executive producer. A graduate of Binghamton University, she was an executive producer of Steve Harvey's Big Time and a talent coordinator for Late Night with David Letterman.[57]

Aiming to parody conventional newscasts, it featured a comedic monologue of the day's headlines from anchor Craig Kilborn (a well-known co-anchor of ESPN's SportsCenter), as well as mockumentary style on-location reports, in-studio segments and debates from regular correspondents Winstead, Brian Unger, Beth Littleford, and A. Whitney Brown.[59]

Common segments included "This Day in Hasselhoff History" and "Last Weekend's Top-Grossing Films, Converted into Lira", in parody of entertainment news shows and their tendency to lead out to commercials with trivia such as celebrity birthdays.[60] Another commercial lead-out featured Winstead's parents, on her answering machine, reading that day's "Final Jeopardy!" question and answer.[61] In each show, Kilborn would conduct celebrity interviews, ending with a segment called "Five Questions" in which the guest was made to answer a series of questions that were typically a combination of obscure fact and subjective opinion.[62] These are highlighted in a 1998 book titled The Daily Show: Five Questions, which contains transcripts of Kilborn's best interviews.[63] Each episode concluded with a segment called "Your Moment of Zen" that showed random video clips of humorous and sometimes morbid interest such as visitors at a Chinese zoo feeding baby chickens to the alligators.[64] Originally the show was recorded without a studio audience, featuring only the laughter of its own off-camera staff members. A studio audience was incorporated into the show for its second season, and has remained since.[65]

The show was much less politically focused than it later became under Jon Stewart, having what Stephen Colbert described as a local news feel and involving more character-driven humor as opposed to news-driven humor.[18] Winstead recalls that when the show was first launched there was constant debate regarding what the show's focus should be. While she wanted a more news-driven focus, the network was concerned that this would not appeal to viewers and pushed for "a little more of a hybrid of entertainment and politics".[66] The show was slammed by some reviewers as being too mean-spirited, particularly towards the interview subjects of field pieces; a criticism acknowledged by some of the show's cast. Describing his time as a correspondent under Kilborn, Colbert says, "You wanted to take your soul off, put it on a wire hanger, and leave it in the closet before you got on the plane to do one of these pieces."[67] One reviewer from The New York Times criticized the show for being too cruel and for lacking a central editorial vision or ideology, describing it as "bereft of an ideological or artistic center... precocious but empty."[68]

There were reports of backstage friction between Kilborn and head writer Lizz Winstead. Winstead had not been involved in the hiring of Kilborn, and disagreed with him over what direction the show should take. "I spent eight months developing and staffing a show and seeking a tone with producers and writers. Somebody else put him in place. There were bound to be problems. I viewed the show as content-driven; he viewed it as host-driven", she said.[69] In a 1997 Esquire magazine interview, Kilborn made a sexually explicit joke about Winstead. Comedy Central responded by suspending Kilborn without pay for one week, and Winstead quit soon after.[70]

In 1998, Kilborn left The Daily Show to replace Tom Snyder on CBS's The Late Late Show. He claimed the "Five Questions" interview segment as intellectual property, disallowing any future Daily Show hosts from using it in their interviews.[71] Correspondents Brian Unger and A. Whitney Brown left the show shortly before him, but the majority of the show's crew and writing staff stayed on.[72] Kilborn's last show as host aired on December 17, 1998, ending a 386-episode tenure. Reruns were shown until Jon Stewart's debut four weeks later.[73] Kilborn made a short appearance on Jon Stewart's final edition of the Daily Show saying "I knew you were going to run this thing into the ground."[74]

Comedian Jon Stewart took over as host of the show, which was retitled The Daily Show with Jon Stewart, on January 11, 1999.[75][76] Stewart had previously hosted Short Attention Span Theater on Comedy Central,[77] two shows on MTV (You Wrote It, You Watch It and The Jon Stewart Show), as well as a syndicated late-night talk show, and had been cast in films and television.[78] In taking over hosting from Kilborn, Stewart initially retained much of the same staff and on-air talent, allowing many pieces to transition without much trouble, while other features like "God Stuff", with John Bloom presenting an assortment of actual clips from various televangelists, and "Backfire", an in-studio debate between Brian Unger and A. Whitney Brown, evolved into the similar pieces of "This Week in God" and Stephen Colbert and Steve Carell's "Even Stevphen". After the change, a number of new features were developed. The ending segment "Your Moment of Zen", previously consisting of a random selection of humorous videos, was diversified to sometimes include recaps or extended versions of news clips shown earlier in the show.[33] The show's theme music, "Dog on Fire" by Bob Mould, was re-recorded by They Might Be Giants after Stewart joined the show.[79][80]

Stewart served not only as host but also as a writer and executive producer of the series. He recalls that he initially struggled with the Kilborn holdover writers to gain control of the show and put his own imprint on the show's voice, a struggle that led to the departure of a number of the holdover writers.[81] Instrumental in shaping the voice of the show under Stewart was former editor of The Onion Ben Karlin who, along with fellow Onion contributor David Javerbaum, joined the staff in 1999 as head writer and was later promoted to executive producer. Their experience in writing for the satirical newspaper, which uses fake stories to mock real print journalism and current events, would influence the comedic direction of the show; Stewart recalls the hiring of Karlin as the point at which things "[started] to take shape". Describing his approach to the show, Karlin said, "The main thing, for me, is seeing hypocrisy. People who know better saying things that you know they don't believe."[19]

Under Stewart and Karlin The Daily Show developed a markedly different style, bringing a sharper political focus to the humor than the show previously exhibited. Then-correspondent Stephen Colbert recalls that Stewart specifically asked him to have a political viewpoint, and to allow his passion for issues to carry through into his comedy.[82] Colbert says that whereas under Kilborn the focus was on "human interest-y" pieces, with Stewart as host the show's content became more "issues and news driven", particularly after the beginning of the 2000 election campaign with which the show dealt in its "Indecision 2000" coverage.[18][83] Stewart himself describes the show's coverage of the 2000 election recount as the point at which the show found its editorial voice. "That's when I think we tapped into the emotional angle of the news for us and found our editorial footing," he says.[84] Following the September 11th attacks, The Daily Show went off the air for nine days. Upon its return, Stewart opened the show with a somber monologue, that, according to Jeremy Gillick and Nonna Gorilovskaya, addressed both the absurdity and importance of his role as a comedian. Commented Stewart:

They said to get back to work, and there were no jobs available for a man in the fetal position. ...We sit in the back and we throw spitballs – never forgetting the fact that it is a luxury in this country that allows us to do that. ...The view from my apartment was the World Trade Center. Now it's gone. They attacked it. This symbol of American ingenuity and strength and labor and imagination and commerce and it is gone. But you know what the view is now? The Statue of Liberty. The view from the south of Manhattan is now the Statue of Liberty. You can't beat that.[85]
Gillick and Gorilovskaya point to the September 11 attacks and the beginning of the wars in Afghanistan and Iraq as the point at which Jon Stewart emerged as a trusted national figure. Robert Thompson, the director of the Bleier Center for Television and Popular Culture at Syracuse University, recalled of this period, "When all the news guys were walking on eggshells, Jon was hammering those questions about WMDs."[85]

During Stewart's tenure, the role of the correspondent broadened to encompass not only field segments but also frequent in-studio exchanges. Under Kilborn, Colbert says that his work as a correspondent initially involved "character driven [field] pieces—like, you know, guys who believe in Bigfoot." However, as the focus of the show has become more news-driven, correspondents have increasingly been used in studio pieces, either as experts discussing issues at the anchor desk or as field journalists reporting from false locations in front of a green screen. Colbert says that this change has allowed correspondents to be more involved with the show, as it has permitted them to work more closely with the host and writers.[18]

The show's 2000 and 2004 election coverage, combined with a new satirical edge, helped to catapult Stewart and The Daily Show to new levels of popularity and critical respect.[86] Since Stewart became host, the show has won 23 Primetime Emmy Awards and three Peabody Awards, and its ratings steadily increased. In 2003, the show was averaging nearly a million viewers, an increase of nearly threefold since the show's inception as Comedy Central became available in more households.[87] By September 2008, the show averaged nearly two million viewers per night.[88] Senator Barack Obama's interview on October 29, 2008, pulled in 3.6 million viewers.[89]

The move towards greater involvement in political issues and the increasing popularity of the show in certain key demographics have led to examinations of where the views of the show fit in the political spectrum. Adam Clymer, among many others, argued in 2004 that The Daily Show was more critical of Republicans than Democrats under Stewart.[90] Stewart, who voted Democratic in the 2004 presidential election,[91] acknowledged that the show had a more liberal point of view, but that it was not "a liberal organization" with a political agenda and its duty first and foremost was to be funny. He acknowledged that the show is not necessarily an "equal opportunity offender", explaining that Republicans tended to provide more comedic fodder because "I think we consider those with power and influence targets and those without it, not."[92] In an interview in 2005, when asked how he responded to critics claiming that The Daily Show is overly liberal, Stephen Colbert, also a self-proclaimed Democrat,[93] said in an interview during the Bush administration, when the Republicans held a majority in the House and Senate: "We are liberal, but Jon's very respectful of the Republican guests, and, listen, if liberals were in power it would be easier to attack them, but Republicans have the executive, legislative and judicial branches, so making fun of Democrats is like kicking a child, so it's just not worth it."[94]

Stewart was critical of Democratic politicians for being weak, timid, or ineffective. He said in an interview with Larry King, prior to the 2006 elections, "I honestly don't feel that [the Democrats] make an impact. They have forty-nine percent of the vote and three percent of the power. At a certain point you go, 'Guys, pick up your game.'"[95] He has targeted them for failing to effectively stand on some issues, such as the war in Iraq, describing them as "incompetent" and "unable... to locate their asses, even when presented with two hands and a special ass map."[96]

Karlin, then the show's executive producer, said in a 2004 interview that while there is a collective sensibility among the staff which, "when filtered through Jon and the correspondents, feels uniform," the principal goal of the show is comedy. "If you have a legitimately funny joke in support of the notion that gay people are an affront to God, we'll put that motherfucker on!"[97]

On September 15, 2003, Senator John Edwards became the first candidate to announce that they were running for president on the show, causing Jon Stewart to jokingly inform him that their show was "fake" and he might have to re-announce elsewhere.[98] On November 17, 2009, Vice President Joe Biden appeared on the show, making him the first sitting vice president to do so.[99] On October 27, 2010, President Barack Obama became the first sitting U.S. president to be interviewed on the show, wherein Obama commented he "loved" the show.[100] Obama took issue with Stewart's suggestion that his health care program was "timid."[101]

After the United States Senate failed to pass and the media failed to cover the James Zadroga 9/11 Health and Compensation Act, which would provide health monitoring and financial aid to sick first responders of the September 11 attacks, Stewart dedicated the entire December 16, 2010, broadcast to the issue. During the next week, a revived version of the bill gained new life, with the potential of being passed before the winter recess.[102][103] Stewart was praised by both politicians and affected first responders for the bill's passage. According to Syracuse University professor of television, radio and film Robert J. Thompson, "Without him, it's unlikely it would've passed. I don't think Brian Williams, Katie Couric or Diane Sawyer would've been allowed to do this."[104]

Due to the 2007–08 Writers Guild of America strike, the show went on hiatus on November 5, 2007. Although the strike continued until February 2008, the show returned to air on January 7, 2008, without its staff of writers. In solidarity with the writers, the show was referred to as A Daily Show with Jon Stewart rather than The Daily Show with Jon Stewart, until the end of the strike.[105] As a member of the Writers Guild of America, Stewart was barred from writing any material for the show himself which he or his writers would ordinarily write.[106] As a result, Stewart and the correspondents largely ad-libbed the show around planned topics.[107]

In an effort to fill time while keeping to the strike-related restrictions, the show aired or re-aired some previously recorded segments, and Stewart engaged in a briefly recurring mock feud with fellow late-night hosts Stephen Colbert and Conan O'Brien.[108] The strike officially ended on February 12, 2008, with the show's writers returning to work the following day, at which point the title of The Daily Show was restored.[109]

Starting in June 2013, Jon Stewart took a twelve-week break to direct Rosewater, a drama about a journalist jailed by Iran for four months. Correspondent John Oliver replaced Stewart at the anchor desk for two months, to be followed by one month of reruns.[110] Oliver received positive reviews for his hosting,[111][112] leading to his departure from the show in December 2013[113] for his own show Last Week Tonight with John Oliver, which debuted April 27, 2014, on HBO.[114]

On February 10, 2015, Stewart announced that he would be leaving the show later in the year. Comedy Central indicated in a statement that The Daily Show would continue without Stewart, saying it would "endure for years to come".[115]

Stewart's final episode aired on August 6 as an hour-long special in three segments. The first featured a reunion of a majority of the correspondents and contributors from throughout the show's history as well as a pre-recorded "anti-tribute" (mocking Stewart) from various frequent guests and "friends" of the show. This included Bill O'Reilly, Hillary Clinton, John McCain, Lindsey Graham, Chris Christie, John Kerry, and Chuck Schumer.[116] The second segment featured a pre-recorded tour of the Daily Show production facility and studio introducing all of the show's staff and crew. The final segment featured a short farewell speech from Stewart followed by the final "Moment of Zen" (being 'his own' moment of zen): a performance of "Land of Hope and Dreams" and "Born to Run" by Bruce Springsteen and the E Street Band.[117]

On March 30, 2015, it was announced that Trevor Noah would replace Stewart as host of The Daily Show.[119] Shortly after his announcement, it was revealed that Amy Schumer, Louis C.K., Amy Poehler, and Chris Rock were all considered for the job.[120][121] His first show was on September 28, 2015,[122] with comedian Kevin Hart as his first guest. Noah's premiere episode was simulcast by Viacom on Comedy Central, the Nick at Nite block on Nickelodeon, Spike, MTV, MTV2, mtvU, VH1, VH1 Classic, BET, Centric, CMT, TV Land, Logo TV, and the NickMom block on the Nick Jr. Channel.[123][124]

On September 14, 2017, it was announced that Comedy Central had extended Trevor Noah's contract as host of The Daily Show for five years, through 2022.[125]

Ratings declined by about 37 percent at the start of Noah's tenure. They gradually increased from there, only to fall to the lowest ratings in 15 years in 2020.[126] Some of the musicians that have been on the shows as guests performed their music as well.[127] Beginning in 2020 until the end of Noah's tenure, the show expanded to a 45-minute time slot.[128]

On September 29, 2022, during a taping of the show, Noah announced that he would step down as the host of The Daily Show so he could focus on his standup career and touring.[2] On October 2, 2022, it was confirmed that the show would continue on Comedy Central following Noah's departure.[129] On October 12, 2022, it was announced that Noah's final episode would air on December 8.[130] On October 18, 2022, it was announced that Comedy Central may replace Noah with more than one comedian.[131]

In addition to changes in the tone of the show, Noah also implemented stylistic changes to the show, with an updated set,[132] new graphics[133] and his monologue sometimes taking place while standing in front of a screen as opposed to sitting at the desk. Noah also increased the usage of more millennial-based references, impersonations and characterizations for his comedy on the show, due to his younger demographic and his ability to speak in multiple accents and eight languages.[134]

The debut of The Daily Show with Trevor Noah brought along three new correspondents: Roy Wood Jr., Desi Lydic and Ronny Chieng.[135]

Additional correspondents were added in 2017. Michael Kosta became the Senior Constitutional Correspondent and Senior American Correspondent on July 11, 2017.[136] Dulcé Sloan became the Senior Fashion Correspondent on September 7, 2017.[137]

In January 2016, The Daily Show with Trevor Noah started to use a modified version of the show's previous theme, remixed by Timbaland and King Logan.[138]

Noah also avoided talking enough about Fox News, as Stewart was previously known for. "The Daily Show was based on an emerging 24 hour news cycle, that’s everything it was, that’s what inspired The Daily Show. Now you look at news and it’s changed. It’s no longer predicated around 24 hour news. There are so many different choices. Half of it is online now. Now you’ve got the Gawkers, the BuzzFeeds. The way people are drawing their news is soundbites and headlines and click-bait links has changed everything. The biggest challenge is going to be an exciting one I'm sure is how are we going to bring all of that together looking at it from a bigger lens as opposed to just going after one source—which was historically Fox News," Noah said at a press conference before the show's debut.[139]

On December 8, 2015, former host Jon Stewart returned to The Daily Show for the first time in an extended-length show to return attention to extending the James Zadroga 9/11 Health and Compensation Act, otherwise referred to as 9/11 First Responders Bill, which Stewart explained had been blocked by Paul Ryan and Mitch McConnell for political reasons.[140][141] On October 20, 2016, Noah was unable to host a scheduled taping of The Daily Show due to illness,[142] so correspondent Jordan Klepper guest hosted.

On November 16, 2017, Stewart once again returned to The Daily Show, in part as a parody of the robocalls of fake Washington Post reporter "Bernie Bernstein" and to promote Night of Too Many Stars on HBO.

In March 2020 due to the COVID-19 pandemic, the show suspended production. On March 18, 2020, Comedy Central began to release webisodes of The Daily Show produced remotely from Noah's home, entitled The Daily Social Distancing Show. This format moved to television beginning March 23.[143][144] Following the cancellation of Lights Out with David Spade, The Daily Show expanded into a 45-minute format beginning April 27, 2020.[145] In July 2020, Comedy Central head Chris McCarthy told Vulture that there were plans to possibly extend the show to an hour-long format by the end of the year.[146]

In May 2020, The Daily Show won the 2020 Webby Award for Humor in the category Social.[147]

The at-home format continued until June 2021, when the show went on an extended hiatus for the summer. The Daily Show returned on September 13, 2021, with the show re-located to studios at ViacomCBS's headquarters at One Astor Plaza in Times Square (its existing studio was being occupied by fellow Comedy Central program Tha God's Honest Truth).[148] Comedy Central stated that the show planned to preserve the "intimacy and creative elements" of the home-based episodes.[149][150] The program continued to be filmed with no studio audience; while there were plans to reinstate an audience,[148] this was delayed due to concerns regarding Omicron variant.[148]

In March 2022, it was announced that The Daily Show would go on a hiatus from March 18 to accommodate Noah's hosting of the Grammy Awards on April 3. On April 11, the show returned to Studio 52 with an audience and a redesigned studio.[151][49]

On December 6, 2022, Comedy Central announced that until the next iteration of the show, The Daily Show would feature weekly celebrity guest hosts including Al Franken, Wanda Sykes, Leslie Jones, Hasan Minhaj, Sarah Silverman, Chelsea Handler, John Leguizamo, Marlon Wayans, Kal Penn, and D.L. Hughley, as well as both current and former correspondents.[152] The show returned from hiatus on January 17 with Leslie Jones guest hosting through January 19.[153][154] Jones was followed by Sykes, Hughley, Handler, and Silverman, each hosting a week through February 16.[155] Correspondent Dulcé Sloan had her first and last guest hosting gig of this era on May 1, 2023, when it was cut short by the 2023 Writers Guild of America strike, bumping originally announced guest hosts Michael Kosta, Charlamagne tha God, Michelle Wolf, Ronny Chieng, Lewis Black, and Desus Nice.[156]

On August 1, 2023, Variety reported that Minhaj was the primary possibility of a permanent replacement host.[157] A day later, The Wrap reported that Penn was also a top candidate.[158]

On September 27, 2023, following the 148-day strike, Comedy Central announced the show would return on October 16 with guest hosts and would not name a permanent host until 2024.[159] The extension of the search for a permanent host has been attributed to the New Yorker article alleging factual inaccuracies in Minhaj's comedy routines.[160]

On January 24, 2024, it was announced that Jon Stewart would return as host for Monday night shows, while the remainder of the week would be hosted by the correspondents, beginning on February 12. Stewart accepted the single day a week contract deal as his initial run left him feeling exhausted.[161] The producers of the show hope that Stewart will serve to cultivate and attract new talent to fill a full host role.[162] In May 2024, it was announced that Stewart would additionally begin hosting The Weekly Show, an original podcast from Comedy Central.[163] Dulcé Sloan was a senior correspondent but only hosted one week and no longer lived in New York.

On July 14, 2024, in the wake of the attempted assassination of Donald Trump, Comedy Central announced that The Daily Show would not air live from Milwaukee, the host city for the 2024 Republican National Convention, and would preempt the Monday evening broadcast for July 15. The show returned to air on July 16, 2024, from its New York studio.[164] In late October, it was announced that Stewart has extended his contract to host until 2025.[165]

The show's correspondents have two principal roles: experts with satirical senior titles that the main host interviews about certain issues, or hosts of field reporting segments which often involve humorous commentary and interviews relating to a current issue. The current team of hosting correspondents collectively known as "The Best F**king News Team" (formerly known as "The World's Fakest News Team" and previously spelled "The Best F#@king News Team Ever")  Ronny Chieng, Michael Kosta, Jordan Klepper, and Desi Lydic. Troy Iwata, Josh Johnson, and Grace Kuhlenschmidt are non-hosting correspondents.[166] Contributors appear on a less frequent basis, often with their own unique recurring segment or topic.[166] Current contributors include Lewis Black and Charlamagne tha God.[167] Ben Karlin says that the on-air talent contribute in many ways to the material they perform, playing an integral role in the creation of their field pieces as well as being involved with their scripted studio segments, either taking part early on in the writing process or adding improvised material during the rehearsal.[51]

The show has featured a number of well-known comedians throughout its run and is notable for boosting the careers of several of these. In 2006, The Onion editor-in-chief Scott Dikkers described it as a key launching pad for comedic talent, saying that "I don't know if there's a better show you could put on your resume right now."[168] Steve Carell, who was a correspondent between 1999 and 2005 before moving on to a movie career and starring television role in The Office, credits Stewart and The Daily Show with his success.[169] In 2005, the show's longest-serving correspondent, Stephen Colbert, became the host of the spin-off The Colbert Report, earning critical and popular acclaim.[170] Colbert would host the program until he was chosen to replace David Letterman as host of CBS's Late Show in 2015.[171] Ed Helms, a former correspondent from 2002 to 2006, also starred on NBC's The Office and was a main character in the 2009 hit The Hangover.[172]

After filling in as host during Stewart's two-month absence in the summer of 2013,[173] John Oliver went on to host his own show on HBO, Last Week Tonight with John Oliver. In 2016, former correspondent Samantha Bee launched her own late-night talk show Full Frontal with Samantha Bee.[174] Bee's husband Jason Jones, also a former correspondent, serves as executive producer for the show.[175] Hasan Minhaj, the last correspondent hired during Stewart's tenure as host, left the show in 2018 to host Patriot Act with Hasan Minhaj on Netflix.[176]

In June 2010, actress-comedian Olivia Munn began a tryout period on the show as a correspondent. Her credentials were questioned by Irin Carmon of the website Jezebel, who suggested that Munn was better known as a sex symbol than as a comedian.[177] Carmon's column was denounced by Munn and the Daily Show's female writers, producers, and correspondents, 32 of whom posted a rebuttal on the show's website in which they asserted that the description of the Daily Show office given by the Jezebel piece was not accurate.[178][179] Munn appeared as a Daily Show correspondent in 16 episodes, from June 2010 to September 2011.[180]

Wyatt Cenac had a tumultuous tenure on the show, revealing in a July 2015 interview on WTF with Marc Maron, that his departure stemmed in part from a heated argument he had with Jon Stewart in June 2011 over a bit about Republican presidential candidate Herman Cain.[181][182] However, Cenac did return for Stewart's final episode to bid him farewell and the two exchanged an intentionally awkward conversation.[183][184][185][186]

After Trevor Noah's departure from The Daily Show at the end of 2022, the program engaged a series of guest hosts beginning in January 2023, each of which hosted four episodes. A permanent replacement was not named as of March 2024[update] until the show transitioned to a format with a Monday show led by Jon Stewart and the News Team rotating hosting from Tuesday through Thursday.[11][187]

Television ratings from 2008 show that the program generally drew 1.45 to 1.6 million viewers nightly, a high figure for cable television.[192] By the end of 2013 The Daily Show's ratings hit 2.5 million viewers nightly.[193] In demographic terms, the viewership is skewed to a relatively young and well-educated audience compared to traditional news shows. A 2004 Nielsen Media Research study commissioned by Comedy Central put the median age at 35. During the 2004 U.S. presidential election, the show received more male viewers in the 18- to 34-year-old age demographic than Nightline, Meet the Press, Hannity & Colmes and all of the evening news broadcasts.[194]

For this reason, commentators such as Howard Dean and Ted Koppel posited that Stewart served as a real source of news for young people, regardless of his intentions.[195][196] In 2016, a New York Times study of the 50 TV shows with the most Facebook Likes found that The Daily Show was "most popular in cities and other more liberal-leaning areas along the coasts. Peak popularity is in San Francisco; it's least popular in Alabama".[197]

From January 2014 to January 2023, The Daily Show lost 75% of its audience, averaging 570,000 nightly viewers, down from 2.2 million. During the same period, the average age of its viewership increased from 48.2 to 63.3, with only 30,000 viewers in the coveted 18–34 demographic per broadcast.[8]

The show's writers reject the idea that The Daily Show has become a source of news for young people. Stewart argues that Americans are living in an "age of information osmosis" in which it is close to impossible to gain one's news from any single source, and says that his show succeeds comedically because the viewers already have some knowledge about current events. "Our show would not be valuable to people who didn't understand the news because it wouldn't make sense," he argues. "We make assumptions about your level of knowledge that... if we were your only source of news, you would just watch our show and think, 'I don't know what's happening.'"[198]

A 2006 study published by Indiana University tried to compare the substantive amount of information of The Daily Show against prime time network news broadcasts, and concluded that when it comes to substance, there is little difference between The Daily Show and other news outlets. The study contended that, since both programs are more focused on the nature of "infotainment" and ratings than on the dissemination of information, both are broadly equal in terms of the amount of substantial news coverage they offer.[199]

As the lines between comedy show and news show have blurred, Jon Stewart has come under pressure in some circles to engage in more serious journalism. Tucker Carlson and Daily Show co-creator Lizz Winstead have chastised Stewart for criticizing politicians and newspeople in his solo segments and then, in interviews with the same people, rarely taking them to task face-to-face. In 2004, Winstead expressed a desire for Stewart to ask harder satirical questions, saying, "When you are interviewing a Richard Perle or a Kissinger, if you give them a pass, then you become what you are satirizing. You have a war criminal sitting on your couch—to just let him be a war criminal sitting on your couch means you are having to respect some kind of boundary."[200] She has argued that The Daily Show's success and access to the youth vote should allow Stewart to press political guests harder without fearing that they will not return to the show.[201] In 2010, Winstead had changed her views, commenting that since 2004, Stewart did some of the hardest-hitting interviews on TV.[202] Stewart said in 2003 that he does not think of himself as a social or media critic and rejects the idea that he has any journalistic role as an interviewer.[203]

During Stewart's appearance on CNN's Crossfire, Stewart criticized that show and said that it was "hurting America" by sensationalizing debates and enabling political spin. When co-host Carlson argued that Stewart himself had not asked John Kerry substantial questions when Kerry appeared on The Daily Show, Stewart countered that it was not his job to give hard-hitting interviews and that a "fake news" comedy program should not be held to the same standards as real journalism. "You're on CNN!" Stewart said, "The show that leads into me is puppets making crank phone calls! What is wrong with you?"[204] Media critic Dan Kennedy says that Stewart came off as disingenuous in this exchange because "you can't interview Bill Clinton, Richard Clarke, Bill O'Reilly, Bob Dole, etc., etc., and still say you're just a comedian."[201]

A 2004 study into the effect of The Daily Show on viewers' attitudes found that participants had a more negative opinion of both President Bush and then Democratic presidential nominee John Kerry. Participants also expressed more cynical views of the electoral system and news media.[205] Political scientists Jody Baumgartner and Jonathan Morris, who conducted the study, state that it is not clear how such cynicism would affect the political behavior of the show's viewers. While disillusionment and negative perceptions of the presidential candidates could discourage watchers from voting, Baumgartner and Morris say it is also possible that discontent could prompt greater involvement and that by following the show, viewers may potentially become more engaged and informed voters, with a broader political knowledge.[206]

Rachel Larris, who has also conducted an academic study of The Daily Show, disputes the findings of Baumgartner and Morris. Larris argues that the study measured cynicism in overly broad terms, and that it would be extremely hard to find a causal link between viewing The Daily Show and thinking or acting in a particular way.[207] Bloggers such as Marty Kaplan of The Huffington Post argue that so long as Stewart's comedy is grounded in truth, responsibility for increased cynicism belongs to the political and media figures themselves, not the comedian who satirizes them.[208]

Stewart himself says that he does not perceive his show as cynical. "It's so interesting to me that people talk about late-night comedy being cynical," he says. "What's more cynical than forming an ideological news network like Fox and calling it 'fair and balanced'? What we do, I almost think, is adorable in its idealism."[209] Stewart has said that he does not take any joy in the failings of American government, despite the comedic fodder they provide. "We're not the guys at the craps table betting against the line," he said on Larry King Live. "If government suddenly became inspiring... we would be the happiest people in the world to turn our attention to idiots like, you know, media people, no offense."[95]

In July 2009, Time magazine held an online poll entitled "Now that Walter Cronkite has passed on, who is America's most trusted newscaster?"[210] Jon Stewart won with 44% of the vote, 15 points ahead of Brian Williams in second place with 29%.[211] Stewart downplayed the results on the show stating "It was an Internet poll and I was the 'None of the above' option".[citation needed]

In June 2013, the Writers Guild of America ranked The Daily Show with Jon Stewart #17 on their list of the 101 Best Written TV Series.[212]

In December 2013, TV Guide ranked it #53 on its list of the 60 Best Series of All Time.[213]

In late 2004, the National Annenberg Election Survey at the University of Pennsylvania ran a study of American television viewers and found that fans of The Daily Show had a more accurate idea of the facts behind the 2004 presidential election than most others, including those who primarily got their news through the national network evening newscasts and through reading newspapers.[214] However, in a 2004 campaign survey conducted by the Pew Research Center those who cited comedy shows such as The Daily Show as a source for news were among the least informed on campaign events and key aspects of the candidates' backgrounds while those who cited the Internet, National Public Radio, and news magazines were the most informed. Even when age and education were taken into account, the people who learned about the campaigns through the Internet were still found to be the most informed, while those who learned from comedy shows were the least informed.[215]

In a survey released by the Pew Research Center in April 2007, viewers who watch The Daily Show tend to be more knowledgeable about news than audiences of other news sources. Approximately 54% of The Daily Show viewers scored in the high knowledge range, followed by Jim Lehrer's program at 53% and Bill O'Reilly's program at 51%, significantly higher than the 34% of network morning show viewers. The survey shows that changing news formats have not made much difference on how much the public knows about national and international affairs, but adds that there is no clear connection between news formats and what audiences know.[216] The Project for Excellence in Journalism released a content analysis report suggesting that The Daily Show comes close to providing the complete daily news.[217]

Under host Jon Stewart, The Daily Show rose to critical acclaim. It has received two Peabody Awards for its coverage of the 2000[218] and 2004 presidential elections.[219] Between 2001 and 2024, it has been awarded 26 Primetime Emmy Awards in the categories of Outstanding Variety, Music, or Comedy Series (winner for 10 consecutive years from 2003 to 2012) and Outstanding Writing for a Variety, Music, or Comedy Program, and a further seven nominations. The show has also been honored by GLAAD,[220] the Television Critics Association, and the Satellite Awards. America (The Book): A Citizen's Guide to Democracy Inaction, the 2004 bestseller written by Stewart and the writing staff of The Daily Show, was recognized by Publishers Weekly as its "Book of the Year", and its abridged audiobook edition received the 2005 Grammy Award for Best Comedy Album.[221]

In September 2010, Time magazine selected the series as one of "The 100 Best TV Shows of All-TIME".[222] In 2015, the show received its third Peabody Award[223] for the show's "lasting impact on political satire, television comedy and even politics itself."[224]

The Daily Show airs on various networks worldwide; in addition, an edited version of the show called The Daily Show: Global Edition was produced each week specifically for overseas audiences until mid-2020. It used to air outside of the U.S. on CNN International and other overseas networks beginning in September 2002.[225][226] This edition ran for a half-hour and contained a selection of segments, including one guest interview from the preceding week's shows, usually from the Monday or Tuesday episode. Stewart provided an exclusive introductory monologue in front of an audience, usually about the week's prevalent international news story, and closing comments without an audience present.[227] When aired on CNN International, the broadcast was prefaced by a written disclaimer: "The show you are about to watch is a news parody. Its stories are not fact checked. Its reporters are not journalists. And its opinions are not fully thought through."[228]
Since February 27, 2017, The Daily Show with Trevor Noah has been regularly simulcast on Black Entertainment Television.

Between 2001 and 2006, Westwood One broadcast small, ninety-second portions of the show to various radio stations across America.[229]

In Canada, The Daily Show was aired on CTV Comedy Channel in simulcast with the Comedy Central airing. However, it was dropped in 2023, leaving the program without a Canadian television home, and exclusive to Paramount+. In August 2024, Corus Entertainment acquired the linear television rights to The Daily Show, with the program scheduled to move to Slice beginning on September 9, 2024.[230][231]

In the United Kingdom and Ireland, the digital television channel More4 used to broadcast episodes of The Daily Show Tuesday through Friday evenings with the Global Edition, which is uncensored, airing on Mondays; regular episodes air the evening following their U.S. airing. More4 was the first international broadcaster to syndicate entire Daily Show episodes, though they made edits to the program due to content, language, length or commercial references. The program was also available to watch via the internet video on demand service 4oD. However, the 'toss' to The Colbert Report was usually included even though it was aired on FX, another channel.[232] In addition, the placement of commercial breaks followed the UK format, with one break midway through the show rather than several short breaks at various points. When The Daily Show was on hiatus, either re-runs or alternative content were aired. Since January 2011, only the Global Edition is broadcast.[233]
In July 2012 Comedy Central announced that The Daily Show would be shown on Comedy Central Extra in the same format as previously on More4, with episodes shown 24 hours after airing in the U.S.[234] The show aired on the channel from July 2012 to April 2015.

The Global Edition of the week of July 20, 2011, was not aired in the UK as it included a segment mocking Rupert Murdoch's appearance before the House of Commons Culture, Media and Sport Committee in relation to the News International phone hacking scandal.[235] Parliamentary rules ban parliamentary proceedings from being broadcast in a satirical context.[235] Stewart dedicated a segment of the show on August 2, 2011, to lampooning the censorship of the episode in Britain.[236] In May that year, The Daily Show mocked the ban on using footage of the Royal Wedding in a satirical context with an animated video that showed Paddington Bear, Gollum and Adolf Hitler as guests at the wedding, and depicted its attendants engaging in various forms of violent and sexual behavior.[237] Stewart later discussed the ban with guest Keira Knightley.[238]

The Daily Show used to be aired in India on Comedy Central India.[239]

The Daily Show aired on Australian Pay TV channel, The Comedy Channel, weeknights at 6:30pm. Free-to-air digital channel ABC2 began broadcasting the show without commercial breaks in March 2010, but discontinued in January 2011 when The Comedy Channel obtained exclusive rights;[240] episodes were also available on the network's online service ABC iView shortly after airing.[241] The Comedy Channel (as well as ABC2 during 2010) used to air the show together with The Colbert Report, and both air the Global Edition on Mondays and the regular edition Tuesday through Friday. The Global Edition was previously shown weekend late nights on SBS before moving to Network TEN. The show now airs on 10 Shake (owned by Comedy Central parent ViacomCBS).

In North Africa and the Middle East, the Daily Show was broadcast on Showtime Arabia starting in 2008 and ending in 2015. When the show transitioned under Noah, OSN decided to wait a bit before airing the new show.[242] Now, the Global Edition of Noah's show is broadcast on OSN's Comedy Central HD channel.[243] Episodes are often edited if they contain topics deemed inappropriate for the region.

Episodes of the American version are also available online the next day at Comedy Central's official Daily Show website, although this service is not available in all countries. However, clips for UK and Ireland viewers became available on the UK Comedy Central website in December 2011.[244]

An official Dutch version of the show called The Daily Show: Nederlandse Editie (The Daily Show: Dutch Edition) premiered on the Dutch Comedy Central on January 31, 2011. The program is similar to the original, except with Dutch news and a Dutch view on international news. The show is hosted by comedian Jan-Jaap van der Wal, who was a team captain on Dit was het nieuws, the Dutch edition of Have I Got News For You.[245] The first episode featured a guest appearance by Jon Stewart (recorded at the New York studio), who gave his official blessing for the show.[246] This is also the first and still only franchise of The Daily Show. The 'Dutch Edition' didn't make it past the test run of 12 episodes due to lack of viewers.

A spin-off, The Colbert Report, was announced in early May 2005. The show starred former correspondent Stephen Colbert, and served as Comedy Central's answer to the programs of media pundits such as Bill O'Reilly. Colbert, Stewart, and Ben Karlin developed the idea for the show based on a series of faux television commercials that had been created for an earlier Daily Show segment. They pitched the concept to Comedy Central chief Doug Herzog, who agreed to run the show for eight weeks without first creating a pilot.[247] The Colbert Report premiered on October 17, 2005, and aired following The Daily Show for nine years. Initial ratings satisfied Comedy Central and less than three weeks after its debut the show was renewed for a year.[248] The Colbert Report was produced by Jon Stewart's production company, Busboy Productions.

In 2014, it was announced that Colbert would leave Comedy Central to host The Late Show with Stephen Colbert on CBS in 2015, following the retirement of David Letterman.[249] The final episode of The Colbert Report aired on December 18, 2014.

On May 9, 2014, it was announced that Larry Wilmore had been selected to host a show on Comedy Central to serve as a replacement for The Colbert Report. On January 19, 2015, Wilmore began hosting The Nightly Show with Larry Wilmore, a late-night panel talk show. It was produced by Busboy Productions. On August 15, 2016, Comedy Central announced that Wilmore's show had been cancelled. The show ended on August 18, 2016, with a total of 259 episodes.[250]

On April 4, 2017, Comedy Central announced a brand-new spinoff to occupy the 11:30 p.m. time slot which had not had an original show since the canceling of The Nightly Show with Larry Wilmore. The Daily Show's senior correspondent Jordan Klepper was enlisted as host, with Klepper, Stuart Miller, and Trevor Noah serving as executive producers.[251] The show intends to "satirize the hyperbolic, conspiracy-laden noise machine that is the alternative-media landscape on both the right and left."[252]

The show aired from September 25, 2017, to June 28, 2018. Comedy Central announced that Klepper would be hosting a new primetime weekly talk show, Klepper, which debuted in 2019.

In February 2018, The Daily Show: Ears Edition podcast was launched as companion piece to the main program, often featuring extended information and additional interviews. In December 19, Comedy Central launched a 5 episode mini series podcast called The Daily Show Podcast Universe.[253]

The Daily Show's satirical format has inspired international versions unaffiliated with Comedy Central.The Sopranos is an American crime drama television series created by David Chase. The series follows Tony Soprano (James Gandolfini), a New Jersey Mafia boss who suffers from panic attacks. He reluctantly begins seeing a psychiatrist, Dr. Melfi (Lorraine Bracco), who encourages him to open up about his difficulties balancing his family life with his criminal life. Other important characters include Tony's family, Mafia colleagues, and rivals, most notably his wife Carmela (Edie Falco) and his protégé and distant cousin Christopher Moltisanti (Michael Imperioli).

Having been greenlit in 1997, the series was broadcast on HBO from January 10, 1999, to June 10, 2007, spanning six seasons and 86 episodes. Broadcast syndication followed in the United States and internationally.[5] The Sopranos was produced by HBO, Chase Films, and Brad Grey Television. It was primarily filmed at Silvercup Studios in New York City, with some on-location filming in New Jersey. The executive producers throughout the show's run were Chase, Brad Grey, Robin Green, Mitchell Burgess, Ilene S. Landress, Terence Winter, and Matthew Weiner.

Widely regarded as one of the greatest and most influential television series of all time,[6][7][8][9][10][11][12][13]The Sopranos has been credited with kickstarting the Second Golden Age of Television.[14] The series won multiple awards, including Peabody Awards for its first two seasons, 21 Primetime Emmy Awards, and five Golden Globe Awards. It has been the subject of critical analysis, controversy, and parody; it has also spawned books,[15] a video game,[16] soundtrack albums, podcasts, and merchandise.[17] Several members of the show's cast and crew were largely unknown to the public when it began, but have since had successful careers.[18][19][20][21]

In March 2018, New Line Cinema announced that they had purchased a film detailing the show's background story, set in the 1960s and 1970s during and after the Newark riots. The film, The Many Saints of Newark (2021), was written by Chase and Lawrence Konner and directed by Alan Taylor.[22][23] It starred Gandolfini's son Michael Gandolfini as a young Tony Soprano.[24]

The series follows Tony Soprano, a North Jersey-based Italian-American mobster, who tries to balance his family life with his role as the boss of the Soprano family. Tony's career causes him frequent rage and anxiety, and after suffering from a particularly bad panic attack, he reluctantly begins therapy sessions with psychiatrist Jennifer Melfi. In Dr. Melfi's office, Tony slowly begins to open up about his emotional problems, even as his career continually leads him into dangerous and even life-threatening scenarios. He finds himself at odds with his uncle Junior, his wife Carmela, and other ambitious members of the Soprano family, as well as the Lupertazzi family of New York.

David Chase had worked as a television writer and producer for more than 20 years before creating The Sopranos.[25][26] He had been employed as a staff writer or producer for several television series, including Kolchak: The Night Stalker, Switch, The Rockford Files, I'll Fly Away, and Northern Exposure.[27] He had also co-created the series Almost Grown in 1988.[28][29] He made his television directorial debut in 1986 with the "Enough Rope for Two" episode of Alfred Hitchcock Presents. He also directed episodes of Almost Grown and I'll Fly Away in 1988 and 1992, respectively. In 1996, he wrote and directed the television film The Rockford Files: Punishment and Crime. He served as showrunner for I'll Fly Away and Northern Exposure in the 1990s. Chase won his first Emmy Award in 1978 for his work on The Rockford Files (shared with fellow producers) and his second for writing the 1980 television film Off the Minnesota Strip.[30][31] By 1996, he was a coveted showrunner.[32]

I want to tell a story about this particular man. I want to tell the story about the reality of being a mobster—or what I perceive to be the reality of life in organized crime. They aren't shooting each other every day. They sit around eating baked ziti and betting and figuring out who owes who money. Occasionally, violence breaks out—more often than it does in the banking world, perhaps.

The story of The Sopranos was initially conceived as a feature film about "a mobster in therapy having problems with his mother".[28] Chase got some input from his manager Lloyd Braun and decided to adapt it into a television series.[28] He signed a development deal in 1995 with production company Brillstein-Grey and wrote the original pilot script.[26][30][34] He drew heavily from his personal life and his experiences growing up in an Italian-American family in New Jersey, and has stated that he tried to apply his own "family dynamic to mobsters".[33] For instance, the tumultuous relationship between series protagonist Tony Soprano and his mother Livia is partially based on Chase's relationship with his own mother.[33] He was also in psychotherapy at the time and modeled the character of Jennifer Melfi after his own psychiatrist.[35]

Chase had been fascinated by organized crime and the mafia from an early age, witnessing such people growing up. He also was raised on classic gangster films such as The Public Enemy and the crime series The Untouchables. The series is partly inspired by the Richard Boiardo family, a prominent New Jersey organized crime family when Chase was growing up, and partly on New Jersey's DeCavalcante family.[36] He has mentioned American playwrights Arthur Miller and Tennessee Williams as influences on the show's writing, and Italian director Federico Fellini as an important influence on the show's cinematic style.[32][37][38]
The series was named after high school friends of his.[25][35]

I said to myself, this show is about a guy who's turning 40. He's inherited a business from his dad. He's trying to bring it into the modern age. He's got all the responsibilities that go along with that. He's got an overbearing mom that he's still trying to get out from under. Although he loves his wife, he's had an affair. He's got two teenage kids, and he's dealing with the realities of what that is. He's anxious; he's depressed; he starts to see a therapist because he's searching for the meaning of his own life. I thought: the only difference between him and everybody I know is he's the Don of New Jersey.

Chase and producer Brad Grey pitched The Sopranos to several networks; Fox showed interest but passed on it after Chase presented them the pilot script.[34] They eventually pitched the show to Chris Albrecht, president of HBO Original Programming, who decided to finance a pilot episode[26][30] which was shot in 1997.[40][41]
Chase directed it himself. They finished the pilot and showed it to HBO executives, but the show was put on hold for several months.[26]

During this time, Chase, who had experienced frustration for a long period with being unable to break out of the TV genre and into film,[26] considered asking HBO for additional funding to shoot 45 more minutes of footage and release The Sopranos as a feature film. In December 1997, HBO decided to produce the series and ordered 12 more episodes for a 13-episode season.[26][30][42]
The show premiered on HBO on January 10, 1999, with the pilot, "The Sopranos".

North Jersey prosecutor and municipal judge Robert Baer filed a breach of contract lawsuit against Chase in Trenton, New Jersey, federal court, alleging that he helped to create the show. Baer lost the suit, but he won a ruling that a jury should decide how much he should be paid for services as a location scout, researcher, and story consultant. Baer argued that he had introduced Chase to Tony Spirito, a restaurateur and gambler with alleged mob ties, and Thomas Koczur, a homicide detective for the Elizabeth, New Jersey police department. Chase had conducted interviews and tours with both, which strongly inspired some characters, settings, and storylines portrayed in The Sopranos.[43][44] On December 19, 2007, a federal jury found against Baer, dismissing all of his claims.[45]

Many of the actors on The Sopranos are Italian American from the New York metropolitan area, like the characters they portray, and many appeared together in films and television series before joining the cast of The Sopranos. The series has 27 actors in common with the 1990 Martin Scorsese gangster film Goodfellas, including main cast members Lorraine Bracco, Michael Imperioli, and Tony Sirico.[46]

The casting directors were Georgianne Walken and Sheila Jaffe.[47][48] The main cast was put together through a process of auditions and readings. Actors often did not know whether Chase liked their performances or not.[26] Michael Imperioli beat out several actors for the part of Christopher Moltisanti; he said that Chase had "a poker face, so I thought he wasn't into me, and he kept giving me notes and having me try it again, which often is a sign that you're not doing it right." Chase said that he wanted Imperioli because of his performance in Goodfellas.[26]

James Gandolfini was invited to audition for the part of Tony Soprano after casting director Susan Fitzgerald saw a short clip of his performance in the 1993 film True Romance.[26] Lorraine Bracco played the role of mob wife Karen Hill in Goodfellas, and she was originally asked to play the role of Carmela Soprano. She took the role of Dr. Jennifer Melfi instead because she wanted to try something different and felt that the part of the highly educated Dr. Melfi would be more of a challenge for her.[49] Tony Sirico had a criminal history,[50] and he signed on to play Paulie Walnuts so long as his character was not to be a "rat".[51] Sirico had originally auditioned for the role of Uncle Junior with Frank Vincent, but Dominic Chianese landed the role.[52]

Chase was impressed with Steven Van Zandt's humorous appearance and presence after seeing him induct The Rascals into the Rock and Roll Hall of Fame in 1997, and invited him to audition.[53] Van Zandt, a guitarist in Bruce Springsteen's E Street Band, had never acted before. He auditioned for the role of Tony Soprano, but HBO felt that the role should go to an experienced actor, so Chase wrote a new part for him.[49][53] Van Zandt eventually agreed to star on the show as consigliere Silvio Dante, and his real-life spouse Maureen was cast as his on-screen wife Gabriella.[54][55][56]

The cast of the debut season of the series consisted of largely unknown actors, with the exception of Bracco, Chianese, and Nancy Marchand, but many cast members were noted for their acting ability and received mainstream attention for their performances.[26][57] Subsequent seasons saw established actors Joe Pantoliano, Robert Loggia, Steve Buscemi, and Frank Vincent[58] join the starring cast, along with well-known actors in recurring roles such as Peter Bogdanovich, John Heard,[59] Robert Patrick,[60] Peter Riegert,[61] Annabella Sciorra,[58] and David Strathairn.[62]

Numerous well-known actors appeared in one or two episodes, such as Lauren Bacall, Daniel Baldwin, Annette Bening, Polly Bergen, Sandra Bernhard, Paul Dano, Charles S. Dutton,[63] Jon Favreau, Janeane Garofalo, Hal Holbrook, Tim Kang, Elias Koteas, Ben Kingsley, Linda Lavin, Ken Leung,[64] Julianna Margulies, Sydney Pollack, Wilmer Valderrama, Alicia Witt, and Burt Young.[65] Ray Liotta, who was eventually cast as two of the Moltisanti brothers in The Many Saints of Newark film prequel, was approached by Chase at one point to appear in the third or fourth seasons of the show, but the plan did not work out.[66]

Series creator and executive producer David Chase served as showrunner and head writer for the production of all six seasons of the show. He was deeply involved with the general production of every episode and is noted for being a very controlling, demanding, and specific producer.[25][31] He wrote or co-wrote between two and seven episodes per season and would oversee all the editing, consult with episode directors, give actors character motivation, approve casting choices and set designs, and do extensive but uncredited rewrites of episodes written by others.[57][67][68] Brad Grey served as executive producer alongside Chase but had no creative input on the show.[69] Many members of the creative team behind The Sopranos were handpicked by Chase, some being old friends and colleagues of his; others were selected after interviews conducted by producers of the show.[26][58]

Many of the show's writers had worked in television before joining the writing staff of The Sopranos. The writing team and married couple Robin Green and Mitchell Burgess worked on the series as writers and producers from the first to the fifth season; they had previously worked with Chase on Northern Exposure.[70]
Terence Winter joined the writing staff during the production of the second season and served as executive producer from season five onwards. He practiced law for two years before deciding to pursue a career as a screenwriter, and he caught the attention of Chase through writer Frank Renzulli.[32][71]

Matthew Weiner served as staff writer and producer for the show's fifth and sixth seasons. He wrote a script for the series Mad Men in 2000 which was passed on to Chase, who was so impressed that he immediately offered Weiner a job as a writer for The Sopranos.[72]
Cast members Michael Imperioli and Toni Kalem portray Christopher Moltisanti and Angie Bonpensiero respectively, and they also wrote episodes for the show. Imperioli wrote five episodes of seasons two through five, and Kalem wrote one episode of season five.[73][74]

Other writers included Frank Renzulli, Todd A. Kessler (co-creator of Damages), writing team Diane Frolov and Andrew Schneider who worked with Chase on Northern Exposure, and Lawrence Konner, who co-created Almost Grown with Chase in 1988. In total, 20 writers or writing teams are credited with writing episodes of The Sopranos. Of these, Tim Van Patten and Maria Laurino receive a single story credit, and eight others are credited with writing a sole episode. The most prolific writers of the series were Chase (30 credited episodes, including story credits), Winter (25 episodes), Green and Burgess (22 episodes), Weiner (12 episodes), and Renzulli (9 episodes).

Many of the directors had previously worked on television series and independent films.[58] The most frequent directors of the series were Tim Van Patten (20 episodes), John Patterson (13 episodes), Allen Coulter (12 episodes), and Alan Taylor (9 episodes), all of whom have a background in television.[58] Recurring cast members Steve Buscemi and Peter Bogdanovich also directed episodes of the series intermittently.[75][76] Chase directed the pilot episode and the series finale.[77] Both episodes were photographed by the show's original director of photography Alik Sakharov, who later alternated episodes with Phil Abraham.[78] The show's photography and directing is noted for its feature film quality.[79][80] This look was achieved by Chase collaborating with Sakharov. "From the pilot, we would sit down with the whole script and break the scenes down into shots. That's what you do with feature films."[78]

The Sopranos is noted for its eclectic music selections and has received considerable critical attention for its effective use of previously recorded songs.[81][82][83][84]
Chase personally selected all of the show's music with producer Martin Bruestle and music editor Kathryn Dayak, sometimes also consulting Steven Van Zandt.[81] The music was usually selected once the production and editing of an episode was completed, but on occasion sequences were filmed to match preselected pieces of music.[67]

The show's opening theme is "Woke Up This Morning" (Chosen One Mix), written by, remixed and performed by British band Alabama 3.[85] With few exceptions, a different song plays over the closing credits of each episode.[83] Many songs are repeated multiple times through an episode, such as "Living on a Thin Line" by The Kinks in the season three episode "University" and "Glad Tidings" by Van Morrison in the season five finale "All Due Respect".[83] Other songs are heard several times throughout the series. A notable example is "Con te partirò", performed by Italian singer Andrea Bocelli,[86]
which plays several times in relation to the character of Carmela Soprano.
While the show utilizes a wealth of previously recorded music, it is also notable for its lack of originally composed incidental music, compared with other television programs.[87]

Two soundtrack albums containing music from the series have been released. The first, titled The Sopranos: Music from the HBO Original Series, was released in 1999. It contains selections from the show's first two seasons and reached No. 54 on the U.S. Billboard 200.[88][89]
A second soundtrack compilation titled The Sopranos – Peppers & Eggs: Music from the HBO Series, was released in 2001. This double-disc album contains songs and selected dialogue from the show's first three seasons.[90]
It reached No. 38 on the U.S. Billboard 200.[91]

The ending to the show has drawn attention and sparked controversy. The song "Don't Stop Believin'" by Journey plays at the end of the series finale. While Journey granted the rights to use the song in the ending of the series, their one request was that the scene not contain any violence or deaths.[92][93]

The majority of the exterior scenes took place in New Jersey and were filmed on location, with the majority of the interior shots filmed at Silvercup Studios in New York City, including most indoor shots of the Soprano residence, the back room of the strip club Bada Bing!, and Dr. Melfi's office.[57] The pork store was called Centanni's Meat Market in the pilot episode, an actual butchery in Elizabeth, New Jersey.[94]
After the series was picked up by HBO, the producers leased a building with a storefront in Kearny, New Jersey[94] which served as the shooting location for exterior and interior scenes for the remainder of production; renamed Satriale's Pork Store.[94]
After the series ended, the building was demolished.[95]

The strip club Bada Bing! was owned and operated by Silvio Dante on the show, and is an actual strip club on Route 17 in Lodi, New Jersey.[94] Exteriors and interiors were shot on location except for the back room.[94] The club is called Satin Dolls and was an existing business before the show started.[96]
The club continued to operate during the eight years that the show was filmed there, and a business arrangement was worked out with the owner.[96] Locations manager Mark Kamine recalls that the owner was "very gracious" as long as the shooting did not "conflict with his business time".[96]

Scenes set at the restaurant Vesuvio, owned and operated in the series by character Artie Bucco, were filmed at a restaurant called Manolo's located in Elizabeth for the first episode. After the destruction of Vesuvio within the context of the series, Artie opened a new restaurant called Nuovo Vesuvio; exterior scenes set there were filmed at an Italian restaurant called Punta Dura located in Long Island City.[94] All the exterior and some interior shots of the Soprano residence were filmed on location at a private residence in North Caldwell, New Jersey.[94]

Tony Soprano is seen emerging from the Lincoln Tunnel out of Manhattan and passing through the tollbooth for the New Jersey Turnpike. Numerous landmarks in and around Newark and Jersey City, New Jersey, are then shown passing by the camera as Tony drives down the highway.[97] The sequence ends with Tony pulling into the driveway of his suburban home. Chase has said that the goal of the title sequence was to show that this particular mafia show was about New Jersey, as opposed to New York, where most similar dramas have been set.[98]

In the first three seasons, between Tony leaving the tunnel and passing through the toll plaza, the title sequence included a shot of the World Trade Center towers in the right side-view mirror. After the September 11 attacks, beginning with the show's fourth season, this shot was removed and replaced with a more generic view.

In a 2010 issue of TV Guide, the show's opening title sequence ranked No. 10 on a list of TV's top 10 credits sequences, as selected by readers.[99]

A parody of the opening sequence was used in an episode of The Simpsons. In "Poppa's Got a Brand New Badge", a variation on the sequence is used, with Fat Tony leaving a Springfield tunnel instead of Tony. Fat Tony then continues to drive through Springfield to the same soundtrack as the original.

The Sopranos features a large cast of characters, many of whom get significant amounts of character development. Some only appear in certain seasons, while others appear (sporadically or constantly) throughout the entire series. All characters were created by David Chase unless otherwise noted.

Tony Soprano (James Gandolfini) is the series' protagonist. Tony is one of the capos of the New Jersey–based DiMeo crime family at the beginning of the series; he eventually becomes its undisputed boss. He is also the patriarch of the Soprano household. Throughout the series, Tony struggles to balance the conflicting requirements of his family with those of the Mafia family he controls.[100] Because he is prone to bouts of clinical depression and reflex syncope, after a fainting spell (triggered by a panic attack), Tony's physician refers him for treatment by psychiatrist Dr. Jennifer Melfi (Lorraine Bracco) in the show's first episode.[101]

She treats Tony to the best of her ability, even when he is triggered to violent, angry outbursts. Melfi is usually thoughtful, rational, and humane, which is a stark contrast to Tony's personality. Tony, a serial womanizer, occasionally divulges his sexual attraction to Dr. Melfi. Melfi harbors some degree of attraction to Tony too, but rarely admits it and never acts on it. Melfi is drawn to the challenge of helping such an unusual client and naively assumes that their doctor–patient relationship will not affect her personal life in any way.[101]

Adding to Tony's complicated life is his relationship with his wife Carmela (Edie Falco),[102] which is strained by his constant infidelity and her struggle to reconcile the reality of Tony's business, of which she is often in denial, with the affluent lifestyle and higher social status it brings her. Tony and Carmela default to anger, criticism and yelling in their parenting, and so have challenging relationships with their two children: the intelligent, but rebellious, Meadow (Jamie-Lynn Sigler)[103] and underachiever Anthony Jr. ("A.J.") (Robert Iler),[104] whose everyday teenage issues are further complicated by their eventual knowledge of their father's criminal activities and reputation.

The starring cast includes members of Tony's extended family, including his narcissistic, disapproving, manipulative mother, Livia (Nancy Marchand),[105] his aimless, histrionic older sister  Janice (Aida Turturro),[106] his crafty, paternal uncle Corrado "Junior" Soprano (Dominic Chianese) who is nominal boss of the crime family following the death of then-acting boss Jackie Aprile Sr. (Michael Rispoli),[107] Christopher Moltisanti (Michael Imperioli)[108] his immature and hot-headed cousin and protégé[109] and his maternal cousin Tony Blundetto (Steve Buscemi).[110]

Both Livia and Janice are scheming  treacherous shrewd manipulators with major unaddressed psychological issues of their own. The single-mindedly ambitious Uncle Junior is chronically frustrated by having not been made boss of the DiMeo family, despite old-school mob traditions entitling him to the position by seniority. He feels his authority is perpetually undermined by Tony's greater influence in the organization and barely contains his seething jealousy at having to watch both his younger brother (Tony's father) and now Tony leapfrog him in the organization. As their professional tensions escalate, Uncle Junior employs increasingly desperate behind-the-scenes measures to solve his problems with Tony, who still idolizes his uncle, and wants to retain Junior's affection and approval.

Uncle Junior and Christopher are fixtures in Tony's real family, as well as his crime family, so their actions in one realm often create further conflicts in the other. Christopher, an entitled, insecure DiMeo associate who is as ambitious as he is insubordinate and incompetent, is also a chronic substance abuser. Tony Blundetto is a well-respected DiMeo family soldier who returns after completing a lengthy prison sentence; he leaves prison committed to "going straight" (to Tony's dismay), but also has an intense violent streak.

Those in Tony's closest circle within the DiMeo crime family include Silvio Dante (Steven Van Zandt), Tony's consigliere and best friend who runs the family's strip club headquarters and other businesses,[111] Paulie "Walnuts" Gualtieri (Tony Sirico), a tough, short-tempered, aging soldier who is fiercely loyal to Tony,[112] and Salvatore "Big Pussy" Bonpensiero (Vincent Pastore), a veteran gangster who runs an automotive body shop. Paulie "Walnuts" and "Big Pussy" (often called just "Pussy") have worked with Tony and his father.[113] Also in Tony's criminal organization are Patsy Parisi (Dan Grimaldi), a soft-spoken soldier with a head for figures,[114] and Furio Giunta (Federico Castelluccio),[115] an Italian national who joins the family later in the series, who serves as Tony's violent enforcer and bodyguard.

Other significant characters in the DiMeo family include Bobby "Bacala" Baccalieri (Steven R. Schirripa);[116] Richie Aprile (David Proval);[117] Ralph Cifaretto (Joe Pantoliano);[118] Eugene Pontecorvo (Robert Funaro);[119] and Vito Spatafore (Joseph R. Gannascoli).[120] Bobby is a subordinate of Uncle Junior's whom Tony initially bullies, but later accepts into his inner circle. Ralph is a clever, ambitious top-earner, but his arrogant, obnoxious, disrespectful, and unpredictably violent tendencies turn Tony resentful. Richie Aprile is released from prison in season 2 and quickly makes waves. Pontecorvo is a young soldier who becomes a "made" man alongside Christopher. Spatafore works his way up through the ranks to become top earner of the Aprile crew but is secretly gay.

Friends of the Soprano family include Herman "Hesh" Rabkin (Jerry Adler);[121] Adriana La Cerva (Drea de Matteo);[122] Rosalie Aprile (Sharon Angela);[123] Angie Bonpensiero (Toni Kalem), along with Artie (John Ventimiglia)[124] and Charmaine Bucco (Kathrine Narducci).[125] Hesh is an invaluable adviser and friend to Tony, as he was when Tony's father ran things. Adriana is Christopher's loyal and long-suffering girlfriend; the two have a volatile relationship but appear destined to stay together. Christopher often ignores Adriana's advice and winds up regretting it. Rosalie is the widow of previous DiMeo boss Jackie Aprile Sr. and a very close friend of Carmela. Angie is Salvatore Bonpensiero's wife. She later goes into "business" for herself, and quite successfully.

Artie and Charmaine are childhood friends of the Sopranos, and owners of the popular restaurant Vesuvio. Charmaine wishes to have no association with Tony and his crew due to fears that Tony's criminal ways will ultimately ruin everything she and Artie have achieved. Artie, however—a law-abiding, hard-working man—is drawn to his childhood friend Tony's glamorous, seemingly carefree lifestyle. Charmaine bitterly resents Artie's chronic tendency to disregard her wishes while catering to Tony's; their marriage suffers greatly as a result. Charmaine also had a brief sexual encounter with Tony (when he and Carmela had temporarily broken-up) when all four were teenagers.

John "Johnny Sack" Sacramoni (Vince Curatola),[126] Phil Leotardo (Frank Vincent)[127] and "Little" Carmine Lupertazzi Jr. (Ray Abruzzo)[128] are all significant characters from the New York City–based Lupertazzi crime family, which shares a good amount of its business with the Soprano organization. Although the Lupertazzis' and DiMeos' interests are often at odds, Tony maintains a cordial, business-like relationship with "Johnny Sack", preferring to make mutually beneficial deals, not war. Johnny Sack's second-in-command and eventual successor, Phil Leotardo, is less friendly and harder for Tony to do business with. Little Carmine is the son of the family's first boss and vies for power with its other members.

When Tony Soprano collapses after suffering a panic attack, he begins therapy with Dr. Jennifer Melfi. Details of Tony's upbringing—with his father's influence looming large on his development as a gangster, but more so that of Tony's mother, Livia, who is vengeful, narcissistic, and possibly psychopathic—are revealed. His complicated relationship with his wife Carmela is also explored, as well as her feelings regarding her husband's cosa nostra ties. Meadow and Anthony Jr., Tony's children, gain increasing knowledge of their father's mob dealings. Later, federal indictments are brought as a result of someone in his organization talking to the FBI.

Tony's uncle Corrado "Junior" Soprano, who controls his own crew, orders the murder of Brendan Filone and the mock execution of Christopher Moltisanti, associates of Tony's, as a reprisal for repeated hijackings of trucks under Corrado's protection. Tony defuses the situation by allowing his uncle to be installed as boss of the family (following the death of the previous boss Jackie Aprile from cancer), while Tony retains actual control of most dealings from behind the scenes. Corrado discovers the subterfuge, after talking to Livia and falling for her subtle manipulation, and he orders an attempt on Tony's life. The assassination is botched and Tony responds violently, before confronting his mother for her role in plotting his downfall; she appears to have a psychologically triggered stroke as a result. Junior is arrested by the FBI on charges related to the federal indictments before Tony gets a chance to murder him in retaliation.

Jackie's brother Richie Aprile is released from prison. He proves to be uncontrollable in the business arena, siding more with Junior than Tony, despite the fact that Tony is the acting boss of the family after Junior's arrest. Richie starts a relationship with Janice, Tony's sister, who has arrived from Seattle to take care of their mother. "Big Pussy" returns to New Jersey after a conspicuous absence.

Christopher Moltisanti becomes engaged to his girlfriend Adriana La Cerva, despite his past abuse. Matthew Bevilaqua and Sean Gismonte, two low-level associates dissatisfied with their perceived lack of success in the Soprano crew, try to make names for themselves by attempting to kill Christopher as a favor to Richie, even though he didn't ask them to. Their plan fails and Christopher kills Sean, but Christopher is critically wounded. He manages to recover after surgery. Tony and Big Pussy locate Matthew and kill him. A witness to the murder goes to the FBI and identifies Tony, but later retracts his statement.

Junior is placed under house arrest as he awaits trial. Richie, frustrated with Tony's authority over him, entreats Junior to have Tony killed. Junior feigns interest, then informs Tony of Richie's intentions, leaving Tony with another problem to address. However, the situation is defused unexpectedly when Janice kills Richie in a violent argument; Tony and his men conceal all evidence of the murder, and Janice returns to Seattle.

After a food poisoning incident that causes vivid dreams, Tony finally comes to terms with his suspicion that Big Pussy might be an FBI informant. He manages to search Pussy's bedroom under false pretenses and discovers damning evidence. Tony kills Pussy on board a boat (with assistance from Silvio Dante and Paulie Gualtieri), disposing of his body at sea.

Following the "disappearance" of Richie Aprile, the return of the ambitious Ralph Cifaretto, having spent an extended period of leisure time in Miami, marks the third season. He renews a relationship with Rosalie Aprile, the widow of Jackie Aprile Sr. With Richie assumed to have joined the Witness Protection Program, Ralph unofficially usurps control over the Aprile crew, proving to be an exceptionally dexterous earner. While Ralph's competitive merit would seemingly have him next in line to ascend to capo, his insubordination inclines Tony not to promote him and he instead gives the promotion to the less qualified but competent Gigi Cestone, causing much resentment and tension between him and Ralph.

Ralph ultimately crosses the line when, in a cocaine-induced rage, he gets into a confrontation with his pregnant girlfriend Tracee and beats her to death. This infuriates Tony, who had come to care for the girl, to the point where he violates the traditional mafia code by beating Ralph in front of the entire family. Bad blood temporarily surfaces between the two but is shortly resolved after Ralph apologizes. Cestone suffers a fatal heart attack, thereby forcing Tony to reluctantly promote Ralph to capo.

After getting arrested at the airport for stolen airplane tickets that Tony gave her, Livia is set to testify against him in court. Before that can happen, Livia dies of a stroke and Tony has to deal with his complicated feelings surrounding their relationship. Junior is diagnosed with stomach cancer; following surgery and chemotherapy, it goes into remission.

One night after work, Dr. Melfi is raped by a stranger in a parking complex. After police mishandle evidence, the suspect is released from custody without facing charges. Dr. Melfi struggles with the fallout of the assault and the notion that she could ask Tony to deal out his brand of justice, which she ultimately decides against. Meanwhile, Tony begins an affair with Gloria Trillo, who is also a patient of Dr. Melfi. Their relationship is brief and tumultuous.

Rosalie's son Jackie Aprile Jr. becomes involved with Meadow and then descends into a downward spiral of recklessness, drugs, and crime. Tony initially attempts to act as a mentor to Jackie and encourages him to stay in school, but he becomes increasingly impatient with Jackie's escalating misbehavior, particularly as Jackie's relationship with Meadow begins to become serious. Inspired by a story from Ralph about how Tony, Jackie Sr., and Silvio Dante got made, Jackie and his friends Dino Zerilli and Carlo Renzi make a similar move and attempt to rob Eugene Pontecorvo's Saturday night card game so they can gain recognition from the family.

The plan takes a turn for the worse when Jackie panics and kills the card dealer, provoking a shoot-out. Dino and Carlo are killed, but Jackie manages to escape. Tony decides to let Ralph handle the decision regarding Jackie Jr.'s punishment, but he strongly implies that he thinks Ralph should kill Jackie. Despite his role as a surrogate father, Ralph decides to have Jackie Jr. killed when other members of the crew play up how badly Jackie had disrespected him.

A.J. continues to get in trouble at school—despite success on the football team—which culminates in his expulsion and his parents considering sending him to military school. When he suffers a panic attack, his second after the one his old school failed to report, Tony realizes A.J. can't attend military school and he blames himself. Meadow is hit hard by Jackie Jr.'s death, resorting to drinking and then storming out of his funeral reception.

New York underboss Johnny Sack becomes enraged after learning Ralph Cifaretto joked about his wife's weight. He seeks permission from boss Carmine Lupertazzi to have Ralph clipped, but is denied. Johnny orders the hit anyway. Tony receives the okay from Carmine to hit Johnny for insubordination. Junior Soprano tips Tony to use an old outfit in Providence for the work. After catching his wife eating sweets secretly instead of following her diet, Johnny Sack talks it out with her and then calls off the hit on Ralph, averting bloodshed.

Tony and Ralph invest in a racehorse named Pie-O-My, who wins several races and makes them both a great deal of money until the horse dies in a stable fire. When Ralph's 12-year-old son Justin is severely injured in an archery accident, Tony comes to believe Ralph started the stable fire himself in order to collect $200,000 in insurance money. Tony confronts Ralph and Ralph denies setting the fire. The two engage in a violent brawl, culminating in Tony strangling Ralph to death. Tony and Christopher dispose of the body and tell the rest of the crew that the likely culprit for Ralph's disappearance is Johnny Sack.

While he is leaving court, Junior is hit in the head with a boom mic and falls down several steps. Tony advises him to take advantage of the opportunity, act mentally incompetent and employ it as a ruse for not continuing the trial. When that fails, Eugene Pontecorvo intimidates a juror, resulting in a deadlocked jury, forcing the judge to declare a mistrial.

Following the death of Bobby Baccalieri's wife, Janice pursues a romantic relationship with him. Bobby is initially reluctant to move on, but after an incident with his kids and Anthony Jr. trying to summon his deceased wife's ghost, he becomes more receptive to Janice's advances.

Christopher's addiction to heroin deepens, prompting his associates and family to organize an intervention, after which he enters a drug rehabilitation center. Adriana's friend Danielle Ciccolella is revealed to be an undercover FBI agent named Deborah Ciccerone-Waldrup, who tells Adriana the only way for her to stay out of prison for cocaine distribution at her bar is to become an informant. Adriana reluctantly agrees and starts sharing information with the FBI.

Carmela, whose relationship with Tony is tense due to financial worries and Tony's infidelities, develops a mutual infatuation with Furio Giunta. Furio, incapable of breaking his personal moral code and that of the Neapolitan mafia, clandestinely returns home to Italy. After Tony's former mistress calls their home, Carmela throws Tony out. As a result, their plan to buy a beach house falls through and Tony pesters the owner until he gets his deposit back.

Anthony Jr. starts attending a new high school, with Tony suggesting he needed to pull some strings to get him in. A.J. gets a girlfriend but is intimidated by her family's wealth. Meadow initially struggles with her ex-boyfriend's death. As she considers taking a gap year or switching schools, she sees a therapist that Dr. Melfi recommended. Eventually, Meadow finds a worthwhile cause by volunteering at a law center. She gets an apartment with some roommates and starts dating again. Her relationship with Carmela becomes strained after several arguments. Both the kids take their parents' separation hard, with A.J. asking to live with his dad instead of his mom.

Tony decides to quit therapy, thinking he isn't making any progress. He thanks Dr. Melfi for all her help and they part amicably. Stuck in a deadlock over a deal with the Lupertazzi family, Tony is approached by Johnny Sack with a proposal to murder Carmine. He considers it, even after managing to reach an agreement with Carmine, but he later becomes suspicious of Johnny's intentions and turns him down.

A string of new characters are introduced, including Tony's cousin Tony Blundetto, who simultaneously along with other Mafiosi, is released from prison. Among the others released are former DiMeo crime family capo Michele "Feech" La Manna, Lupertazzi family capo Phil Leotardo, and semi-retired Lupertazzi consigliere Angelo Garepe. Tony offers Tony B a job, but he respectfully declines, as he is determined to lead a straight life. He initially begins to take courses to earn a degree in massage therapy and aspires to open up a massage parlor. After Carmine Lupertazzi dies of a stroke, his death leaves a vacancy for the boss of the Lupertazzi family, which will soon be fought over by underboss Johnny Sack and Carmine's son Carmine Lupertazzi Jr. After Feech proves to be an insubordinate presence, Tony arranges for him to be sent back to prison by setting him up with stolen property, violating his parole.

The war between Johnny Sack and Carmine Jr. begins when Johnny has Phil kill "Lady Shylock" Lorraine Calluzzo. Tony B's attempt to stay straight comes to a head when he gets into a brawl with his employer. Angelo, who was a good friend to Tony B in prison, and Lupertazzi capo Rusty Millio offer Tony B the job of taking out Joey Peeps in retaliation for Lorraine's death. Tony B initially declines but, desperate to earn, accepts the job. He catches Joey outside a bordello, shoots him, and quickly flees the scene. Johnny believes Tony B is involved and retaliates by having Phil and his brother Billy Leotardo kill Angelo. Tony B finds the Leotardo brothers and opens fire, killing Billy and wounding Phil.

Separated from Carmela, Tony is living at his parents' house. Carmela, the sole authority figure in the home, becomes frustrated as her rules lead A.J. to resent her so she allows him to live with his father. She has a brief relationship with Robert Wegler, A.J.'s guidance counselor; he breaks it off abruptly when he suspects that she is manipulating him to improve A.J.'s grades. Tony and Carmela reconcile; Tony promises to be more loyal and agrees to pay for a piece of real estate Carmela wishes to develop.

Tony gets Meadow's boyfriend Finn De Trolio a summer job at a construction site, which is run by Aprile crew capo Vito Spatafore. Finn comes in early one morning and catches Vito performing fellatio on a security guard. Vito tries to buddy up to Finn so that he keeps quiet, but Finn soon quits the job out of fear.

After covering up a murder that occurred at The Crazy Horse, Adriana is arrested and pressured by the FBI to start sharing more relevant information about the family to avoid being charged as an accomplice. Rather than taking the risk of wearing a wire, Adriana confesses to Christopher and tries to persuade him to co-operate and become an informant against Tony. A grief-stricken Christopher instead informs Tony, who has Silvio pick up Adriana under the pretense of taking her to the hospital to see Christopher after he supposedly attempted suicide, but Silvio instead drives her out to the woods and executes her. Adriana's betrayal and subsequent execution are too much for Christopher to handle and he briefly relapses into drug use to deal with the pain.

Phil Leotardo and his henchmen beat Benny Fazio while trying to acquire the whereabouts of Tony B; Phil also threatens to have Christopher taken out if Tony B's whereabouts are not disclosed soon. To pacify New York and give his cousin a painless death, Tony tracks Tony B to their Uncle Pat's farm and executes him. Phil is furious that he did not get the opportunity to do it himself. Tony and Johnny meet at Johnny's house in a reconciliatory manner, but Johnny is arrested by Federal agents, while Tony escapes.

A senile and confused Uncle Junior shoots Tony one night in his house. Rendered comatose, Tony dreams he is a salesman on a business trip who mistakenly exchanges his briefcase and identification with a man named Kevin Finnerty. Tony's recovery from the shooting changes his outlook and he tries to mend his ways. However, he is faced with more problems in his business and personal life. Eugene Pontecorvo makes a request to Tony to get out of the life, which is subsequently denied, and coupled with the stress and problems with his home life, hangs himself in his garage. 

Once Tony is out of the hospital, Johnny Sack's daughter is about to get married and the Soprano family attends the wedding. Johnny is approved to leave prison for six hours to attend, but he is humiliated by having to pay for the metal detectors and the presence of U.S. marshals at the event. As his daughter is about to drive away with her husband, Johnny's time expires and the marshals publicly take him back to prison. In a moment of weakness and despair, Johnny bursts into tears as he is handcuffed, dismantling the remaining respect his and Tony's crews had for him.

Vito Spatafore is outed as gay after running into a friend making collections at a New York gay nightclub. The rumor spreads quickly, and once word gets to Meadow that everyone else knows, she tells Tony and Carmela about the incident between Finn and Vito. Finn is forced to tell Tony's entire crew what happened with Vito and the security guard at the construction site, solidifying their suspicions about Vito's sexuality. Tony is urged to deal with the problem by the intensely homophobic Phil Leotardo, now the acting boss of New York, whose cousin is married to Vito.

Once Vito is confronted by other members of the crew, he flees to a New Hampshire town, where he poses as an author and starts a romantic relationship with a male cook at a local diner. Despite finally living an authentic life, Vito misses the benefits his old job afforded him, so he eventually returns to New Jersey. He asks Tony to allow him to return to work, making a case that he could bring in a lot of money in Atlantic City. Vito visits his wife and children and continues to maintain that he is not a homosexual.

Tony mulls over the decision to let him back into the crew, as well as whether to let him live. When Tony fails to act, Phil intervenes and brutally executes Vito. When one of the members of the New York family, Fat Dom Gamiello, pays a visit to the Jersey office and won't stop making jokes about Vito and his death, Silvio Dante and Carlo Gervasi kill Fat Dom out of anger at his disrespect. Once more, it appears that the families are on the verge of an all-out war.

During the first half of the season, Christopher and Little Carmine head to Los Angeles in an ultimately unsuccessful attempt to try to sign Ben Kingsley for a slasher film they are trying to make called Cleaver, which is a mix of The Godfather and Saw. While in Los Angeles, Chris goes back to drinking and using cocaine for a short period, and he robs famous actress Lauren Bacall. When Cleaver comes out, Carmela is upset that the boss, who is based on Tony, sleeps with his underling's girlfriend, who seems to be based on Christopher's ex Adrianna. Tony's negative portrayal in the movie further strains his relationship with Christopher, along with the fact that Christopher had an affair with realtor Julianna Skiff, a woman Tony was romantically interested in. When Christopher's new girlfriend Kelli Lombardo accidentally gets pregnant, they decide to get married in Atlantic City. Later they welcome a baby girl.

Tony considers killing several of his associates for relatively minor infractions, including Paulie Gualtieri. Christopher is unable to thrive in the business because of his addiction, deflecting his problems by relapsing and killing his friend from Narcotics Anonymous and co-writer of Cleaver, J. T. Dolan. He is then seriously injured in a car accident while driving under the influence of narcotics. Tony, the sole passenger, finally loses patience with Christopher's failings and suffocates him. He later tries to justify his actions by bringing up the infant car seat that was impaled by a branch in the accident, implying that Christopher was a danger to his daughter.

A.J. is dumped by his fiancée and he slips into depression, culminating in a suicide attempt in the backyard pool. After spending some time in a mental institution, he returns home but is still haunted by existential questions and he ultimately decides to join the army. Tony and Carmela come up with a movie set job to keep him from enlisting, with Tony promising he would one day finance A.J.'s nightclub. Dr. Melfi is convinced by colleagues that Tony is making no progress and may even be using talk therapy to excuse his own actions and as practice for manipulative behavior. She drops him as a patient and he fully quits therapy.

Johnny dies from lung cancer while imprisoned, and Phil officially takes over the Lupertazzi family after having his rivals killed. Phil renews his past feud with Tony and refuses to compromise with New Jersey on a garbage deal. When Tony assaults a Lupertazzi soldier for harassing Meadow while she was on a date, Phil initiates open war on the Soprano crew. He orders the executions of Bobby Baccalieri, who is shot to death; Silvio Dante, who ends up comatose; and Tony, who goes into hiding. Since Phil won't back down until Tony is executed, a deal is eventually brokered whereby the rest of the Lupertazzi family agrees to ignore the hit on Tony, allowing him to go after Phil without fear of repercussions. FBI agent Dwight Harris informs Tony of Phil's location, allowing Tony to have him killed.

Tony starts suspecting that Carlo Gervasi, a capo from New Jersey, has become an informant in an attempt to help out his son, who has recently been arrested for dealing ecstasy. Tony meets his lawyer, who informs him that subpoenas are being delivered to New Jersey and New York crews alike. Tony visits Uncle Junior for the first time since the shooting, and although he does not forgive him, he comes to understand the full extent of his dementia and that his uncle likely had not meant to kill him.

Tony plans to have a quiet dinner at a diner with his family. As Meadow arrives at the door, the camera cuts to Tony. A bell signals the door opening, Tony looks up and the show smash cuts to black; after a few seconds, the credits roll in silence.

The Sopranos was a major ratings success throughout its run, despite being aired on premium cable network HBO, which had been available in significantly fewer American homes than regular networks. The show frequently attracted equal or larger audiences than most popular network shows of the time.[129] The Nielsen ratings for the first four seasons are not entirely accurate, as Nielsen reported aggregate numbers for cable networks prior to January 2004, meaning that people who were included in the ratings estimates were actually watching HBO channels other than the main one on which The Sopranos aired.[130]

The Sopranos has been hailed by many critics as the greatest and most groundbreaking television series of all time.[a] The writing, acting, and directing have often been singled out for praise. The show has also received considerable attention from critics and journalists for its technical merit, music selections, cinematography, and willingness to deal with difficult and controversial subjects including crime, family, gender roles, mental illness, and American and Italian-American culture.[80][152][153]

The Sopranos is credited for creating a new era in the mafia genre deviating from the traditional dramatized image of the gangster in favor of a simpler, more accurate reflection of ordinary day-to-day mob life in a suburb.[156] The series sheds light on Italian family dynamics through the depiction of Tony's tumultuous relationship with his mother.[157] Edie Falco's character Carmela Soprano is praised in Kristyn Gorton's essay "Why I Love Carmela Soprano" for challenging Italian-American gender roles.[158] New Yorker editor David Remnick described The Sopranos as mirroring the "mindless commerce and consumption" of modern America.[159] The series has an overall rating of 92 percent on Rotten Tomatoes,[160] and 94 out of 100 on Metacritic.[161]

The Sopranos has been called "perhaps the greatest pop-culture masterpiece of its day" by Vanity Fair contributor Peter Biskind.[26] Remnick called the show "the richest achievement in the history of television."[159]
In 2002, TV Guide ranked The Sopranos fifth on their list of the "Top 50 TV Shows of All Time",[162] while the series was only in its fourth season. In 2007, Channel 4 (UK) named The Sopranos the greatest television series of all time.[163]

The first season of the series received overwhelmingly positive reviews.[137] Following its initial airing in 1999, The New York Times stated, "[The Sopranos] just may be the greatest work of American popular culture of the last quarter century."[30] In 2007, Roger Holland of PopMatters wrote, "the debut season of The Sopranos remains the crowning achievement of American television."[164]

Time Out New York's Andrew Johnston had high praise for the series, stating: "Together, Chase and his fellow writers (including Terence Winter and Mad Men creator Matthew Weiner) produced the legendary Great American Novel, and it's 86 episodes long."[165] Johnston asserted the preeminence of The Sopranos as opposed to The Wire and Deadwood in a debate with television critics Alan Sepinwall and Matt Zoller Seitz,[166] both of whom would later include The Sopranos in their 2016 book titled TV (The Book) as the 2nd greatest American television series of all time, behind only The Simpsons and ahead of The Wire, with Seitz considering the show's ending to be the greatest ending for any television show.[167]

In November and December 2009, many television critics named The Sopranos the best series of the decade and all time in articles summarizing the decade in television. In numbered lists over the best television programs, The Sopranos frequently ranked first or second, almost always competing with The Wire.[153] In 2013, TV Guide ranked The Sopranos No. 2 in its list of The 60 Greatest Dramas of All Time,[168] In the same year, the Writers Guild of America named it the best-written television series of all time[169] and TV Guide ranked it as the greatest show of all time.[13]

A 2015 The Hollywood Reporter survey of 2,800 actors, producers, directors, and other industry people named The Sopranos as their #6 favorite show.[170] In 2016 and 2022, Rolling Stone ranked it first on the magazine's list of 100 Greatest TV Shows of All Time.[9][171] In September 2019, The Guardian ranked the show first on its list of the 100 best TV shows of the 21st century, stating that it "hastened TV's transformation into a medium where intelligence, experimentation and depth were treasured" and describing it as "something to aspire to" for anyone currently making TV.[172] In 2021, Empire ranked The Sopranos at number one on their list of The 100 Greatest TV Shows of All Time.[173] In 2023, Variety ranked The Sopranos #3 on its own list of the 100 greatest TV shows of all time.[11]

Certain episodes have frequently been singled out by critics as the show's best. These include the pilot, titled "The Sopranos", "College" and "I Dream of Jeannie Cusamano" of the first season; "The Knight in White Satin Armor" and "Funhouse" of the second; "Employee of the Month", "Pine Barrens" and "Amour Fou" of the third; "Whoever Did This" and "Whitecaps" of the fourth; "Irregular Around the Margins" and "Long Term Parking" of the fifth and "Members Only", "Join the Club", "Kennedy and Heidi", "The Second Coming", "The Blue Comet" and "Made in America" of the sixth season.[174][175][176][177][178][179][180]

Chase's decision to end the last episode abruptly with just a black screen was controversial. While Chase has insisted that it was not his intention to stir controversy, the ambiguity over the ending and question of whether Tony was murdered has continued for years after the finale's original broadcast and has spawned numerous websites devoted to finding out his true intention.[181][182][183]

The Sopranos won and was nominated for many awards throughout its original broadcast. It was nominated for the Primetime Emmy Award for Outstanding Drama Series in every year it was eligible and is the first cable TV series to receive a nomination for the award. After being nominated for and losing the award in 1999, 2000, 2001, and 2003 (losing the first time to The Practice and the last three to The West Wing), The Sopranos won the award in 2004, and again in 2007. Its 2004 win made The Sopranos the first series on a cable network to win the award,[184] while its 2007 win made the show the first drama series since Upstairs, Downstairs in 1977 to win the award after it had finished airing.[185] The show earned 21 nominations for Outstanding Writing for a Drama Series and won the award six times, with creator David Chase receiving three awards.[186] The Sopranos won American Film Institute's Drama Series of the Year Award in 2001.[187]

The Sopranos won at least one Emmy Award for acting in every eligible year except 2006 and 2007. James Gandolfini and Edie Falco were each nominated six times for Outstanding Lead Actor and Actress, respectively, both winning a total of three awards. Joe Pantoliano won an Emmy for Outstanding Supporting Actor in 2003, and Michael Imperioli and Drea de Matteo also won Emmys in 2004 for their supporting roles on the show. Other actors who have received Emmy nominations for the series include Lorraine Bracco (in the Lead Actress and Supporting Actress categories), Dominic Chianese, Nancy Marchand, Aida Turturro, Tim Daly, John Heard, Annabella Sciorra and Steve Buscemi, who was also nominated for directing the episode "Pine Barrens".[186]

In 1999 and 2000, The Sopranos earned two consecutive George Foster Peabody Awards.[188][189] Only two other series have won the award in consecutive years: Northern Exposure (1991 and 1992) and The West Wing (1999 and 2000).[190] The show also received numerous nominations at the Golden Globe Awards (winning the award for Best Drama Series in 2000)[191] and the major guild awards (Directors,[192] Producers,[193] Writers,[194] and Actors).[195]

In 2001, the American Psychoanalytic Association presented the producers and writers with an award for "the artistic depiction of psychoanalysis and psychoanalytic psychotherapy" and also presented Lorraine Bracco with an award for creating "the most credible psychoanalyst ever to appear in the cinema or on television."[196]

The Sopranos has been characterized by critics as one of the most influential artistic works of the 2000s and has been cited as helping to turn serial television into a legitimate art form on the same level as feature films, literature, and theater.[79][152][197] Time Magazine editor James Poniewozik wrote in 2007, "This mafia saga showed just how complex and involving TV storytelling could be, inspiring an explosion of ambitious dramas on cable and off."[152]

Maureen Ryan of PopMatters described The Sopranos as the most influential television drama ever. "No one-hour drama series has had a bigger impact on how stories are told on the small screen, or more influence on what kind of fare we've been offered by an ever-growing array of television networks."[79]

Hal Boedeker stated in PopMatters in 2007 that the series was "widely influential for revealing that cable would accommodate complex series about dark characters. The Sopranos ushered in Six Feet Under, The Shield, Rescue Me, and Big Love."[197] Breaking Bad creator Vince Gilligan said in 2013 shortly after Gandolfini's death, "Without Tony Soprano, there would be no Walter White."[198]

Weiner said that when he became a writer for The Sopranos after having written the Mad Men pilot, "Whatever I had intended [Mad Men] to be ... was very different after seeing how seriously David Chase took human behavior. Real human behavior", giving "Maidenform" and how Peggy Olson's baby affects her as examples.[199]

The series helped establish HBO as producers of critically acclaimed and commercially successful original television series. Michael Flaherty of The Hollywood Reporter has stated that The Sopranos "helped launch [HBO's] reputation as a destination for talent looking for cutting-edge original series work."[41]

The show has frequently been accused of perpetuating negative stereotypes about Italian Americans. Several major organizations have voiced their concern that The Sopranos presents a very distorted and harmful stereotype of Italian Americans and their cultural values, including the National Italian American Foundation, Order Sons of Italy in America, Unico National, and the Italic Institute of America.[200][201][202]

The show had a strong female cast and expressions of feminism during the six seasons. The creators challenge the traditional gender roles in society. Around the third season of the show, the creators were pushing the violence against women, highlighting the show's impact on viewers and the ongoing debates surrounding its depiction of feminist issues. The show uses the Dr. Melfi rape episode to show the internal strength of the female perspective on the show, by not asking Tony for help to seek revenge.[203]

In 2000, officials in Essex County, New Jersey, denied producers permission to film scenes in the South Mountain Reservation, which is county-owned property, by Essex County, New Jersey Executive James Treffinger, who argued that the show depicts Italian Americans "in stereotypical fashion".[204] In 2002, organizers of the New York City Columbus Day Parade won an injunction preventing Mayor Michael Bloomberg from inviting cast members of The Sopranos to participate in the parade.[205]

Fairleigh Dickinson University's PublicMind conducted a national survey in August 2001 that polled 800 people, out of which 37% said that they watched the show regularly, and 65% of this group (192 people, or 24% of the total) disagreed that the show negatively portrayed Italian Americans. Professor William Roberts, who was associated with the poll, said that "The show's inflated image of organized crime casts a shadow over both the state [of New Jersey] and its Italian American community."[206]

He further stated "The show helped to perpetuate one of the more problematic and stereotypical images of Italian Americans. Both Italian and Italian American cultures have much more diverse and interesting heritages than the American public generally realizes."[207] Humanities professor Camille Paglia, herself an Italian American, has spoken negatively about The Sopranos, arguing that its depiction of Italian Americans was inaccurate, inauthentic, and dated.[208]

Chase has defended his show, saying that "It is not meant to stereotype all Italian Americans, only to depict a small criminal subculture".[209]

Actors from The Sopranos have reprised their roles, or at the very least parodied their roles, in various other media. Tony Sirico and Steve Schirripa appear in two separate Muppet-related Christmas specials, A Muppets Christmas: Letters to Santa and Elmo's Christmas Countdown, parodying their roles on The Sopranos. Sirico also appeared in a series of commercials for Denny's in-character as Paulie Gualtieri, a nod to the restaurant chain's mention in "Pine Barrens".[210] James Gandolfini appeared on Weekend Update as a "New Jersey Resident" on the October 2, 2004, episode of Saturday Night Live to comment on the recent resignation of New Jersey governor Jim McGreevey. Gandolfini's character went unnamed, and hosts Tina Fey and Amy Poehler insisted at the segment's conclusion that he was "unidentified", but the character was clearly meant to be Tony Soprano.[211]

Jamie-Lynn Sigler and Robert Iler reprised their roles as Meadow and A.J. Soprano in a Chevrolet television commercial initially broadcast in 2022 during Super Bowl LVI. David Chase directed the commercial and treated it as a continuation of The Sopranos story. At Chase's insistence, former Sopranos director of photography Phil Abraham performed the filming.[212] The ad recreates the opening-title sequence of The Sopranos, with Meadow driving a Silverado EV (as opposed to Tony's Chevrolet Suburban) and meeting A.J. at Bahrs Landing, featured in The Many Saints of Newark. Along the way, she passes some Sopranos landmarks including Satriale's. Chase wanted the commercial to continue the intrigue surrounding The Sopranos finale: besides the visual allusion to the episode with Meadow's parking, Chase intentionally left open why Meadow and A.J. were at the restaurant and who they could be meeting there.[212]

The first four seasons of The Sopranos were released on VHS in five-volume box sets which lack bonus material.[213][214][215]

All six Sopranos seasons were released as DVD box sets, with the sixth season released in two parts. A complete series box set was released in 2008.

The sixth season was released on Blu-ray Disc and HD DVD in 2006 and 2007. The first season was released on Blu-ray in 2009.[216] A complete series box set was released in 2014.[217]

October 1, 2014(Blu-ray)

Three companion books, written by Allen Rucker, were published during The Sopranos' run:

On September 17, 2020, Michael Imperioli and Steve Schirripa signed a deal with HarperCollins book imprint William Morrow and Company to write an oral history of the show;[230] the book, titled Woke Up This Morning: The Definitive Oral History of The Sopranos, was released on November 2, 2021.[231]

Two official soundtrack compilations were released featuring music used in The Sopranos:

A video game based on the series, titled The Sopranos: Road to Respect, was developed by 7 Studios and released by THQ for the PlayStation 2 in November 2006. The game features the voices and likenesses of key Sopranos cast members.[234]

In 2005, Stern Pinball released a Sopranos pinball machine designed by George Gomez.[235][236]

Several cast members of The Sopranos have started podcasts regarding the series. Michael Imperioli and Steve Schirripa began hosting a podcast called Talking Sopranos on April 6, 2020, where the two provide inside info as they follow The Sopranos series episode-by-episode and interview cast and crew from the series.[237][238] By September 2020, the podcast had reached over five million downloads.[230] In May 2021, the podcast won a Webby Award for Best Television & Film Podcast by method of "People's Voice Winner".[239]

Drea de Matteo and Chris Kushner began hosting a re-watch podcast on March 13, 2020, called Made Women;[240] in July, the podcast was retooled and renamed Gangster Goddess Broad-Cast.[241]

In March 2018, New Line Cinema announced that they had purchased a film detailing The Sopranos background story, set in the 1960s and '70s during, and in the wake of, the Newark riots. The 2021 film, The Many Saints of Newark, was written by David Chase and Lawrence Konner and directed by Alan Taylor.[22][23] Alessandro Nivola was cast in the film as Christopher Moltisanti's father Dickie, and Michael Gandolfini, James Gandolfini's son, as the younger version of Tony Soprano.[24][242] Vera Farmiga, Jon Bernthal, Ray Liotta, Corey Stoll, Billy Magnussen and John Magaro are other cast members.[243][244][245]

The film was initially scheduled to be released on September 25, 2020,[246] however, the film's release was delayed multiple times due to the COVID-19 pandemic in the United States; it was released on October 1, 2021, in theaters and on HBO Max.[247]

Chase has expressed interest in producing a sequel to The Many Saints of Newark that follows Tony Soprano in his 20s, provided he could collaborate with former Sopranos writer Terence Winter.[248] Upon hearing this, Winter replied he would do it "in a heartbeat. Absolutely."[249]Titanfall is a media franchise that mainly features first-person shooter games. The series was created by Respawn Entertainment and debuted on Xbox and Microsoft Windows; it has expanded to other consoles and platforms.

In Titanfall, players control "Pilots" and their mech-style Titans, and fight in six-on-six matches set in the war-torn outer space colonies of the Frontier. The game is optimized for fast-paced, continual action, aided by wall-running abilities and populations of computer-controlled soldiers. Other titles in the series includes unique characters who are able to use special abilities.

Titanfall, the first game in the series, was released for Xbox One and Microsoft Windows on March 11, 2014. On April 8, 2014, it released for Xbox 360. The game was mainly multiplayer focused with no real single-player campaign included. Instead, there was a single-player tutorial included that served as a way for the player to learn the mechanics of the game.

Titanfall 2 was released on October 28, 2016, for the Xbox One, PlayStation 4, and Microsoft Windows. This time, the game included a single-player campaign with a full-fledged story. Many of the mechanics from Titanfall returned, including maps in multiplayer. The game received several free updates after its launch, including a returning multiplayer mode known as Frontier Defense. The level "Effect and Cause" in particular was well received.

Titanfall: Assault was a top-down real-time strategy game for mobile platforms in the style of Clash Royale. It was developed by Particle City and Respawn Entertainment, published by Nexon, and released for iOS and Android[1] in August 2017.[2][3] All servers for Titanfall: Assault were shut down on July 30, 2018.[4] On July 31, 2018, Titanfall: Assault was removed from Google Play shortly after the servers were shut down.

Apex Legends is a battle royale game that features hero shooter mechanics and the first major platform spin-off for the series. While the game is not directly tied to Titanfall, many of its assets and gameplay features were based on Titanfall 2. While Apex Legends takes place in the Titanfall universe, the games do not run parallel to each other.[a][5] The game was released in 2019 for the Xbox One, PlayStation 4, and Microsoft Windows, and in 2021 for Nintendo Switch as a free-to-play game. The game was met with positive critical reception.[6] The game holds the record for the most players signed up in 1 week at 25 million.[7] The game currently has a playerbase of 100 million people.[8] The mobile game version called Apex Legends Mobile was released on May 17, 2022 [9]

Titanfall: Frontline was a collectible card game that was played in real-time. The player collects and places Pilot, Titan, and burn cards to damage and defeat their opponent. Pilot and Titan cards can combine to perform extra damage.[10] In January 2017, Titanfall: Frontline was cancelled.[11]

In 2016, EA announced that it was partnering with Nexon to create an Asian market-specific version of Titanfall called Titanfall Online, similar to Counter-Strike Online and Call of Duty Online.[12] This version was based on the first Titanfall rather than its sequel, with some slight differences like four main pilots in the game, the introduction of a new titan, and a new map. Titanfall Online had a closed beta in 2017.[13] Titanfall Online was cancelled on July 9, 2018, primarily due to poor reception during testing and a changing market.[14]

On 2 February 2023, rumored single-player Titanfall and Apex Legends game codenamed TFL (short for Titanfall Legends) was cancelled prior to any public announcement.[15]

A Titanfall project codenamed "R7" was cancelled in April 2025. According to people familiar with the project, the game had been in an early stage, and was developed by Respawn Entertainment.[16]

Titanfall is the first game developed by Respawn Entertainment, a developer founded by Jason West and Vince Zampella. As ex-employees of Infinity Ward, they helped create the Call of Duty franchise. The two were fired after contract disputes.[17][18]