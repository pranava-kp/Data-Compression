The Simpsons is an American animated television sitcom starring the animated Simpson family, which was created by Matt Groening. He conceived of the characters in the lobby of James L. Brooks's office and named them after his own family members, substituting "Bart" for his own name. The family debuted as shorts on The Tracey Ullman Show on April 19, 1987. After a three-season run, the sketch was developed into a half-hour prime time show called The Simpsons, which debuted on December 17, 1989. The show was an early hit for Fox, becoming the first Fox series to land in the top 30 ratings in a season (1990).[1]

The show was controversial from its beginning and has made the news several times. In the early seasons, some parents characterized Bart as a poor role model for children and several United States public schools even banned The Simpsons merchandise and T-shirts. In January 1992, then-President George H. W. Bush made a speech during his re-election campaign in which he said: "We are going to keep on trying to strengthen the American family, to make American families a lot more like the Waltons and a lot less like the Simpsons." In 2002, the show was nearly sued by the Rio de Janeiro tourist board for creating an unreal image of the city on the show.

The Simpsons Movie, a feature-length film, was released in theaters worldwide on July 26 and 27, 2007. Previous attempts to create a film version of The Simpsons failed due to the lack of a script of appropriate length and production crew members. Eventually, producers Brooks, Groening, Al Jean, Mike Scully, and Richard Sakai began development of the film in 2001. They conceived numerous plot ideas, with Groening's being the one developed into a film. The script was re-written over a hundred times, and this creativity continued after animation had begun in 2006. The film was a box office success, and received overwhelmingly positive reviews.

The Simpsons eventually became the longest-running American sitcom, the longest-running American animated program, and in 2009 it surpassed Gunsmoke as the longest-running American primetime, scripted television series.[2] Since its debut on December 17, 1989, the show has broadcast 788 episodes and its 35th season began airing on October 1, 2023.

When producer James L. Brooks was working on the television variety show The Tracey Ullman Show for the fledgling Fox network, he decided that he wanted to include small animated sketches before and after the commercial breaks. After Polly Platt gifted James L. Brooks the original "The Los Angeles Way of Death" comic from cartoonist Matt Groening's Life in Hell comic strips,[3] Brooks asked Groening to pitch an idea for a series of animated shorts, which Groening initially intended to present as his Life in Hell series.[4] Groening later realized that animating Life in Hell would require the rescinding of publication rights for his life's work, and passed on the offer. Richard Sakai contacted Groening to see if he had any other characters he would be willing to let Fox merchandise, and, in short, Groening developed a dysfunctional family that became the Simpsons. Garth Ancier contends that Groening created the characters at home and brought them the next day, while Phil Roman claims Groening sketched out the characters on the drive to Fox.[5] Groening's account states he hurriedly formulated The Simpsons while waiting in the lobby of Brooks's office for the pitch meeting, which is the most common and famous story.[4][6] He named the characters after his own family members, substituting "Bart" for his own name, adapting an anagram of the word "brat".[4]

Fox negotiated a deal which would prove extremely lucrative for Groening, in which he retained a large portion of revenue from merchandising.[7] To animate the short segments, Brooks and company settled on Klasky Csupo, a small animation house who offered to produce the cartoons cheaply. Brooks initially just wanted to animate the shorts through the basic line drawings, and Klasky-Csupo offered color for the same cost. The studio only employed three young animators—CalArts graduates Bill Kopp, Wes Archer, and David Silverman—who adapted Groening's scripts for animation in one week, doing layouts, animation and inbetweening by hand in a very short amount of time.[8][9] Groening submitted only basic sketches to the three,[10] and assumed that the figures would be cleaned-up in production. However, the animators merely re-traced his drawings, which led to the crude appearance of the characters in the initial short episodes.[4] Colorist Gyorgyi Peluce was the person who decided to make the characters yellow.[10]

Appearing initially alongside cartoons by M. K. Brown, the Simpson family first appeared in short subjects in The Tracey Ullman Show on April 19, 1987, and were featured the first three seasons.[11] The actors who voiced the characters would later reprise their roles in The Simpsons. Dan Castellaneta, a Tracey Ullman cast member, performed the voices of Homer Simpson, Abraham Simpson, and Krusty the Clown.[12] Homer's voice in the shorts is a loose impression of Walter Matthau, whereas it became more robust and humorous on the half-hour show, allowing Homer to cover a fuller range of emotions.[13] Julie Kavner (another Tracey Ullman cast member), Nancy Cartwright, and Yeardley Smith performed the voices of Marge Simpson, Bart Simpson, and Lisa Simpson respectively.[12] The crew began to string the clips together on tape to play for the show's live audience, and The Simpsons generated "the biggest laughs of the show" according to John Ortved, author of The Simpsons: An Uncensored, Unauthorized History.[8] The writing staff of Ullman soon began to view The Simpsons as poor relations due to the popularity, and Brooks began to consider adapting the shorts for its own half-hour series. Brooks' decision was partly inspired by the cheerleading of David Silverman, who drunkenly approached him at a Christmas party and suggested the idea, passionately emphasizing what a primetime series would mean for the animation industry.[8]

In 1989, a team of production companies adapted The Simpsons into a half-hour series for the Fox Broadcasting Company. The team included what is now the Klasky Csupo animation house. Due to the increased workload of the full-length episodes, production was subcontracted to South Korean animation studio AKOM.[9] While character and background layout is done by the domestic studio, tweening, coloring and filming is done by the overseas studio.[9]

The Simpsons was co-developed by Groening, Brooks, and Sam Simon, a writer-producer with whom Brooks had worked on previous projects. Groening said his goal was to offer the audience an alternative to what he called "the mainstream trash".[14] Brooks negotiated a provision in the contract with the Fox network that prevented Fox from interfering with the show's content.[15] The Fox network was unsure if the show could sustain the audience's attention for the duration of the episode.[16] They proposed doing three seven-minute shorts per episode and four specials until the audience adjusted,[16] but the producers gambled by asking Fox for 13 full-length episodes.[17]

Simon assembled and led the initial team of writers,[18][19] and has been credited as "developing [the show's] sensibility".[20] Writer Ken Levine says he "brought a level of honesty to the characters" and made them "three-dimensional", adding that Simon's "comedy is all about character, not just a string of gags".[20] Simon saw The Simpsons as a chance to solve what he did not like about Saturday-morning cartoon shows. He wanted all the actors in a room together, instead of reading their lines separated from each other.[20] In addition to Castellaneta, Kavner, Cartwright and Smith, actors Harry Shearer and Hank Azaria were added to the show's cast.[21][22]

Groening developed a lengthy opening sequence to cut down on the animation necessary for each episode, but devised two gags as compensation for the repeated material each week.[23] In the first gag, the camera zooms in on Springfield Elementary School, where Bart can be seen writing a message on the chalkboard. This message, which changes from episode to episode, has become known as the "chalkboard gag".[24] The other gag is known as a "couch gag", in which a twist of events occur when the family meets to sit on their couch and watch television.[24] Groening, who had not paid much attention to television since childhood, was unaware that title sequences of such length were uncommon by that time.[23] The theme, which plays over the sequence, was composed by Danny Elfman in 1989, after Groening approached him requesting a "retro-style" theme. The piece, which took two days to create, has been noted by Elfman as the most popular of his career.[25]

The half-hour series premiered on December 17, 1989, with "Simpsons Roasting on an Open Fire".[26] The series was originally set to debut in the fall of 1989 with the episode "Some Enchanted Evening",[16] but the producers discovered that the animation was so appalling that 70 percent of the episode needed to be redone.[27] At the time there were only a few choices for animation style; usually, they would follow the style of Disney, Warner Bros., or Hanna-Barbera.[16] The producers wanted a realistic environment in which the characters and objects could not do anything that was not possible in the real world.[16] They considered aborting the series if the next episode "Bart the Genius" turned out as bad, but it only suffered from easily fixable problems. The debut was moved to December, and "Simpsons Roasting on an Open Fire" became the first episode of the series.[16] In some of the episodes of the first season, the characters act completely differently from in later seasons; Lisa, for example, is undisciplined and short-tempered, while Homer is the voice of reason; these roles are reversed in later episodes.[28]

During the second season, The Simpsons aired the first Halloween special, "Treehouse of Horror".[29] The annual series typically consist of four parts: an opening and Halloween-themed version of the credits, followed by three segments. These segments usually have a horror, science fiction or supernatural theme and quite often are parodies of films, novels, plays, television shows, Twilight Zone episodes, or old issues of EC Comics.[30] Part of the attraction for the writers is that they are able to break the rules and include violence that would not make a regular episode.[31] In some cases, the writers will have an idea that is too violent and far-fetched or too short for a normal episode, but can be used as a segment in the seasonal special.[29] The first "Treehouse of Horror" episode was the first time that an alternate version of the theme airs over the end credits.[31]

The show was controversial from its beginning. The rebellious lead character at the time, Bart, frequently received no punishment for his misbehavior, which led some parents to characterize him as a poor role model for children.[32][33] Several US public schools banned The Simpsons merchandise and T-shirts, such as one featuring Bart and the caption "Underachiever ('And proud of it, man!')".[34] In the season two opening episode "Bart Gets an 'F'", Bart fails four consecutive history exams and the school psychiatrist recommends that Bart repeat the fourth grade.[35] Several critics thought that the episode "Bart Gets an 'F'" was a response to these controversies.[36][37] However, Brooks denied that it was a response and added, "we're mindful of it. I do think it's important for us that Bart does badly in school. There are students like that. Besides, I'm very wary of television where everybody is supposed to be a role model. You don't run across that many role models in real life. Why should television be full of them?"[38]

In the October 1, 1990, edition of People, First Lady Barbara Bush called The Simpsons "the dumbest thing [she] had ever seen" which led to the writers sending a letter to Bush posing as Marge Simpson. Bush immediately sent a reply in which she apologized.[39] A few years later, on January 27, 1992, then-President of the United States George H. W. Bush made a speech during his re-election campaign where he said, "We are going to keep on trying to strengthen the American family, to make American families a lot more like the Waltons and a lot less like the Simpsons."[39] The writers decided that they wanted to respond by adding a response to the next broadcast of The Simpsons, which was a rerun of "Stark Raving Dad" on January 30. The broadcast included a new tongue-in-cheek opening where they watch Bush's speech. Bart replies, "Hey, we're just like the Waltons. We're praying for an end to the Depression, too".[40][41] The criticism eventually led to the idea for the episode "Two Bad Neighbors", which has George and Barbara move into the house across the street from the Simpsons.[42]

The Simpsons first season was the Fox network's first TV series to rank among a season's top 30 highest-rated shows.[43] Due to its success, the Fox network decided to switch The Simpsons timeslots in hopes that it would result in higher ratings for the lead out shows.[44] It would move from 8:00 PM on Sunday night to the same time on Thursday where it would compete with The Cosby Show, the number one show at the time.[45] Many of the producers were against the move, as The Simpsons had been in the top 10 while airing on Sunday and they felt the move would destroy its ratings.[46]

"Bart Gets an 'F'" was the first episode to air against The Cosby Show and averaged an 18.4 Nielsen rating and 29% of the audience. In the weeks ratings, it finished tied for eighth behind The Cosby Show which had an 18.5 rating. However, an estimated 33.6 million viewers watched the episode, making it the number one show in terms of actual viewers that week. At the time, it was the most watched episode in the history of the Fox Network and still remains the most watched episode in the history of The Simpsons.[47] Ratings wise, new episodes of The Cosby Show beat The Simpsons every time during the second season and The Simpsons eventually fell out of the top 10.[48] At the end of the season Cosby averaged as the fifth highest rated show on television while The Simpsons ranked 38th.[44] It would not be until the third-season episode "Homer at the Bat" that The Simpsons would beat The Cosby Show in the ratings.[48] The show remained in its Thursday timeslot until the sixth season, when it moved back to its original timeslot on Sundays.[45]

David Geffen, founder of Geffen Records, had the idea to record the album The Simpsons Sing the Blues based on The Simpsons, to be released in time for Christmas 1990.[49] The writers wrote humorous lyrics for the actors to perform over blues and hip hop.[50] The album faced great publicity before its release. One particular element that was highly publicized was Michael Jackson's involvement, which was denied around the time of the album's release.[50] Early published reports attributed Jackson as the composer of "Do the Bartman", which Groening denied in a press release.[50] However, Groening revealed in 1998 that "Do the Bartman" was actually co-written and co-produced by Jackson,[51][52] but he could not receive credit for it because he was under contract to another record label.[53] Jackson was a fan of The Simpsons, especially Bart,[54] and had called the producers one night offering to write Bart a number one single and do a guest spot on the show, which is how "Do the Bartman" came about.[55] Jackson eventually guest-starred in the episode "Stark Raving Dad".[56]

The album The Simpsons Sing the Blues was certified triple platinum by the Recording Industry Association of America for sales of over 3 million copies.[57] The producers followed up on the album with The Yellow Album in 1998, which featured original recordings by Prince, Linda Ronstadt, C+C Music Factory, and George Clinton of Funkadelic as well as the cast of The Simpsons.[58] The soundtrack albums Songs in the Key of Springfield (1997),[59] Go Simpsonic with The Simpsons (1999),[60] The Simpsons Movie: The Music (2007),[61] and The Simpsons: Testify (2007)[62] were also released.

Although they initially worked well together, Simon and Groening's relationship became "very contentious" according to Groening.[63] According to John Ortved's book The Simpsons: An Uncensored, Unauthorized History, Simon resented the media attention Groening received, particularly the praise for the show's writing; Simon felt that Groening's involvement was limited, and that he should have been the one receiving credit for the show.[64] As well as Groening, Simon was often at odds with Brooks and production company Gracie Films and left the show in 1993.[65][66] Before leaving, he negotiated a deal that saw him receive a share of the show's profits every year, and an executive producer credit despite not having worked on the show since.[20][65] Al Jean and Mike Reiss, who had written for The Simpsons since the start of the show, took over as showrunners for the third season.[67] Compared to being an executive producer, the showrunner position is more involved with the show and acts as head writer and manages the show's production for an entire season.[10] As well as a turnover in the staff, The Simpsons moved the production of the animation from Klasky Csupo to Film Roman in season four.[68]

During the fourth season the episode "A Streetcar Named Marge" was produced. The musical within the episode contains a controversial song about New Orleans, which describes the city as a "home of pirates, drunks and whores", among other things. Jeff Martin, the writer of the episode, had meant the song to be a parody of the opening number in Sweeney Todd: The Demon Barber of Fleet Street, which speaks of London in unflattering terms.[69] A New Orleans critic viewed "A Streetcar Named Marge" and published the song lyrics in his newspaper before the episode aired.[70] Many readers took the lyrics out of context, and New Orleans' Fox affiliate, WNOL, received about one hundred complaints on the day the episode aired. Several local radio stations also held on-air protests in response to the song.[71] The Simpsons' producers rushed out an apologetic chalkboard gag for "Homer the Heretic", which aired a week after "A Streetcar Named Marge". It read, "I will not defame New Orleans".[69]

Ullman filed a lawsuit in 1992, claiming that her show was the source of The Simpsons' success and therefore should receive a share of the show's profit. "I breast-fed those little devils," Ullman once said of The Simpsons. She wanted a share of The Simpsons' merchandising and gross profits and believed she was entitled to $2.5 million of Fox's estimated $50 million in 1992. The Fox network had paid her $58,000 in royalties for The Simpsons as well as $3 million for the 3½ seasons her show was on the air. Eventually the courts ruled in favor of the network.[72][73]

Several of the show's original writers who had worked on The Simpsons since the first season had left following the completion of season four. David Mirkin took over as showrunner and executive producer for the fifth and sixth season.[74] In The Simpsons: An Uncensored, Unauthorized History (2009), John Ortved describes Mirkin as an "outsider" on the show as, unlike the bulk of the writing staff, Mirkin was not a Harvard University graduate.[75] The writing staff were, at least initially, divided on Mirkin's abilities as a leader.[76] Mirkin conducted the show's writing sessions in one room, rather than splitting the writers into two groups as other showrunners had done, and often worked late into the night.[76] Writer Richard Appel praised Mirkin's leadership and comedy style, saying that "the shows were great under him."[76] In contrast to much of Ortved's account, in a 2004 interview with Animation Magazine, Mirkin stated that he "really wasn't at all intimidat[ed] to join [the show's writing] crew," because he "had worked with and written with" many of his fellow writers previously.[77]

Mirkin said that he "brought [the show] back to a more story-oriented" approach and increased the character and emotion focus, while "at the same time still keeping it surreal and weird".[77] During his tenure, Mirkin moved the show's focus towards Homer, and developed some of the secondary characters, such as Apu.[77][78] He also strongly opposed censorship and network interference.[79] Mirkin's era and style of humor are popular amongst the show's fans,[78] but the writing staff were divided on his style of humor, which saw the show move away from more "realistic" emotional and character based stories to "pure comedy" and "surreal" humor.[80] The episode "Deep Space Homer" was controversial when the episode was in production. Some of the writers felt that having Homer go into space was too "large" of an idea and Groening felt that the idea was so big that it gave the writers "nowhere to go".[81]

The writing staff wanted to do an episode where the Simpsons family traveled to Australia.[82] They had previously poked fun at several American institutions and thought it would be interesting to poke fun at a whole nation.[83] They purposefully designed Australia and the Australian people very inaccurately and many things were completely made up for fun.[84] The episode "Bart vs. Australia" received a mixed reception in Australia, with some Australian fans saying the episode was a mockery of their country. Shortly after it had aired, the Simpsons staff received over 100 letters from Australians who were insulted by the episode.[83] Reiss claimed that this episode is Australia's least favorite, and that "whenever we have the Simpsons visit another country, that country gets furious, including Australia". He also claimed that they were "condemned in the Australian Parliament after the episode had aired".[85] However, it has been accepted as typical American satire and laughed off.

Former showrunners Jean and Reiss had left to produce their own series, The Critic,[74] along with The Simpsons co-creator Brooks.[86] The Critic was an animated series that revolved around the life of movie critic Jay Sherman.[86] For the second season of The Critic, Brooks cut a deal with the Fox network to have the series switch over.[87] The episode "A Star Is Burns" was pitched by Brooks, who had wanted a crossover that would help launch The Critic on Fox, and he thought having a film festival in Springfield would be a good way to introduce Sherman.[88] In addition, Jean and Reiss returned to produce two episodes ("A Star is Burns" and "'Round Springfield") with the staff of The Critic, to relieve some of the stress on The Simpsons' writing staff.[89][90] Groening felt that the crossover was a thirty-minute advertisement for another show and blamed Brooks, calling it an attempt to get attention for one of his unsuccessful shows. After unsuccessful attempts to get the episode pulled, he decided to go public with his concerns shortly before the episode aired and had his name removed from the credits.[91] In response, Brooks said, "for years, Al [Jean] and Mike [Reiss] were two guys who worked their hearts out on this show, staying up until 4 in the morning to get it right. The point is, Matt's name has been on Mike's and Al's scripts and he has taken plenty of credit for a lot of their great work. In fact, he is the direct beneficiary of their work. 'The Critic' is their shot and he should be giving them his support."[91]

Groening conceived the idea of an episode in which the character Mr. Burns was shot, which could be used as a publicity stunt.[92] The writers decided to write the episode "Who Shot Mr. Burns?" in two parts with a mystery that could be used in a contest.[93] Part one was the final episode of the sixth season and originally aired on the Fox network on May 21, 1995.[94] Part two was the premiere of the seventh season and originally aired on September 17, 1995.[95] It was important for the writers to design a mystery that had clues, took advantage of freeze frame technology, and was structured around one character who seemed the obvious culprit.[93] In the months following the broadcast of the first part, there was widespread debate among fans of the series as to who shot Mr. Burns. Fox offered a contest to tie in with the mystery where the viewers could guess who the culprit was.[96] It ran from August 13 to September 10 and was one of the first contests to tie together elements of television and the internet.[97] Fox launched a new website, www.Springfield.com, devoted to the mystery which got over 500,000 hits during the summer of 1995.[96] The winner would be animated on an episode of the show. No one, however, was ever animated on the show. This was because no one officially guessed the right answer, so the chosen winner did not have the right answer and was paid a cash prize in lieu of being animated.[93]

After season six, Mirkin suggested that Bill Oakley and Josh Weinstein take over as showrunners, but remained on the show in an advisory capacity, helping them with technical aspects of the show such as editing and sound mixing, and attending the scripts' table readings.[98][99] Oakley and Weinstein wanted to produce Treehouse of Horror episodes, episodes about Sideshow Bob, Itchy & Scratchy and several "format-bending" episodes such as "22 Short Films About Springfield".[100] They aimed for "at least two episodes per season that 'pushed the envelope', [and] expanded the definition of what an episode could be."[99] Season eight featured several episodes in which focus was given to secondary characters and in which new issues, such as divorce, were explored.[89] Their preferred choice of guest stars were those with unique and interesting voices, and several of their guest stars were "old grizzled men with distinctive voices".[101]

Their goal for the episodes was to be realistic and focus more on the five members of the Simpson family and explore their feelings and emotions towards each other.[102] Oakley considered season three to be the single greatest comedic season of television ever produced and so attempted to recreate the feel of that season,[103] focusing on stories with real emotions and situations, as well as some off-the-wall episodes.[99] Season three was their basis for Homer: "We liked Homer the way he was in the second and third seasons. That was what we consciously used as our model. Dimwitted, loving, hyper-enthusiastic, creatively goofy, parody of the American father – drawn with real emotions, though admittedly amplified."[104]

The script supervisor for the show and voice of the character Lunchlady Doris, Doris Grau, died on December 30, 1995. The episode "Team Homer", which aired eight days later, was one of the last episodes to feature her voice and featured a dedication to her.[105] From season nine until season eighteen, Lunchlady Doris appeared only as a background character. She returned as a speaking character in several episodes since "The Mook, the Chef, the Wife and Her Homer", and is now voiced by Tress MacNeille.[106]

The episode "Lisa the Vegetarian" featured a permanent character development when Lisa becomes a vegetarian. The story had been pitched by David S. Cohen and the producers felt it would be a surefire way to get Paul McCartney to guest star. McCartney agreed, but only on the condition that Lisa would stay a vegetarian and not revert.[107] The trait stayed and is one of the few permanent character changes made in the show.[108][109] In the season 13 episode "She of Little Faith", Lisa underwent another permanent character change when she converted to Buddhism.[110]

On February 9, 1997, The Simpsons surpassed The Flintstones with the episode "The Itchy & Scratchy & Poochie Show" as the longest-running prime-time animated series in the United States.[111] The producers took this milestone and made the episode deal with the issue of longevity and the problems that arise when the producers try to make a show "fresh" again;[112] themes commonly known as "jumping the shark".[113] Alan Sepinwall of The Star-Ledger, in a review printed two days after the episode originally aired, praised the writers for not airing a "very special" episode to celebrate the milestone of overtaking The Flintstones. He noted "[the episode is] so self-aware it put the best in-jokes on St. Elsewhere to shame."[114]

Oakley and Weinstein stood down as showrunners after season eight because they "didn't want to break [the show]"[115] and Mike Scully took over as showrunner in 1997.[116] As showrunner and executive producer, Scully said his aim was to "not wreck the show",[117] Scully was popular with the staff members, many of whom praised his organization and management skills. Writer Tom Martin said he was "quite possibly the best boss I've ever worked for" and "a great manager of people" while writer Don Payne commented that for Scully "it was really important that we kept decent hours".[118] Scully noted: "I wrote a lot of Lisa's shows. I have five daughters, so I like Lisa a lot. I like Homer, too. Homer comes very naturally to me: I don't know if that's a good or a bad thing. A lot of my favorite episodes are the ones when Homer and Lisa are in conflict with each other... They're very human, I think that's their appeal."[117]

Despite this, Scully's tenure as showrunner of The Simpsons has been the subject of criticism from the show's fans.[119] John Ortved wrote "Scully's episodes excel when compared to what The Simpsons airs nowadays, but he was the man at the helm when the ship turned towards the iceberg."[118] The BBC noted "the common consensus is that The Simpsons' golden era ended after season nine",[120] while an op-ed in Slate by Chris Suellentrop argued The Simpsons changed from a realistic show about family life into a typical cartoon during Scully's years.[121] The Simpsons under Scully has been negatively labelled as a "gag-heavy, Homer-centric incarnation" by Jon Bonné of MSNBC,[122] while some fans have bemoaned the transformation in Homer's character during the era, from dumb yet well-meaning to "a boorish, self-aggrandizing oaf",[123] dubbing him "Jerkass Homer".[122][124][125] Martin said that he does not understand the criticism against Scully and that he thinks the criticism "bothered [Scully], and still bothers him, but he managed to not get worked up over it."[126] Ortved noted in his book that it is hard to tell how much of the decline is Scully's fault, and that blaming a single showrunner for lowering the quality of the show "is unfair."[127]

UGO Networks' Brian Tallerico has defended the season against the criticism. He wrote in a 2007 review that comparing "tenth season Simpsons episodes to the prime of the series (3–7) is just unfair and really kind of self-defeating. 'Yeah, I laughed, but not as hard as a couple of years ago. So it sucks.' That's nonsense. The fact is that even the tenth season of The Simpsons was funnier than most [other] show's best years."[128] PopMatters' Hassenger commented in his review that although the show had declined in quality, "this is not to say that these episodes are without their charm; many, in fact, are laugh-out-loud funny and characteristically smart."[129]

On May 28, 1998, Phil Hartman, voice actor of Troy McClure and Lionel Hutz, was murdered by his wife, Brynn Omdahl. In the weeks following his death, Hartman was celebrated in a wave of tributes. Dan Snierson of Entertainment Weekly opined that Hartman was "the last person you'd expect to read about in lurid headlines in your morning paper...a decidedly regular guy, beloved by everyone he worked with".[130] After Hartman's death in 1998, rather than replace him with a new voice actor, the production staff retired McClure and Hutz from the show.[6] McClure last appeared in the season ten episode "Bart the Mother", which was dedicated to Hartman.[131]

In the season 10 episode "Thirty Minutes over Tokyo", the family travels to Japan.[132] The episode references and mocks several aspects of Japanese and American culture, as well as differences between the two. At a sumo wrestling match, Bart and Homer encounter the Japanese emperor, Akihito. After Homer throws him into a trunk of sumo thongs, Bart and Homer are put in jail, where they have to re-enact a kabuki play about the forty-seven Ronin, do origami, flower arranging and meditation. The episode also references the Japanese's adaption to American culture.[133] Although all other episodes of The Simpsons have been dubbed and broadcast on Japanese television, "Thirty Minutes Over Tokyo" has never aired in Japan. The episode, which contains a scene showing Homer throwing the Emperor of Japan into a box filled with sumo thongs, was considered disrespectful.[134]

Up until the production of season ten in 1998, the six main voice actors were paid $30,000 per episode. In 1998, a salary dispute between them and the Fox Broadcasting Company arose, with the actors threatening to strike.[135] Fox went as far as preparing for casting of new voices, but an agreement was soon made and their salaries were raised to $125,000 per episode.[135] Groening expressed his sympathy for the actors in an issue of Mother Jones a while after the salary dispute had been settled. He told the magazine: "[The cast members] are incredibly talented, and they deserve a chance to be as rich and miserable as anyone else in Hollywood."[136] The show also made a change for the writers to become covered under a Writers Guild of America (WGA) agreement. Most writers on primetime series television belong to the WGA, but The Simpsons as well as other animated shows on Fox were different. Scully commented that "everyone expected a big fight with the studio" and continued that "it never materialized, because they conceded that prime-time animation was successful and everyone was benefiting."[137]

Voice actress Maggie Roswell left The Simpsons in spring 1999 after a pay dispute with Fox.[138][139] The network originally reported that she decided to quit only because she was tired of flying between Denver and Los Angeles for the recording sessions.[140][141] It was then announced by Roswell that she had asked for a raise, not only because she was tired of the traveling, but because of the increasing cost of flight tickets.[142] Roswell was paid $1,500 to $2,000 per episode during the three seasons before she left, and she asked Fox for a raise to $6,000 per episode. However, Fox only offered her a $150 raise, which did not even cover the travel costs, so she decided to quit.[143] As a result of Roswell's departure, the Maude Flanders character was killed off in the episode "Alone Again, Natura-Diddily".[144][145] Voice actress Marcia Mitzman Gaven was hired to fill in for Roswell's other characters.[146] Roswell returned to The Simpsons in 2002 in the season premiere of the fourteenth season.[147][148] She reached a deal with Fox to record her lines from her Denver home[149] and thus the dispute ended.[147]

Jean returned full-time to The Simpsons during the tenth season.[150] He once again became showrunner with the start of the thirteenth season in 2001,[151] this time without Reiss.[152] Jean said that "the hardest thing at this point is just thinking of fresh ideas. People are so on top of things that we've done before, so the challenge now is to think of an idea that's good, but hasn't been seen."[152] In April 2001, in an interview with The New York Times, Jean stated that he wanted "to take the show back to the family".[63]
His return was welcomed, with MSNBC's Jon Bonné stating: "[Jean] has guided the show away from its gag-heavy, Homer-centric incarnation... these are certainly brighter days for the show's long-time fans."[122] Bill Gibron of PopMatters.com noted that "the show corralled much of its craziness for more personal stories" and that "Homer's Neanderthal nonsense and bratty Bart gave way to 'softer' episodes focusing on Marge and Lisa."[153]

In the season 13 episode "Blame It on Lisa", The Simpsons visit Rio de Janeiro in Brazil. In the week following the episode's original broadcast, it faced intense controversy involving the country of Brazil, most specifically the Rio de Janeiro Tourist Board (Riotur). The board claimed that the city was portrayed as having rampant street crime, kidnappings, slums and a rat infestation.[154][155] The tourist board asserted that the show "went too far" and undermined an $18m (£12.5m) advertising campaign to attract visitors to the city.[154] Fernando Henrique Cardoso, then the president of Brazil, stated that the episode "brought a distorted vision of Brazilian reality."[156][157] By April 9, Riotur, was preparing to sue the producers and Fox, for damage to its international image and loss of revenue. The issue threatened to become a diplomatic incident.[156] Upon knowledge of an impending lawsuit, the show's producers contacted Fox lawyers, who informed them that a city could not technically sue for defamation.[158] In response, executive producer Brooks apologized, stating "we apologize to the lovely city and people of Rio de Janeiro".[159] Jean commented that it was "one of the biggest controversies in the history of the show".[158]

In season 14, production switched from traditional cel animation to digital ink and paint.[160] The first episode to experiment with digital coloring was "Radioactive Man" in 1995. Animators used digital ink and paint during production of the season 12 episode "Tennis the Menace", but Gracie Films delayed the regular use of digital ink and paint until two seasons later. The already completed "Tennis the Menace" was broadcast as made.[161]

As the show's revenue continued to rise through syndication and DVD sales, the main cast stopped appearing for script readings in April 2004. The work stoppage occurred after weeks of unsuccessful negotiations with Fox, in which the cast asked for an increase in their pay to $360,000 per episode, or $8 million over a 22-episode season.[135] The strike was resolved a month later[162] and their salary was raised to something between $250,000[163] and $360,000 per episode.[164]

Season 16 featured one of the few major character developments since the show's inception. It was reported a long time in advance of the airing of the episode "There's Something About Marrying" that a major character would come out as gay during the episode. At San Diego Comic-Con in July 2004, Al Jean revealed: "We have a show where, to raise money, Springfield legalises gay marriage. Homer becomes a minister by going on the internet and filling out a form. A long-time character comes out of the closet, but I'm not saying who."[165] This led to much media speculation and publicity in the press for the episode.[166][167] Many fans correctly guessed that it would be one of Homer's sisters-in-law, either Patty or Selma, while others believed it to be Waylon Smithers.[165][168]

20th Century Fox, Gracie Films, and Film Roman produced an animated The Simpsons film that was released on July 27, 2007.[169] The production staff of The Simpsons had entertained the thought of a film since early in the series, but production never came together. Groening felt a feature-length film would allow them to increase the show's scale and animate sequences too complex for a TV series.[170] The film was directed by David Silverman and written by a team of Simpsons writers comprising Groening, Brooks, Jean, Reiss, Mirkin, Scully, George Meyer, John Swartzwelder, Jon Vitti, Matt Selman, and Ian Maxtone-Graham.[169] Work continued on the screenplay from 2003 onwards and did not cease,[171] taking place in the small bungalow where Groening first pitched The Simpsons in 1987.[172] Groening read about a town that had to get rid of pig feces in their water supply, which inspired the plot of the film.[173] He also wanted to make the film dramatically stronger than a TV episode, as "we wanna really give you something that you haven't seen before."[174] Production of the film occurred alongside continued writing of the series despite long-time claims by those involved in the show that a film would enter production only after the series had concluded.[169]

After winning a Fox and USA Today competition, Springfield, Vermont, hosted the film's world premiere.[175] The Simpsons Movie grossed a combined total of $74 million in its opening weekend in the US, taking it to the top of the box office,[176] and set the record for highest grossing opening weekend for a film based on a television series, surpassing Mission: Impossible 2.[177] It opened at the top of the international box office, taking $96 million from seventy-one overseas territories — including $27.8 million in the United Kingdom, making it Fox's second highest opening ever in that country.[178] In Australia, it grossed A$13.2 million, the biggest opening for an animated film and third largest opening weekend in the country.[179] As of November 23, 2007, the film has a worldwide gross of $525,267,904.[180] The film garnered a 90% approval rating on Rotten Tomatoes, with 171 of a total 191 reviews being determined as positive.[181] It received a rating of 80 out of 100 (signifying "generally favorable reviews") on Metacritic from 36 reviews.[182]

Jean continued as showrunner after the movie.[150] Critics have argued that the quality of the show has declined in Jean's tenure. Jacob Burch, an administrator of the website NoHomers.com, said in an interview that the show "seems less cohesive, more about trying to get the jokes in there, instead of make a story and let the jokes come off of that" and adds "I just think there's only so much you can do [with the characters]."[183] Steven Hyden of The A.V. Club argues in an online debate over this issue that "The Simpsons has come to rely too much on wacky Homer shtick and tired, meaningless guest stars" and that the writers are "content to amuse themselves with in-jokes, non sequiturs, and self-consciously silly plot twists."[184] Jean responded to this criticism by saying: "Well, it's possible that we've declined. But honestly, I've been here the whole time and I do remember in season two people saying, 'It's gone downhill.' If we'd listened to that then we would have stopped after episode 13. I'm glad we didn't."[185]

The writers of The Simpsons went on strike together with the Writers Guild of America at the end of 2007. The broadcasting of The Simpsons was not affected by the strike. Since it takes a long time to produce an episode of an animated show, the episodes are written up to a year in advance. So the strike would have had to go on for a while for the show to have run out of new episodes.[186] Production of season 19 was further delayed because of contract negotiations with the six main voice actors.[164] The dispute was resolved, and the actors' salary was raised to $400,000 per episode. The delay in production has caused the planned 22 episodes to be shortened to 20.[187]

To celebrate the 20th anniversary of the premiere of The Simpsons, Fox announced a year-long celebration of the show titled "Best. 20 Years. Ever.", which ran from January 14, 2009, to January 14, 2010.[188] Morgan Spurlock, an Academy Award-nominated documentary filmmaker (Best Documentary Feature for Super Size Me in 2004) and fan of The Simpsons since his college days,[189] was asked to direct the special The Simpsons 20th Anniversary Special – In 3-D! On Ice! in February 2009.[190] Spurlock believes "the reason [the producers] called [him] to begin with was to not have a show that would be a glad-hand, pat-everyone-on-the-back special, that's why rooting it in the people who kept this show on the air for the last 20 years is important."[189] It was shown on January 10, 2010, alongside "Once Upon a Time in Springfield", which was promoted as the 450th episode of the series.[191]

The episode "Million Dollar Maybe" featured a new character created by the winner of the "Best. Character. Ever." contest, in which fans could submit their own ideas for a new, and possibly recurring, Simpsons character.[192][193] Over 25,000 entries were sent in. The winner of the contest was Peggy Black from Orange, Connecticut, who created the character Ricardo Bomba. She described Ricardo as "someone that all the women love and all the men want to be" and "something like a Casanova." Jean was one of the judges of the contest, which he described as "a thank you to loyal fans." He also noted that there is a possibility the Ricardo character might appear on the show again.[194] Another change was to air The Simpsons in 720p high-definition television with the episode "Take My Life, Please" on February 15, 2009. With the new broadcasting system came a new opening sequence. It was the first major permanent change to the show's introduction since the beginning of the show's second season in 1990; previous changes have included variations in the duration of the intro. This new intro also includes some 3D animation when the camera pans over Springfield.[195]

To commemorate the show's twentieth anniversary, the United States Postal Service unveiled a series of five 44 cent stamps featuring Homer, Marge, Bart, Lisa and Maggie.[196] The stamps, designed by Groening, were made available for purchase on May 7, 2009[197] and approximately one billion stamps were printed.[198] The Simpsons is the first television series still in production to receive this recognition.[199][200] After entering its 21st season in late 2009, the show beat Gunsmoke's record as the longest-running American primetime, scripted television series.[201][202]

On October 4, 2011, 20th Century Fox Television released a statement saying: "23 seasons in, The Simpsons is as creatively vibrant as ever and beloved by millions around the world. We believe this brilliant series can and should continue, but we cannot produce future seasons under its current financial model. We are hopeful that we can reach an agreement with the voice cast that allows The Simpsons to go on entertaining audiences with original episodes for many years to come."[203] One of the problems was that The Simpsons was possibly worth more cancelled than on the air. A 17-year-old syndication deal with local TV stations prohibits Fox from selling the show to cable networks. As long as The Simpsons still produces new episodes, Fox cannot break this deal. In the meantime, cable networks have grown to become just as big a market as the local TV stations.[204] Another consideration was that Fox's parent company News Corporation was having meetings discussing the possibility of a cable channel that would only air The Simpsons episodes.[205] Analysts consider a cancellation and subsequent second-run deal that includes cable networks to be worth $750 million.[204] On this issue, Jean commented in an interview with TV Guide that "It's a big company, and there are definitely people whose interests would have been better served by ending it. Those interests were superseded because we're still valuable to the network in terms of our ratings."[206]

For the negotiations, the studio requested that the cast members accept a 45 percent cut of their salaries so that more seasons could be produced after season 23, or else that season would be the series' last.[203] The actors were willing to take a pay cut, but wanted a percentage of the back-end payments instead.[207] At one point Shearer even offered a 70 percent pay cut in exchange for back-end percentages, but the studio was unwilling to make any deal involving back-end percentages.[208] In the end, the studio and the actors reached a deal, in which the actors would take a pay cut of 30 percent, down to just over $300,000 per episode, prolonging the show to its 25th season.[209] As well as the voice actors, everybody involved in the show took a pay cut. This included animators, writers, the post-production crew and even Jean himself. The further use of digital animation also saves money, as the animation of the show becomes more efficient.[206]

In 2013, FXX purchased the exclusive American cable rights to the series.[210] In August 2014, a new website and app was launched called Simpsons World, which contained every episode from the show's first 25 seasons that were available for viewing with a valid cable login.[211] The website, which updated regularly, was only available in the United States.

In 2016, The Simpsons moved the production of the animation from Film Roman to Fox Television Animation in season 28.

In February 2019, the series was renewed for a 31st and 32nd season bringing the series up to 713 episodes, making it the first scripted primetime series to surpass 700 episodes.[212]

In March 2019, the episode "Stark Raving Dad" was pulled from circulation following the release of the Leaving Neverland documentary and renewed discussion of Michael Jackson's sexual abuse allegations.[213] The episode also was omitted from the Disney+ streaming service.[214]

In March 2019, following the acquisition of 21st Century Fox by Disney, The Simpsons, among other franchises and studios owned by 21st Century Fox, became properties of The Walt Disney Company.

On April 11, 2019, it was announced that the series would stream exclusively on Disney+ at launch; as a result, Simpsons World was officially shut down on November 16 of that year, four days after Disney+'s launch. Initially, episodes from the first 20 seasons that were originally broadcast in the 4:3 aspect ratio were only available in a cropped 16:9 format, a move which received heavy criticism from fans. On May 28, 2020, Disney+ introduced a new feature that allows viewers to toggle between the original 4:3 aspect ratio and the remastered 16:9 ratio for seasons 1–20.

In May 2019, FXX's sister network Freeform began sharing the off-network rights to the series and began airing it on October 2, 2019.[215]

As part of the series' 30th anniversary, FXX (in association with Disney+) aired a fourteen-day marathon titled The Simpsons: Plus Sized Holiday Marathon, airing 661 episodes and the movie. The marathon premiered exactly 30 years after the series premiere on December 17, 2019, at 8pm ET and concluded on January 1, 2020.

On February 27, 2020, Disney announced that a second short film based on the series, titled Playdate with Destiny, would release ahead of Pixar's Onward, making it the third piece of Simpsons media to be released in theaters.[216]

On March 3, 2021, The Simpsons was renewed for a 33rd and 34th season,[217] with a further extension for a 35th and 36th season on January 26, 2023.[218]The history of artificial intelligence (AI) began in antiquity, with myths, stories, and rumors of artificial beings endowed with intelligence or consciousness by master craftsmen. The study of logic and formal reasoning from antiquity to the present led directly to the invention of the programmable digital computer in the 1940s, a machine based on abstract mathematical reasoning. This device and the ideas behind it inspired scientists to begin discussing the possibility of building an electronic brain.

The field of AI research was founded at a workshop held on the campus of Dartmouth College in 1956.[1] Attendees of the workshop became the leaders of AI research for decades. Many of them predicted that machines as intelligent as humans would exist within a generation. The U.S. government provided millions of dollars with the hope of making this vision come true.[2]

Eventually, it became obvious that researchers had grossly underestimated the difficulty of this feat.[3] In 1974, criticism from James Lighthill and pressure from the U.S.A. Congress led the U.S. and British Governments to stop funding undirected research into artificial intelligence. Seven years later, a visionary initiative by the Japanese Government and the success of expert systems  reinvigorated investment in AI, and by the late 1980s, the industry had grown into a billion-dollar enterprise. However, investors' enthusiasm waned in the 1990s, and the field was criticized in the press and avoided by industry (a period known as an "AI winter"). Nevertheless, research and funding continued to grow under other names.

In the early 2000s, machine learning was applied to a wide range of problems in academia and industry. The success was due to the availability of powerful computer hardware, the collection of immense data sets, and the application of solid mathematical methods. Soon after, deep learning proved to be a breakthrough technology, eclipsing all other methods. The transformer architecture debuted in 2017 and was used to produce impressive generative AI applications, amongst other use cases.

Investment in AI boomed in the 2020s. The recent AI boom, initiated by the development of transformer architecture, led to the rapid scaling and public releases of large language models (LLMs) like ChatGPT. These models exhibit human-like traits of knowledge, attention, and creativity, and have been integrated into various sectors, fueling exponential investment in AI. However, concerns about the potential risks and ethical implications of advanced AI have also emerged, prompting debate about the future of AI and its impact on society.

In Greek mythology, Talos was a giant made of bronze who acted as guardian for the island of Crete. He would throw boulders at the ships of invaders and would complete 3 circuits around the island's perimeter daily.[4] According to pseudo-Apollodorus' Bibliotheke, Hephaestus forged Talos with the aid of a cyclops and presented the automaton as a gift to Minos.[5] In the Argonautica, Jason and the Argonauts defeated Talos by removing a plug near his foot, causing the vital ichor to flow out from his body and rendering him lifeless.[6]

Pygmalion was a legendary king and sculptor of Greek mythology, famously represented in Ovid's Metamorphoses. In the 10th book of Ovid's narrative poem, Pygmalion becomes disgusted with women when he witnesses the way in which the Propoetides prostitute themselves. Despite this, he makes offerings at the temple of Venus asking the goddess to bring to him a woman just like a statue he carved.[7]

In Of the Nature of Things, the Swiss alchemist Paracelsus describes a procedure that he claims can fabricate an "artificial man". By placing the "sperm of a man" in horse dung, and feeding it the "Arcanum of Mans blood" after 40 days, the concoction will become a living infant.[8]

The earliest written account regarding golem-making is found in the writings of Eleazar ben Judah of Worms in the early 13th century.[9] During the Middle Ages, it was believed that the animation of a Golem could be achieved by insertion of a piece of paper with any of God's names on it, into the mouth of the clay figure.[10] Unlike legendary automata like Brazen Heads,[11] a Golem was unable to speak.[12]

Takwin, the artificial creation of life, was a frequent topic of Ismaili alchemical manuscripts, especially those attributed to Jabir ibn Hayyan. Islamic alchemists attempted to create a broad range of life through their work, ranging from plants to animals.[13]

In Faust: The Second Part of the Tragedy by Johann Wolfgang von Goethe, an alchemically fabricated homunculus, destined to live forever in the flask in which he was made, endeavors to be born into a full human body. Upon the initiation of this transformation, however, the flask shatters and the homunculus dies.[14]

By the 19th century, ideas about artificial men and thinking machines became a popular theme in fiction. Notable works like Mary Shelley's Frankenstein  and Karel Čapek's R.U.R. (Rossum's Universal Robots)[15]
explored the concept of artificial life. Speculative essays, such as Samuel Butler's "Darwin among the Machines",[16] and Edgar Allan Poe's "Maelzel's Chess Player"[17] reflected society's growing interest in machines with artificial intelligence. AI remains a common topic in science fiction today.[18]

Realistic humanoid automata were built by craftsman from many civilizations, including Yan Shi,[19] Hero of Alexandria,[20] Al-Jazari,[21] Haroun al-Rashid,[22] Jacques de Vaucanson,[23][24] Leonardo Torres y Quevedo,[25] Pierre Jaquet-Droz and Wolfgang von Kempelen.[26][27]

The oldest known automata were the sacred statues of ancient Egypt and Greece.[28][29] The faithful believed that craftsman had imbued these figures with very real minds, capable of wisdom and emotion—Hermes Trismegistus wrote that "by discovering the true nature of the gods, man has been able to reproduce it".[30] English scholar Alexander Neckham asserted that the Ancient Roman poet Virgil had built a palace with automaton statues.[31]

During the early modern period, these legendary automata were said to possess the magical ability to answer questions put to them. The late medieval alchemist and proto-Protestant Roger Bacon was purported to have fabricated a brazen head, having developed a legend of having been a wizard.[32][33] These legends were similar to the Norse myth of the Head of Mímir. According to legend, Mímir was known for his intellect and wisdom, and was beheaded in the Æsir-Vanir War. Odin is said to have "embalmed" the head with herbs and spoke incantations over it such that Mímir's head remained able to speak wisdom to Odin. Odin then kept the head near him for counsel.[34]

Artificial intelligence is based on the assumption that the process of human thought can be mechanized. The study of mechanical—or "formal"—reasoning has a long history. Chinese, Indian and Greek philosophers all developed structured methods of formal deduction by the first millennium BCE. Their ideas were developed over the centuries by philosophers such as Aristotle (who gave a formal analysis of the syllogism),[35] Euclid (whose Elements was a model of formal reasoning), al-Khwārizmī (who developed algebra and gave his name to the word algorithm) and European scholastic philosophers such as William of Ockham and Duns Scotus.[36]

Spanish philosopher Ramon Llull (1232–1315) developed several logical machines devoted to the production of knowledge by logical means;[37][38] Llull described his machines as mechanical entities that could combine basic and undeniable truths by simple logical operations, produced by the machine by mechanical meanings, in such ways as to produce all the possible knowledge.[39] Llull's work had a great influence on Gottfried Leibniz, who redeveloped his ideas.[40]

In the 17th century, Leibniz, Thomas Hobbes and René Descartes explored the possibility that all rational thought could be made as systematic as algebra or geometry.[41] Hobbes famously wrote in Leviathan: "For reason ... is nothing but reckoning, that is adding and subtracting".[42] Leibniz envisioned a universal language of reasoning, the characteristica universalis, which would reduce argumentation to calculation so that "there would be no more need of disputation between two philosophers than between two accountants. For it would suffice to take their pencils in hand, down to their slates, and to say each other (with a friend as witness, if they liked): Let us calculate."[43] These philosophers had begun to articulate the physical symbol system hypothesis that would become the guiding faith of AI research.

The study of mathematical logic provided the essential breakthrough that made artificial intelligence seem plausible. The foundations had been set by such works as Boole's The Laws of Thought and Frege's Begriffsschrift.[44] Building on Frege's system, Russell and Whitehead presented a formal treatment of the foundations of mathematics in their masterpiece, the Principia Mathematica in 1913. Inspired by Russell's success, David Hilbert challenged mathematicians of the 1920s and 30s to answer this fundamental question: "can all of mathematical reasoning be formalized?"[36] His question was answered by Gödel's incompleteness proof,[45] Turing's machine[45] and Church's Lambda calculus.[a]

Their answer was surprising in two ways. First, they proved that there were, in fact, limits to what mathematical logic could accomplish. But second (and more important for AI) their work suggested that, within these limits, any form of mathematical reasoning could be mechanized. The Church-Turing thesis implied that a mechanical device, shuffling symbols as simple as 0 and 1, could imitate any conceivable process of mathematical deduction.[45] The key insight was the Turing machine—a simple theoretical construct that captured the essence of abstract symbol manipulation.[48] This invention would inspire a handful of scientists to begin discussing the possibility of thinking machines.

Calculating machines were designed or built in antiquity and throughout history by many people, including 
Gottfried Leibniz,[38][49]
Joseph Marie Jacquard,[50]
Charles Babbage,[50][51]
Percy Ludgate,[52]
Leonardo Torres Quevedo,[53]
Vannevar Bush,[54]
and others. Ada Lovelace speculated that Babbage's machine was "a thinking or ... reasoning machine", but warned "It is desirable to guard against the possibility of exaggerated ideas that arise as to the powers" of the machine.[55][56]

The first modern computers were the massive machines of the Second World War (such as Konrad Zuse's Z3, Alan Turing's Heath Robinson and Colossus, Atanasoff and Berry's ABC and ENIAC at the University of Pennsylvania).[57] ENIAC was based on the theoretical foundation laid by Alan Turing and developed by John von Neumann,[58] and proved to be the most influential.[57]

The earliest research into thinking machines was inspired by a confluence of ideas that became prevalent in the late 1930s, 1940s, and early 1950s. Recent research in neurology had shown that the brain was an electrical network of neurons that fired in all-or-nothing pulses. Norbert Wiener's cybernetics described control and stability in electrical networks. Claude Shannon's information theory described digital signals (i.e., all-or-nothing signals). Alan Turing's theory of computation showed that any form of computation could be described digitally. The close relationship between these ideas suggested that it might be possible to construct an "electronic brain".

In the 1940s and 50s, a handful of scientists from a variety of fields (mathematics, psychology, engineering, economics and political science) explored several research directions that would be vital to later AI research.[59] Alan Turing was among the first people to seriously investigate the theoretical possibility of "machine intelligence".[60] The field of "artificial intelligence research" was founded as an academic discipline in 1956.[61]

In 1950 Turing published a landmark paper "Computing Machinery and Intelligence", in which he speculated about the possibility of creating machines that think.[63][b] In the paper, he noted that "thinking" is difficult to define and devised his famous Turing Test: If a machine could carry on a conversation (over a teleprinter) that was indistinguishable from a conversation with a human being, then it was reasonable to say that the machine was "thinking".[64] This simplified version of the problem allowed Turing to argue convincingly that a "thinking machine" was at least plausible and the paper answered all the most common objections to the proposition.[65] The Turing Test was the first serious proposal in the philosophy of artificial intelligence.

Donald Hebb was a Canadian psychologist whose work laid the foundation for modern neuroscience, particularly in understanding learning, memory, and neural plasticity. His most influential book, The Organization of Behavior (1949), introduced the concept of Hebbian learning, often summarized as "cells that fire together wire together."
[66]

Hebb began formulating the foundational ideas for this book in the early 1940s, particularly during his time at the Yerkes Laboratories of Primate Biology from 1942 to 1947. He made extensive notes between June 1944 and March 1945 and sent a complete draft to his mentor Karl Lashley in 1946. The manuscript for The Organization of Behavior wasn’t published until 1949. The delay was due to various factors, including World War II and shifts in academic focus. By the time it was published, several of his peers had already published related ideas, making Hebb’s work seem less groundbreaking at first glance. However, his synthesis of psychological and neurophysiological principles became a cornerstone of neuroscience and machine learning.
[67]
[68]

Walter Pitts and Warren McCulloch analyzed networks of idealized artificial neurons and showed how they might perform simple logical functions in 1943. They were the first to describe what later researchers would call a neural network.[69] The paper was influenced by Turing's paper 'On Computable Numbers' from 1936 using similar two-state boolean 'neurons', but was the first to apply it to neuronal function.[60] One of the students inspired by Pitts and McCulloch was Marvin Minsky who was a 24-year-old graduate student at the time. In 1951 Minsky and Dean Edmonds built the first neural net machine, the SNARC.[70] Minsky would later become one of the most important leaders and innovators in AI.

Experimental robots such as W. Grey Walter's turtles and the Johns Hopkins Beast, were built in the 1950s. These machines did not use computers, digital electronics or symbolic reasoning; they were controlled entirely by analog circuitry.[71]

In 1951, using the Ferranti Mark 1 machine of the University of Manchester, Christopher Strachey wrote a checkers program[72] and Dietrich Prinz wrote one for chess.[73] Arthur Samuel's checkers program, the subject of his 1959 paper "Some Studies in Machine Learning Using the Game of Checkers", eventually achieved sufficient skill to challenge a respectable amateur.[74] Samuel's program was among the first uses of what would later be called machine learning.[75] Game AI would continue to be used as a measure of progress in AI throughout its history.

When access to digital computers became possible in the mid-fifties, a few scientists instinctively recognized that a machine that could manipulate numbers could also manipulate symbols and that the manipulation of symbols could well be the essence of human thought. This was a new approach to creating thinking machines.[76][77]

In 1955, Allen Newell and future Nobel Laureate Herbert A. Simon created the "Logic Theorist", with help from J. C. Shaw. The program would eventually prove 38 of the first 52 theorems in Russell and Whitehead's Principia Mathematica, and find new and more elegant proofs for some.[78] Simon said that they had "solved the venerable mind/body problem, explaining how a system composed of matter can have the properties of mind."[79][c] The symbolic reasoning paradigm they introduced would dominate AI research and funding until the middle 90s, as well as inspire the cognitive revolution.

The Dartmouth workshop of 1956 was a pivotal event that marked the formal inception of AI as an academic discipline.[61] It was organized by Marvin Minsky and John McCarthy, with the support of two senior scientists Claude Shannon and Nathan Rochester of IBM. The proposal for the conference stated they intended to test the assertion that "every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it".[80][d] The term "Artificial Intelligence" was introduced by John McCarthy at the workshop.[e] 
The participants included Ray Solomonoff, Oliver Selfridge, Trenchard More, Arthur Samuel, Allen Newell and Herbert A. Simon, all of whom would create important programs during the first decades of AI research.[86][f] At the workshop Newell and Simon debuted the "Logic Theorist".[87] The workshop was the moment that AI gained its name, its mission, its first major success and its key players, and is widely considered the birth of AI.[g]

In the autumn of 1956, Newell and Simon also presented the Logic Theorist at a meeting of the Special Interest Group in Information Theory at the Massachusetts Institute of Technology (MIT). At the same meeting, Noam Chomsky discussed his generative grammar, and George Miller described his landmark paper "The Magical Number Seven, Plus or Minus Two". Miller wrote "I left the symposium with a conviction, more intuitive than rational, that experimental psychology, theoretical linguistics, and the computer simulation of cognitive processes were all pieces from a larger whole."[89][57]

This meeting was the beginning of the "cognitive revolution"—an interdisciplinary paradigm shift in psychology, philosophy, computer science and neuroscience. It inspired the creation of the sub-fields of symbolic artificial intelligence, generative linguistics, cognitive science, cognitive psychology, cognitive neuroscience and the philosophical schools of computationalism and functionalism. All these fields used related tools to model the mind and results discovered in one field were relevant to the others.

The cognitive approach allowed researchers to consider "mental objects" like thoughts, plans, goals, facts or memories, often analyzed using high level symbols in functional networks. These objects had been forbidden as "unobservable" by earlier paradigms such as behaviorism.[h] Symbolic mental objects would become the major focus of AI research and funding for the next several decades.

The programs developed in the years after the Dartmouth Workshop were, to most people, simply "astonishing":[i] computers were solving algebra word problems, proving theorems in geometry and learning to speak English. Few at the time would have believed that such "intelligent" behavior by machines was possible at all.[93][94][92] Researchers expressed an intense optimism in private and in print, predicting that a fully intelligent machine would be built in less than 20 years.[95] Government agencies like the Defense Advanced Research Projects Agency (DARPA, then known as "ARPA") poured money into the field.[96] Artificial Intelligence laboratories were set up at a number of British and US universities in the latter 1950s and early 1960s.[60]

There were many successful programs and new directions in the late 50s and 1960s. Among the most influential were these:

Many early AI programs used the same basic algorithm. To achieve some goal (like winning a game or proving a theorem), they proceeded step by step towards it (by making a move or a deduction) as if searching through a maze, backtracking whenever they reached a dead end.[97] The principal difficulty was that, for many problems, the number of possible paths through the "maze" was astronomical (a situation known as a "combinatorial explosion"). Researchers would reduce the search space by using heuristics that would eliminate paths that were unlikely to lead to a solution.[98]

Newell and Simon tried to capture a general version of this algorithm in a program called the "General Problem Solver".[99][100] Other "searching" programs were able to accomplish impressive tasks like solving problems in geometry and algebra, such as Herbert Gelernter's Geometry Theorem Prover (1958)[101] and Symbolic Automatic Integrator (SAINT), written by Minsky's student James Slagle in 1961.[102][103] Other programs searched through goals and subgoals to plan actions, like the STRIPS system developed at Stanford to control the behavior of the robot Shakey.[104]

An important goal of AI research is to allow computers to communicate in natural languages like English. An early success was Daniel Bobrow's program STUDENT, which could solve high school algebra word problems.[105]

A semantic net represents concepts (e.g. "house", "door") as nodes, and relations among concepts as links between the nodes (e.g. "has-a"). The first AI program to use a semantic net was written by Ross Quillian[106] and the most successful (and controversial) version was Roger Schank's Conceptual dependency theory.[107]

Joseph Weizenbaum's ELIZA could carry out conversations that were so realistic that users occasionally were fooled into thinking they were communicating with a human being and not a computer program (see ELIZA effect). But in fact, ELIZA simply gave a canned response or repeated back what was said to it, rephrasing its response with a few grammar rules. ELIZA was the first chatbot.[108][109]

In the late 60s, Marvin Minsky and Seymour Papert of the MIT AI Laboratory proposed that AI research should focus on artificially simple situations known as micro-worlds.[j] They pointed out that in successful sciences like physics, basic principles were often best understood using simplified models like frictionless planes or perfectly rigid bodies. Much of the research focused on a "blocks world," which consists of colored blocks of various shapes and sizes arrayed on a flat surface.[110]

This paradigm led to innovative work in machine vision by Gerald Sussman, Adolfo Guzman, David Waltz (who invented "constraint propagation"), and especially Patrick Winston. At the same time, Minsky and Papert built a robot arm that could stack blocks, bringing the blocks world to life. Terry Winograd's SHRDLU could communicate in ordinary English sentences about the micro-world, plan operations and execute them.[110]

In the 1960s funding was primarily directed towards laboratories researching symbolic AI, however several people still pursued research in neural networks.

The perceptron, a single-layer neural network was introduced in 1958 by Frank Rosenblatt[111] (who had been a schoolmate of Marvin Minsky at the Bronx High School of Science).[112] Like most AI researchers, he was optimistic about their power, predicting that a perceptron "may eventually be able to learn, make decisions, and translate languages."[113] Rosenblatt was primarily funded by Office of Naval Research.[114]

Bernard Widrow and his student Ted Hoff built ADALINE (1960) and MADALINE (1962), which had up to 1000 adjustable weights.[115][116] A group at Stanford Research Institute led by Charles A. Rosen and Alfred E. (Ted) Brain built two neural network machines named MINOS I (1960) and II (1963), mainly funded by U.S. Army Signal Corps. MINOS II[117] had 6600 adjustable weights,[118] and was controlled with an SDS 910 computer in a configuration named MINOS III (1968), which could classify symbols on army maps, and recognize hand-printed characters on Fortran coding sheets.[119][120] Most of neural network research during this early period involved building and using bespoke hardware, rather than simulation on digital computers.[k]

However, partly due to lack of results and partly due to competition from symbolic AI research, the MINOS project ran out of funding in 1966. Rosenblatt failed to secure continued funding in the 1960s.[121] In 1969, research came to a sudden halt with the publication of Minsky and Papert's 1969 book Perceptrons.[122] It suggested that there were severe limitations to what perceptrons could do and that Rosenblatt's predictions had been grossly exaggerated. The effect of the book was that virtually no research was funded in connectionism for 10 years.[123] The competition for government funding ended with the victory of symbolic AI approaches over neural networks.[120][121]

Minsky (who had worked on SNARC) became a staunch objector to pure connectionist AI. Widrow (who had worked on ADALINE) turned to adaptive signal processing. The SRI group (which worked on MINOS) turned to symbolic AI and robotics.[120][121]

The main problem was the inability to train multilayered networks (versions of backpropagation had already been used in other fields but it was unknown to these researchers).[124][123] The AI community became aware of backpropogation in the 80s,[125] and, in the 21st century, neural networks would become enormously successful, fulfilling all of Rosenblatt's optimistic predictions. Rosenblatt did not live to see this, however, as he died in a boating accident in 1971.[126]

The first generation of AI researchers made these predictions about their work:

In June 1963, MIT received a $2.2 million grant from the newly created Advanced Research Projects Agency (ARPA, later known as DARPA). The money was used to fund project MAC which subsumed the "AI Group" founded by Minsky and McCarthy five years earlier. DARPA continued to provide $3 million each year until the 70s.[133] DARPA made similar grants to Newell and Simon's program at Carnegie Mellon University and to Stanford University's AI Lab, founded by John McCarthy in 1963.[134] Another important AI laboratory was established at Edinburgh University by Donald Michie in 1965.[135] These four institutions would continue to be the main centers of AI research and funding in academia for many years.[136][m]

The money was given with few strings attached: J. C. R. Licklider, then the director of ARPA, believed that his organization should "fund people, not projects!" and allowed researchers to pursue whatever directions might interest them.[138]  This created a freewheeling atmosphere at MIT that gave birth to the hacker culture,[139] but this "hands off" approach did not last.

In the 1970s, AI was subject to critiques and financial setbacks. AI researchers had failed to appreciate the difficulty of the problems they faced. Their tremendous optimism had raised public expectations impossibly high, and when the promised results failed to materialize, funding targeted at AI was severely reduced.[140] The lack of success indicated the techniques being used by AI researchers at the time were insufficient to achieve their goals.[141][142]

These setbacks did not affect the growth and progress of the field, however. The funding cuts only impacted a handful of major laboratories[143] and the critiques were largely ignored.[144] General public interest in the field continued to grow,[143] the number of researchers increased dramatically,[143] and new ideas were explored in logic programming, commonsense reasoning and many other areas. Historian Thomas Haigh argued in 2023 that there was no winter,[143] and AI researcher Nils Nilsson described this period as the most "exciting" time to work in AI.[145]

In the early seventies, the capabilities of AI programs were limited. Even the most impressive could only handle trivial versions of the problems they were supposed to solve;[n] all the programs were, in some sense, "toys".[147] AI researchers had begun to run into several limits that would be only conquered decades later, and others that still stymie the field in the 2020s:

The agencies which funded AI research, such as the British government, DARPA and the National Research Council (NRC) became frustrated with the lack of progress and eventually cut off almost all funding for undirected AI research. The pattern began in 1966 when the Automatic Language Processing Advisory Committee (ALPAC) report criticized machine translation efforts. After spending $20 million, the NRC ended all support.[157] In 1973, the Lighthill report on the state of AI research in the UK criticized the failure of AI to achieve its "grandiose objectives" and led to the dismantling of AI research in that country.[158] (The report specifically mentioned the combinatorial explosion problem as a reason for AI's failings.)[142][146][s] DARPA was deeply disappointed with researchers working on the Speech Understanding Research program at CMU and canceled an annual grant of $3 million.[160][t]

Hans Moravec blamed the crisis on the unrealistic predictions of his colleagues. "Many researchers were caught up in a web of increasing exaggeration."[161][u] However, there was another issue: since the passage of the Mansfield Amendment in 1969, DARPA had been under increasing pressure to fund "mission-oriented direct research, rather than basic undirected research". Funding for the creative, freewheeling exploration that had gone on in the 60s would not come from DARPA, which instead directed money at specific projects with clear objectives, such as autonomous tanks and battle management systems.[162][v]

The major laboratories (MIT, Stanford, CMU and Edinburgh) had been receiving generous support from their governments, and when it was withdrawn, these were the only places that were seriously impacted by the budget cuts. The thousands of researchers outside these institutions and the many more thousands that were joining the field were unaffected.[143]

Several philosophers had strong objections to the claims being made by AI researchers. One of the earliest was John Lucas, who argued that Gödel's incompleteness theorem showed that a formal system (such as a computer program) could never see the truth of certain statements, while a human being could.[164] Hubert Dreyfus ridiculed the broken promises of the 1960s and critiqued the assumptions of AI, arguing that human reasoning actually involved very little "symbol processing" and a great deal of embodied, instinctive, unconscious "know how".[w][166] John Searle's Chinese Room argument, presented in 1980, attempted to show that a program could not be said to "understand" the symbols that it uses (a quality called "intentionality"). If the symbols have no meaning for the machine, Searle argued, then the machine can not be described as "thinking".[167]

These critiques were not taken seriously by AI researchers. Problems like intractability and commonsense knowledge seemed much more immediate and serious. It was unclear what difference "know how" or "intentionality" made to an actual computer program. MIT's Minsky said of Dreyfus and Searle "they misunderstand, and should be ignored."[168] Dreyfus, who also taught at MIT, was given a cold shoulder: he later said that AI researchers "dared not be seen having lunch with me."[169] Joseph Weizenbaum, the author of ELIZA, was also an outspoken critic of Dreyfus' positions, but he "deliberately made it plain that [his AI colleagues' treatment of Dreyfus] was not the way to treat a human being,"[x] and was unprofessional and childish.[171]

Weizenbaum began to have serious ethical doubts about AI when Kenneth Colby wrote a "computer program which can conduct psychotherapeutic dialogue" based on ELIZA.[172][173][y] Weizenbaum was disturbed that Colby saw a mindless program as a serious therapeutic tool. A feud began, and the situation was not helped when Colby did not credit Weizenbaum for his contribution to the program. In 1976, Weizenbaum published Computer Power and Human Reason which argued that the misuse of artificial intelligence has the potential to devalue human life.[175]

Logic was introduced into AI research as early as 1958, by John McCarthy in his Advice Taker proposal.[176][101] In 1963, J. Alan Robinson had discovered a simple method to implement deduction on computers, the resolution and unification algorithm.[101] However, straightforward implementations, like those attempted by McCarthy and his students in the late 1960s, were especially intractable: the programs required astronomical numbers of steps to prove simple theorems.[176][177] A more fruitful approach to logic was developed in the 1970s by Robert Kowalski at the University of Edinburgh, and soon this led to the collaboration with French researchers Alain Colmerauer and Philippe Roussel [fr] who created the successful logic programming language Prolog.[178] Prolog uses a subset of logic (Horn clauses, closely related to "rules" and "production rules") that permit tractable computation. Rules would continue to be influential, providing a foundation for Edward Feigenbaum's expert systems and the continuing work by Allen Newell and Herbert A. Simon that would lead to Soar and their unified theories of cognition.[179]

Critics of the logical approach noted, as Dreyfus had, that human beings rarely used logic when they solved problems. Experiments by psychologists like Peter Wason, Eleanor Rosch, Amos Tversky, Daniel Kahneman and others provided proof.[z] McCarthy responded that what people do is irrelevant. He argued that what is really needed are machines that can solve problems—not machines that think as people do.[aa]

Among the critics of McCarthy's approach were his colleagues across the country at MIT. Marvin Minsky, Seymour Papert and Roger Schank were trying to solve problems like "story understanding" and "object recognition" that required a machine to think like a person. In order to use ordinary concepts like "chair" or "restaurant" they had to make all the same illogical assumptions that people normally made. Unfortunately, imprecise concepts like these are hard to represent in logic. MIT chose instead to focus on writing programs that solved a given task without using high-level abstract definitions or general theories of cognition, and measured performance by iterative testing, rather than arguments from first principles. Schank described their "anti-logic" approaches as scruffy, as opposed to the neat paradigm used by McCarthy, Kowalski, Feigenbaum, Newell and Simon.[180][ab]

In 1975, in a seminal paper, Minsky noted that many of his fellow researchers were using the same kind of tool: a framework that captures all our common sense assumptions about something. For example, if we use the concept of a bird, there is a constellation of facts that immediately come to mind: we might assume that it flies, eats worms and so on (none of which are true for all birds). Minsky associated these assumptions with the general category and they could be inherited by the frames for subcategories and individuals, or over-ridden as necessary. He called these structures frames. Schank used a version of frames he called "scripts" to successfully answer questions about short stories in English.[181] Frames would eventually be widely used in software engineering under the name object-oriented programming.

The logicians rose to the challenge. Pat Hayes claimed that "most of 'frames' is just a new syntax for parts of first-order logic." But he noted that "there are one or two apparently minor details which give a lot of trouble, however, especially defaults".[182]

Ray Reiter admitted that "conventional logics, such as first-order
logic, lack the expressive power to adequately represent the knowledge required for reasoning by default".[183] He proposed augmenting first-order logic with a closed world assumption that a conclusion holds (by default) if its contrary cannot be shown. He showed how such an assumption corresponds to the common sense assumption made in reasoning with frames. He also showed that it has its "procedural equivalent" as negation as failure in Prolog. The closed world assumption, as formulated by Reiter, "is not a first-order notion. (It is a meta notion.)"[183] However, Keith Clark showed that negation as finite failure can be understood as reasoning implicitly with definitions in first-order logic including a unique name assumption that different terms denote different individuals.[184]

During the late 1970s and throughout the 1980s, a variety of logics and extensions of first-order logic were developed both for negation as failure in logic programming and for default reasoning more generally. Collectively, these logics have become known as non-monotonic logics.

In the 1980s, a form of AI program called "expert systems" was adopted by corporations around the world and knowledge became the focus of mainstream AI research. Governments provided substantial funding, such as Japan's fifth generation computer project and the U.S. Strategic Computing Initiative. "Overall, the AI industry boomed from a few million dollars in 1980 to billions of dollars in 1988."[125]

An expert system is a program that answers questions or solves problems about a specific domain of knowledge, using logical rules that are derived from the knowledge of experts.[185]
The earliest examples were developed by Edward Feigenbaum and his students. Dendral, begun in 1965, identified compounds from spectrometer readings.[186][123] MYCIN, developed in 1972, diagnosed infectious blood diseases.[125] They demonstrated the feasibility of the approach.

Expert systems restricted themselves to a small domain of specific knowledge (thus avoiding the commonsense knowledge problem)[123] and their simple design made it relatively easy for programs to be built and then modified once they were in place. All in all, the programs proved to be useful: something that AI had not been able to achieve up to this point.[187]

In 1980, an expert system called R1 was completed at CMU for the Digital Equipment Corporation. It was an enormous success: it was saving the company 40 million dollars annually by 1986.[188] Corporations around the world began to develop and deploy expert systems and by 1985 they were spending over a billion dollars on AI, most of it to in-house AI departments.[189] An industry grew up to support them, including hardware companies like Symbolics and Lisp Machines and software companies such as IntelliCorp and Aion.[190]

In 1981, the Japanese Ministry of International Trade and Industry set aside $850 million for the Fifth generation computer project. Their objectives were to write programs and build machines that could carry on conversations, translate languages, interpret pictures, and reason like human beings.[191] Much to the chagrin of scruffies, they initially chose Prolog as the primary computer language for the project.[192]

Other countries responded with new programs of their own. The UK began the £350 million Alvey project.[193] A consortium of American companies formed the Microelectronics and Computer Technology Corporation (or "MCC") to fund large scale projects in AI and information technology.[194][193] DARPA responded as well, founding the Strategic Computing Initiative and tripling its investment in AI between 1984 and 1988.[195][196]

The power of expert systems came from the expert knowledge they contained. They were part of a new direction in AI research that had been gaining ground throughout the 70s. "AI researchers were beginning to suspect—reluctantly, for it violated the scientific canon of parsimony—that intelligence might very well be based on the ability to use large amounts of diverse knowledge in different ways,"[197] writes Pamela McCorduck. "[T]he great lesson from the 1970s was that intelligent behavior depended very much on dealing with knowledge, sometimes quite detailed knowledge, of a domain where a given task lay".[198] Knowledge based systems and knowledge engineering became a major focus of AI research in the 1980s.[199] It was hoped that vast databases would solve the commonsense knowledge problem and provide the support that commonsense reasoning required.

In the 1980s some researchers attempted to attack the commonsense knowledge problem directly, by creating a massive database that would contain all the mundane facts that the average person knows. Douglas Lenat, who started a database called Cyc, argued that there is no shortcut ― the only way for machines to know the meaning of human concepts is to teach them, one concept at a time, by hand.[200]

Although symbolic knowledge representation and logical reasoning produced useful applications in the 80s and received massive amounts of funding, it was still unable to solve problems in perception, robotics, learning and common sense. A small number of scientists and engineers began to doubt that the symbolic approach would ever be sufficient for these tasks and developed other approaches, such as "connectionism", robotics, "soft" computing and reinforcement learning. Nils Nilsson called these approaches "sub-symbolic".

In 1982, physicist John Hopfield was able to prove that a form of neural network (now called a "Hopfield net") could learn and process information, and provably converges after enough time under any fixed condition. It was a breakthrough, as it was previously thought that nonlinear networks would, in general, evolve chaotically.[201] Around the same time, Geoffrey Hinton and David Rumelhart popularized a method for training neural networks called "backpropagation".[ac] These two developments helped to revive the exploration of artificial neural networks.[125][202]

Neural networks, along with several other similar models, received widespread attention after the 1986 publication of the Parallel Distributed Processing, a two volume collection of papers edited by Rumelhart and psychologist James McClelland. The new field was christened "connectionism" and there was a considerable debate between advocates of symbolic AI and the "connectionists".[125] Hinton called symbols the "luminous aether of AI" – that is, an unworkable and misleading model of intelligence.[125] This was a direct attack on the principles that inspired the cognitive revolution.

In 1990, Yann LeCun at Bell Labs used convolutional neural networks to recognize handwritten digits. The system was used widely in 90s, reading zip codes and personal checks. This was the first genuinely useful application of neural networks.[203][204]

Rodney Brooks, Hans Moravec and others argued that, in order to show real intelligence, a machine needs to have a body — it needs to perceive, move, survive and deal with the world.[205] Sensorimotor skills are essential to higher level skills such as commonsense reasoning. They can't be efficiently implemented using abstract symbolic reasoning, so AI should solve the problems of perception, mobility, manipulation and survival without using symbolic representation at all. These robotics researchers advocated building intelligence "from the bottom up".[ad]

A precursor to this idea was David Marr, who had come to MIT in the late 1970s from a successful background in theoretical neuroscience to lead the group studying vision. He rejected all symbolic approaches (both McCarthy's logic and Minsky's frames), arguing that AI needed to understand the physical machinery of vision from the bottom up before any symbolic processing took place. (Marr's work would be cut short by leukemia in 1980.)[207]

In his 1990 paper "Elephants Don't Play Chess,"[208] robotics researcher Brooks took direct aim at the physical symbol system hypothesis, arguing that symbols are not always necessary since "the world is its own best model. It is always exactly up to date. It always has every detail there is to be known. The trick is to sense it appropriately and often enough."[209]

In the 1980s and 1990s, many cognitive scientists also rejected the symbol processing model of the mind and argued that the body was essential for reasoning, a theory called the "embodied mind thesis".[210]

Soft computing uses methods that work with incomplete and imprecise information. They do not attempt to give precise, logical answers, but give results that are only "probably" correct. This allowed them to solve problems that precise symbolic methods could not handle. Press accounts often claimed these tools could "think like a human".[211][212]

Judea Pearl's Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, an influential 1988 book[213] brought probability and decision theory into AI.[214] Fuzzy logic, developed by Lofti Zadeh in the 60s, began to be more widely used in AI and robotics. Evolutionary computation and artificial neural networks also handle imprecise information, and are classified  as "soft". In the 90s and early 2000s many other soft computing tools were developed and put into use, including Bayesian networks,[214] hidden Markov models,[214] information theory and stochastic modeling. These tools in turn depended on advanced mathematical techniques such as classical optimization. For a time in the 1990s and early 2000s, these soft tools were studied by a subfield of AI called "computational intelligence".[215]

Reinforcement learning[216] gives an agent a reward every time it performs a desired action well, and may give negative rewards (or "punishments") when it performs poorly. It was described in the first half of the twentieth century by psychologists using animal models, such as Thorndike,[217][218] Pavlov[219] and Skinner.[220] In the 1950s, Alan Turing[218][221] and Arthur Samuel[218] foresaw the role of reinforcement learning in AI.

A successful and influential research program was led by Richard Sutton and Andrew Barto beginning 1972. Their collaboration revolutionized the study of reinforcement learning and decision making over the four decades.[222][223] In 1988, Sutton described machine learning in terms of decision theory (i.e., the Markov decision process). This gave the subject a solid theoretical foundation and access to a large body of theoretical results developed in the field of operations research.[223]

Also in 1988, Sutton and Barto developed the "temporal difference" (TD) learning algorithm, where the agent is rewarded only when its predictions about the future show improvement. It significantly outperformed previous algorithms.[224] TD-learning was used by Gerald Tesauro in 1992 in the program TD-Gammon, which played backgammon as well as the best human players. The program learned the game by playing against itself with zero prior knowledge.[225] In an interesting case of interdisciplinary convergence, neurologists discovered in 1997 that the dopamine reward system in brains also uses a version of the TD-learning algorithm.[226][227][228] TD learning would be become highly influential in the 21st century, used in both AlphaGo and AlphaZero.[229]

The business community's fascination with AI rose and fell in the 1980s in the classic pattern of an economic bubble. As dozens of companies failed, the perception in the business world was that the technology was not viable.[230] The damage to AI's reputation would last into the 21st century. Inside the field there was little agreement on the reasons for AI's failure to fulfill the dream of human level intelligence that had captured the imagination of the world in the 1960s. Together, all these factors helped to fragment AI into competing subfields focused on particular problems or approaches, sometimes even under new names that disguised the tarnished pedigree of "artificial intelligence".[231]

Over the next 20 years, AI consistently delivered working solutions to specific isolated problems. By the late 1990s, it was being used throughout the technology industry, although somewhat behind the scenes. The success was due to increasing computer power, by collaboration with other fields (such as mathematical optimization and statistics) and using the highest standards of scientific accountability.                                                                            By 2000, AI had achieved some of its oldest goals. The field was both more cautious and more successful than it had ever been.

The term "AI winter" was coined by researchers who had survived the funding cuts of 1974 when they became concerned that enthusiasm for expert systems had spiraled out of control and that disappointment would certainly follow.[ae] Their fears were well founded: in the late 1980s and early 1990s, AI suffered a series of financial setbacks.[125]

The first indication of a change in weather was the sudden collapse of the market for specialized AI hardware in 1987. Desktop computers from Apple and IBM had been steadily gaining speed and power and in 1987 they became more powerful than the more expensive Lisp machines made by Symbolics and others. There was no longer a good reason to buy them. An entire industry worth half a billion dollars was demolished overnight.[233]

Eventually the earliest successful expert systems, such as XCON, proved too expensive to maintain. They were difficult to update, they could not learn, and they were "brittle" (i.e., they could make grotesque mistakes when given unusual inputs). Expert systems proved useful, but only in a few special contexts.[234]

In the late 1980s, the Strategic Computing Initiative cut funding to AI "deeply and brutally". New leadership at DARPA had decided that AI was not "the next wave" and directed funds towards projects that seemed more likely to produce immediate results.[235]

By 1991, the impressive list of goals penned in 1981 for Japan's Fifth Generation Project had not been met. Indeed, some of them, like "carry on a casual conversation" would not be accomplished for another 40 years. As with other AI projects, expectations had run much higher than what was actually possible.[236][af]

Over 300 AI companies had shut down, gone bankrupt, or been acquired by the end of 1993, effectively ending the first commercial wave of AI.[238] In 1994, HP Newquist stated in The Brain Makers that "The immediate future of artificial intelligence—in its commercial form—seems to rest in part on the continued success of neural networks."[238]

In the 1990s, algorithms originally developed by AI researchers began to appear as parts of larger systems. AI had solved a lot of very difficult problems[ag] and their solutions proved to be useful throughout the technology industry,[239][240] such as data mining, industrial robotics, logistics, speech recognition,[241] banking software,[242] medical diagnosis[242] and Google's search engine.[243][244]

The field of AI received little or no credit for these successes in the 1990s and early 2000s. Many of AI's greatest innovations have been reduced to the status of just another item in the tool chest of computer science.[245] Nick Bostrom explains: "A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore."[242]

Many researchers in AI in the 1990s deliberately called their work by other names, such as informatics, knowledge-based systems, "cognitive systems" or computational intelligence. In part, this may have been because they considered their field to be fundamentally different from AI, but also the new names help to procure funding.[241][246][247] In the commercial world at least, the failed promises of the AI Winter continued to haunt AI research into the 2000s, as the New York Times reported in 2005: "Computer scientists and software engineers avoided the term artificial intelligence for fear of being viewed as wild-eyed dreamers."[248]

AI researchers began to develop and use sophisticated mathematical tools more than they ever had in the past.[249][250] Most of the new directions in AI relied heavily on mathematical models, including artificial neural networks, probabilistic reasoning, soft computing and reinforcement learning. In the 90s and 2000s, many other highly mathematical tools were adapted for AI. These tools were applied to machine learning, perception and mobility.

There was a widespread realization that many of the problems that AI needed to solve were already being worked on by researchers in fields like statistics, mathematics, electrical engineering, economics or operations research. The shared mathematical language allowed both a higher level of collaboration with more established and successful fields and the achievement of results which were measurable and provable; AI had become a more rigorous "scientific" discipline.

Another key reason for the success in the 90s was that AI researchers focussed on specific problems with verifiable solutions (an approach later derided as narrow AI). This provided useful tools in the present, rather than speculation about the future.

A new paradigm called "intelligent agents" became widely accepted during the 1990s.[251][252][ah] Although earlier researchers had proposed modular "divide and conquer" approaches to AI,[ai] the intelligent agent did not reach its modern form until Judea Pearl, Allen Newell, Leslie P. Kaelbling, and others brought concepts from decision theory and economics into the study of AI.[253] When the economist's definition of a rational agent was married to computer science's definition of an object or module, the intelligent agent paradigm was complete.

An intelligent agent is a system that perceives its environment and takes actions which maximize its chances of success. By this definition, simple programs that solve specific problems are "intelligent agents", as are human beings and organizations of human beings, such as firms. The intelligent agent paradigm defines AI research as "the study of intelligent agents".[aj] This is a generalization of some earlier definitions of AI: it goes beyond studying human intelligence; it studies all kinds of intelligence.

The paradigm gave researchers license to study isolated problems and to disagree about methods, but still retain hope that their work could be combined into an agent architecture that would be capable of general intelligence.[254]

On May 11, 1997, Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov.[255] In 2005, a Stanford robot won the DARPA Grand Challenge by driving autonomously for 131 miles along an unrehearsed desert trail. Two years later, a team from CMU won the DARPA Urban Challenge by autonomously navigating 55 miles in an urban environment while responding to traffic hazards and adhering to traffic laws.[256]

These successes were not due to some revolutionary new paradigm, but mostly on the tedious application of engineering skill and on the tremendous increase in the speed and capacity of computers by the 90s.[ak] In fact, Deep Blue's computer was 10 million times faster than the Ferranti Mark 1 that Christopher Strachey taught to play chess in 1951.[al] This dramatic increase is measured by Moore's law, which predicts that the speed and memory capacity of computers doubles every two years. The fundamental problem of "raw computer power" was slowly being overcome.

In the first decades of the 21st century, access to large amounts of data (known as "big data"), cheaper and faster computers and advanced machine learning techniques were successfully applied to many problems throughout the economy. A turning point was the success of deep learning around 2012 which improved the performance of machine learning on many tasks, including image and video processing, text analysis, and speech recognition.[258] Investment in AI increased along with its capabilities, and by 2016, the market for AI-related products, hardware, and software reached more than $8 billion, and the New York Times reported that interest in AI had reached a "frenzy".[259]

In 2002, Ben Goertzel and others became concerned that AI had largely abandoned its original goal of producing versatile, fully intelligent machines, and argued in favor of more direct research into artificial general intelligence. By the mid-2010s several companies and institutions had been founded to pursue Artificial General Intelligence (AGI), such as OpenAI and Google's DeepMind. During the same period, new insights into superintelligence raised concerns that AI was an existential threat. The risks and unintended consequences of AI technology became an area of serious academic research after 2016.

The success of machine learning in the 2000s depended on the availability of vast amounts of training data and faster computers.[260] Russell and Norvig wrote that the "improvement in performance obtained by increasing the size of the data set by two or three orders of magnitude outweighs any improvement that can be made by tweaking the algorithm."[203] Geoffrey Hinton recalled that back in the 90s, the problem was that "our labeled datasets were thousands of times too small. [And] our computers were millions of times too slow."[261] This was no longer true by 2010.

The most useful data in the 2000s came from curated, labeled data sets created specifically for machine learning and AI. In 2007, a group at UMass Amherst released Labeled Faces in the Wild, an annotated set of images of faces that was widely used to train and test face recognition systems for the next several decades.[262] Fei-Fei Li developed ImageNet, a database of three million images captioned by volunteers using the Amazon Mechanical Turk. Released in 2009, it was a useful body of training data and a benchmark for testing for the next generation of image processing systems.[263][203] Google released word2vec in 2013 as an open source resource. It used large amounts of data text scraped from the internet and word embedding to create a numeric vector to represent each word. Users were surprised at how well it was able to capture word meanings, for example, ordinary vector addition would give equivalences like China + River = Yangtze, London-England+France = Paris.[264] This database in particular would be essential for the development of large language models in the late 2010s.

The explosive growth of the internet gave machine learning programs access to billions of pages of text and images that could be scraped. And, for specific problems, large privately held databases contained the relevant data. McKinsey Global Institute reported that "by 2009, nearly all sectors in the US economy had at least an average of 200 terabytes of stored data".[265] This collection of information was known in the 2000s as big data.

In a Jeopardy! exhibition match in February 2011, IBM's question answering system Watson defeated the two best Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin.[266] Watson's expertise would have been impossible without the information available on the internet.[203]

In 2012, AlexNet, a deep learning model,[am] developed by Alex Krizhevsky, won the ImageNet Large Scale Visual Recognition Challenge, with significantly fewer errors than the second-place winner.[268][203] Krizhevsky worked with Geoffrey Hinton at the University of Toronto.[an] This was a turning point in machine learning: over the next few years dozens of other approaches to image recognition were abandoned in favor of deep learning.[260]

Deep learning uses a multi-layer perceptron. Although this architecture has been known since the 60s, getting it to work requires powerful hardware and large amounts of training data.[269] Before these became 
available, improving performance of image processing systems required hand-crafted ad hoc features that were difficult to implement.[269] Deep learning was simpler and more general.[ao]

Deep learning was applied to dozens of problems over the next few years (such as speech recognition, machine translation, medical diagnosis, and game playing). In every case it showed enormous gains in performance.[260] Investment and interest in AI boomed as a result.[260]

It became fashionable in the 2000s to begin talking about the future of AI again and several popular books considered the possibility of superintelligent machines and what they might mean for human society. Some of this was optimistic (such as Ray Kurzweil's The Singularity is Near), but others warned that a sufficiently powerful AI was existential threat to humanity, such as Nick Bostrom and Eliezer Yudkowsky.[270] The topic became widely covered in the press and many leading intellectuals and politicians commented on the issue.

AI programs in the 21st century are defined by their goals – the specific measures that they are designed to optimize. Nick Bostrom's influential 2005 book Superintelligence argued that, if one isn't careful about defining these goals, the machine may cause harm to humanity in the process of achieving a goal. Stuart J. Russell used the example of an intelligent robot that kills its owner to prevent it from being unplugged, reasoning "you can't fetch the coffee if you're dead".[271] (This problem is known by the technical term "instrumental convergence".) The solution is to align the machine's goal function with the goals of its owner and humanity in general. Thus, the problem of mitigating the risks and unintended consequences of AI became known as "the value alignment problem" or AI alignment.[272]

At the same time, machine learning systems had begun to have disturbing unintended consequences. Cathy O'Neil explained how statistical algorithms had been among the causes of the 2008 economic crash,[273] Julia Angwin of ProPublica argued that the COMPAS system used by the criminal justice system exhibited racial bias under some measures,[274][ap] others showed that many machine learning systems exhibited some form of racial bias,[276] and there were many other examples of dangerous outcomes that had resulted from machine learning systems.[aq]

In 2016, the election of Donald Trump and the controversy over the COMPAS system illuminated several problems with the current technological infrastructure, including misinformation, social media algorithms designed to maximize engagement, the misuse of personal data and the trustworthiness of predictive models.[277] Issues of fairness and unintended consequences became significantly more popular at AI conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The value alignment problem became a serious field of academic study.[278][ar]

In the early 2000s, several researchers became concerned that mainstream AI was too focused on "measurable performance in specific applications"[280] (known as "narrow AI") and had abandoned AI's original goal of creating versatile, fully intelligent machines. An early critic was Nils Nilsson in 1995, and similar opinions were published by AI elder statesmen John McCarthy, Marvin Minsky, and Patrick Winston in 2007–2009. Minsky organized a symposium on "human-level AI" in 2004.[280] Ben Goertzel adopted the term "artificial general intelligence" for the new sub-field, founding a journal and holding conferences beginning in 2008.[281] The new field grew rapidly, buoyed by the continuing success of artificial neural networks and the hope that it was the key to AGI.

Several competing companies, laboratories and foundations were founded to develop AGI in the 2010s. DeepMind was founded in 2010 by three English scientists, Demis Hassabis, Shane Legg and Mustafa Suleyman, with funding from Peter Thiel and later Elon Musk. The founders and financiers were deeply concerned about AI safety and the existential risk of AI. DeepMind's founders had a personal connection with Yudkowsky and Musk was among those who was actively raising the alarm.[282] Hassabis was both worried about the dangers of AGI and optimistic about its power; he hoped they could "solve AI, then solve everything else."[283] The New York Times wrote in 2023 "At the heart of this competition is a brain-stretching paradox. The people who say they are most worried about AI are among the most determined to create it and enjoy its riches. They have justified their ambition with their strong belief that they alone can keep AI from endangering Earth."[282]

In 2012, Geoffrey Hinton (who been leading neural network research since the 80s) was approached by Baidu, which wanted to hire him and all his students for an enormous sum. Hinton decided to hold an auction and, at a Lake Tahoe AI conference, they sold themselves to Google for a price of $44 million. Hassabis took notice and sold DeepMind to Google in 2014, on the condition that it would not accept military contracts and would be overseen by an ethics board.[282]

Larry Page of Google, unlike Musk and Hassabis, was an optimist about the future of AI. Musk and Paige became embroiled in an argument about the risk of AGI at Musk's 2015 birthday party. They had been friends for decades but stopped speaking to each other shortly afterwards. Musk attended the one and only meeting of the DeepMind's ethics board, where it became clear that Google was uninterested in mitigating the harm of AGI. Frustrated by his lack of influence he founded OpenAI in 2015, enlisting Sam Altman to run it and hiring top scientists. OpenAI began as a non-profit, "free from the economic incentives that were driving Google and other corporations."[282] Musk became frustrated again and left the company in 2018. OpenAI turned to Microsoft for continued financial support and Altman and OpenAI formed a for-profit version of the company with more than $1 billion in financing.[282]

In 2021, Dario Amodei  and 14 other scientists left OpenAI over concerns that the company was putting profits above safety. They formed Anthropic, which soon had $6 billion in financing from Microsoft and Google.[282]

The AI boom started with the initial development of key architectures and algorithms such as the transformer architecture in 2017, leading to the scaling and development of large language models exhibiting human-like traits of knowledge, attention and creativity. The new AI era began since 2020, with the public release of scaled large language models (LLMs) such as ChatGPT.[285]

In 2017, the transformer architecture was proposed by Google researchers. It exploits an attention mechanism and became widely used in large language models.[286]

Large language models, based on the transformer, were developed by AGI companies: OpenAI released GPT-3 in 2020, and DeepMind released Gato in 2022. These are foundation models: they are trained on vast quantities of unlabeled data and can be adapted to a wide range of downstream tasks.[citation needed]

These models can discuss a huge number of topics and display general knowledge. The question naturally arises: are these models an example of artificial general intelligence? Bill Gates was skeptical of the new technology and the hype that surrounded AGI. However, Altman presented him with a live demo of ChatGPT4 passing an advanced biology test. Gates was convinced.[282] In 2023, Microsoft Research tested the model with a large variety of tasks, and concluded that "it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system".[287]

In 2024, OpenAI o3, a type of advanced reasoning model developed by OpenAI was announced. On the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) benchmark developed by François Chollet in 2019, the model achieved an unofficial score of 87.5% on the semi-private test, surpassing the typical human score of 84%. The benchmark is supposed to be a necessary, but not sufficient test for AGI. Speaking of the benchmark, Chollet has said "You’ll know AGI is here when the exercise of creating tasks that are easy for regular humans but hard for AI becomes simply impossible."[288]

Investment in AI grew exponentially after 2020, with venture capital funding for generative AI companies increasing dramatically. Total AI investments rose from $18 billion in 2014 to $119 billion in 2021, with generative AI accounting for approximately 30% of investments by 2023.[289] According to metrics from 2017 to 2021, the United States outranked the rest of the world in terms of venture capital funding, number of startups, and AI patents granted.[290] The commercial AI scene became dominated by American Big Tech companies, whose investments in this area surpassed those from U.S.-based venture capitalists.[291] OpenAI's valuation reached $86 billion by early 2024,[292] while NVIDIA's market capitalization surpassed $3.3 trillion by mid-2024, making it the world's largest company by market capitalization as the demand for AI-capable GPUs surged.[293]

15.ai, launched in March 2020[294] by an anonymous MIT researcher,[295][296] was one of the earliest examples of generative AI gaining widespread public attention during the initial stages of the AI boom.[297] The free web application demonstrated the ability to clone character voices using neural networks with minimal training data, requiring as little as 15 seconds of audio to reproduce a voice—a capability later corroborated by OpenAI in 2024.[298] The service went viral on social media platforms in early 2021,[299][300] allowing users to generate speech for characters from popular media franchises, and became particularly notable for its pioneering role in popularizing AI voice synthesis for creative content and memes.[301]

Contemporary AI systems are now becoming human-competitive at general tasks, and we must ask ourselves: Should we let machines flood our information channels with propaganda and untruth? Should we automate away all the jobs, including the fulfilling ones? Should we develop nonhuman minds that might eventually outnumber, outsmart, obsolete and replace us? Should we risk loss of control of our civilization? Such decisions must not be delegated to unelected tech leaders. Powerful AI systems should be developed only once we are confident that their effects will be positive and their risks will be manageable. This confidence must be well justified and increase with the magnitude of a system’s potential effects. OpenAI’s recent statement regarding artificial general intelligence, states that "At some point, it may be important to get independent review before starting to train future systems, and for the most advanced efforts to agree to limit the rate of growth of compute used for creating new models." We agree. That point is now.

Therefore, we call on all AI labs to immediately pause for at least 6 months the training of AI systems more powerful than GPT-4. This pause should be public and verifiable, and include all key actors. If such a pause cannot be enacted quickly, governments should step in and institute a moratorium.

ChatGPT was launched on November 30, 2022, marking a pivotal moment in artificial intelligence's public adoption. Within days of its release it went viral, gaining over 100 million users in two months and becoming the fastest-growing consumer software application in history.[303] The chatbot's ability to engage in human-like conversations, write code, and generate creative content captured public imagination and led to rapid adoption across various sectors including education, business, and research.[304] ChatGPT's success prompted unprecedented responses from major technology companies—Google declared a "code red" and rapidly launched Gemini (formerly known as Google Bard), while Microsoft incorporated the technology into Bing Chat.[305]

The rapid adoption of these AI technologies sparked intense debate about their implications. Notable AI researchers and industry leaders voiced both optimism and concern about the accelerating pace of development. In March 2023, over 20,000 signatories, including computer scientist Yoshua Bengio, Elon Musk, and Apple co-founder Steve Wozniak, signed an open letter calling for a pause in advanced AI development, citing "profound risks to society and humanity."[306] However, other prominent researchers like Juergen Schmidhuber took a more optimistic view, emphasizing that the majority of AI research aims to make "human lives longer and healthier and easier."[307]

By mid-2024, however, the financial sector began to scrutinize AI companies more closely, particularly questioning their capacity to produce a return on investment commensurate with their massive valuations. Some prominent investors raised concerns about market expectations becoming disconnected from fundamental business realities. Jeremy Grantham, co-founder of GMO LLC, warned investors to "be quite careful" and drew parallels to previous technology-driven market bubbles.[308] Similarly, Jeffrey Gundlach, CEO of DoubleLine Capital, explicitly compared the AI boom to the dot-com bubble of the late 1990s, suggesting that investor enthusiasm might be outpacing realistic near-term capabilities and revenue potential.[309] These concerns were amplified by the substantial market capitalizations of AI-focused companies, many of which had yet to demonstrate sustainable profitability models.

In March 2024, Anthropic released the Claude 3 family of large language models, including Claude 3 Haiku, Sonnet, and Opus.[310] The models demonstrated significant improvements in capabilities across various benchmarks, with Claude 3 Opus notably outperforming leading models from OpenAI and Google.[311] In June 2024, Anthropic released Claude 3.5 Sonnet, which demonstrated improved performance compared to the larger Claude 3 Opus, particularly in areas such as coding, multistep workflows, and image analysis.[312]

In 2024, the Royal Swedish Academy of Sciences awarded Nobel Prizes in recognition of groundbreaking contributions to artificial intelligence. The recipients included:

In January 2025, OpenAI announced a new AI, ChatGPT-Gov, which would be specifically designed for US government agencies to use securely. [314] Open AI said that agencies could utilize ChatGPT Gov on a Microsoft Azure cloud or Azure Government cloud, "on top of Microsoft’s Azure’s OpenAI Service." OpenAI's announcement stated that "Self-hosting ChatGPT Gov enables agencies to more easily manage their own security, privacy, and compliance requirements, such as stringent cybersecurity frameworks (IL5, CJIS, ITAR, FedRAMP High). Additionally, we believe this infrastructure will expedite internal authorization of OpenAI’s tools for the handling of non-public sensitive data."[314]

In January 2025, a significant development in AI infrastructure investment occurred with the formation of Stargate LLC. The joint venture, created by OpenAI, SoftBank, Oracle, and MGX, announced plans to invest US$500 billion in AI infrastructure across the United States by 2029, starting with US$100 billion. The venture was formally announced by U.S. President Donald Trump on January 21, 2025, with SoftBank CEO Masayoshi Son appointed as chairman.[315][316]

.The history of private equity, venture capital, and the development of these asset classes has occurred through a series of boom-and-bust cycles since the middle of the 20th century. Within the broader private equity industry, two distinct sub-industries, leveraged buyouts and venture capital experienced growth along parallel, although interrelated tracks.

Since the origins of the modern private equity industry in 1946, there have been four major epochs marked by three boom and bust cycles. The early history of private equity—from 1946 through 1981—was characterized by relatively small volumes of private equity investment, rudimentary firm organizations and limited awareness of and familiarity with the private equity industry. The first boom and bust cycle, from 1982 through 1993, was characterized by the dramatic surge in leveraged buyout activity financed by junk bonds and culminating in the massive buyout of RJR Nabisco before the near collapse of the leveraged buyout industry in the late 1980s and early 1990s. The second boom and bust cycle (from 1992 through 2002) emerged from the ashes of the savings and loan crisis, the insider trading scandals, the real estate market collapse and the recession of the early 1990s.  This period saw the emergence of more institutionalized private equity firms, ultimately culminating in the massive dot-com bubble in 1999 and 2000. The third boom and bust cycle (from 2003 through 2007) came in the wake of the collapse of the dot-com bubble—leveraged buyouts reach unparalleled size and the institutionalization of private equity firms is exemplified by the Blackstone Group's 2007 initial public offering.

In its early years through to roughly the year 2000, the private equity and venture capital asset classes were primarily active in the United States. With the second private equity boom in the mid-1990s and liberalization of regulation for institutional investors in Europe, a mature European private equity market emerged.

Investors have been acquiring businesses and making minority investments in privately held companies since the dawn of the industrial revolution.  Merchant bankers in London and Paris financed industrial concerns in the 1850s; most notably Crédit Mobilier, founded in 1854 by Jacob and Isaac Pereire, who together with New York-based Jay Cooke financed the United States Transcontinental Railroad.

Later, J. Pierpont Morgan's J.P. Morgan & Co. would finance railroads and other industrial companies throughout the United States.  In certain respects, J. Pierpont Morgan's 1901 acquisition of Carnegie Steel Company from Andrew Carnegie and Henry Phipps for $480 million represents the first true major buyout as they are thought of today.[according to whom?]

Due to structural restrictions imposed on American banks under the Glass–Steagall Act and other regulations in the 1930s, there was no private merchant banking industry in the United States, a situation that was quite exceptional in developed nations.[according to whom?]  As late as the 1980s, Lester Thurow, a noted economist, decried the inability of the financial regulation framework in the United States to support merchant banks. US investment banks were confined primarily to advisory businesses, handling mergers and acquisitions transactions and placements of equity and debt securities.  Investment banks would later enter the space, however long after independent firms had become well established.

With few exceptions, private equity in the first half of the 20th century was the domain of wealthy individuals and families.  The Vanderbilts, Whitneys, Rockefellers and Warburgs were notable investors in private companies in the first half of the century.  In 1938, Laurance S. Rockefeller helped finance the creation of both Eastern Air Lines and Douglas Aircraft and the Rockefeller family had vast holdings in a variety of companies. Eric M. Warburg founded E.M. Warburg & Co. in 1938, which would ultimately become Warburg Pincus, with investments in both leveraged buyouts and venture capital.

It was not until after World War II that what is considered today to be true private equity investments began to emerge marked by the founding of the first two venture capital firms in 1946: American Research and Development Corporation (ARDC) and J.H. Whitney & Company.[1]

ARDC was founded by Georges Doriot, the "father of venture capitalism"[2] (founder of INSEAD and former dean of Harvard Business School), with Ralph Flanders and Karl Compton (former president of MIT), to encourage private sector investments in businesses run by soldiers who were returning from World War II.  ARDC's significance was primarily that it was the first institutional private equity investment firm that raised capital from sources other than wealthy families although it had several notable investment successes as well.[3]  ARDC is credited with the first major venture capital success story when its 1957 investment of $70,000 in Digital Equipment Corporation (DEC) would be valued at over $35.5 million after the company's initial public offering in 1968 (representing a return of over 500 times on its investment and an annualized rate of return of 101%).[4]  Former employees of ARDC went on to found several prominent venture capital firms including Greylock Partners (founded in 1965 by Charlie Waite and Bill Elfers) and Morgan, Holland Ventures, the predecessor of Flagship Ventures (founded in 1982 by James Morgan).[5]  ARDC continued investing until 1971 with the retirement of Doriot.  In 1972, Doriot merged ARDC with Textron after having invested in over 150 companies.

J.H. Whitney & Company was founded by John Hay Whitney and his partner Benno Schmidt.  Whitney had been investing since the 1930s, founding Pioneer Pictures in 1933 and acquiring a 15% interest in Technicolor Corporation with his cousin Cornelius Vanderbilt Whitney.  By far, Whitney's most famous investment was in Florida Foods Corporation. The company, having developed an innovative method for delivering nutrition to American soldiers, later came to be known as Minute Maid orange juice and was sold to The Coca-Cola Company in 1960.  J.H. Whitney & Company continues to make investments in leveraged buyout transactions and raised $750 million for its sixth institutional private equity fund in 2005.

Before World War II, venture capital investments (originally known as "development capital") were primarily the domain of wealthy individuals and families.  One of the first steps toward a professionally managed venture capital industry was the passage of the Small Business Investment Act of 1958.  The 1958 Act officially allowed the U.S. Small Business Administration (SBA) to license private "Small Business Investment Companies" (SBICs) to help the financing and management of the small entrepreneurial businesses in the United States.  Passage of the Act addressed concerns raised in a Federal Reserve Board report to Congress that concluded that a major gap existed in the capital markets for long-term funding for growth-oriented small businesses. It was thought that fostering entrepreneurial companies would spur technological advances to compete against the Soviet Union. Facilitating the flow of capital through the economy up to the pioneering small concerns in order to stimulate the U.S. economy was and still is the main goal of the SBIC program today.[6]  The 1958 Act provided venture capital firms structured either as SBICs or Minority Enterprise Small Business Investment Companies (MESBICs) access to federal funds which could be leveraged at a ratio of up to 4:1 against privately raised investment funds.  The success of the Small Business Administration's efforts are viewed primarily in terms of the pool of professional private equity investors that the program developed as the rigid regulatory limitations imposed by the program minimized the role of SBICs.  In 2005, the SBA significantly reduced its SBIC program, though SBICs continue to make private equity investments.

The real growth in Private Equity surged in 1984 to 1991 period when Institutional Investors, e.g. Pension Plans, Foundations and Endowment Funds such as the Shell Pension Plan, the Oregon State Pension Plan, the Ford Foundation and the Harvard Endowment Fund started investing a small part of their trillion dollars portfolios into Private Investments - particularly venture capital and Leverage Buyout Funds.

During the 1960s and 1970s, venture capital firms focused their investment activity primarily on starting and expanding companies. More often than not, these companies were exploiting breakthroughs in electronic, medical or data-processing technology. As a result, venture capital came to be almost synonymous with technology finance.

It is commonly noted that the first venture-backed startup was Fairchild Semiconductor (which produced the first commercially practicable integrated circuit), funded in late 1957 by a loan from Sherman Fairchild's Fairchild Camera with the help of Arthur Rock, an early venture capitalist with the firm of Hayden Stone in New York (which received 20% of the equity of the newly formed company). Another early VC firm was Venrock Associates.[7]  Venrock was founded in 1969 by Laurance S. Rockefeller, the fourth of John D. Rockefeller's six children as a way to allow other Rockefeller children to develop exposure to venture capital investments.

It was also in the 1960s that the common form of private equity fund, still in use today, emerged. Private equity firms organized limited partnerships to hold investments in which the investment professionals served as general partner and the investors, who were passive limited partners, put up the capital.  The compensation structure, still in use today, also emerged with limited partners paying an annual management fee of 1–2% and a carried interest typically representing up to 20% of the profits of the partnership.

An early West Coast venture capital company was Draper and Johnson Investment Company, formed in 1962[8] by William Henry Draper III and Franklin P. Johnson Jr. In 1964 Bill Draper and Paul Wythes founded Sutter Hill Ventures, and Pitch Johnson formed Asset Management Company.

The growth of the venture capital industry was fueled by the emergence of the independent investment firms on Sand Hill Road, beginning with Kleiner, Perkins, Caufield & Byers and Sequoia Capital in 1972.  Located in Menlo Park, California, Kleiner Perkins, Sequoia and later venture capital firms would have access to the burgeoning technology industries in the area.  Kleiner Perkins was the first venture capital firm to open an office on Sand Hill Road in 1972.[9]

By the early 1970s, there were many semiconductor companies based in the Santa Clara Valley as well as early computer firms using their devices and programming and service companies.[10]  Throughout the 1970s, a group of private equity firms, focused primarily on venture capital investments, would be founded that would become the model for later leveraged buyout and venture capital investment firms.  In 1973, with the number of new venture capital firms increasing, leading venture capitalists formed the National Venture Capital Association (NVCA).  The NVCA was to serve as the industry trade group for the venture capital industry.[11]  Venture capital firms suffered a temporary downturn in 1974, when the stock market crashed and investors were naturally wary of this new kind of investment fund. It was not until 1978 that venture capital experienced its first major fundraising year, as the industry raised approximately $750 million.  During this period, the number of venture firms also increased.  Among the firms founded in this period, in addition to Kleiner Perkins and Sequoia, that continue to invest actively are AEA Investors, TA Associates, Mayfield Fund, Apax Partners, New Enterprise Associates, Oak Investment Partners and Sevin Rosen Funds.

Venture capital played an instrumental role in developing many of the major technology companies of the 1980s.  Some of the most notable venture capital investments were made in firms that include: Tandem Computers, Genentech, Apple Inc., Electronic Arts, Compaq, Federal Express and LSI Corporation.

Although not strictly private equity, and certainly not labeled so at the time, the first leveraged buyout may have been the purchase by Malcolm McLean's McLean Industries, Inc. of Pan-Atlantic Steamship Company in January 1955 and Waterman Steamship Corporation in May 1955.[12] Under the terms of the transactions, McLean borrowed $42 million and raised an additional $7 million through an issue of preferred stock. When the deal closed, $20 million of Waterman cash and assets were used to retire $20 million of the loan debt. The newly elected board of Waterman then voted to pay an immediate dividend of $25 million to McLean Industries.[13]

Similar to the approach employed in the McLean transaction, the use of publicly traded holding companies as investment vehicles to acquire portfolios of investments in corporate assets would become a new trend in the 1960s popularized by the likes of Warren Buffett (Berkshire Hathaway) and Victor Posner (DWG Corporation)[14] and later adopted by Nelson Peltz (Triarc), Saul Steinberg (Reliance Insurance) and Gerry Schwartz (Onex Corporation). These investment vehicles would utilize a number of the same tactics and target the same type of companies as more traditional leveraged buyouts and in many ways could be considered a forerunner of the later private equity firms.  In fact, it is Posner who is often credited with coining the term "leveraged buyout" (LBO).[15]

Posner, who had made a fortune in real estate investments in the 1930s and 1940s acquired a major stake in DWG Corporation in 1966.[14][16] Having gained control of the company, he used it as an investment vehicle that could execute takeovers of other companies. Posner and DWG are perhaps best known for the hostile takeover of Sharon Steel Corporation in 1969, one of the earliest such takeovers in the United States.[16][17] Posner's investments were typically motivated by attractive valuations, balance sheets and cash flow characteristics.  Because of its high debt load, Posner's DWG would generate attractive but highly volatile returns and would ultimately land the company in financial difficulty.  In 1987, Sharon Steel sought Chapter 11 bankruptcy protection.[17]

Warren Buffett, who is typically described as a stock market investor rather than a private equity investor, employed many of the same techniques in the creation on his Berkshire Hathaway conglomerate as Posner's DWG Corporation and in later years by more traditional private equity investors.  In 1965, with the support of the company's board of directors, Buffett assumed control of Berkshire Hathaway.  At the time of Buffett's investment, Berkshire Hathaway was a textile company, however, Buffett used Berkshire Hathaway as an investment vehicle to make acquisitions and minority investments in dozens of the insurance and reinsurance industries (GEICO) and varied companies including: American Express, The Buffalo News, the Coca-Cola Company, Fruit of the Loom, Nebraska Furniture Mart and See's Candies.  Buffett's value investing approach and focus on earnings and cash flows are characteristic of later private equity investors.  Buffett would
distinguish himself relative to more traditional leveraged buyout practitioners through his reluctance to use leverage and hostile techniques in his investments.

Lewis Cullman's acquisition of Orkinin Exterminating Company in 1963 is among the first significant leveraged buyout transactions.[18][19][20] However, the industry that is today described as private equity was conceived by a number of corporate financiers, most notably Jerome Kohlberg Jr. and later his protégé, Henry Kravis.  Working for Bear Stearns at the time, Kohlberg and Kravis along with Kravis' cousin George Roberts began a series of what they described as "bootstrap" investments.  They targeted family-owned businesses, many of which had been founded in the years following World War II and by the 1960s and 1970s were facing succession issues.  Many of these companies lacked a viable or attractive exit for their founders as they were too small to be taken public and the founders were reluctant to sell out to competitors, making a sale to a financial buyer potentially attractive. In the following years, the three Bear Stearns bankers would complete a series of buyouts including Stern Metals (1965), Incom (a division of Rockwood International, 1971), Cobblers Industries (1971) and Boren Clay (1973) as well as Thompson Wire, Eagle Motors and Barrows through their investment in Stern Metals.  Although they had a number of highly successful investments, the $27 million investment in Cobblers ended in bankruptcy.[21]

By 1976, tensions had built up between Bear Stearns and Kohlberg, Kravis and Roberts leading to their departure and the formation of Kohlberg Kravis Roberts in that year.  Most notably, Bear Stearns executive Cy Lewis had rejected repeated proposals to form a dedicated investment fund within Bear Stearns and Lewis took exception to the amount of time spent on outside activities.[22]  Early investors included the Hillman Family[23]

By 1978, with the revision of the Employee Retirement Income Security Act regulations, the nascent KKR was successful in raising its first institutional fund with approximately $30 million of investor commitments.[24] That year, the firm signed a risky precedent-setting deal to buy the publicly traded conglomerate Houdaille Industries, which made machine tools, industrial pipes, chrome-plated car bumpers and torsional viscous dampers, for $380 million. The leveraged buyout was by far the largest take-private at the time[25] and soon ended in a spectacular failure, breakup of the half-a-century-old company and loss of thousands of jobs, even though creditors earned a profit.[26]

In 1974, Thomas H. Lee founded a new investment firm to focus on acquiring companies through leveraged buyout transactions, one of the earliest independent private equity firms to focus on leveraged buyouts of more mature companies rather than venture capital investments in growth companies.  Lee's firm, Thomas H. Lee Partners, while initially generating less fanfare than other entrants in the 1980s, would emerge as one of the largest private equity firms globally by the end of the 1990s.

The second half of the 1970s and the first years of the 1980s saw the emergence of several private equity firms that would survive the various cycles both in leveraged buyouts and venture capital.  Among the firms founded during these years were: Cinven, Forstmann Little & Company, Welsh, Carson, Anderson & Stowe, Candover, and GTCR.

Management buyouts also came into existence in the late 1970s and early 1980s.  One of the most notable early management buyout transactions was the acquisition of Harley-Davidson.  A group of managers at Harley-Davidson, the motorcycle manufacturer, bought the company from AMF in a leveraged buyout in 1981, but racked up big losses the following year and had to ask for protection from Japanese competitors.[citation needed]

The advent of the boom in leveraged buyouts in the 1980s was supported by three major legal and regulatory events:

In the years that would follow these events, private equity would experience its first major boom, acquiring some of the famed brands and major industrial powers of American business.

The decade of the 1980s is perhaps more closely associated with the leveraged buyout than any decade before or since.  For the first time, the public became aware of the ability of private equity to affect mainstream companies and "corporate raiders" and "hostile takeovers" entered the public consciousness.  The decade would see one of the largest booms in private equity culminating in the 1989 leveraged buyout of RJR Nabisco, which would reign as the largest leveraged buyout transaction for nearly 17 years.  In 1980, the private equity industry would raise approximately $2.4 billion of annual investor commitments and by the end of the decade in 1989 that figure stood at $21.9 billion marking the tremendous growth experienced.[30]

The beginning of the first boom period in private equity would be marked by the well-publicized success of the Gibson Greetings acquisition in 1982 and would roar ahead through 1983 and 1984 with the soaring stock market driving profitable exits for private equity investors.

In January 1982, former US Secretary of the Treasury William E. Simon, Ray Chambers and a group of investors, which would later come to be known as Wesray Capital Corporation, acquired Gibson Greetings, a producer of greeting cards.  The purchase price for Gibson was $80 million, of which only $1 million was rumored to have been contributed by the investors.  By mid-1983, just sixteen months after the original deal, Gibson completed a $290 million IPO and Simon made approximately $66 million.[31][32]  Simon and Wesray would later complete the $71.6 million acquisition of Atlas Van Lines.  The success of the Gibson Greetings investment attracted the attention of the wider media to the nascent boom in leveraged buyouts.

Between 1979 and 1989, it was estimated that there were over 2,000 leveraged buyouts valued in excess of $250 million[33]  Notable buyouts of this period (not described elsewhere in this article) include: Malone & Hyde (1984), Wometco Enterprises (1984), Beatrice Companies (1985), Sterling Jewelers (1985), Revco Drug Stores  (1986), Safeway (1986), Southland Corporation (1987), Jim Walter Corp (later Walter Industries, Inc., 1987), BlackRock (1988), Federated Department Stores (1988), Marvel Entertainment (1988), Uniroyal Goodrich Tire Company (1988) and Hospital Corporation of America (1989).

Because of the high leverage on many of the transactions of the 1980s, failed deals occurred regularly. However, the promise of attractive returns on successful investments attracted more capital.  With the increased leveraged buyout activity and investor interest, the mid-1980s saw a major proliferation of private equity firms.  Among the major firms founded in this period were: Bain Capital, Chemical Venture Partners, Hellman & Friedman, Hicks & Haas, (later Hicks Muse Tate & Furst), The Blackstone Group, Doughty Hanson, BC Partners, and The Carlyle Group.

As the market developed, new niches within the private equity industry began to emerge.  In 1982, Venture Capital Fund of America, the first private equity firm focused on acquiring secondary market interests in existing private equity funds was founded and then, two years later in 1984, First Reserve Corporation, the first private equity firm focused on the energy sector, was founded.

The public successes of the venture capital industry in the 1970s and early 1980s (e.g., DEC, Apple, Genentech) gave rise to a major proliferation of venture capital investment firms.  From just a few dozen firms at the start of the decade, there were over 650 firms by the end of the 1980s, each searching for the next major "home run". The capital managed by these firms increased from $3 billion to $31 billion over the course of the decade.[34]

The growth the industry was hampered by sharply declining returns and certain venture firms began posting losses for the first time.  In addition to the increased competition among firms, several other factors impacted returns.  The market for initial public offerings cooled in the mid-1980s before collapsing after the stock market crash in 1987 and foreign corporations, particularly from Japan and Korea, flooded early stage companies with capital.[34]

In response to the changing conditions, corporations that had sponsored in-house venture investment arms, including General Electric and Paine Webber, either sold off or closed these venture capital units. Venture capital units within Chemical Bank (today CCMP Capital) and Continental Illinois National Bank (today CIVC Partners), among others, began shifting their focus from funding early stage companies toward investments in more mature companies. Even industry founders J.H. Whitney & Company and Warburg Pincus began to transition toward leveraged buyouts and growth capital investments.[34][35][36]  Many of these venture capital firms attempted to stay close to their areas of expertise in the technology industry by acquiring companies in the industry that had reached certain levels of maturity.  In 1989, Prime Computer was acquired in a $1.3 billion leveraged buyout by J.H. Whitney & Company in what would prove to be a disastrous transaction.  Whitney's investment in Prime proved to be nearly a total loss with the bulk of the proceeds from the company's liquidation paid to the company's creditors.[37]

Although lower profile than their buyout counterparts, new leading venture capital firms were also formed including Draper Fisher Jurvetson (originally Draper Associates) in 1985 and Canaan Partners in 1987 among others.

Although buyout firms generally had different aims and methods, they were often lumped in with the "corporate raiders" who came on the scene in the 1980s. The raiders were best known for hostile bids—takeover attempts that were opposed by management. By contrast, private equity firms generally attempted to strike deals with boards and CEOs, though in many cases in the 1980s they allied with managements that were already under pressure from raiders. But both groups bought companies through leveraged buyouts; both relied heavily on junk bond financing; and under both types of owners in many cases major assets were sold, costs were slashed and employees were laid off. Hence, in the public mind, they were lumped together.[38]

Management of many large publicly traded corporations reacted negatively to the threat of potential hostile takeover or corporate raid and pursued drastic defensive measures including poison pills, golden parachutes and increasing debt levels on the company's balance sheet. The threat of the corporate raid would lead to the practice of "greenmail", where a corporate raider or other party would acquire a significant stake in the stock of a company and receive an incentive payment (effectively a bribe) from the company in order to avoid pursuing a hostile takeover of the company.  Greenmail represented a transfer payment from a company's existing shareholders to a third party investor and provided no value to existing shareholders but did benefit existing managers.  The practice of "greenmail" is not typically considered a tactic of private equity investors and is not condoned by market participants.

Among the most notable corporate raiders of the 1980s were Carl Icahn, Victor Posner, Nelson Peltz, Robert M. Bass, T. Boone Pickens, Harold Clark Simmons, Kirk Kerkorian, Sir James Goldsmith, Saul Steinberg and Asher Edelman. Carl Icahn developed a reputation as a ruthless corporate raider after his hostile takeover of TWA in 1985.[39]  The result of that takeover was Icahn systematically selling TWA's assets to repay the debt he used to purchase the company, which was described as asset stripping.[40]  In 1985, Pickens was profiled on the cover of Time magazine as "one of the most famous and controversial businessmen in the U.S." for his pursuit of Unocal, Gulf Oil and Cities Services.[41] In later years, many of the corporate raiders would be re-characterized as "Activist shareholders".

Many of the corporate raiders were onetime clients of Michael Milken, whose investment banking firm Drexel Burnham Lambert helped raise blind pools of capital with which corporate raiders could make a legitimate attempt to take over a company and provided high-yield debt financing of the buyouts.

Drexel Burnham raised a $100 million blind pool in 1984 for Nelson Peltz and his holding company Triangle Industries (later Triarc) to give credibility for takeovers, representing the first major blind pool raised for this purpose.  Two years later, in 1986, Wickes Companies, a holding company run by Sanford Sigoloff raised a $1.2 billion blind pool.[42]

In 1985, Milken raised $750 million for a similar blind pool for Ronald Perelman which would ultimately prove instrumental in acquiring his biggest target: The Revlon Corporation.  In 1980, Ronald Perelman, the son of a wealthy Philadelphia businessman, and future "corporate raider" having made several small but successful buyouts, acquired MacAndrews & Forbes, a distributor of licorice extract and chocolate that Perelman's father had tried and failed to acquire 10 years earlier.[43]  Perelman would ultimately divest the company's core business and use MacAndrews & Forbes as a holding company investment vehicle for subsequent leveraged buyouts including Technicolor, Inc., Pantry Pride and Revlon.  Using the Pantry Pride subsidiary of his holding company, MacAndrews & Forbes Holdings, Perelman's overtures were rebuffed.  Repeatedly rejected by the company's board and management, Perelman continued to press forward with a hostile takeover raising his offer from an initial bid of $47.50 per share until it reached $53.00 per share.  After receiving a higher offer from a white knight, private equity firm Forstmann Little & Company, Perelman's Pantry Pride finally was able to make a successful bid for Revlon, valuing the company at $2.7 billion.[44]  The buyout would prove troubling, burdened by a heavy debt load.[45][46][47]  Under Perelman's control, Revlon sold four divisions: two were sold for $1 billion, its vision care division was sold for $574 million and its National Health Laboratories division was spun out to the public market in 1988.  Revlon also made acquisitions including Max Factor in 1987 and Betrix in 1989 later selling them to Procter & Gamble in 1991.[48]  Perelman exited the bulk of his holdings in Revlon through an IPO in 1996 and subsequent sales of stock.  As of December 31, 2007, Perelman still retains a minority ownership interest in Revlon.  The Revlon takeover, because of its well-known brand, was profiled widely by the media and brought new attention to the emerging boom in leveraged buyout activity.

In later years, Milken and Drexel would shy away from certain of the more "notorious" corporate raiders as Drexel and the private equity industry attempted to move upscale.

Leveraged buyouts in the 1980s including Perelman's takeover of Revlon came to epitomize the "ruthless capitalism" and "greed" popularly seen to be pervading Wall Street at the time.  One of the final major buyouts of the 1980s proved to be its most ambitious and marked both a high-water mark and a sign of the beginning of the end of the boom that had begun nearly a decade earlier.  In 1989, Kohlberg Kravis Roberts (KKR) closed on a $31.1 billion takeover of RJR Nabisco. It was, at that time and for over 17 years, the largest leverage buyout in history. The event was chronicled in the book, Barbarians at the Gate: The Fall of RJR Nabisco, and later made into a television movie starring James Garner.

F. Ross Johnson was the President and CEO of RJR Nabisco at the time of the leveraged buyout and Henry Kravis was a general partner at KKR. The leveraged buyout was in the amount of $25 billion (plus assumed debt), and the battle for control took place between October and November 1988. KKR would eventually prevail in acquiring RJR Nabisco at $109 per share marking a dramatic increase from the original announcement that Shearson Lehman Hutton would take RJR Nabisco private at $75 per share. A fierce series of negotiations and horse-trading ensued which pitted KKR against Shearson Lehman Hutton and later Forstmann Little & Co.  Many of the major banking players of the day, including Morgan Stanley, Goldman Sachs, Salomon Brothers, and Merrill Lynch were actively involved in advising and financing the parties.

After Shearson Lehman's original bid, KKR quickly introduced a tender offer to obtain RJR Nabisco for $90 per share—a price that enabled it to proceed without the approval of RJR Nabisco's management. RJR's management team, working with Shearson Lehman and Salomon Brothers, submitted a bid of $112, a figure they felt certain would enable them to outflank any response by Kravis's team.  KKR's final bid of $109, while a lower dollar figure, was ultimately accepted by the board of directors of RJR Nabisco.  KKR's offer was guaranteed, whereas the management offer (backed by Shearson Lehman and Salomon) lacked a "reset", meaning that the final share price might have been lower than their stated $112 per share. Many in RJR's board of directors had grown concerned at recent disclosures of Ross Johnson' unprecedented golden parachute deal. Time magazine featured Ross Johnson on the cover of their December 1988 issue along with the headline, "A
Game of Greed: This man could pocket $100 million from the largest corporate takeover in history. Has the buyout craze gone too far?".[49]  KKR's offer was welcomed by the board, and, to some observers, it appeared that their elevation of the reset issue as a deal-breaker in KKR's favor was little more than an excuse to reject Ross Johnson's higher payout of $112 per share.  F. Ross Johnson received $53 million from the buyout.

At $31.1 billion of transaction value, RJR Nabisco was by far the largest leveraged buyouts in history.  In 2006 and 2007, a number of leveraged buyout transactions were completed that for the first time surpassed the RJR Nabisco leveraged buyout in terms of nominal purchase price.  However, adjusted for inflation, none of the leveraged buyouts of the 2006–2007 period would surpass RJR Nabisco.  Unfortunately for KKR, size would not equate with success as the high purchase price and debt load would burden the performance of the investment.  It had to pump additional equity into the company a year after the buyout closed and years later, when it sold the last of its investment, it had chalked up a $700 million loss.[50]

Two years earlier, in 1987, Jerome Kohlberg Jr. resigned from Kohlberg Kravis Roberts & Co. over differences in strategy. Kohlberg did not favor the larger buyouts (including Beatrice Companies (1985) and Safeway (1986) and would later likely have included the 1989 takeover of RJR Nabisco), highly leveraged transactions or hostile takeovers being pursued increasingly by KKR.[51] The split would ultimately prove acrimonious as Kohlberg sued Kravis and Roberts for what he alleged were improper business tactics. The case was later settled out of court.[52] Instead, Kohlberg chose to return to his roots, acquiring smaller, middle-market companies and in 1987, he would found a new private equity firm Kohlberg & Company along with his son James A. Kohlberg, at the time a KKR executive. Jerome Kohlberg would continue investing successfully for another seven years before retiring from Kohlberg & Company in 1994 and turning his firm over to his son.[53]

As the market reached its peak in 1988 and 1989, new private equity firms were founded which would emerge as major investors in the years to follow, including: ABRY Partners, Coller Capital, Landmark Partners, Leonard Green & Partners and Providence Equity Partners.

By the end of the 1980s the excesses of the buyout market were beginning to show, with the bankruptcy of several large buyouts including Robert Campeau's 1988 buyout of Federated Department Stores, the 1986 buyout of the Revco drug stores, Walter Industries, FEB Trucking and Eaton Leonard. The RJR Nabisco deal was showing signs of strain, leading to a recapitalization in 1990 that involved the contribution of $1.7 billion of new equity from KKR.[54] In response to the threat of unwelcome LBOs, certain companies adopted a number of techniques, such as the poison pill, to protect them against hostile takeovers by effectively self-destructing the company if it were to be taken over (these practices are increasingly discredited).

Drexel Burnham Lambert was the investment bank most responsible for the boom in private equity during the 1980s due to its leadership in the issuance of high-yield debt. The firm was first rocked by scandal on May 12, 1986, when Dennis Levine, a Drexel managing director and investment banker, was charged with insider trading. Levine pleaded guilty to four felonies, and implicated one of his recent partners, arbitrageur Ivan Boesky. Largely based on information Boesky promised to provide about his dealings with Milken, the Securities and Exchange Commission initiated an investigation of Drexel on November 17. Two days later, Rudy Giuliani, the United States Attorney for the Southern District of New York, launched his own investigation.[55]

For two years, Drexel steadfastly denied any wrongdoing, claiming that the criminal and SEC cases were based almost entirely on the statements of an admitted felon looking to reduce his sentence. However, it was not enough to keep the SEC from suing Drexel in September 1988 for insider trading, stock manipulation, defrauding its clients and stock parking (buying stocks for the benefit of another). All of the transactions involved Milken and his department. Giuliani began seriously considering indicting Drexel under the powerful Racketeer Influenced and Corrupt Organizations Act (RICO), under the doctrine that companies are responsible for an employee's crimes.[55]

The threat of a RICO indictment, which would have required the firm to put up a performance bond of as much as $1 billion in lieu of having its assets frozen, unnerved many at Drexel. Most of Drexel's capital was borrowed money, as is common with most investment banks and it is difficult to receive credit for firms under a RICO indictment.[55]  Drexel's CEO, Fred Joseph said that he had been told that if Drexel were indicted under RICO, it would only survive a month at most.[56]

With literally minutes to go before being indicted, Drexel reached an agreement with the government in which it pleaded nolo contendere (no contest) to six felonies – three counts of stock parking and three counts of stock manipulation.[55] It also agreed to pay a fine of $650 million – at the time, the largest fine ever levied under securities laws. Milken left the firm after his own indictment in March 1989.[56] Effectively, Drexel was now a convicted felon.

In April 1989, Drexel settled with the SEC, agreeing to stricter safeguards on its oversight procedures.  Later that month, the firm eliminated 5,000 jobs by shuttering three departments – including the retail brokerage operation.

The high-yield debt markets had begun to shut down in 1989, a slowdown that accelerated into 1990.  On February 13, 1990, after being advised by United States Secretary of the Treasury Nicholas F. Brady, the U.S. Securities and Exchange Commission (SEC), the New York Stock Exchange (NYSE) and the Federal Reserve System, Drexel Burnham Lambert officially filed for Chapter 11 bankruptcy protection.[56]

In the 1980s, the boom in private equity transactions, specifically leveraged buyouts, was driven by the availability of financing, particularly high-yield debt, also known as "junk bonds". The collapse of the high yield market in 1989 and 1990 would signal the end of the LBO boom. At that time, many market observers were pronouncing the junk bond market "finished".  This collapse would be due largely to three factors:

Despite the adverse market conditions, several of the largest private equity firms were founded in this period including: Apollo Management, Madison Dearborn and TPG Capital.

Beginning roughly in 1992, three years after the RJR Nabisco buyout, and continuing through the end of the decade the private equity industry once again experienced a tremendous boom, both in venture capital (as will be discussed below) and leveraged buyouts with the emergence of brand name firms managing multibillion-dollar sized funds. After declining from 1990 through 1992, the private equity industry began to increase in size raising approximately $20.8 billion of investor commitments in 1992 and reaching a high-water mark in 2000 of $305.7 billion, outpacing the growth of almost every other asset class.[30]

Private equity in the 1980s was a controversial topic, commonly associated with corporate raids, hostile takeovers, asset stripping, layoffs, plant closings and outsized profits to investors. As private equity reemerged in the 1990s it began to earn a new degree of legitimacy and respectability. Although in the 1980s, many of the acquisitions made were unsolicited and unwelcome, private equity firms in the 1990s focused on making buyouts attractive propositions for management and shareholders. According to The Economist, "[B]ig companies that would once have turned up their noses at an approach from a private-equity firm are now pleased to do business with them."[3] Private equity investors became increasingly focused on the long-term development of companies they acquired, using less leverage in the acquisition. In the 1980s leverage would routinely represent 85% to 95% of the purchase price of a company as compared to average debt levels between 20% and 40% in leveraged buyouts in the 1990s and the first decade of the 21st century. KKR's 1986 acquisition of Safeway, for example, was completed with 97% leverage and 3% equity contributed by KKR, whereas KKR's acquisition of TXU in 2007 was completed with approximately 19% equity contributed ($8.5 billion of equity out of a total purchase price of $45 billion). Private equity firms are more likely to make investments in capital expenditures and provide incentives for management to build long-term value.

The Thomas H. Lee Partners acquisition of Snapple Beverages, in 1992, is often described as the deal that marked the resurrection of the leveraged buyout after several dormant years.[59] Only eight months after buying the company, Lee took Snapple Beverages public and in 1994, only two years after the original acquisition, Lee sold the company to Quaker Oats for $1.7 billion. Lee was estimated to have made $900 million for himself and his investors from the sale. Quaker Oats would subsequently sell the company, which performed poorly under new management, three years later for only $300 million to Nelson Peltz's Triarc. As a result of the Snapple deal, Thomas H. Lee, who had begun investing in private equity in 1974, would find new prominence in the private equity industry and catapult his Boston-based Thomas H. Lee Partners to the ranks of the largest private equity firms.

It was also in this time that the capital markets would start to open up again for private equity transactions.  During the 1990–1993 period, Chemical Bank established its position as a key lender to private equity firms under the auspices of pioneering investment banker, James B. Lee Jr. (known as Jimmy Lee, not related to Thomas H. Lee). By the mid-1990s, under Jimmy Lee, Chemical had established itself as the largest lender in the financing of leveraged buyouts.  Lee built a syndicated leveraged finance business and related advisory businesses including the first dedicated financial sponsor coverage group, which covered private equity firms in much the same way that investment banks had traditionally covered various industry sectors.[60][61]

The following year, David Bonderman and James Coulter, who had worked for Robert M. Bass during the 1980s completed a buyout of Continental Airlines in 1993, through their nascent Texas Pacific Group, (today TPG Capital). TPG was virtually alone in its conviction that there was an investment opportunity with the airline. The plan included bringing in a new management team, improving aircraft utilization and focusing on lucrative routes. By 1998, TPG had generated an annual internal rate of return of 55% on its investment. Unlike Carl Icahn's hostile takeover of TWA in 1985,[39] Bonderman and Texas Pacific Group were widely hailed as saviors of the airline, marking the change in tone from the 1980s. The buyout of Continental Airlines would be one of the few successes for the private equity industry which has suffered several major failures, including the 2008 bankruptcies of ATA Airlines, Aloha Airlines and Eos Airlines.

Among the most notable buyouts of the mid-to-late 1990s included: Duane Reade (1992 and 1997), Sealy Corporation (1997), KinderCare Learning Centers (1997), J. Crew (1997), Domino's Pizza (1998), Regal Entertainment Group (1998), Oxford Health Plans (1998) and Petco (2000).

As the market for private equity matured, so too did its investor base. The Institutional Limited Partner Association was initially founded as an informal networking group for limited partner investors in private equity funds in the early 1990s. However the organization would evolve into an advocacy organization for private equity investors with more than 200 member organizations from 10 countries. As of the end of 2007, ILPA members had total assets under management in excess of $5 trillion with more than $850 billion of capital commitments to private equity investments.

In the 1980s, FedEx and Apple Inc. were able to grow because of private equity or venture funding, as were Cisco, Genentech, Microsoft and Avis.[62] However, by the end of the 1980s, venture capital returns were relatively low, particularly in comparison with their emerging leveraged buyout cousins, due in part to the competition for hot startups, excess supply of IPOs and the inexperience of many venture capital fund managers. Unlike the leveraged buyout industry, after total capital raised increased to $3 billion in 1983, growth in the venture capital industry remained limited through the 1980s and the first half of the 1990s increasing to just over $4 billion more than a decade later in 1994.

After a shakeout of venture capital managers, the more successful firms retrenched, focusing increasingly on improving operations at their portfolio companies rather than continuously making new investments. Results would begin to turn very attractive, successful and would ultimately generate the venture capital boom of the 1990s. Former Wharton Professor Andrew Metrick refers to these first 15 years of the modern venture capital industry beginning in 1980 as the "pre-boom period" in anticipation of the boom that would begin in 1995 and last through the bursting of the Internet bubble in 2000.[63]

The late 1990s were a boom time for the venture capital, as firms on Sand Hill Road in Menlo Park and Silicon Valley benefited from a huge surge of interest in the nascent Internet and other computer technologies. Initial public offerings of stock for technology and other growth companies were in abundance and venture firms were reaping large windfalls. Among the highest profile technology companies with venture capital backing were Amazon.com, America Online, eBay, Intuit, Macromedia, Netscape, Sun Microsystems and Yahoo!.

The Nasdaq crash and technology slump that started in March 2000 shook virtually the entire venture capital industry as valuations for startup technology companies collapsed. Over the next two years, many venture firms had been forced to write-off large proportions of their investments and many funds were significantly "under water" (the values of the fund's investments were below the amount of capital invested).  Venture capital investors sought to reduce size of commitments they had made to venture capital funds and in numerous instances, investors sought to unload existing commitments for cents on the dollar in the secondary market. By mid-2003, the venture capital industry had shriveled to about half its 2001 capacity.  Nevertheless, PricewaterhouseCoopers' MoneyTree Survey shows that total venture capital investments held steady at 2003 levels through the second quarter of 2005.

Although the post-boom years represent just a small fraction of the peak levels of venture investment reached in 2000, they still represent an increase over the levels of investment from 1980 through 1995. As a percentage of GDP, venture investment was 0.058% percent in 1994, peaked at 1.087% (nearly 19x the 1994 level) in 2000 and ranged from 0.164% to 0.182% in 2003 and 2004. The revival of an Internet-driven environment (thanks to deals such as eBay's purchase of Skype, the News Corporation's purchase of MySpace.com, and the very successful Google.com and Salesforce.com IPOs) have helped to revive the venture capital environment. However, as a percentage of the overall private equity market, venture capital has still not reached its mid-1990s level, let alone its peak in 2000.

As the venture sector collapsed, the activity in the leveraged buyout market also declined significantly. Leveraged buyout firms had invested heavily in the telecommunications sector from 1996 to 2000 and profited from the boom which suddenly fizzled in 2001. In that year at least 27 major telecommunications companies, (i.e., with $100 million of liabilities or greater) filed for bankruptcy protection. Telecommunications, which made up a large portion of the overall high yield universe of issuers, dragged down the entire high yield market. Overall corporate default rates surged to levels unseen since the 1990 market collapse rising to 6.3% of high yield issuance in 2000 and 8.9% of issuance in 2001. Default rates on junk bonds peaked at 10.7 percent in January 2002 according to Moody's.[64][65] As a result, leveraged buyout activity ground to a halt.[66][67] The major collapses of former high-fliers including WorldCom, Adelphia Communications, Global Crossing and Winstar Communications were among the most notable defaults in the market. In addition to the high rate of default, many investors lamented the low recovery rates achieved through restructuring or bankruptcy.[65]

Among the most affected by the bursting of the internet and telecom bubbles were two of the largest and most active private equity firms of the 1990s: Tom Hicks' Hicks Muse Tate & Furst and Ted Forstmann's Forstmann Little & Company. These firms were often cited as the highest profile private equity casualties, having invested heavily in technology and telecommunications companies.[68] Hicks Muse's reputation and market position were both damaged by the loss of over $1 billion from minority investments in six telecommunications and 13 Internet companies at the peak of the 1990s stock market bubble.[69][70][71]  Similarly, Forstmann suffered major losses from investments in McLeodUSA and XO Communications.[72][73] Tom Hicks resigned from Hicks Muse at the end of 2004 and Forstmann Little was unable to raise a new fund.  The treasure of the State of Connecticut, sued Forstmann Little to return the state's $96 million investment to that point and to cancel the commitment it made to take its total investment to $200 million.[74] The humbling of these private equity titans could hardly have been predicted by their investors in the 1990s and forced fund investors to conduct due diligence on fund managers more carefully and include greater controls on investments in partnership agreements.

Deals completed during this period tended to be smaller and financed less with high yield debt than in other periods. Private equity firms had to cobble together financing made up of bank loans and mezzanine debt, often with higher equity contributions than had been seen. Private equity firms benefited from the lower valuation multiples. As a result, despite the relatively limited activity, those funds that invested during the adverse market conditions delivered attractive returns to investors. In Europe LBO activity began to increase as the market continued to mature. In 2001, for the first time, European buyout activity exceeded US activity with $44 billion of deals completed in Europe as compared with just $10.7 billion of deals completed in the US.  This was a function of the fact that just six LBOs in excess of $500 million were completed in 2001, against 27 in 2000.[75]

As investors sought to reduce their exposure to the private equity asset class, an area of private equity that was increasingly active in these years was the nascent secondary market for private equity interests.  Secondary transaction volume increased from historical levels of 2% or 3% of private equity commitments to 5% of the addressable market in the early years of the new decade.[76][77] Many of the largest financial institutions (e.g., Deutsche Bank, Abbey National, UBS AG) sold portfolios of direct investments and "pay-to-play" funds portfolios that were typically used as a means to gain entry to lucrative leveraged finance and mergers and acquisitions assignments but had created hundreds of millions of dollars of losses. Some of the most notable financial institutions to complete publicly disclosed secondary transactions during this period include: Chase Capital Partners (2000), National Westminster Bank (2000), UBS AG (2003), Deutsche Bank (MidOcean Partners) (2003) Abbey National (2004) and Bank One (2004).

As 2002 ended and 2003 began, the private equity sector, which had spent the previous two and a half years reeling from major losses in telecommunications and technology companies and had been severely constrained by tight credit markets.  As 2003 got underway, private equity began a five-year resurgence that would ultimately result in the completion of 13 of the 15 largest leveraged buyout transactions in history, unprecedented levels of investment activity and investor commitments and a major expansion and maturation of the leading private equity firms.

The combination of decreasing interest rates, loosening lending standards and regulatory changes for publicly traded companies would set the stage for the largest boom private equity had seen.  The Sarbanes–Oxley legislation, officially the Public Company Accounting Reform and Investor Protection Act, passed in 2002, in the wake of corporate scandals at Enron, WorldCom, Tyco, Adelphia, Peregrine Systems and Global Crossing among others, would create a new regime of rules and regulations for publicly traded corporations.  In addition to the existing focus on short term earnings rather than long term value creation, many public company executives lamented the extra cost and bureaucracy associated with Sarbanes–Oxley compliance.  For the first time, many large corporations saw private equity ownership as potentially more attractive than remaining public.  Sarbanes–Oxley would have the opposite effect on the venture capital industry.  The increased compliance costs would make it nearly impossible for venture capitalists to bring young companies to the public markets and dramatically reduced the opportunities for exits via IPO.  Instead, venture capitalists have been forced increasingly to rely on sales to strategic buyers for an exit of their investment.[78]

Interest rates, which began a major series of decreases in 2002 would reduce the cost of borrowing and increase the ability of private equity firms to finance large acquisitions.  Lower interest rates would encourage investors to return to relatively dormant high-yield debt and leveraged loan markets, making debt more readily available to finance buyouts. Alternative investments also became increasingly important as investors focused on yields despite increases in risk.  This search for higher yielding investments would fuel larger funds, allowing larger deals, never before thought possible, to become reality.

Certain buyouts were completed in 2001 and early 2002, particularly in Europe where financing was more readily available.  In 2001, for example, BT Group agreed to sell its international yellow pages directories business (Yell Group) to Apax Partners and Hicks, Muse, Tate & Furst for £2.14 billion (approximately $3.5 billion at the time),[79] making it then the largest non-corporate LBO in European history. Yell later bought US directories publisher McLeodUSA for about $600 million, and floated on London's FTSE in 2003.

Marked by the two-stage buyout of Dex Media at the end of 2002 and 2003, large multibillion-dollar U.S. buyouts could once again obtain significant high yield debt financing and larger transactions could be completed. The Carlyle Group, Welsh, Carson, Anderson & Stowe, along with other private investors, led a $7.5 billion buyout of QwestDex. The buyout was the third largest corporate buyout since 1989.  QwestDex's purchase occurred in two stages: a $2.75 billion acquisition of assets known as Dex Media East in November 2002 and a $4.30 billion acquisition of assets known as Dex Media West in 2003. R. H. Donnelley Corporation acquired Dex Media in 2006.  Shortly after Dex Media, other larger buyouts would be completed signaling the resurgence in private equity was underway.  The acquisitions included Burger King (by Bain Capital), Jefferson Smurfit (by Madison Dearborn), Houghton Mifflin[80][81] (by Bain Capital, the Blackstone Group and Thomas H. Lee Partners) and TRW Automotive by the Blackstone Group.

In 2006 USA Today reported retrospectively on the revival of private equity:[82]

By 2004 and 2005, major buyouts were once again becoming common and market observers were stunned by the leverage levels and financing terms obtained by financial sponsors in their buyouts.  Some of the notable buyouts of this period include: Dollarama (2004), Toys "R" Us (2004), The Hertz Corporation (2005), Metro-Goldwyn-Mayer (2005) and SunGard (2005).

As 2005 ended and 2006 began, new "largest buyout" records were set and surpassed several times with nine of the top ten buyouts at the end of 2007 having been announced in an 18-month window from the beginning of 2006 through the middle of 2007. The buyout boom was not limited to the United States as industrialized countries in Europe and the Asia-Pacific region also saw new records set.  In 2006, private equity firms bought 654 U.S. companies for $375 billion, representing 18 times the level of transactions closed in 2003.[84] U.S. based private equity firms raised $215.4 billion in investor commitments to 322 funds, surpassing the previous record set in 2000 by 22% and 33% higher than the 2005 fundraising total.[85]  However, venture capital funds, which were responsible for much of the fundraising volume in 2000 (the height of the dot-com bubble), raised only $25.1 billion in 2006, a 2% percent decline from 2005 and a significant decline from its peak.[86]  The following year, despite the onset of turmoil in the credit markets in the summer, saw yet another record year of fundraising with $302 billion of investor commitments to 415 funds.[87]

Among the largest buyouts of this period included: Georgia-Pacific Corp (2005), Albertson's (2006), EQ Office (2006), Freescale Semiconductor (2006), Ally Financial GMAC (2006), HCA (2006), Kinder Morgan (2006), Harrah's Entertainment (2006), TDC A/S (2006), Sabre Holdings (2006), Travelport (2006), Alliance Boots (2007), Biomet (2007), Chrysler (2007), First Data (2007) and TXU (2007).

Although there had previously been certain instances of publicly traded private equity vehicles, the convergence of private equity and the public equity markets attracted significantly greater attention when several of the largest private equity firms pursued various options through the public markets.  Taking private equity firms and private equity funds public appeared an unusual move since private equity funds often buy public companies listed on exchange and then take them private.  Private equity firms are rarely subject to the quarterly reporting requirements of the public markets and tout this independence to prospective sellers as a key advantage of going private.  Nevertheless, there are fundamentally two separate opportunities that private equity firms pursued in the public markets.  These options involved a public listing of either:

In May 2006, Kohlberg Kravis Roberts (KKR) raised $5 billion in an initial public offering for a new permanent investment vehicle (KKR Private Equity Investors or KPE) listing it on the Euronext exchange in Amsterdam (ENXTAM: KPE).  KKR raised more than three times what it had expected at the outset as many of the investors in KPE were hedge funds that sought exposure to private equity but that could not make long-term commitments to private equity funds.  Because private equity had been booming in the preceding years, the proposition of investing in a KKR fund appeared attractive to certain investors.[88]
KPE's first-day performance was lackluster, trading down 1.7% and trading volume was limited.[89]  Initially, a handful of other private equity firms, including Blackstone, and hedge funds had planned to follow KKR's lead but when KPE was increased to $5 billion, it soaked up all the demand.[90] That, together with the slump of KPE's shares, caused the other firms to shelve their plans. KPE's stock declined from an IPO price of €25 per share to €18.16 (a 27% decline) at the end of 2007 and a low of €11.45 (a 54.2% decline) per share in Q1 2008.[91]  KPE disclosed in May 2008 that it had completed approximately $300 million of secondary sales of selected limited
partnership interests in and undrawn commitments to certain KKR-managed funds in order to generate liquidity and repay borrowings.[92]

On March 22, 2007, after nine months of secret preparations, the Blackstone Group filed with the SEC[93] to raise $4 billion in an initial public offering. On June 21, Blackstone sold a 12.3% stake in its ownership to the public for $4.13 billion in the largest U.S. IPO since 2002.[94] Traded on the New York Stock Exchange under the ticker symbol BX, Blackstone priced at $31 per share on June 22, 2007.[95][96]

Less than two weeks after the Blackstone Group IPO, rival firm Kohlberg Kravis Roberts filed with the SEC[97] in July 2007 to raise $1.25 billion by selling an ownership interest in its management company.[98]  KKR had previously listed its KKR Private Equity Investors (KPE) private equity fund vehicle in 2006.  The onset of the credit crunch and the shutdown of the IPO market would dampen the prospects of obtaining a valuation that would be attractive to KKR and the flotation was repeatedly postponed.

Other private equity investors were seeking to realize a portion of the value locked into their firms.  In September 2007, the Carlyle Group sold a 7.5% interest in its management company to Mubadala Development Company, which is owned by the Abu Dhabi Investment Authority (ADIA) for $1.35 billion, which valued Carlyle at approximately $20 billion.[99]  Similarly, in January 2008, Silver Lake Partners sold a 9.9% stake in its management company to the California Public Employees' Retirement System (CalPERS) for $275 million.[100]

Apollo Management completed a private placement of shares in its management company in July 2007.  By pursuing a private placement rather than a public offering, Apollo would be able to avoid much of the public scrutiny applied to Blackstone and KKR.[101][102]  In April 2008, Apollo filed with the SEC[103] to permit some holders of its privately traded stock to sell their shares on the New York Stock Exchange.[104]  In April 2004, Apollo raised $930 million for a listed business development company, Apollo Investment Corporation (NASDAQ: AINV), to invest primarily in middle-market companies in the form of mezzanine debt and senior secured loans, as well as by making direct equity investments in companies. The company also invests in the securities of public companies.[105]

Historically, in the United States, there had been a group of publicly traded private equity firms that were registered as business development companies (BDCs) under the Investment Company Act of 1940.[106]  Typically, BDCs are structured similar to real estate investment trusts (REITs) in that the BDC structure reduces or eliminates corporate income tax. In return, REITs are required to distribute 90% of their income, which may be taxable to its investors.  As of the end of 2007, among the largest BDCs (by market value, excluding Apollo Investment Corp, discussed earlier) are: American Capital Strategies (NASDAQ: ACAS), Allied Capital Corp (NASDAQ:ALD), Ares Capital Corporation (NASDAQ:ARCC), Gladstone Investment Corp (NASDAQ:GAIN) and Kohlberg Capital Corp (NASDAQ:KCAP).

In the wake of the collapse of the equity markets in 2000, many investors in private equity sought an early exit from their outstanding commitments.[107]  The surge in activity in the secondary market, which had previously been a relatively small niche of the private equity industry, prompted new entrants to the market, however the market was still characterized by limited liquidity and distressed prices with private equity funds trading at significant discounts to fair value.

Beginning in 2004 and extending through 2007, the secondary market transformed into a more efficient market in which assets for the first time traded at or above their estimated fair values and liquidity increased dramatically.  During these years, the secondary market transitioned from a niche sub-category in which the majority of sellers were distressed to an active market with ample supply of assets and numerous market participants.[108]  By 2006 active portfolio management had become far more common in the increasingly developed secondary market and an increasing number of investors had begun to pursue secondary sales to rebalance their private equity portfolios.  The continued evolution of the private equity secondary market reflected the maturation and evolution of the larger private equity industry. Among the most notable publicly disclosed secondary transactions (it is estimated that over two-thirds of secondary market activity is never disclosed publicly): CalPERS (2008), Ohio Bureau of Workers' Compensation (2007), MetLife (2007), Bank of America (2006 and 2007), Mellon Financial Corporation (2006), American Capital Strategies (2006), JPMorgan Chase, Temasek Holdings, Dresdner Bank and Dayton Power & Light.

In July 2007, turmoil that had been affecting the mortgage markets, spilled over into the leveraged finance and high-yield debt markets.[109][110]  The markets had been highly robust during the first six months of 2007, with highly issuer friendly developments including PIK and PIK Toggle (interest is "Payable In Kind") and covenant light debt widely available to finance large leveraged buyouts.  July and August saw a notable slowdown in issuance levels in the high yield and leveraged loan markets with only few issuers accessing the market.  Uncertain market
conditions led to a significant widening of yield spreads, which coupled with the typical summer slowdown led to many companies and investment banks to put their plans to issue debt on hold until the autumn.  However, the expected rebound in the market after Labor Day 2007 did not materialize and the lack of market confidence prevented deals from pricing.  By the end of September, the full extent of the credit situation became obvious as major lenders including Citigroup and UBS AG announced major writedowns due to credit losses.  The leveraged finance markets came to a near standstill.[111]  As a result of the sudden change in the market, buyers would begin to withdraw from or renegotiate the deals completed at the top of the market, most notably in transactions involving: Harman International  (announced and withdrawn 2007), Sallie Mae (announced 2007 but
withdrawn 2008), Clear Channel Communications (2007) and BCE (2007).

The credit crunch has prompted buyout firms to pursue a new group of transactions in order to deploy their massive investment funds.  These transactions have included Private Investment in Public Equity (or PIPE) transactions as well as purchases of debt in existing leveraged buyout transactions.  Some of the most notable of these transactions completed in the depths of the credit crunch include Apollo Management's acquisition of the Citigroup Loan Portfolio (2008) and TPG Capital's PIPE investment in Washington Mutual (2008).  According to investors and fund managers, the consensus among industry members in late 2009 was that private equity firms will need to become more like asset managers, offering buyouts as just part of their portfolio, or else focus tightly on specific sectors in order to prosper.  The industry must also become better in adding value by turning businesses around rather than pure financial engineering.[112]

Although private equity rarely received a thorough treatment in popular culture, several films did feature stereotypical "corporate raiders" prominently.  Among the most notable examples of private equity featured in motion pictures included:

Two other works were pivotal in framing the image of buyout firms.[113] Barbarians at the Gate, the 1990 best seller about the fight over RJR Nabisco linked private equity to hostile takeovers and assaults on management. A blistering story on the front page of the Wall Street Journal the same year about KKR's buyout of the Safeway supermarket chain painted a much more damaging picture.[114] The piece, which later won a Pulitzer Prize, began with the suicide of a Safeway worker in Texas who had been laid off and went on to chronicle how KKR had sold off hundreds of stores after the buyout and slashed jobs.

Carlyle group featured prominently in Michael Moore's 2003 film Fahrenheit 9/11.  The film suggested that The Carlyle Group exerted tremendous influence on U.S. government policy and contracts through their relationship with the president's father, George H. W. Bush, a former senior adviser to the Carlyle Group. Moore cited relationships with the Bin Laden family.  The movie quotes author Dan Briody claiming that the Carlyle Group "gained" from September 11 because it owned United Defense, a military contractor, although the firm's $11 billion XM2001 Crusader artillery rocket system developed for the U.S. Army is one of the few weapons systems canceled by the Bush administration.[115][116]

Over the next few years, attention intensified on private equity as the size of transactions and profile of the companies increased.  The attention would increase significantly following a series of events involving The Blackstone Group: the firm's initial public offering and the birthday celebration of its CEO.  The Wall Street Journal observing  Blackstone Group's Steve Schwarzman's 60th birthday celebration in February 2007 described the event as follows:[117]

The Armory's entrance hung with banners painted to replicate Mr. Schwarzman's sprawling Park Avenue apartment. A brass band and children clad in military uniforms ushered in guests. A huge portrait of Mr. Schwarzman, which usually hangs in his living room, was shipped in for the occasion.
The affair was emceed by comedian Martin Short. Rod Stewart performed. Composer Marvin Hamlisch did a number from A Chorus Line. Singer Patti LaBelle led the Abyssinian Baptist Church choir in a tune about Mr. Schwarzman. Attendees included Colin Powell and New York Mayor Michael Bloomberg. The menu included lobster, baked Alaska and a 2004 Maison Louis Jadot Chassagne Montrachet, among other fine wines.
Schwarzman received a severe backlash from both critics of the private equity industry and fellow investors in private equity.  The lavish event which reminded many of the excesses of notorious executives including Bernie Ebbers (WorldCom) and Dennis Kozlowski (Tyco International). David Bonderman, the founder of TPG Capital remarked, "We have all wanted to be private – at least until now.  When Steve Schwarzman's biography with all the dollar signs is posted on the web site none of us will like the furor that results – and that's even if you like Rod Stewart."[117] As the IPO drew closer, there were moves by a number of congressman and senators to block the stock offering and to raise taxes on private equity firms and/or their partners—proposals many attributed in part to the extravagance of the party.[118]

David Rubenstein's fears would be confirmed when in 2007, the Service Employees International Union (SEIU) launched a campaign against private equity firms, specifically the largest buyout firms through public events, protests as well as leafleting and web campaigns.[119][120][121]  A number of leading private equity executives were targeted by the union members.[122] However the SEIU's campaign was non nearly as effective at slowing the buyout boom as the credit crunch of 2007 and 2008 would ultimately prove to be.

In 2008, the SEIU would shift part of its focus from attacking private equity firms directly toward the highlighting the role of sovereign wealth funds in private equity.  The SEIU pushed legislation in California that would disallow investments by state agencies (particularly CalPERS and CalSTRS) in firms with ties to certain sovereign wealth funds.[123] The SEIU has attempted to criticize the treatment of taxation of carried interest.  The SEIU, and other critics, point out that many wealthy private equity investors pay taxes at lower rates (because the majority of their income is derived from carried interest, payments received from the profits on a private equity fund's investments) than many of the rank and file employees of a private equity
firm's portfolio companies.[124]The history of the Royal Australian Navy traces the development of the Royal Australian Navy (RAN) from the colonisation of Australia by the British in 1788. Until 1859, vessels of the Royal Navy made frequent trips to the new colonies. In 1859, the Australia Squadron was formed as a separate squadron and remained in Australia until 1913. Until Federation, five of the six Australian colonies operated their own colonial naval force, which formed on 1 March 1901 the Australian Navy's (AN) Commonwealth Naval Force which received Royal patronage in July 1911 and was from that time referred to as Royal Australian Navy (RAN).[1] On 4 October 1913 the new replacement fleet for the foundation fleet of 1901 steamed through Sydney Heads for the first time.

The Royal Australian Navy has seen action in every ocean of the world.[2][3] It first saw action in World War I, in the Pacific, Indian and Atlantic oceans. Between the wars the RAN's fortunes shifted with the financial situation of Australia: it experienced great growth during the 1920s, but was forced to reduce its fleet and operations during the 1930s. Consequently, when it entered World War II, the RAN was smaller than it had been at the start of World War I. During the course of World War II, the RAN operated more than 350 fighting and support ships; a further 600 small civilian vessels were put into service as auxiliary patrol boats.[4] (Contrary to some claims, however, the RAN was not the fifth-largest navy in the world at any point during World War II.[5])

Following World War II, the RAN saw action in Korea, Vietnam, and other smaller conflicts. Today, the RAN consists of a small but modern force, widely regarded as one of the most powerful forces in the Asia Pacific Region.

In the years following the establishment of the British colony of New South Wales in 1788, Royal Navy ships of the East Indies Squadron under the command of the East Indies Station would be station in or visit Australian waters. From the 1820s, a ship was sent annually to New South Wales, and occasionally to New Zealand.[6]

In 1848, an Australian Division of the East Indies Station was established,[7] and in 1859 the British Admiralty established an independent command, the Australia Station, under the command of a Commodore who was assigned as Commander-in-Chief, Australia Station.[8] The Australian Squadron was created to which British naval ships serving on the Australia Station were assigned.[8][9] The changes were partially in recognition of the fact that a large part of the East Indies Station had been detached to Australian waters, and also reflecting growing concern for the strategic situation in the western Pacific in general, and in Tahiti and New Zealand in particular.[8] In 1884, the commander of the Australia Station was upgraded to the rank of rear admiral.[8]

At its establishment, the Australia Station encompassed Australia and New Zealand, with its eastern boundary including Samoa and Tonga, its western edge in the Indian Ocean, south of India and its southern edge defined by the Antarctic Circle. The boundaries were modified in 1864, 1872 and 1893.[10] At its largest, the Australia Station reached from the Equator to the Antarctic in its greatest north–south axis, and covered 1⁄4 of the Southern Hemisphere in its extreme east–west dimension, including Papua New Guinea, New Zealand, Melanesia and Polynesia.[11]

In 1911 the Australia Station passed to the Commonwealth Naval Forces (initially under the command of RN officers) and the Australian Squadron was disbanded. The Station, now under nominal Australian command, was reduced to only cover Australia and its island dependencies to the north and east. In 1911, the Commonwealth Naval Forces was renamed the Royal Australian Navy, which in 1913 came under Australian command. The Royal Navy's Australia Station's Sydney based depots, dockyards and structures were gifted to the Commonwealth of Australia. The Royal Navy continued to support the RAN and provided additional blue-water defence capability in the Pacific up to the early years of World War II.

Before the Federation of Australia in 1901, five of the six self-governing colonies in Australia operated a navy, the exception being Western Australia which did not have a naval force. The colonial navies were supported by the ships of the Royal Navy's Australian Station which was established in 1859. In 1856, Victoria received its own naval vessel, HMCSS Victoria, which in 1860 was deployed to assist the New Zealand colonial government during the First Taranaki War. When Victoria returned to Australia, the vessel had taken part in several minor actions, with the loss of one crew member. The deployment of Victoria to New Zealand marked the first occasion that an Australian warship had been deployed overseas. In the years leading up to Federation, Victoria had the most powerful of the colonial navies. Victoria had HMVS Cerberus since 1870, as well as HMVS Nelson, three small gunboats and five torpedo-boats. NSW had two very small torpedo boats, and the corvette Wolverine. The colonial navies were expanded greatly in the mid-1880s and usually consisted of gunboats and torpedo-boats for coastal defence of harbours and rivers, and naval brigades to man vessels and forts.

On 1 January 1901, Australia became a federation of six States, as the Commonwealth of Australia, which on 1 March 1901 took over the defence forces from the States, to form the Commonwealth Naval Forces.[12][13] The Australian and New Zealand governments agreed with the Imperial government to help fund the Royal Navy's Australian Squadron, while the Admiralty committed itself to maintain the Squadron at a constant strength.[9] In 1902, the commander of the Australia Station was upgraded to the rank of vice admiral. The boundaries were again modified in 1908.

A growing number of people, among them Captain William Rooke Creswell, the director of the Commonwealth Naval Forces, demanded an autonomous Australian navy, financed and controlled by Australia. In 1907 Prime Minister Alfred Deakin and Creswell, while attending the Imperial Conference in London, sought the British Government's agreement to end the subsidy system and develop an Australian navy. The Admiralty rejected the idea, but suggested diplomatically that a small fleet of destroyers and submarines would be sufficient. Deakin was not impressed with the Admiralty, and in 1908 invited the United States Great White Fleet to visit Australia. The visit prompted public enthusiasm for a modern navy and led to the order of two 700-ton River-class torpedo-boat destroyers. The surge in German naval construction in 1909 led the Admiralty to change its position on an Australian navy, which resulted in the Naval Defence Act of 1910 being passed which created the Australian navy.

The first Australian warship, the destroyer HMAS Parramatta, was launched at Govan in Scotland on Wednesday 9 February 1910. Sister ship HMAS Yarra was launched at Dumbarton in Scotland on Saturday 9 April 1910.  Both ships were commissioned into the Royal Navy on 19 September 1910 and sailed for Australia, arriving at Port Phillip on 10 December 1910. The event was marred by the death of Engineer Lieutenant W. Robertson, RN, who suffered a heart attack 8 miles (13 km) outside Port Phillip Heads whilst onboard HMAS Yarra, and drowned.[14]

The British Australia Station passed to the Commonwealth Naval Forces in 1911 and the Australian Squadron was disbanded. On 10 July 1911, King George V granted the title of "Royal Australian Navy" to the Commonwealth Naval Forces,[15] and RAN ships could carry the prefix "His Majesty's Australian Ship" (HMAS). The Station was reduced to cover Australia and its island dependencies to the north and east, excluding New Zealand and its surrounds, which became part of the China Station and called the New Zealand Naval Forces.[8] The Navy was to operate under the authority of the Australian Commonwealth Naval Board, which functioned from 1 March 1911.

At the 1911 Imperial Conference Australia expressed concern about Japan's growing naval power and it was agreed that the British government would consult Australia when negotiating renewal of the Anglo-Japanese Alliance.[16] It was also decided that the Royal Navy would continue to support the RAN and provide blue-water defence capability in the Pacific and that if there was war the ships of the RAN would be transferred to British Admiralty control. Under the Naval Defence Act (1912) the power to make the transfer was conferred in the Governor-General. The RAN would become the Australia Squadron of the Royal Navy with all ships and personnel under the direct control of the British Admiralty, while the RAN remained responsible for the upkeep of the ships and training.

In 1913, responsibility for the reduced Australia Station passed to the new Royal Australian Navy[8][17] under nominal Australian command, with the Australia Squadron of the Royal Navy's Australia Station coming to an end and its Sydney based depots, dockyards and structures being gifted to the Commonwealth of Australia. The first commanding officer was Admiral George Edwin Patey, Rear Admiral Commanding HM Australian Fleet, on loan from the Royal Navy. On Saturday 4 October 1913 the Australian fleet, consisting of the battle cruiser Australia, the cruisers Melbourne and Sydney, the protected cruiser Encounter, and the torpedo-boat destroyers Parramatta, Yarra and Warrego, entered Sydney Harbour for the first time.[17] The manpower of the fleet stood at four hundred officers and men and, for the next two years, ships were built for the fledgling navy.

The Royal Navy continued to support the RAN and provide blue-water defence capability in the Pacific up to the early years of World War II. In 1958, the boundaries of Australia Station was redrawn again, now to include Papua New Guinea.[8]

On 3 August 1914, as the prospect of war with the German Empire loomed, the Australian Government sent the following message to the Admiralty.[18]

In the event of war Government prepared place vessels of Australian Navy under control British Admiralty when desired.
 The United Kingdom declared war on Germany the next day, and on 8 August, the Australian Government received a reply, requesting that the transfer be made immediately, if not already done. Two days later, on 10 August, the Governor-General officially transferred control of the Royal Australian Navy to the British Admiralty, which would retain control until 19 August 1919.[19]

At the outbreak of war, the RAN stood at 3,800 personnel and consisted of sixteen ships, including the battlecruiser Australia, the light cruisers Sydney and Melbourne, the destroyers Parramatta, Yarra, and Warrego, and the submarines AE1 and AE2. The light cruiser Brisbane and three destroyers were under construction, and a small fleet of auxiliary ships was also being maintained. As a consequence the Royal Australian Navy at the start of the war was a small but formidable force.[20]

Australian ships first saw action in the Asian and Pacific theatre; assisting in the attack on German New Guinea by the Australian Naval and Military Expeditionary Force (AN&MEF). Germany had colonised the northeastern part of New Guinea and several nearby island groups in 1884, and the colony was currently used as a wireless radio base, Britain required the wireless installations to be destroyed because they were used by the German East Asia Squadron which threatened merchant shipping in the region. The objectives of the force were the German stations at Yap in the Caroline Islands, Nauru, and Rabaul in New Britain. On 30 August 1914, the AN&MEF left Sydney under the protection of Australia and Melbourne for Port Moresby, where the force met the Queensland contingent, aboard the transport HMAHS Kanowna. The force then sailed for German New Guinea on 7 September, leaving Kanowna behind when her stokers refused to work. Sydney and her escorting destroyers met the AN&MEF off the eastern tip of New Guinea. Melbourne was detached to destroy the wireless station on Nauru, while on 14 September, Encounter bombarded a ridge near Rabaul,[21] while half a battalion advanced towards the town. The only major loss of the campaign was the disappearance of the submarine AE1 during a patrol off Rabaul on 14 September 1914.[22][23]

On 9 November 1914, the German light cruiser SMS Emden attacked the Allied radio and telegraph station at Direction Island in the Cocos (Keeling) Islands. The inhabitants of the island managed to transmit a distress signal, which was received by Sydney, only 50 miles (80 km) away. Sydney arrived within two hours, and was engaged by Emden. Sydney was the larger, faster and better armed of the two, and eventually overpowered Emden, with captain Karl von Müller running the ship aground on North Keeling Island at 11:15 am. At first, Emden refused to strike its colours and surrender; Sydney fired on the stationary Emden until it eventually struck its colours. The Battle of Cocos was the first battle the RAN participated in.[15]

On 6 February 1915, the obsolescent light cruiser HMAS Pioneer joined the East African campaign. On 6 July, she engaged the German cruiser SMS Königsberg and German shore batteries, during the Battle of Rufiji Delta. Pioneer remained off East Africa and took part in many bombardments of German East Africa, including Dar-es-Salaam on 13 June 1916. Pioneer then returned to Australia, to be decommissioned in October 1916.

During the Naval operations in the Dardanelles Campaign the Australian submarine AE2 became the first Allied warship to breach the Turkish defences of the Dardanelles. AE2 spent five days in the area, was unsuccessfully attacked several times, but was unable to find any large enemy troop transports. On 29 April 1915, she was damaged in an attack by the Turkish torpedo-boat Sultan Hisar in Artaki Bay and was scuttled by her crew. The wreck of AE2 remained undiscovered until June 1998.[24]

Ships of the Royal Australian Navy also assisted the Royal Navy in the blockade of the German High Seas Fleet. In February 1915, HMAS Australia joined the British Grand Fleet, and was made flagship of the 2nd Battle Cruiser Squadron.[25] Australia was not involved in the Battle of Jutland; in April, the battlecruiser was damaged in a collision with sister ship HMS New Zealand, and she did not return to service until June.[25][26] Three RAN ships were present during the surrender of the German High Seas Fleet; Australia, Sydney, and Melbourne, with Australia leading the port division of the Grand Fleet as it sailed out to meet the Germans.[27][28]

The most decorated Australian Naval unit of World War One, however was not a ship at all, but the Royal Australian Navy Bridging Train,[29] a land-based unit composed mostly of reservists which landed at Suvla Bay with the British IX Corps and was responsible for receiving, storing and distributing the supplies, including potable water, of the British troops at Suvla.[30] Due to their position working the piers and landings at Suvla, the Train was the last Australian unit to depart the Gallipoli Peninsula.[31] After Gallipoli, the Train was sent to the Middle East, where they made a second amphibious landing at the Battle of Magdhaba, before returning to Australia and being disbanded after a series of miscommunications during May 1917.[32]

Expansion during the war had been limited, with the RAN growing to include thirty-seven ships and more than 5,000 personnel by 1918.[33] The RAN's losses had also been modest, only losing the two submarines AE1 and AE2, whilst casualties included 171 fatalities – 108 Australians and 63 officers and men on loan from the Royal Navy, with less than a third the result of enemy action.[34]

Between April 1918 and May 1919, the Spanish flu killed approximately 25 million people worldwide, far more than had been killed in four years of war. A rigorous quarantine policy was implemented in Australia; although this reduced the immediate impact of the flu, the nation's death toll surpassed 11,500.[35]

When the pandemic struck in 1918, the ships of the Royal Australian Navy were dispersed throughout the world. The speed at which the flu spread, coupled with the cramped mess decks and poorly ventilated living spaces on early 20th century warships, created a favourable environment for the disease. The pandemic swept through the British Grand Fleet in 1918; the Australian cruisers assigned to the fleet suffered high casualties, with up to 157 casualties in one ship alone. Outbreaks in the Mediterranean fleets were more severe than those in the Atlantic. HMAS Brisbane recorded 183 casualties between November and December 1918, of those casualties 2 men died of pneumonia. The RAN lost a total of 26 men to the disease; further loss prevented primarily by the ready availability of professional medical treatment.[36]

The disease arrived in the South Pacific on the cargo vessel SS Talune, which sailed from Auckland on 30 October 1918 whilst knowingly carrying sick passengers. Talune stopped in Fiji, Samoa, Tonga and Nauru: the first outbreaks in these locations occurred within days of the ships visits. The local authorities were generally unprepared for the size of the outbreak, allowing the infection to spread uncontrollably. The German territory of Samoa was the worst affected of the small islands, the New Zealand administration carried out no efforts to lessen the outbreak and rejected offers of assistance from nearby American Samoa. The New Zealand government officially apologised to Samoa in 2002 for their reaction to the outbreak.[37] On 29 November 1918 the military governor of Apia requested assistance from Wellington; the request was turned down because all doctors were needed in New Zealand. Australia offered the only alternate source of aid.

The Commonwealth Naval Board was aware of the worsening situation in the region; the sloop HMAS Fantome reported its first case on 11 November 1918 while stationed in Fiji, with half her complement eventually affected. On 20 November 1918, the Naval Board began forming a joint relief expedition from available military medical personnel. The commanding officer of HMAS Encounter was then ordered to embark the expedition in Sydney and sail as soon as possible. Encounter departed Sydney on 24 November 1918, ten minutes after completing loading.[35] As a precaution, all 450 members of Encounter's crew were doubly inoculated; the ship had suffered 74 cases earlier in the year at Fremantle and the captain did not want a repeat. Encounter arrived in Suva on 30 November and took on half of the available coal and 39 tonnes of water.[38] Spanish flu was rampant in Suva; Captain Thring implemented a strict quarantine, placed guards on the wharf, and ordered that coaling be carried out by the crew instead of native labour. Encounter departed Suva in the evening of the same day and arrived off Apia on 3 December. Within six hours, the medical landing party assigned to Apia and their stores were ashore. Encounter then departed for the Tongan capital of Nukuʻalofa, arriving on 5 December. The last of the medical staff and supplies were unloaded, and  Encounter sailed for Suva on 7 December to re-coal. On arriving in Suva, Encounter received orders to return to Sydney, where reached on 17 December and was immediately placed into quarantine. The South Pacific aid mission is regarded as Australia's first overseas relief expedition, and set a precedent for future relief missions conducted by the RAN.[35]

Following the end of World War I, the Australian Government believed that an immediate evaluation of the RAN was necessary. Australia had based its naval policy on the Henderson Recommendations of 1911, developed by Sir Reginald Henderson. The government sent an invitation to Admiral John Jellicoe, he arrived in Australia in May 1919. Jellicoe remained in Australia for three months, before returning to England via New Zealand and Canada. Jellicoe submitted his findings in August 1919, titled the Report on the Naval Mission to the Commonwealth. The report outlined several policies designed to strengthen British naval strength in the Pacific Ocean. The report heavily stressed a close relationship between the RAN and the Royal Navy. This would be achieved by strict adherence to the procedures and administration methods of the Royal Navy. The report also suggested constant officer exchange between the two forces. Jellicoe also called for the creation of a large Far East Imperial Fleet, which would be based in Singapore and include capital ships and aircraft carriers. The creation cost for this fleet was to be divided between Great Britain, Australia and New Zealand: contributing 75%, 20%, and 5% respectively. The suggested makeup of the RAN would include; one aircraft carrier, two battlecruisers, eight light cruisers, one flotilla leader, twelve destroyers, a destroyer depot ship, eight submarines, one submarine depot ship, and a small number of additional auxiliary ships. The annual cost and depreciation of the fleet was estimated to be £4,024,600. Except for implementing closer ties with the Royal Navy, none of Jellicoe's major recommendations were carried out.[39]

With the end of World War I, the Australian Government began to worry about the threat Japan posed to Australia. Japan had extended its empire 3,000 kilometres (1,900 mi) to the south, bringing it right to Australia's doorstep. Japan had continued to build up its naval force, and had reached the point where it outgunned the Royal Navy in the Pacific. The RAN and the government believed that the possibility of a Japanese invasion was highly likely. In his report, Admiral Jellicoe believed that the threat of a Japanese invasion of Australia would remain as long as the White Australia Policy remained in place. Due to the perceived threat, and bilateral support in Australia for the White Australia Policy, the Australian Government became a vocal supporter of the continuance of the 1902 Anglo-Japanese Alliance. Australia was joined in its support for the alliance by New Zealand but was heavily opposed by Canada, which believed that the alliance had hindered the British Empire's relationship with China and the United States. No decision on the alliance was agreed on, and the discussion was shelved pending the outcome of the Washington Naval Treaty. The results of the treaty, which allowed the British to retain naval supremacy in the Pacific Ocean, created a sense of security in Australia. Many Australians saw the Four Powers Pact as replacing the Anglo-Japanese Alliance. This sense of security became known as the Ten Year Rule. This led to defence retrenchments in Australia, following the international trend, and a £500,000 reduction in expenditure. The Governor-General Henry Forster when opening parliament on 22 June 1922 was quoted as saying:[39]

In view of the result of attained at the Washington Treaty which, my advisors believe, guarantee peace in the Pacific for some time to come, it is proposed to reduce the establishment of the navy and army, and postpone the expansion of the air force.
Between World War I and World War II, the Royal Australian Navy underwent a severe reduction in ships and manpower. As a result of the Washington Naval Treaty, the flagship HMAS Australia was scrapped with her main armaments and sunk outside Sydney Heads in 1924.[40] In the same year, the RAN began a five-year program of obtaining new ships from Britain: the heavy cruisers Australia and Canberra and the seaplane carrier Albatross. This purchase was partly paid for by scrapping Brisbane, Melbourne, Sydney, and most of the destroyers. The Great Depression of 1929 led to another reduction of manpower; although reduced in size, the available posts were easily filled as many men were unemployed and the offered pay was greater than most jobs. The RAN's personnel strength fell to 3,117 personnel, plus 131 members of the Naval Auxiliary Services. By 1932, the strength of the Reserves stood at 5,446. In the early 1930s, lack of funds forced the transfer of the Royal Australian Naval College from Jervis Bay to Flinders Naval Depot in Victoria. In 1933 the Australian Government ordered three light cruisers; HMA Ships Perth, Hobart, and Sydney; selling the seaplane carrier Albatross to fund Hobart. During this time, the RAN also purchased destroyers of the V and W destroyer classes, the ships that would become known as the Scrap Iron Flotilla. With the ever-increasing threat of Germany and Japan in the late 1930s, the RAN was not in the position it was at the outbreak of World War I.[41]

Australia declared war on Nazi Germany one hour after the United Kingdom's declaration of war on 3 September 1939. Unlike the arrangements with the British Admiralty at the start of the First World War, during World War II RAN ships remained under Australian command.

At the onset of war the RAN was relatively modest, even if it was arguably the most combat-ready of the three services. Major units included:[42]

Following the call up of reserves in 1939 the permanent forces grew from 5,440 to 10,259.[43]

During the war the men and vessels of the RAN served in every theatre of operations, from the tropical Pacific to the frigid Russian convoys and grew exponentially. The table illustrates the growth of the RAN between the outbreak of war on 3 September 1939 and 30 June 1945.[44]

From mid-1940, ships of the RAN, at the request of the Admiralty, began to deploy to the Mediterranean Sea to take part in the Battle of the Mediterranean against Nazi Germany and Fascist Italy. In September 1939, the Admiralty and the Australian Commonwealth Naval Board agreed to deploy the RAN Destroyer Flotilla outside the Australia Station; the five ships of what was to become known as the Scrap Iron Flotilla arrived at Malta in mid-December.[45] HMAS Sydney deployed in May 1940 and was later joined by Hobart. When Italy declared war on 10 June 1940, the Australian warships made up five of the twenty-two Allied destroyers and one of the five modern light cruisers on station in the Mediterranean. The RAN then offered the services of Australia to the Admiralty, and was accepted. When Australia arrived in the Mediterranean, the RAN had sent nearly the entire combat fleet to the Northern Hemisphere, leaving Australia open to possible attack.[46]

The entry of Italy into the war also lead to a far more active role for the few remaining RAN vessels on the Australian Station. Indeed, on 12 June 1940, after a prolonged chase, the Armed Merchant Cruiser (AMC) HMAS Manoora forced the Italian merchant ship Romolo (9,780 tons) to scuttle south-west of Nauru.[47]

On 27 June 1940, Admiral Cunningham commander of the Mediterranean Fleet ordered the 7th Cruiser Squadron, which included HMAS Sydney, to rendezvous with an Egypt-bound convoy near Cape Matapan.[48] The cruiser squadron sighted three Italian destroyers at 18.00 on 28 June 1940 and immediately engaged them.[49] Within an hour, the Espero was incapacitated and Sydney was signalled to sink her.[49] As Sydney approached, Espero launched torpedoes, but failed to hit any targets.[49] Sydney fired four salvos, scoring ten direct hits on Espero.[49] Sydney remained at the scene for two hours picking up survivors.[49]

Also on 27 June 1940, the Console Generale Liuzzi was scuttled south of Crete after being depth-charged by HMAS Voyager and the British destroyers Dainty, Ilex, Decoy, and Defender. On 29 June 1940, another Italian submarine, the Uebi Scebeli, was sunk west of Crete by the same ships.[50]

On 7 July 1940, a 25-ship fleet departed Alexandria, intending to meet a convoy east of Malta.[51] The next day, a submarine sighted an Italian fleet 500 miles (800 km) away; the Allied fleet altered course to intercept.[52] The two fleets sighted each other at 15.00 on 9 July 1940, and a battle that became known as the Battle of Calabria began.[53] Four vessels of the RAN took part in the battle; HMA Ships Sydney, Stuart, Vampire, and Voyager. Sydney was the first RAN vessel to engage the enemy, and at 15.20 opened fire on an Italian cruiser.[53] When the Italian fleet began to withdraw, the Allied destroyer squadron was ordered forward. Stuart, leading the destroyer force, was the first to open fire; her opening salvo was a direct hit at a range of 12,600 yards (11,500 m). Both fleets retired, with the Italians withdrawing under smoke, but Italian aircraft continued to attack Allied ships.[54] Sydney, which came under heavy air attack, was believed to have sunk.[55] The fleet arrived back in Alexandria on 13 July.[54]

On 17 July 1940, HMAS Sydney and the destroyer HMS Havock were ordered to support a Royal Navy destroyer squadron on a sweep north of the island of Crete.[56] At 07.20 on 19 July, the Italian cruisers Giovanni dalle Bande Nere and Bartolomeo Colleoni, which opened fire seven minutes later.[57] The four British destroyers retreated to the north-east, while Sydney and Havock, 40 miles (60 km) away, began to close in.[58] Sydney sighted the cruisers at 08.29, and fired the first shots of the Battle of Cape Spada at a range of 17,360 metres (56,960 ft).[59] Within minutes, Sydney had successfully damaged Bande Nere, and when the Italians withdrew to the south, the six Allied ships pursued.[60] At 0848, with Bande Nere hiding behind a smoke screen, Sydney shifted her fire to Bartolomeo Colleoni, which was disabled by 0933.[61] The Australian cruiser left to pursue Bande Nere, but broke off at 10.27 as the Italian warship was out of range, and Sydney was dangerously low on ammunition.[59][62] The only damage to Sydney during the battle was caused by a shell at 09.21, which knocked a hole in the forward funnel, and wounded a sailor through splinter damage.[63]

On 30 September 1940, HMAS Stuart destroyed the Italian 600-Serie Adua class submarine Gondar, killing two of its crew. Twenty-eight survivors was subsequently rescued by Stuart, with a further nineteen picked up by other vessels.[64]

On 27 March 1941, an Allied fleet under Admiral Cunningham was ambushed by an Italian naval force off Cape Matapan, Greece.[65] Three vessels of the RAN took part in the battle; HMA Ships Perth, Stuart, and Vampire. The victory at Cape Matapan allowed the evacuation of thousands of Allied troops from Crete.[66]

HMAS Parramatta was torpedoed and sunk on 27 November 1941 by U-559 whilst escorting transports resupplying the Allied garrison at Tobruk. There were 24 survivors, but 138 men, including all officers, lost their lives.[67]

The Australians experienced further success on 15 December 1941 when HMAS Nestor attacked and sank the German submarine U-127 off Cape St. Vincent, Portugal.[68]

On 6 September 1940, HMAS Australia was ordered to sail to Freetown, Sierra Leone to join Operation Menace, the invasion of Vichy French-controlled Dakar in French West Africa. On 19 September, Australia and the cruiser HMS Cumberland sighted three Vichy cruisers heading south and shadowed them. When the French cruiser Gloire developed engine trouble, Australia escorted her towards Casablanca and returned to the fleet two days later. On 23 September Australia came under heavy fire from shore batteries, then drove two Vichy destroyers back into port. Australia then engaged and sunk the destroyer L'Audacieux with eight salvos in sixteen minutes. Over the next two days French and Allied forces exchanged fire; Australia was struck twice and lost her Walrus amphibian. Australia and the rest of the fleet retired on 25 September the battle became known as the Battle of Dakar.[69][70]

The Scrap Iron Flotilla was an Australian destroyer group that operated in the Mediterranean and Pacific during World War II. The name was bestowed upon the group by Nazi Propaganda Minister Joseph Goebbels who described the fleet as a "consignment of junk" and "Australia's Scrap-Iron Flotilla". The flotilla consisted of five vessels; Scott-class destroyer HMAS Stuart, which acted as flotilla leader, and four V-class destroyers; Vampire, Vendetta, Voyager, Waterhen. The ships were all built to fight in World War I, and were slow and poorly armed compared to newer ships.[71] The five destroyers—the entirety of the RAN's destroyer force—departed Australia in November 1939 destined for Singapore where they carried out anti-submarine exercises with the Royal Navy submarine HMS Rover.[72] On 13 November 1939, the flotilla left Singapore for the Mediterranean Sea, following a request from the Admiralty for assistance.

The Australian destroyer flotilla took part in multiple actions while in the Mediterranean, including the Allied evacuation following the battle of Greece in April 1941, though the flotilla came to fame in the mission to resupply the besieged city of Tobruk. The resupply routes from Alexandria and Mersa Matruh to Tobruk became known as "Bomb Alley" and was subject to constant Axis air attacks.[71] The flotilla, which by this time was in poor condition, managed to make 138 supply runs to Tobruk, carrying in ammunition and stores and taking out wounded soldiers. On 28 May 1941 Vampire became the first of the flotilla to leave the Mediterranean. Vendetta, the last to leave, sailed in October 1941.

Of the five destroyers, three were lost during the war; Waterhen was sunk in the Mediterranean on 30 June 1941, Vampire was sunk by Japanese aircraft during the Indian Ocean Raid and Voyager ran aground at Betano, during the Timor campaign and was abandoned.[73]

As well as serving in the Mediterranean Sea, ships of the RAN also served in the Red Sea. In August 1940, Italian forces invaded British Somaliland. After a fighting withdrawal, the small British garrison was evacuated from Berbera, with HMAS Hobart assisting in the destruction of the port and its facilities. To aid in the delaying action, Hobart sent a 3-pounder gun ashore, operated by volunteers from the crew. The seamen were captured by the Italians, but were later liberated. Two RAN sloops joined the Red Sea force in 1940: Parramatta on 30 July and Yarra in September. In October, Yarra engaged and drove off two Italian destroyers attempting to raid a convoy. Although vessels of the RAN served in the Red Sea throughout the war, after 1941 the larger RAN ships were deployed to Australian waters in response to the threat from Japan.[74]

On 19 November 1941, the Australian light cruiser HMAS Sydney and the German auxiliary cruiser Kormoran engaged each other in the Indian Ocean, off Western Australia. The two ships sank each other: Sydney was lost with all 645 hands, while the majority of the Kormoran's crew were rescued and became prisoners of war. The location of both wrecks remained a mystery to many and subject to much controversy until 16–17 March 2008, when both ships were found.

RAN units continued to serve in the Mediterranean campaign, with HMAS Quiberon, taking part in Operation Torch, the invasion of North Africa. On 28 November 1942 Quiberon assisted in sinking the Italian submarine Dessiè and three days later also took part in the destruction of a four-ship convoy and a destroyer.[75]

During early 1943, eight Australian-designed and built Bathurst-class corvettes were transferred to Egypt from the Indian Ocean, in preparation for Operation Husky, the Allied invasion of Sicily.[76] They were part of a 3,000-ship Allied force. The corvettes arrived in the Mediterranean in May and were formed into the 21st and 22nd Minesweeping Flotillas. All eight ships survived the campaign without damage or casualties sustained in action, although HMAS Maryborough experienced a near-miss from a German bomber. When the captain of HMAS Gawler enquired what damage had been sustained, the response from Maryborough read: "no damage except to my underpants".[76]

After the Imperial Japanese Navy's attacks on the Allies in December 1941, the RAN redeployed its larger ships to home waters to protect the Australian mainland from Japanese attack, while several smaller ships remained in the Mediterranean. From 1940 onwards, there was considerable Axis naval activity in Australian waters first from German commerce raiders and submarines and later by the Imperial Japanese Navy.

Initially, RAN ships served as part of the British-Australian component of the American-British-Dutch-Australian Command (ABDACOM) naval forces or in the ANZAC Force. ABDACOM was wound up following the fall of the Netherlands East Indies and was succeeded by the South West Pacific Area (command) (SWPA). The United States Seventh Fleet was formed at Brisbane on 15 March 1943, for service in the SWPA. RAN ships in the Pacific generally served at part of Seventh Fleet taskforces.

From February 1942, the RAN played a critical role in resupplying Australian and Dutch commandos on Timor. Voyager was not the only loss during the campaign. On 1 December 1942, HMAS Armidale was attacked by thirteen Japanese aircraft while attempting to land Dutch soldiers off Betano, Portuguese Timor. Armidale sank with the loss of 40 of her crew and 60 Dutch personnel. During the engagement, Ordinary Seaman Teddy Sheean operated an Oerlikon anti-aircraft gun and was wounded by strafing Japanese planes, he went down with the ship, still strapped into the gun and still shooting at the attacking aircraft.[77]

On 28 February 1942, a joint ABDA naval force met a Japanese invasion force in the Java Sea. The Leander-class cruiser HMAS Perth and the American heavy cruiser USS Houston fought in and survived the Battle of the Java Sea.

On 1 March 1942, the Perth and Houston attempted to move through the Sunda Strait to Tjilatjap however they found their path blocked by the main Japanese invasion fleet from western Java. The Allied ships were engaged by at least three cruisers and several destroyers and in a ferocious night action, known as the Battle of Sunda Strait, both Perth and Houston were torpedoed and sunk. Casualties aboard Perth included 350 crew and 3 civilians killed, while 324 survived the sinking and were taken prisoner by the Japanese (106 of whom later died in captivity). The loss of Perth so soon after the sinking of her sister Sydney, had a major psychological effect on the Australian people. Japanese losses included a minesweeper and a troop transport sunk by friendly fire, whilst three other transports were damaged and had to be beached.[78]

On 2 May 1942, two ships of the RAN were part of the Allied force in the Battle of the Coral Sea; HMA Ships Australia and Hobart as part of Task Force 44. Both ships came under intense air attack, while part of a force guarding the approaches to Port Moresby.[79]

In late May and early June 1942, a group of five Imperial Japanese Navy submarines made a series of attacks on Sydney and the nearby port of Newcastle. On the night of 31 May – 1 June, the submarines launched three Ko-hyoteki-class midget submarines against Allied shipping in Sydney Harbour. A torpedo intended for the cruiser USS Chicago exploded under the depot ship HMAS Kuttabul, killing 21. On 8 June, two of the submarines shelled Sydney and Newcastle, with little effect.[80] In response, the RAN instituted convoys between Brisbane and Adelaide. All ships of over 1,200 tons and with speeds of less than 12 knots (22 km/h; 14 mph) were required to sail in convoy when travelling between cities on the east coast.[81]

The attack on Sydney and Newcastle marked the start of a sustained Japanese submarine campaign against Australia. During 1942, Japanese submarines sank 17 ships in Australian waters, although none of these ships were sailing as part of a convoy.[82] 16 ships were sunk in Australian waters during 1943, before the Japanese ended the campaign in July. Five of these ships were sunk while sailing in escorted convoys.[83] The Australian naval authorities gradually dismantled the coastal convoy system between December 1943 and March 1944.[84] By the end of the war, the RAAF and RAN had escorted over 1,100 convoys along the Australian coastline.[4]

While the scale of the Japanese naval offensive directed against Australia was small compared to other naval campaigns of the war such as the Battle of the Atlantic, these attacks were "the most comprehensive and widespread series of offensive operations ever conducted by an enemy against Australia".[85] Although the RAN only sank a single full-sized Japanese submarine in Australian waters (I-124 in January 1942) convoy escorts may have successfully reduced the threat to shipping in Australian waters by making it harder for Japanese submarines to carry out attacks.[86]

Whilst escorting convoys between Australia and New Guinea, HMAS Arunta attacked and sank the Japanese Kaichu type submarine Ro-33 off Port Moresby on 24 August 1942, killing all 42 men aboard.[87]

The loss of HMAS Canberra at the Battle of Savo Island in August 1942 was the largest single ship loss the RAN experienced during World War II. In the early hours of the morning of 9 August 1942, Canberra was severely damaged off Guadalcanal in a surprise attack by a powerful Japanese naval force. Canberra was hit by 24 shells in less than two minutes, with 84 of her crew killed, including Captain Frank Getting. Following an order to abandon ship, Canberra was sunk the next day by a torpedo from a US destroyer, to prevent it being captured.

The loss of Canberra, following the losses of Sydney and Perth, attracted unprecedented international attention and sympathy for the RAN. US President Franklin D. Roosevelt wished to commemorate the loss of Canberra and requested that a US heavy cruiser under construction be named Canberra. USS Canberra was launched on 19 April 1943.[88] The British Government approved the transfer of HMS Shropshire to the RAN as a replacement, and the ship was commissioned as HMAS Shropshire on 20 April 1943.

Between 23 and 25 October 1944 four RAN warships – HMA Ships Australia, Shropshire, Arunta, and Warramunga – took part in the Battle of Leyte Gulf, one of the largest naval battles in history. In the lead-up, on 21 October, Australia became the first Allied ship to be hit by a kamikaze aircraft near Leyte Island.[89] Gunners from Australia and Shropshire fired at, and reportedly hit, an unidentified Japanese aircraft. The plane then flew away from the ships, before turning and flying into Australia, striking the ship's superstructure above the bridge, and spewing burning fuel and debris over a large area, before falling into the sea. A 200-kilogram (440 lb) bomb carried by the plane failed to explode; if it had, the ship might have been effectively destroyed. At least 30 crew members died as a result of the attack, including the commanding officer, Captain Emile Dechaineux; among the wounded was Commodore John Collins, the Australian force commander. Australia remained on duty, but on 25 October, was hit again and was forced to retire to the New Hebrides for repairs.

Shropshire and Arunta remained at Leyte and were part of the United States Seventh Fleet Support Force at the Battle of Surigao Strait on 25 October. During this action both ships contributed to the sinking of the Japanese battleship Yamashiro, with Shropshire firing thirty-two eight-gun broadsides into the battleship with her 8-inch guns in a period of 14 minutes.[90]

HMAS Australia returned to combat at the Battle of Lingayen Gulf in January 1945. During the battle Australia was repeatedly attacked between 5–9 January, suffering significant damage which forced it to retire once more.[91]

In 1940–42, five N class and two Q class were built in the UK and commissioned into the RAN for service with the British Eastern Fleet: HMA Ships Napier, Nepal, Nestor, Nizam, Norman, Quiberon, and Quickmatch. These ships were predominantly crewed by RAN personnel, although they were often commanded by British officers and remained the property of the British government.

Following the Japanese raid on Ceylon of March–April 1942, the Eastern Fleet was transferred from its base at Trincomalee, to the other side of the Indian Ocean: Kilindi in Kenya. From there the fleet undertook local patrols, escorted convoys and occasionally despatched ships to operations in the Mediterranean. During Operation Vigorous, a convoy to Malta in June 1942, Nestor was serious damaged in an air raid and slowly sank.

On 11 February 1944 the corvettes HMA Ships Ipswich and Launceston, in conjunction with the Indian sloop Jumna, sank the Japanese submarine Ro-110 in the Bay of Bengal after the latter had torpedoed a ship in a Calcutta-bound convoy.[90]

From late 1944, Nepal, Norman and Quiberon were transferred, along with many other Eastern Fleet ships, to the new British Pacific Fleet (BPF). Among other operations with the BPF, they took part in the Battle of Okinawa.

In late 1945, following the end of hostilities, the RAN acquired three more Q-class destroyers: Queenborough, Quality, and Quadrant.

By the end of World War II, the RAN's combat strength numbered 150 ships with an additional 200 auxiliary craft with the service reaching its peak in June 1945, when it ranks swelled to 39,650 personnel.[4] During the six years of war, the RAN lost three cruisers, four destroyers, two sloops, a corvette, and an auxiliary minesweeper to enemy action.[68] Casualties included 1,740 personnel from the 19 ships sunk, and another 436 personnel killed aboard other ships or at other posts.[4] By most measures, such losses were heavy for such a small service, representing over half its pre-war strength in ships and one-fifth in men. Against this the RAN destroyed one cruiser, an armed merchant raider, three destroyers or torpedo boats, a minesweeper, many light craft and seven submarines. It also destroyed or captured more than 150,000 tons of Axis merchant shipping and shot down more than a hundred aircraft. Although difficult to quantify the RAN also played a role in numerous other successes.[68]

Ten RAN vessels were present at the signing of the Japanese surrender in Tokyo Bay on 2 September 1945; HMA Ships Ballarat, Cessnock, Gascoyne, Hobart, Ipswich, Napier, Nizam, Pirie, Shropshire, and Warramunga.[92] Following the surrender ceremony, the majority of the RAN vessels left Japanese waters for other duties. As part of the surrender agreement, Japan agreed to an Allied occupation and disarmament. On 17 August 1945, the Australian Government agreed to provide two cruisers and two destroyers for service with the British Commonwealth Occupation Force (BCOF). A total of 15 RAN ships served with the BCOF, the ships performed a variety of tasks but were mainly employed on the Kyushu Patrol, preventing Korean nationals from illegally entering Japan.

The RAN also played a role in the disarmament of Japan, assisting in the scuttling of former Imperial Japanese Navy ships, in one instance Quiberon took part in the sinking of seven submarines of Kyushu as part of Operation Bottom. When Indian and New Zealand contingents began to withdraw from the BCOF, the operation became a predominantly Australian operation. In 1948, Kure naval base was turned over to Australia, and became known as HMAS Commonwealth. When North Korea invaded South Korea on 25 June 1950, one RAN ship was on station as part of BCOF. The Australian Government immediately offered HMAS Shoalhaven for United Nations service. Eventually, all RAN ships in the area were transferred to the command of British Commonwealth Forces Korea (BCFK).[93]

Clearing mines from Australian and New Guinean waters was another focus for the RAN in the years after the war. Minesweeping began in December 1945 and was conducted by HMAS Swan, eight Bathurst-class corvettes and several smaller craft from a base at Cairns. The work was arduous and dangerous, and HMAS Warrnambool was sunk with the loss of four men killed and another 25 wounded when she struck a mine off North Queensland on 13 September 1947. The RAN completed this task in August 1948 after sweeping 1,816 mines.[94]

Following World War II, the RAN reduced its surface fleet but continued to expand in other ways, acquiring two Royal Navy Majestic-class aircraft carriers then under construction (HMAS Melbourne and HMAS Sydney) to build up a Fleet Air Arm. In the 1960s, the RAN began to move away from British-designed ships; the last major British design used was the Type 12 frigate, which formed the basis of the River-class frigates.

When it was decided that the RAN should commission a destroyer armed with guided missiles, the obvious British design was the County class; however, the RAN had reservations regarding the gas turbine propulsion, the Seaslug missile system, and the ability to adapt the design to Australian needs. Instead, the Australian government chose the United States-built, steam turbine-powered Charles F. Adams-class destroyer, armed with the Tartar missile as the basis for its Perth class, the first major US warship design chosen for the RAN.[95]

By the mid-late 1960s, the RAN was at the zenith of its operational capabilities; it was capable of dispatching a full carrier battle group in support of major operations by having in service an aircraft carrier (HMAS Melbourne), three large area defence destroyers of the Perth class, six modern River-class frigates and four Oberon-class submarines.

With the retreat of British forces west of the Suez Canal in the 1960s, the RAN began to take a more defensive role, and in co-operation with the United States, allied though the ANZUS treaty. The RAN saw service in many of the world's post war conflicts, including Korea, Vietnam, and the Indonesian Confrontation.

On 27 June 1950, the United Nations Security Council called on member nations to aid South Korea. On 29 June, Prime Minister Robert Menzies announced that the frigate HMAS Shoalhaven, stationed in Japan, and the destroyer HMAS Bataan, in Hong Kong, would be placed under UN command in Korea. On 1 July, one day after President Truman committed American ground forces to Korea, the first Australian operation in Korea took place; Shoalhaven escorted an American ammunition ship from Japan to Pusan.

The destroyer Warramunga was deployed in July 1950, and provided gunfire support during the X Corps landing at Wonsan in October. In December, Bataan and Warramunga assisted the mass evacuation of troops and refugees from Hungnam. The aircraft carrier Sydney was deployed to Korea between September 1951 and January 1952—the first carrier owned by a Commonwealth Dominion to see wartime service.[96] During this time, 2,366 sorties were flown from Sydney, with only fifteen aircraft lost and three pilots killed.[96]

Over the course of the Korean War, nine ships of the RAN participated in the naval blockade of North Korea.[97]

The Malayan Emergency was declared on 18 June 1948, prompted by a rise in Malayan Communist guerrillas in Malaya (later Malaysia).[98] Australia, as a member of the Southeast Asia Treaty Organization, first deployed two RAAF squadrons to the region in 1950.[98] In 1955, the Far East Strategic Reserve was created as a concentration of Commonwealth military forces (primarily British, New Zealand, and Australian) in Malaya for the protection of that nation from communist threats.[98] Australia's commitment included two destroyers or frigates on station at any time, plus an annual visit by an aircraft carrier, and additional ships as needed.[98] Training for the potentiality of war was the main occurrence for ships deployed to the Strategic Reserve, with RAN personnel gaining experience in working as part of a larger naval organisation.[98]

The first ships of the RAN to arrive in the area were the Tribal-class destroyers Warramunga and Arunta in June 1955.[98] Between 1955 and 1960, eleven other ships of the RAN operated with the Strategic Reserve: Anzac, Melbourne, Quadrant, Queenborough, Quiberon, Quickmatch, Sydney Tobruk,Vampire, Vendetta, and Voyager.[98]

In response to the Indonesian invasion of Borneo and Malaya in 1963, Australia increased its presence in the region. At the outbreak of hostilities, the RAN frigates Yarra and Parramatta were on duty in the area. As tension mounted, Australia increased its presence by sending Sydney, Vampire, Vendetta, Duchess, and Derwent to the area. On 19 May 1964, the 16th Minesweeping Squadron, comprising six Ton-class minesweepers, was also deployed.[99]

On 13 December 1964, the minesweeper HMAS Teal was fired upon with automatic weapons by an unlit vessel whilst operating as part of the Singapore Strait patrol. The vessel was overpowered and arrested by Teal, following a further small arms engagement that resulted in the deaths of three Indonesian crew members. On 23 February 1965, Teal was again involved in another engagement, she detected an unlit vessel off Cape Rachado. The suspicious vessel was closed on and illuminated, and revealed nine armed men in uniform who surrendered immediately upon challenge. On 13 March 1964, HMAS Hawk became the second vessel of the 16th Minesweeping Squadron to see action, when she was fired on by an Indonesian shore battery while patrolling off Raffles Lighthouse. Eleven high-explosive rounds were fired at the ship, some landing within 200 yards (200 m) of the vessel, and Hawk withdrew from the area at speed. The following morning, Hawk intercepted a sampan with five Indonesians on board who were promptly arrested.

When Indonesian forces crossed the border into Sebatik Island, Sabah on 28 June 1965, HMAS Yarra was called on to carry out bombardments disrupting the withdrawal of the Indonesians. Yarra carried out two more bombardments of the border area on 5 and 10 July. During three runs, Yarra fired a total of 70 rounds on the enemy. On 13 August 1966, an agreement between Indonesia and Malaysia brought an end to the conflict.

During the night of 10 February 1964, the worst peacetime disaster in the RAN's history occurred when the destroyer HMAS Voyager was cut in two by the bow of the aircraft carrier HMAS Melbourne, killing 82 of the 293 men on board Voyager.[100][101] Melbourne was conducting air group exercises off Jervis Bay with Voyager acting as the plane guard destroyer. After a series of manoeuvres to reverse the course of the ships, Voyager ended up to starboard of Melbourne, and was ordered to resume her position (behind the carrier and to port) at 20.52.[102] Instead of turning away from Melbourne, Voyager unexpectedly turned towards the carrier, and did not alter course until it was too late.[100][103] At 20.56, Melbourne's bow hit the destroyer just behind the bridge, and cut her in half, with the bow sinking quickly.[104] The search for survivors went on through the night; of the 314 aboard, 14 officers, 67 sailors, and 1 civilian dockyard worker were killed, including Captain Duncan Stevens.[100][105]

Following the collision Prime Minister Menzies ordered a Royal Commission to investigate the event.[106] The Commissioner concluded that the collision was primarily the fault of Voyager's bridge crew not maintaining an effective lookout, but also placed blame on Melbourne's Captain John Robertson (who resigned shortly after) and two other officers for failing to alert Voyager or take effective measures to avoid collision.[107][108] The handling of the Royal Commission was seen as poor, and after a combination of public pressure and claims that Stevens had a drinking problem, a second Royal Commission was announced: the only time two Commissions have been held for the same incident.[109] The second Royal Commission found that Stevens was likely medically unfit for command, that some of the first Commission's conclusions were therefore incorrect, and the Melbourne officers were not at fault.[110] The two commissions caused great anguish in the hierarchy of the RAN, which was not accustomed to such tight scrutiny, and led to the eventual dismantling of the Naval Board's isolation from the civilian world.[108]

Ships of the Royal Australian Navy were stationed on continuous operational service in Vietnam between 1965 and 1972; a total 18 ships served in Vietnam waters during the war. During this period, the navy performed a wide variety of operational tasks at sea, ashore, and in the air. The RAN's primary contribution consisted of destroyers, Fleet Air Arm personnel attached to a United States Army helicopter company and No. 9 Squadron RAAF, a Clearance Diving Team, and a logistical support force consisting of transport and escort ships. Other RAN personnel served ashore in medical teams or performed staff duties at the Australian Embassy in Saigon or the 1st Australian Task Force Headquarters at Nui Dat.

The RAN did not deploy operationally until 1965, but in 1962 HMAS Vampire and HMAS Quickmatch made goodwill visits to Saigon. They were followed a year later by similar visits by HMAS Quiberon and HMAS Queenborough. In 1967, HMAS Hobart became the first RAN destroyer to be operationally deployed to Vietnam. Hobart served three tours in Vietnam from March to September in 1967, 1968, and 1970. During her operations, she fired 10,000 rounds at 1,000 shore targets and came under fire around 10 times, including on one occasion by a United States F-4 Phantom.[111] Hobart was awarded the United States Navy Unit Commendation in recognition of her service in Vietnam, while sister ship Perth received both the United States Navy Unit Commendation and the Meritorious Unit Commendation. Clearance Diving Team 3 was awarded the US Presidential Citation, two US Navy Unit Commendations and a US Meritorius Unit Commendation. The only non US Unit to ever receive all 3 awards.  After their five years of service in Vietnam, the four gunline destroyers; Perth, Brisbane, Hobart and Vendetta steamed over 397,000 miles and fired 102,546 rounds.[112]

The aircraft carrier HMAS Sydney was converted for troopship duties in the early 1960s, and began her first voyage to Vietnam in May 1965, transporting the 1st Battalion, Royal Australian Regiment, from Sydney to Vung Tau in southern Vietnam. Sydney became known as the Vung Tau Ferry and made 25 voyages to Vietnam: carrying 16,094 troops, 5,753 deadweight tons (5,845 t) of cargo and 2,375 vehicles.[101]

In 1969, the aircraft carrier HMAS Melbourne rammed and sank another destroyer.[113] During the night of 2–3 June, USS Frank E. Evans was escorting the carrier during multinational wargames in the South China Sea.[113] Ordered to the plane guard station, Evans crosses the carrier's bows and was cut in two, killing 74 United States personnel.[114] A Joint RAN-USN Board of Inquiry was established, which found Melbourne's Captain John Stevenson and three officers from Evans at fault.[114] Despite being cleared by a RAN court-martial, Stevenson resigned after receiving similar treatment to Robertson in the first collision.[115] HMAS Melbourne is believed to be the only warship to sink two friendly vessels in peacetime.[116]

In April 1971, Prime Minister John Gorton announced that Australian forces in Vietnam would be reduced. This led to the withdrawal of the clearance divers in May and the Fleet Air Arm in June. The final RAN destroyer on the gunline, Brisbane, returned to Sydney on 15 October 1971. The Whitlam government withdrew all Australian forces from and stopped military aid to South Vietnam. HMAS Jeparit returned to Sydney on 11 March 1972 and was followed the next day by HMAS Sydney. During the 10 years that the RAN was involved in the war, eight officers and sailors were killed, and another 46 were either wounded or suffered other injuries.[112][117]

During the morning of 25 December 1974, Tropical Cyclone Tracy struck the city of Darwin, killing 71 people and causing $4 billion of damage (1998 A$).[118] In response to the cyclone, the RAN embarked upon Operation Navy Help Darwin; the largest peacetime disaster relief operation in its history, involving 13 ships, 11 aircraft and some 3,000 personnel.[119]

When Tracy struck Darwin, the RAN had a total of 351 personnel based in the city, along with four Attack-class patrol boat; the small number of men limited the capability of the RAN to render immediate assistance to the citizens of Darwin.[120] All four patrol boats were damaged in some way: Advance and Assail were able to weather the cyclone with minor damage, but Attack was forced aground, and Arrow sank after colliding with Stokes Hill Wharf, killing two personnel.[119] Land-based naval installations were also heavily damaged by the cyclone, Darwin Naval Headquarters was destroyed, as were large sections of the patrol boat base and the married quarters. The oil fuel supply installation and naval communications station at HMAS Coonawarra were also damaged. The initial RAN relief which was limited to search and rescue in the area of Darwin Harbour and Melville Island, which was hindered by the lack of reliable communications.[119]

As the severity of the disaster was realised, a naval task force was established to render aid to the people of Darwin; Operation Navy Help Darwin. A general recall was issued to all personnel; volunteers from shore bases and ships unable to sail were used to replace those who could not return to their ships in time.[119] The first RAN assets arrived in Darwin on 26 December, a HS 748 aircraft carrying blood transfusion equipment and Red Cross workers, followed shortly by another HS 748 carrying Clearance Diving Team 1 (CDT1). Ships also began departing for Darwin on 26 December: Balikpapan and Betano departed from Brisbane, Flinders sailed from Cairns, while Melbourne (with Rear Admiral Wells aboard), Brisbane, and Stuart left Sydney. The next day, Hobart, Stalwart, Supply, and Vendetta left Sydney, while Brunei and Tarakan sailed from Brisbane.[119] The last ship, Wewak, left Brisbane on 2 January.[119]

The first vessels, HMA Ships Brisbane and Flinders, arrived in Darwin on 31 December. Flinders surveyed the approaches to Darwin, ensuring the safety of the taskforce, while Brisbane landed working parties and established communications. The entire 13-ship task force had arrived in Darwin by 13 January 1975, bringing over 3,000 personnel.[119] RAN personnel was primarily assigned to clear the suburbs of Nightcliff, Rapid Creek, Northern Territory, and Casuarina, while aircraft and helicopters were used to move evacuees and supplies, and CDT1 inspected ships in the harbour for damage and cleared several wharves.[119] Vessels of the task force began to depart Darwin as early as 7 January, with HMA Ships Brisbane and Stalwart the last to depart on 31 January, after command of the relief operation was turned over to the Commandant of the Army's 7th Military District.[119]

Following the introduction of the 1982 United Nations Convention on the Law of the Sea (UNCLOS) the exclusive economic zone (EEZ) of many coastal nations was increased from 12 to 200 Nmi. The sudden expansion of responsibility dramatically increased the area of ocean requiring surveillance, monitoring and policing by these nations, increasing the strain on existing maritime patrol resources, and highlighting the need for countries without a maritime patrol force to obtain one, especially in the South West Pacific area.[121]

In 1979, the Australian and New Zealand Governments, at the request of Pacific Island nations, sent defence representatives into the South-West Pacific region to assess surveillance and maritime patrol requirements. The governments of a number of the Pacific nations expressed their concern about the need for a suitable naval patrol force to meet their new surveillance requirements. The Australian government responded by creating the Defence Cooperation Project (DCP), to provide suitable patrol vessels, training and infrastructure to island nations in the region. The Pacific Patrol Boat Systems Program Office was created within the Minor War Vessels Branch of the RAN procurement organisation.[121]

The tender for the vessels was released in August 1984, and was awarded to Australian Shipbuilding Industries Pty Ltd (now Tenix Western Australia) in September 1985. The first of ten vessels was to be delivered in early 1987. The first vessel, HMPNGS Tarangau, was officially handed over to the Papua New Guinea Defence Force on 16 May 1987. Over the course of the project the number of participating countries increased. By the end of the construction phase of the project, a total of 22 boats had been delivered to 12 countries, compared to the original order of 10 boats for 8 countries. In total, the project cost for 22 vessels and associated support was A$155.25 million.[121]

The RAN never operated the Pacific-class patrol boat (PPB), although the project has given the RAN a number of advantages in the Pacific region. The introduction of self-reliant patrol forces throughout the region has eased the strain on Australia's own maritime patrol force. Cooperation between Australia and its Pacific neighbours has allowed for a greater allocation of RAN patrol boats to protecting Australia's maritime resources, patrolling the Sea Lines of Communication (SLOC), and conducting border protection operations. The PPB's have recently undergone a mid life refit which could potentially see them operating in the region until at least 2027.[121]

The main role of the Royal Australian Navy in the two decades following the end of Australia's involvement in the Vietnam War was supporting Australian diplomatic initiatives.  In line with this goal the RAN exercised with the navies of Australia's allies and provided support to civil authorities in Australia and the South Pacific.[122] The RAN's main military concern from the 1970s was the activities of the Soviet Navy in the Indian Ocean. In 1971 the Marxist government of South Yemen permitted the Soviet Navy to use the former British naval base at Aden, thus allowing the Soviet Navy to maintain a squadron in the Indian Ocean.[123] These concerns lead to increased co-operation with the United States Navy and the development of the RAN's main base in Western Australia, HMAS Stirling.[124]

During the late 1970s, the RAN replaced many of its ageing ships with modern equivalents.  While it planned to purchase the British aircraft carrier HMS Invincible to replace Melbourne, Britain's offer of the carrier was withdrawn after the Falklands War. As a result, Melbourne was decommissioned without replacement in 1982 and the Fleet Air Arm retired almost all of its fixed wing aircraft on 30 June 1983.[125]

In 1987, the Hawke Government's Defence White Paper called for the RAN to become a more self-reliant two-ocean navy with major fleet bases in New South Wales and Western Australia. The plan called for the expansion of Stirling on Garden Island and Jervis Bay to accommodate an expanded RAN combat surface and submarine fleets.  The plan originally called for the major combat units and submarines to be split between the two fleet bases, providing similar capabilities on both sides of the continent. The proposed Jervis Bay naval base never became a reality; Fleet Base East was built up around HMAS Kuttabul in Sydney while HMAS Stirling is home to half the surface fleet and the entire submarine fleet.

The rationale behind the policy included the possibility of savings in fuel and maintenance that would result from Indian Ocean deployments beginning their journey from Western Australia rather than New South Wales. The report also classed the Indian Ocean as an area where contingencies might arise. The new facilities would increase Australia's worth to the United States, particularly to do with maintenance of submarines.  Expansion at Jervis Bay would allow intensified east coast visits by the United States Pacific Fleet, and its nuclear warship visits would not run into as much opposition as they do in Sydney and Melbourne.[126]

The 1987 White Paper was seen by many as an attempt to strengthen Australia's relationship with the United States, which had been damaged by New Zealand's stance against nuclear weapons in its ports. In line with this policy, the RAN was structured to become more self-reliant and its activities during the late 1980s were focused on operating within Australia's local region.[127]

The Two Ocean Policy remains in place today[when?] and is supported by the current Australian Government and the opposition. The success of the policy is especially evident at HMAS Stirling. The base is thriving and its location both in a global and local context gives it an advantage over Fleet Base East. It has been suggested that all eight Anzac class ships be relocated to Stirling, this would create an easier training environment for sailors and would lead to significant cost savings.[128]

Australia's contribution to the 1991 Gulf War centred on a Naval Task Group, initially Task Group 627.4,[129] which formed part of the multi-national fleet in the Persian Gulf and Gulf of Oman. In addition, medical teams were deployed aboard a US hospital ship and a naval clearance diving team took part in de-mining Kuwait's port facilities at the end of the war. Over the period from 6 September 1990 to 4 September 1991 the RAN deployed a total of six ships to the area: HMA Ships Adelaide, Brisbane, Darwin, Success, Sydney, and Westralia. Clearance Diving Team 3 operated in the theatre from 27 January 1991 to 10 May 1991. It was involved in mine clearing operations in Kuwait from 5 March to 19 April 1991.[130]

After the end of the first Gulf War the Royal Australian Navy periodically deployed a ship to the Gulf or Red Sea to assist in maintaining sanctions against Iraq. Until the outbreak of the Second Gulf War the Australian naval force in the Persian Gulf continued to enforce the sanctions against Iraq. These operations were conducted by boarding parties from the RAN warships.[131]

Upon the outbreak of war, the RAN's focus shifted to supporting the coalition land forces and clearing the approaches to Iraqi ports. HMAS Anzac provided gunfire support to Royal Marines during fighting on the Al-Faw Peninsula and the Clearance Diving Team took part in clearing the approaches to Umm Qasr. Boarding operations continued during the war, and on 20 March, boarding parties from HMAS Kanimbla seized an Iraqi ship carrying 86 naval mines.[132]

Since the end of the war the RAN has continuously maintained a frigate in the Persian Gulf to protect Iraq's oil infrastructure and participate in counter-smuggling operations. Twelve Australian sailors were deployed to Umm Qasr, Iraq between January and October 2004 to join the multi-national training team working with the Iraqi Coastal Defense Force.[133] The RAN has also assumed command of coalition forces in the Persian Gulf on two occasions; Combined Task Force 58 in 2005[134] and Combined Task Force 158 in 2006.[135]

On 5 May 1998, a fire broke out onboard HMAS Westralia while off the Western Australia coast. The fire was caused by the rupture of a flexible fuel line (one of a number used to replace rigid hoses) on cylinder number nine, starboard engine. This sprayed diesel fuel onto a hot indicator cock, which ignited a spray fire, resulting in the deaths of four crew. Following the fire, the Australian Government and the RAN began a major investigation known as the Westralia Board of Inquiry. The enquiry found that the RAN and the contractor Australian Defence Industries (ADI) did not critically examine their course of action and that key personnel in both the RAN and the contractor were insufficiently trained and qualified. The inquiry also found that the hoses were not properly designed and were unfit for the intended purpose.[136] In 2005, ADI was fined $75,000 for failing to provide a safe workplace. Seven sailors who were severely traumatised by the fire have also sued ADI and subcontractor Jetrock. In August 2006, the Australian Government decided to accept liability after it reached settlement with the ADI and Jetrock. The seven sailors stand to receive compensation totalling up to $10 million.[137]

During the Australian-led United Nations peacekeeping mission to East Timor in 1999 known as INTERFET, the RAN deployed a total of 16 ships to the mission: HMA Ships Adelaide, Anzac, Balikpapan, Brunei, Darwin, Farncomb, Jervis Bay, Labuan, Success, Sydney, Tarakan, Tobruk, Waller, Westralia, Newcastle and Melbourne.[138] The RAN played a vital role in transporting troops and providing protection to transports and were vital to the success of INTERFET.

The RAN returned to East Timor in 2006 under Operation Astute the United Nations-authorised, Australian-led military deployment to East Timor to quell unrest and return stability during the 2006 East Timor crisis. The Royal Australian Navy deployed the Amphibious Ready Group, including the ships; Kanimbla, Manoora, Tobruk (until approximately 8 June), Balikpapan, Tarakan and Success (until 28 May). The RAN also deployed the Adelaide-class frigate HMAS Adelaide (until 28 May). The Fleet Air Arm contributed one S-70B-2 Seahawk helicopter from 816 Squadron RAN (until 28 May) and two Sea King helicopters from 817 Squadron RAN. The Royal Australian Navy force committed to Operation Astute is apparently the largest amphibious task force in the navy's history.[139]

On 24 July 2003, HMAS Manoora arrived off Honiara, marking the beginning of Operation Anode, Australia's contribution to the Regional Assistance Mission to the Solomon Islands (RAMSI). The deployment of a 2,200 strong multinational force followed several years of unrest in the Solomon Islands. Manoora was soon joined by HMA Ships Hawkesbury, Labuan, Wewak and Whyalla. Following the initial deployment, two vessels were generally kept on station in the area. By the time the RAN deployment ended, 19 Australian warships had taken part. The last ship to leave was Fremantle, which sailed home in October 2004.[140]

Operation Anode was not the first time units of the RAN had been deployed to the Solomon Islands; Anode was unique in that the navy's primary role was to support and facilitate the work of the Participating Police Force (PPF). Moreover, along with being the first time the RAN had supported a police-led mission,[141]

On 2 November 2006, in response to the Fijian military threats to overthrow the Fijian Government, the Australian government began Operation Quickstep by deploying military resources to support Australian citizens in Fiji in the event of a coup d'état. The contribution from the RAN was the deployment of three vessels (Kanimbla, Newcastle, and Success) to international waters south of Fiji; with the mission to evacuate the estimated 7,000 Australian citizens present in Fiji if the need arose.[142] Along with the three vessels a detachment of the Special Air Service Regiment (SASR), helicopters from the 171st Aviation Squadron, and an evacuation team were also deployed.[143]

On 29 November 2006, an Australian Army S-70A Black Hawk helicopter operating from Kanimbla, and carrying ten Army personnel on board, crashed whilst attempting to land on the ship's deck, killing 1 person, injuring 7 more and leaving one missing (later confirmed dead).[144] Melville arrived on task the morning of 15 December 2006, equipped with a Towed Pinger Locating Drone supplied from the United States Navy set about locating the downed Black Hawk. Melville detected the locator beacon during its first pass over the crash site and pinpointed its exact location in subsequent passes. The helicopter was sitting in around 2900 metres of water.[145]

The coup took place on 5 December, but was bloodless and almost completely without violence. The evacuation of Australians was deemed unnecessary, and vessels of the task force began arriving back in Australia on 17 December, with Kanimbla docking in Townsville, and both Newcastle and Success returning to Sydney.[146] Melville returned to Australia in late December. The RAN decided to attempt to recover the downed Black Hawk and identified the United States Navy Supervisor of Salvage (SUPSALV) as the preferred organisation.[145] MV Seahorse Standard recovered the remains of Trooper Joshua Porter on 5 March and the Blackhawk helicopter on 9 March, with the assistance of specialist equipment provided by the SUPSALV team. The soldier's body was repatriated on 13 March, escorted by members of the SASR. Seahorse Standard arrived in Australia with the aircraft wreckage at the end of March. The wreckage would become evidence in the Board of Inquiry into the crash.[147]

Prior to 1989, the battle honour system of the Royal Australian Navy (RAN) was linked to that of the Royal Navy. The British Ministry of Defence and the Admiralty were responsible for approving and assigning battle honours, although from 1947, this was done on advice from the RAN Badges, Names and Honours Committee.[148] The only uniquely Australian battle honour during this time was "Vietnam 1965–72" (and smaller date units thereof) for deployments to the Vietnam War.[149] Ships of the RAN inherited honours from British ships of the same name, in addition to Australian predecessors.[148]

In 1989, the RAN Chief of Naval Staff, Admiral Michael Hudson approved a decision to have Australian warships only carry battle honours earned by previous Australian vessels.[148] The creation and awarding of battle honours came completely under RAN control.[148]

A complete overhaul of the RAN battle honours system was unveiled on 1 March 2010, to celebrate the navy's 109th anniversary of creation.[150] New honours were created for operations during the 1990s and 2000s—the last approved honour prior to this was "Kuwait 1991", for Gulf War service—and the service history of previous vessels was updated to include 'due recognition' of previous actions.[150]

From 1911 to 1941 women were forbidden from serving in the RAN; the demands World War II placed on personnel and resources led to a change of policy.[151] On 21 April 1941, the Australian Naval Board sent a letter authorising the entry of women into the RAN to the Commodore-in-Charge, Sydney. The letter led to the formation of the Women's Royal Australian Naval Service (WRANS) and the Royal Australian Naval Nursing Service (RANNS). The two separate women's services existed until 1984, when they were incorporated into the permanent force. Today,[when?] female members of the RAN have a wide variety of roles open to them; women serve on submarines, command ships and shore postings and are expected to play an increasingly important role in the future of the RAN.[152]

The Royal Australian Navy today[when?] is a medium-sized modern navy in world terms but is one of the strongest navies in the Asia Pacific Region. The combat fleet of the RAN is made up of three Hobart-class destroyer, eight Anzac-class frigate, twelve patrol boats of the Armidale class, and six Collins-class submarine. The RAN also comprises an amphibious and supply force to transport the Australian Army and to resupply the combat arm of the navy.[153] The RAN is divided into seven Force Element Groups (FEGs): Surface Combatants, Amphibious Warfare Forces and Afloat Support Force, Naval Aviation, Submarine Force, Mine Warfare and Clearance Diving, Patrol Boat Force and the Hydrographic Force. The FEG's were formed to manage the operations of the separate sections of the RAN in a more efficient way.[154]

The modern RAN began to form during the late 1970s when the Fraser Government announced the purchase of four Oliver Hazard Perry-class frigates, all to be built in America; in 1980 they announced an additional two vessels both to be built in Australia. The fifteen Australian-built vessels of the Fremantle class made up Australia's patrol boat from 1979 to 2007; they have now been replaced by the fourteen Armidale-class patrol boats.

The Collins class is the newest class of Australian submarines, built in Australia for the Royal Australian Navy. They were constructed by the Australian Submarine Corporation in Adelaide, South Australia, and replaced the six Oberon-class submarines in the Australian fleet. The first vessel, HMAS Collins, was laid down in 1990 and commissioned in 1996, with all six vessels of the class in service and based at HMAS Stirling in Western Australia.

The Anzac class is the current main fleet unit of the Royal Australian Navy; the class has eight vessels. The lead vessel of the class, HMAS Anzac, was commissioned in 1996 and the final vessel, HMAS Perth, was commissioned on 26 August 2006. Along with the eight Australian vessels, two Anzacs were also constructed for the Royal New Zealand Navy. The Anzac class were jointly constructed in New Zealand and Australia with the final fitout in Williamstown, Victoria.

The amphibious and supply arm of the RAN as at January 2021 is made up of; HMAS Choules, a Bay-class LSD, HMAS Adelaide and HMAS Canberra, both of which are Canberra-class LHD's, twelve LHD Landing Craft (LLC) of the LCM-1E type, two Leeuwin-class survey vessels, four Paluma-class motor launches, the fleet oiler HMAS Sirius, and the Dual Stores Replenishment Vessel HMAS Success. The RAN also has six Huon-class minehunters.

The Royal Australian Navy maintains several bases around Australia. Under the RAN's Two-Ocean Policy, HMAS Stirling (Fleet Base West) and HMAS Kuttabul (Fleet Base East) are the primary bases for all major fleet unit of the RAN. The majority of the patrol boat and amphibious forces are located at HMAS Cairns and HMAS Coonawarra, while all Fleet Air Arm squadrons are based at HMAS Albatross.The history of the metric system began during the Age of Enlightenment with measures of length and weight derived from nature, along with their decimal multiples and fractions. The system became the standard of France and Europe within half a century.  Other measures with unity ratios[Note 1] were added, and the system went on to be adopted across the world.

The first practical realisation of the metric system came in 1799, during the French Revolution, after the existing system of measures had become impractical for trade, and was replaced by a decimal system based on the kilogram and the metre.  The basic units were taken from the natural world. The unit of length, the metre, was based on the dimensions of the Earth, and the unit of mass, the kilogram, was based on the mass of a volume of water of one litre (a cubic decimetre). Reference copies for both units were manufactured in platinum and remained the standards of measure for the next 90 years. After a period of reversion to the mesures usuelles due to unpopularity of the metric system, the metrication of France and much of Europe was complete by the 1850s.

In the middle of the 19th century, James Clerk Maxwell conceived a coherent system where a small number of units of measure were defined as base units, and all other units of measure, called derived units, were defined in terms of the base units. Maxwell proposed three base units for length, mass and time. Advances in electromagnetism in the 19th century necessitated additional units to be defined, and multiple incompatible systems of such units came into use; none could be reconciled with the existing dimensional system. The impasse was resolved by Giovanni Giorgi, who in 1901 proved that a coherent system that incorporated electromagnetic units required a fourth base unit, of electromagnetism.

The seminal 1875 Treaty of the Metre resulted in the fashioning and distribution of metre and kilogram artefacts, the standards of the future coherent system that became the SI, and the creation of an international body Conférence générale des poids et mesures or CGPM to oversee systems of weights and measures based on them.

In 1960, the CGPM launched the International System of Units (in French the Système international d'unités or SI) with six "base units": the metre, kilogram, second, ampere, degree Kelvin (subsequently renamed the "kelvin") and candela, plus 16 more units derived from the base units. A seventh base unit, the mole, and six other derived units were added later in the 20th century. During this period, the metre was redefined in terms of the speed of light, and the second was redefined based on the microwave frequency of a caesium atomic clock.

Due to the instability of the international prototype of the kilogram, a series of initiatives were undertaken, starting in the late 20th century, to redefine the ampere, kilogram, mole and kelvin in terms of invariant constants of physics, ultimately resulting in the 2019 revision of the SI, which finally eliminated the need for any physical reference artefacts—notably, this enabled the retirement of the standard kilogram.

A fleeting hint of an ancient decimal or metric system may be found in the Mohenjo-Daro ruler, which uses a base length of 1.32 inches (33.5 mm) and is very precisely divided with decimal markings. Bricks from that period are consistent with this unit, but this usage appears not to have survived, as later systems in India are non-metric, employing divisions into eighths, twelfths, and sixteenths.

Foundational aspects of mathematics, together with an increased understanding of the natural world during the Enlightenment, set the stage for the emergence in the late 18th century of a system of measurement with rationally related units and rules for combining them.

In the early ninth century, when much of what later became Holy Roman Empire was part of France, units of measure had been standardised by the Emperor Charlemagne. He had introduced standard units of measure for length and for mass throughout his empire. As the empire disintegrated into separate nations, including France, these standards diverged. In England, Magna Carta (1215) had stipulated that "There shall be standard measures of wine, ale, and corn (the London quarter), throughout the kingdom. There shall also be a standard width of dyed cloth, russet, and haberject, namely two ells within the selvedges. Weights are to be standardised similarly."[1]

During the early medieval era, Roman numerals were used in Europe to represent numbers,[2] but the Arabs represented numbers using the Hindu numeral system, a positional notation that used ten symbols. In about 1202, Fibonacci published his book Liber Abaci (Book of Calculation) which introduced the concept of positional notation into Europe. These symbols evolved into the numerals "0", "1", "2", etc.[3][4] At that time, there was dispute regarding the difference between rational numbers and irrational numbers and there was no consistency in the way in which decimal fractions were represented.

Simon Stevin is credited with introducing the decimal system into general use in Europe.[5] In 1586, he published a small pamphlet called De Thiende ("the tenth") which historians credit as being the basis of modern notation for decimal fractions.[6] Stevin felt that this innovation was so significant that he declared the universal introduction of decimal coinage, measures, and weights to be merely a question of time.[5][7]: 70 [8]: 91 

Since the time of Charlemagne, the standard of length had been a measure of the body, that from fingertip to fingertip of the outstretched arms of a large man,[Note 2] from a family of body measures called fathoms, originally used among other things, to measure the depth of water. An artifact to represent the standard was cast in the most durable substance available in the Middle Ages, an iron bar [citation needed]. The problems of a non-reproducible artefact became apparent over the ages: it rusted, was stolen, beaten into a mortised wall until it bent, and was, at times, lost. When a new royal standard had to be cast, it was a different standard than the old one, so replicas of old ones and new ones came into existence and use. The artefact existed through the 18th century, and was called a teise or later, a toise (from Latin tense: outstretched (arms)). This would lead to a search in the 18th century for a reproducible standard based on some invariant measure of the natural world.

In 1656, Dutch scientist Christiaan Huygens invented the pendulum clock, with its pendulum marking the seconds. This gave rise to proposals to use its length as a standard unit. But it became apparent that the pendulum lengths of calibrated clocks in different locations varied (due to local variations in the acceleration due to gravity), and this was not a good solution. A more uniform standard was needed.


In 1670, Gabriel Mouton, a French abbot and astronomer, published the book Observationes diametrorum solis et lunae apparentium ("Observations of the apparent diameters of the Sun and Moon") in which he proposed a decimal system of measurement of length for use by scientists in international communication, to be based on the dimensions of the Earth. The milliare would be defined as a minute of arc along a meridian (such as the Paris meridian) and would be divided into 10 centuria, the centuria into 10 decuria and so on, successive units being the virga, virgula, decima, centesima, and the millesima. Mouton used Riccioli's estimate that one degree of arc was 321,185 Bolognese feet. Mouton's experiments showed that a pendulum of length one virgula would beat 3959.2 times[Note 3] in half an hour.[9][Note 4] Mouton believed that, with this information, scientists in a foreign country would be able to construct a copy of the virgula for their own use.[10] Mouton's ideas attracted interest at the time; Picard in his work Mesure de la Terre (1671) and Huygens in his work Horologium Oscillatorium sive de motu pendulorum ("Of oscillating clocks, or concerning the motion of pendulums", 1673) both proposing that a standard unit of length be tied to the beat frequency of a pendulum.[11][10]

Since at least the Middle Ages, the Earth had been perceived as eternal, unchanging, and of symmetrical shape (close to a sphere), so it was natural that some fractional measure of its surface should be proposed as a standard of length. But first, scientific information about the shape and size of the Earth had to be obtained. One degree of arc would be 60 minutes of arc, on the equator; one milliare would be one minute of arc, or 1 nautical mile, so 60 nautical miles would be one degree of arc on Earth's surface, taken as a sphere.[12] Thus Earth's circumference in nautical miles would be 21 600 (viz., 60 minutes of arc × 360 degrees in four 90-degree quadrants; a quadrant being the length of the quarter-circle from the North Pole to the equator).

In 1669, Jean Picard, a French astronomer, was the first person to measure the Earth accurately. In a survey spanning one degree of latitude, he erred by only 0.44% (Picard's arc measurement).

In Philosophiæ Naturalis Principia Mathematica (1686), Isaac Newton gave a theoretical explanation for the "bulging equator",[Note 5] which also explained the differences found in the lengths of the "second pendulums",[13] theories that were confirmed by the French Geodesic Mission to Peru undertaken by the French Academy of Sciences in 1735.[14][a]

By the mid-18th century, it had become apparent that it was necessary to standardise of weights and measures between nations who traded and exchanged scientific ideas with each other. Spain, for example, had aligned her units of measure with the royal units of France[17] and Peter the Great aligned the Russian units of measure with those of England.[18] In 1783, the British inventor James Watt, who was having difficulties in communicating with German scientists, called for the creation of a global decimal measurement system, proposing a system which used the density of water to link length and mass,[16] and, in 1788, the French chemist Antoine Lavoisier commissioned a set of nine brass cylinders (a [French] pound and decimal subdivisions thereof) for his experimental work.[7]: 71 

In 1790, a proposal floated by the French to Britain and the United States, to establish a uniform measure of length, a metre based on the period of a pendulum with a beat of one second, was defeated in the British Parliament and United States Congress. The underlying issue was failure to agree on the latitude for the definition, since gravitational acceleration, and, therefore, the length of the pendulum, varies (inter alia) with latitude: each party wanted a definition according to a major latitude passing through their own country. The direct consequences of the failure were the French unilateral development and deployment of the metric system and its spread by trade to the continent; the British adoption of the Imperial System of Measures throughout the realm in 1824; and the United States' retention of the British common system of measures in place at the time of the independence of the colonies. This was the position that continued for nearly the next 200 years.[Note 6]

It has been estimated that, on the eve of the Revolution in 1789, the eight hundred or so units of measure in use in France had up to a quarter of a million different definitions because the quantity associated with each unit could differ from town to town, and even from trade to trade.[8]: 2–3  Although certain standards, such as the pied du roi (the King's foot) had a degree of pre-eminence and were used by scientists, many traders chose to use their own measuring devices, giving scope for fraud and hindering commerce and industry.[19] These variations were promoted by local vested interests, but hindered trade and taxation.[20][21]

In 1790, a panel of five leading French scientists was appointed by the Académie des sciences to investigate weights and measures. They were Jean-Charles de Borda, Joseph-Louis Lagrange, Pierre-Simon Laplace, Gaspard Monge, and Nicolas de Condorcet.[8]: 2–3 [22]: 46  
Over the following year, the panel, after studying various alternatives, made a series of recommendations regarding a new system of weights and measures, including that it should have a decimal radix, that the unit of length should be based on a fractional arc of a quadrant of the Earth's meridian, and that the unit of weight should be that of a cube of water whose dimension was a decimal fraction of the unit of length.[23][24][7]: 50–51 [25][26] The proposals were accepted by the French Assembly on 30 March 1791.[27]

Following acceptance, the Académie des sciences was instructed to implement the proposals. The Académie broke the tasks into five operations, allocating each part to a separate working group:[7]: 82 

The panel decided that the new measure of length should be equal to one ten-millionth of the distance from the North Pole to the Equator (Earth quadrant), measured along the Paris meridian.[20]

Using Jean Picard's survey of 1670 and Jacques Cassini's survey of 1718,[a] a provisional value of 443.44 lignes was assigned to the metre which, in turn, defined the other units of measure.[8]: 106 

While Méchain and Delambre were completing their survey, the commission had ordered a series of platinum bars to be made based on the provisional metre. When the final result was known, the bar whose length was closest to the meridional definition of the metre would be selected.

After 1792, the name of the original defined unit of mass, "gramme", which was too small to serve as a practical realisation for many purposes, was adopted, the new prefix "kilo" was added to it to form the name "kilogramme". Consequently, the kilogram is the only SI base unit that has an SI prefix as part of its unit name.
A provisional kilogram standard was made and work was commissioned to determine the precise mass of a cubic decimetre (later to be defined as equal to one litre) of water.
The regulation of trade and commerce required a "practical realisation": a single-piece, metallic reference standard that was one thousand times more massive that would be known as the grave.[Note 8] This mass unit defined by Lavoisier and René Just Haüy had been in use since 1793.[28] This new, practical realisation would ultimately become the base unit of mass. On 7 April 1795, the gramme, upon which the kilogram is based, was decreed to be equal to "the absolute weight of a volume of pure water equal to a cube of one hundredth of a metre, and at the temperature of the melting ice".[26] Although the definition of the kilogramme specified water at 0 °C—a highly stable temperature point—it was replaced with the temperature at which water reaches maximum density. This temperature, about 4 °C, was not accurately known, but one of the advantages of the new definition was that the precise Celsius value of the temperature was not actually important.[29][Note 9] The final conclusion was that one cubic decimetre of water at its maximum density was equal to 99.92072% of the mass of the provisional kilogram.[32]

On 7 April 1795, the metric system was formally defined in French law.[Note 10] It defined six new decimal units:[26]

Decimal multiples of these units were defined by Greek prefixes: "myria-" (10,000), "kilo-" (1000), "hecto-" (100), and  "deka-" (10) and submultiples were defined by the Latin prefixes "deci-" (0.1), "centi-" (0.01), and "milli-" (0.001).[33]

For purposes of commerce, units and prefixed-units of weight (mass) and capacity (volume) were prependable by the binary multipliers "double-" (2) and "demi-" (1⁄2), as in double-litre, demi-litre; or double-hectogramme, demi-hectogramme, etc.[Note 11]

The 1795 draft definitions enabled provisional copies of the kilograms and metres to be constructed.[34][35]

The task of surveying the meridian arc, which was estimated to take two years, fell to Pierre Méchain and Jean-Baptiste Delambre. The task eventually took more than six years (1792–1798) with delays caused not only by unforeseen technical difficulties but also by the convulsed period of the aftermath of the Revolution.[8] Apart from the obvious nationalistic considerations, the Paris meridian was also a sound choice for practical scientific reasons: a portion of the quadrant from Dunkirk to Barcelona (about 1000 km, or one-tenth of the total) could be surveyed with start- and end-points at sea level, and that portion was roughly in the middle of the quadrant, where the effects of the Earth's oblateness were expected to be the largest.[20]

The project was split into two parts—the northern section of 742.7 km from the Belfry, Dunkirk to Rodez Cathedral which was surveyed by Delambre and the southern section of 333.0 km from Rodez to the Montjuïc Fortress, Barcelona which was surveyed by Méchain.[8]: 227–230 [Note 12]

Delambre used a baseline of about 10 km in length along a straight road, located close to Melun. In an operation taking six weeks, the baseline was accurately measured using four platinum rods, each of length two toises (about 3.9 m).[8]: 227–230  Thereafter he used, where possible, the triangulation points used by Cassini in his 1744 survey of France. Méchain's baseline, of a similar length, and also on a straight section of road was in the Perpignan area.[8]: 240–241  Although Méchain's sector was half the length of Delambre, it included the Pyrenees and hitherto unsurveyed parts of Spain. After the two surveyors met, each computed the other's baseline in order to cross-check their results and they then recomputed the metre as 443.296 lignes,[20][Note 13] notably shorter than the 1795 provisional value of 443.44 lignes. On 15 November 1798, Delambre and Méchain returned to Paris with their data, having completed the survey. The final value of the mètre was defined in 1799 as the computed value from the survey.

In June 1799, platinum prototypes were fabricated according to the measured quantities, the mètre des archives defined to be a length of 443.296 lignes, and the kilogramme des archives defined to be a weight of 18827.15 grains of the livre poids de marc,[36] and entered into the French National Archives. In December of that year, the metric system based on them became by law the sole system of weights and measures in France from 1801 until 1812.

Despite the law, the populace continued to use the old measures. In 1812, Napoleon revoked the law and issued one called the mesures usuelles, restoring the names and quantities of the customary measures but redefined as round multiples of the metric units, so it was a kind of hybrid system. In 1837, after the collapse of the Napoleonic Empire, the new Assembly reimposed the metric system defined by the laws of 1795 and 1799, to take effect in 1840. The metrication of France took until about 1858 to be completed. Some of the old unit names, especially the livre, originally a unit of mass derived from the Roman libra (as was the English pound), but now meaning 500 grams, are still in use today.

At the start of the nineteenth century, the French Academy of Sciences' artefacts for length and mass were the only nascent units of the metric system that were defined in terms of formal standards.  Other units based on them, except the litre, proved to be short-lived. Pendulum clocks that could keep time in seconds had been in use for about 150 years, but their geometries were local to both latitude and altitude, so there was no standard of timekeeping. Nor had a unit of time been recognised as an essential base unit for the derivation of things like force and acceleration. Some quantities of electricity, like charge and potential, had been identified, but names and interrelationships of units were not yet established.[Note 15] Both Fahrenheit (ca. 1724) and Celsius (ca. 1742) scales of temperature existed, and varied instruments for measuring units or degrees of them. The base/derived unit model had not yet been elaborated, nor was it known how many physical quantities might be interrelated.

A model of interrelated units was first proposed in 1861 by the British Association for the Advancement of Science (BAAS) based on what came to be called the "mechanical" units (length, mass, and time). Over the following decades, this foundation enabled mechanical, electrical, and thermal[when?] units to be correlated.

In 1832, German mathematician Carl-Friedrich Gauss made the first absolute measurements of the Earth's magnetic field using a decimal system based on the use of the millimetre, milligram, and second as the base unit of time.[37]: 109  Gauss's second was based on astronomical observations of the rotation of the Earth, and was the sexagesimal second of the ancients: a partitioning of the solar day into two cycles of 12 periods, and each period divided into 60 intervals, and each interval so divided again, so that a second was 1/86,400th of the day.[Note 16] This effectively established a time dimension as a necessary constituent of any useful system of measures, and the astronomical second as the base unit.

In a paper published in 1843, James Prescott Joule first demonstrated a means of measuring the energy transferred between different systems when work is done thereby relating Nicolas Clément's calorie, defined in 1824 as "the amount of heat required to raise the temperature of 1 kg of water from 0 to 1 °C at 1 atmosphere of pressure" to mechanical work.[38][39] Energy became the unifying concept of nineteenth century science,[40] initially by bringing thermodynamics and mechanics together and later adding electrical technology.

In 1861, a committee of the British Association for the Advancement of Science (BAAS) including William Thomson (later Lord Kelvin), James Clerk Maxwell, and James Prescott Joule among its members was tasked with investigating the "Standards of Electrical Resistance".[clarification needed] In their first report (1862),[41] they laid the ground rules for their work—the metric system was to be used, measures of electrical energy must have the same units as measures of mechanical energy, and two sets of electromagnetic units would have to be derived—an electromagnetic system and an electrostatic system. In the second report (1863),[42] they introduced the concept of a coherent system of units whereby units of length, mass, and time were identified as "fundamental units" (now known as base units). All other units of measure could be derived (hence derived units) from these base units. The metre, gram, and second were chosen as base units.[43][44]

In 1861, before[clarification needed][at?] a meeting of the BAAS, Charles Bright and Latimer Clark proposed the names of ohm, volt, and farad in honour of Georg Ohm, Alessandro Volta, and Michael Faraday respectively for the practical units based on the CGS absolute system. This was supported by Thomson (Lord Kelvin).[45] The concept of naming units of measure after noteworthy scientists was subsequently used for other units.

In 1873, another committee of the BAAS (which also included Maxwell and Thomson) tasked with "the Selection and Nomenclature of Dynamical and Electrical Units" recommended using the cgs system of units. The committee also recommended the names of "dyne" and "erg" for the cgs units of force and energy.[46][44][47] The cgs system became the basis for scientific work for the next seventy years.

The reports recognised two centimetre–gram–second based systems for electrical units: the Electromagnetic (or absolute) system of units (EMU) and the Electrostatic system of units (ESU).

In the 1820s, Georg Ohm formulated Ohm's Law, which can be extended to relate power to current, electric potential (voltage), and resistance.[49][50] During the following decades, the realisation of a coherent system of units that incorporated the measurement of electromagnetic phenomena and Ohm's law was beset with problems—several different systems of units were devised.

In the three CGS systems, the constants 




k

e




{\displaystyle k_{\text{e}}}

 and 




k

m




{\displaystyle k_{\text{m}}}

 and consequently 




ϵ

0




{\displaystyle \epsilon _{0}}

 and 




μ

0




{\displaystyle \mu _{0}}

 were dimensionless, and thus did not require any units to define them.

The electrical units of measure did not easily fit into the coherent system of mechanical units defined by the BAAS. Using dimensional analysis, the dimensions of voltage 






M




1
2






L




1
2






T



−
1




{\displaystyle {\mathsf {M}}^{\frac {1}{2}}{\mathsf {L}}^{\frac {1}{2}}{\mathsf {T}}^{-1}}

 in the ESU system were identical to the dimensions of current in the EMU system, while resistance had dimensions of velocity in the EMU system, but the inverse of velocity in the ESU system.[44]

The Electromagnetic system of units (EMU) was developed from André-Marie Ampère's discovery in the 1820s of a relationship between currents in two conductors and the force between them now known as Ampere's law:

In 1833, Gauss pointed out the possibility of equating this force with its mechanical equivalent. This proposal received further support from Wilhelm Weber in 1851.[51] In this system, current is defined by setting the magnetic force constant 




k


m





{\displaystyle k_{\mathrm {m} }}

 to unity and electric potential is defined in such a way as to ensure the unit of power calculated by the relation 



P
=
V
I


{\displaystyle P=VI}

 is an erg/second. The electromagnetic units of measure were known as the abampere, abvolt, and so on.[52] These units were later scaled for use in the International System.[53]

The Electrostatic system of units (ESU) was based on Coulomb's quantification in 1783 of the force acting between two charged bodies. This relationship, now known as Coulomb's law, can be written

In this system, the unit for charge is defined by setting the Coulomb force constant (




k

e




{\displaystyle k_{\text{e}}}

) to unity and the unit for electric potential was defined to ensure the unit of energy calculated by the relation 



E
=
Q
V


{\displaystyle E=QV}

 is one erg. The electrostatic units of measure were the statampere, statvolt, and so on.[54]

The Gaussian system of units was based on Heinrich Hertz's realisation,[citation needed] while verifying Maxwell's equations in 1888, that the electromagnetic and electrostatic units were related by:

Using this relationship, he proposed merging the EMU and the ESU systems into one system using the EMU units for magnetic quantities (subsequently named the gauss and maxwell) and ESU units elsewhere. He named this combined set of units "Gaussian units". This set of units has been recognised as being particularly useful in theoretical physics.[37]: 128 

The CGS units of measure used in scientific work were not practical for engineering, leading to the development of a more applicable system of electric units especially for telegraphy. The unit of length was 107 m (the hebdometre, nominally the Earth quadrant), the unit of mass was an unnamed unit equal to 10−11 g and the unit of time was the second. The units of mass and length were scaled incongruously to yield more consistent and usable electric units in terms of mechanical measures. Informally called the "practical" system, it was properly termed the quadrant–eleventhgram–second (QES) system of units according to convention.

The definitions of electrical units incorporated the magnetic constant like the EMU system, and the names of the units were carried over from that system, but scaled according to the defined mechanical units.[57] The system was formalised as the International system late in the 19th century and its units later designated the "international ampere", "international volt", etc.[58]: 155–156 

The factor 



4
π


{\displaystyle 4\pi }

 that occurs in Maxwell's equations in the gaussian system (and the other CGS systems) comes from the 



4
π


{\displaystyle 4\pi }

 steradians surrounding a point, such as a point electric charge. This factor could be eliminated from contexts that do not involve spherical coordinates by incorporating the factor into the definitions of the quantities involved. The system was proposed by Oliver Heaviside in 1883 and is also known as the "rationalised Gaussian system of units". The SI later adopted rationalised units based on Heaviside's rationalisation scheme.

Maxwell and Boltzmann had produced theories describing the interrelationship of temperature, pressure, and volume of a gas on a microscopic scale but otherwise, in 1900, there was no understanding of the microscopic nature of temperature.[59][60]

By the end of the nineteenth century, the fundamental macroscopic laws of thermodynamics had been formulated and, although techniques existed to measure temperature using empirical techniques, the scientific understanding[clarification needed] of the nature of temperature was minimal.

With increasing international adoption of the metre, the shortcomings of the mètre des Archives as a standard became ever more apparent. Countries which adopted the metre as a legal measure purchased standard metre bars that were intended to be equal in length to the mètre des Archives, but there was no systematic way of ensuring that the countries were actually working to the same standard. The meridional definition, which had been intended to ensure international reproducibility, quickly proved so impractical that it was all but abandoned in favour of the artefact standards, but the mètre des Archives (and most of its copies) were "end standards": such standards (bars which are exactly one metre in length) are prone to wear with use, and different standard bars could be expected to wear at different rates.[61]

In 1867, it was proposed that a new international standard metre be created, and the length was taken to be that of the mètre des Archives "in the state in which it shall be found".[62][63] The International Conference on Geodesy in 1867 called for the creation of a new international prototype of the metre[62][63][Note 19] and of a system by which national standards could be compared with it. The international prototype would also be a "line standard", that is the metre was defined as the distance between two lines marked on the bar, so avoiding the wear problems of end standards. The French government gave practical support to the creation of an International Metre Commission, which met in Paris in 1870 and again in 1872 with the participation of about thirty countries.[62]

On 20 May 1875, an international treaty known as the Convention du Mètre (Metre Convention) was signed by 17 states.[21][64] This treaty established the following organisations to conduct international activities relating to a uniform system for measurements:

The international prototype of the metre and international prototype of the kilogram were both made from a 90% platinum, 10% iridium alloy which is exceptionally hard and which has good electrical and thermal conductivity properties. The prototype had a special X-shaped (Tresca) cross section to minimise the effects of torsional strain during length comparisons[21] and the prototype kilograms were cylindrical in shape. The London firm Johnson Matthey delivered 30 prototype metres and 40 prototype kilograms. At the first meeting of the CGPM in 1889, bar No. 6 and cylinder No. X were accepted as the international prototypes. The remainder were either kept as BIPM working copies or distributed to member states as national prototypes.[65]

Following the Convention of the Metre, in 1889, the BIPM had custody of two artefacts—one to define length and the other to define mass. Other units of measure which did not rely on specific artefacts were controlled by other bodies.

Although the definition of the kilogram remained unchanged throughout the 20th century, the 3rd CGPM in 1901 clarified that the kilogram was a unit of mass, not of weight. The original batch of 40 prototypes (adopted in 1889) were supplemented from time to time with further prototypes for use by new signatories to the Metre Convention.[66]

In 1921, the Treaty of the Metre was extended to cover electrical units, with the CGPM merging its work with that of the IEC.

The 20th century history of measurement is marked by five periods: the 1901 definition of the coherent MKS system; the intervening 50 years of coexistence of the MKS, cgs and common systems of measures; the 1948 Practical system of units prototype of the SI; the introduction of the SI in 1960; and the evolution of the SI in the latter half century.

The need for an independent electromagnetic dimension to resolve the difficulties related to defining such units in terms of length, mass, and time was identified by Giorgi in 1901. This led to Giorgi presenting a paper in October 1901 to the congress of the Associazione Elettrotecnica Italiana (A.E.I.)[67] in which he showed that a coherent electro-mechanical system of units could be obtained by adding a fourth base unit of an electrical nature (e.g., ampere, volt, or ohm) to the three base units proposed in the 1861 BAAS report. This gave physical dimensions to the constants ke and km and hence also to the electro-mechanical quantities ε0 (permittivity of free space) and μ0 (permeability of free space).[68] His work also recognised the relevance of energy in the establishment of a coherent, rational system of units, with the joule as the unit of energy, and the electrical units in the International System of Units remaining unchanged.[58]: 156  However, it took more than thirty years before Giorgi's work was accepted in practice by the IEC.

As industry developed around the world, the cgs system of units as adopted by the British Association for the Advancement of Science in 1873 with its plethora of electrical units continued to be the dominant system of measurement, and remained so for at least the next 60 years. The advantages were several: it had a comprehensive set of derived units which, while not quite coherent, were at least homologous; the MKS system lacked a defined unit of electromagnetism at all; the MKS units were inconveniently large for the sciences; customary systems of measures held sway in the United States, Britain, and the British empire, and even to some extent in France, the birthplace of the metric system, which inhibited adoption of any competing system. Finally, war, nationalism, and other political forces inhibited development of the science favouring a coherent system of units.

At the 8th CGPM in 1933, the need to replace the "international" electrical units with "absolute" units was raised. The IEC proposal that Giorgi's 'system', denoted informally as MKSX, be adopted was accepted, but no decision was made as to which electrical unit should be the fourth base unit. In 1935, J. E. Sears[69][citation needed] proposed that this should be the ampere, but World War II prevented this being formalised until 1946. The first (and only) follow-up comparison of the national standards with the international prototype of the metre was carried out between 1921 and 1936,[21][63] and indicated that the definition of the metre was preserved to within 0.2 μm.[70] During this follow-up comparison, the way in which the prototype metre should be measured was more clearly defined—the 1889 definition had defined the metre as being the length of the prototype at the temperature of melting ice, but, in 1927, the 7th CGPM extended this definition to specify that the prototype metre shall be "supported on two cylinders of at least one centimetre diameter, symmetrically placed in the same horizontal plane at a distance of 571 mm from each other".[37]: 142–43, 148  The choice of 571 mm represents the Airy points of the prototype—the points at which the bending or droop of the bar is minimised.[71]

The 9th CGPM met in 1948, fifteen years after the 8th CGPM. In response to formal requests made by the International Union of Pure and Applied Physics and by the French government to establish a practical system of units of measure, the CGPM requested the CIPM to prepare recommendations for a single practical system of units of measurement, suitable for adoption by all countries adhering to the Metre Convention.[72] The CIPM's draft proposal was an extensive revision and simplification of the metric unit definitions, symbols, and terminology based on the MKS system of units.

Following astronomical observations, the second was set as a fraction of the year 1900. The electromagnetic base unit, as required by Giorgi, was accepted as the ampere. After negotiations with the CIS and IUPAP, two additional units—the degree kelvin and the candela—were also proposed as base units.[73] For the first time, the CGPM made recommendations concerning derived units. At the same time, the CGPM adopted conventions for the writing and printing of unit symbols and numbers and catalogued the symbols for the most important MKS and CGS units of measure.[74]

Until the advent of the atomic clock, the most reliable timekeeper available to humanity was the Earth's rotation. It was natural, therefore, that the astronomers under the auspices of the International Astronomical Union (IAU) took the lead in maintaining the standards relating to time. During the 20th century, it became apparent that the Earth's rotation was slowing down, resulting in days becoming 1.4 milliseconds longer each century[75]—this was verified by comparing the calculated timings of eclipses of the Sun with those observed in antiquity going back to Chinese records of 763 BC.[76] In 1956, the 10th CGPM instructed the CIPM to prepare a definition of the second; in 1958, the definition was published stating that the second (called an ephemeris second) would be calculated by extrapolation using Earth's rotational speed in 1900.[75]

Per Giorgi's proposals of 1901, the CIPM also recommended that the ampere be the base unit from which electromechanical units would be derived. The definitions for the ohm and volt that had previously been in use were discarded, and these units became derived units based on the ampere. In 1946, the CIPM formally adopted a definition of the ampere based on the original EMU definition and redefined the ohm in terms of other base units.[77] The definitions for the absolute electrical system,[clarification needed] based on the ampere, were formalised in 1948.[78] The draft proposed units with these names are very close, but not identical, to the international units.[79]

In the Celsius scale from the 18th century, temperature was expressed in degrees Celsius with the definition that ice melted at 0 °C and (at standard atmospheric pressure) water boiled at 100 °C. A series of lookup tables defined temperature in terms of interrelated empirical measurements made using various devices. In 1948, definitions relating to temperature had to be clarified. (The degree, as an angular measure, was adopted for general use in many countries, so, in 1948, the General Conference on Weights and Measures (CGPM) recommended that the degree Celsius, as used for the measurement of temperature, be renamed the degree Celsius.)[80]

At the 9th CGPM, the Celsius temperature scale was renamed the Celsius scale, and the scale itself was fixed by defining the triple point of water as 0.01 °C,[81] though the CGPM left the formal definition of absolute zero until the 10th CGPM when the name "Kelvin" was assigned to the absolute temperature scale, and the triple point of water was defined as being 273.16 °K.[82]

Before 1937, the International Commission on Illumination (CIE from its French title, the Commission Internationale de l'Eclairage), in conjunction with the CIPM, produced a standard for luminous intensity to replace the various national standards. This standard, the candela (cd), which was defined as "the brightness of the full radiator at the temperature of solidification of platinum is 60 new candles per square centimetre",[83] was ratified by the CGPM in 1948.

The newly accepted definition of the ampere allowed practical and useful coherent definitions of a set of electromagnetic derived units, including farad, henry, watt, tesla, weber, volt, ohm, and coulomb. Two derived units, lux and lumen, were based on the new candela, and one, degree Celsius, equivalent to the degree Kelvin. Five other miscellaneous derived units completed the draft proposal: radian, steradian, hertz, joule, and newton.

In 1952, the CIPM proposed the use of wavelength of a specific light source as the standard for defining length, and, in 1960, the CGPM accepted this proposal using radiation corresponding to a transition between specified energy levels of the krypton 86 atom as the new standard for the metre. The standard metre artefact was retired.

In 1960, Giorgi's proposals were adopted as the basis of the Système International d'Unités (International System of Units), the SI.[37]: 109  This initial definition of the SI included six base units, the metre, kilogram, second, ampere, degree Kelvin, and candela, and sixteen coherent derived units.[84]

The evolution of the SI after its publication in 1960 has seen the addition of a seventh base unit, the mole, and six more derived units, the pascal for pressure, the gray, sievert, and becquerel for radiation, the siemens for electrical conductance, and katal for catalytic (enzymatic) activity. Several units have also been redefined in terms of physical constants.

Over the ensuing years, the BIPM developed and maintained cross-correlations relating various measuring devices such as thermocouples, light spectra, and the like to the equivalent temperatures.[85]

The mole was originally known as a gram-atom or a gram-molecule—the amount of a substance measured in grams divided by its atomic weight. Originally chemists and physicists had differing views regarding the definition of the atomic weight—both assigned a value of 16 atomic mass units (amu) to oxygen, but physicists defined oxygen in terms of the 16O isotope whereas chemists assigned 16 amu to 16O, 17O and 18O isotopes mixed in the proportion that they occur in nature. Finally, an agreement between the International Union of Pure and Applied Physics[86] (IUPAP) and the International Union of Pure and Applied Chemistry (IUPAC) brought this duality to an end in 1959/60, both parties agreeing to define the atomic weight of 12C as being exactly 12 amu. This agreement was confirmed by ISO and in 1969 the CIPM recommended its inclusion in SI as a base unit. This was done in 1971 at the 14th CGPM.[37]: 114–115 

The second major trend in the post-modern SI was the migration of unit definitions in terms of physical constants of nature.

In 1967, at the 13th CGPM, the degree Kelvin (°K) was renamed the "kelvin" (K).[87]

Astronomers from the US Naval Observatory (USNO) and the National Physical Laboratory determined a relationship between the frequency of radiation corresponding to the transition between the two hyperfine levels of the ground state of the caesium 133 atom and the estimated rate of rotation of the earth in 1900. Their atomic definition of the second was adopted in 1968 by the 13th CGPM.

By 1975, when the second had been defined in terms of a physical phenomenon rather than the earth's rotation, the CGPM authorised the CIPM to investigate the use of the speed of light as the basis for the definition of the metre. This proposal was accepted in 1983.[88]

The candela definition proved difficult to implement so, in 1979, the definition was revised and the reference to the radiation source was replaced by defining the candela in terms of the power of a specified frequency of monochromatic yellowish-green visible light,[37]: 115  which is close to the frequency where the human eye, when adapted to bright conditions, has greatest sensitivity.

After the metre was redefined in 1960, the kilogram remained the only SI base defined by a physical artefact. During the years that followed, the definitions of the base units and particularly the mise en pratique[90] to realise these definitions have been refined.

The third periodic recalibration in 1988–1989 revealed that the average difference between the IPK and adjusted baseline for the national prototypes was 50 μg—in 1889, the baseline of the national prototypes had been adjusted so that the difference was zero. As the IPK is the definitive kilogram, there is no way of telling whether the IPK had been losing mass or the national prototypes had been gaining mass.[89]

During the course of the century, the various national prototypes of the kilogram were recalibrated against the international prototype of the kilogram (IPK) and, therefore, against each other. The initial 1889 starting-value offsets of the national prototypes relative to the IPK were nulled,[89] with any subsequent mass changes being relative to the IPK.

A number of replacements were proposed for the IPK.

From the early 1990s, the International Avogadro Project worked on creating a 1 kg, 94 mm, sphere made of a uniform silicon-28 crystal, with the intention of being able replace the IPK with a physical object which would be precisely reproducible from an exact specification. Due to its precise construction, the Avogadro Project's sphere is likely to be the most precisely spherical object ever created by humans.[91]

Other groups worked on concepts such as creating a reference mass via precise electrodeposition of gold or bismuth atoms, and defining the kilogram in terms of the ampere by relating it to forces generated by electromagnetic repulsion of electric currents.[92]

Eventually, the choices were narrowed down to the use of the Watt balance and the International Avogadro Project sphere.[92]

Ultimately, a decision was made not to create any physical replacement for the IPK, but instead to define all SI units in terms of assigning precise values to a number of physical constants which had previously been measured in terms of the earlier unit definitions.

At its 23rd meeting (2007), the CGPM mandated the CIPM to investigate the use of natural constants as the basis for all units of measure rather than the artefacts that were then in use.

The following year, this was endorsed by the International Union of Pure and Applied Physics (IUPAP).[93] At a meeting of the CCU held in Reading, United Kingdom, in September 2010, a resolution[94] and draft changes to the SI brochure that were to be presented to the next meeting of the CIPM in October 2010 were agreed in principle.[95] The CIPM meeting of October 2010 found that "the conditions set by the General Conference at its 23rd meeting have not yet been fully met.[Note 21] For this reason the CIPM does not propose a revision of the SI at the present time".[97] The CIPM, however, presented a resolution for consideration at the 24th CGPM (17–21 October 2011) to agree to the new definitions in principle, but not to implement them until the details had been finalised.[98]

In the revision, four of the seven SI base units—the kilogram, ampere, kelvin, and mole—were redefined by setting exact numerical values for the Planck constant (h), the elementary electric charge (e), the Boltzmann constant (kB), and the Avogadro constant (NA), respectively. The second, metre, and candela were already defined by physical constants and were subject to correction to their definitions. The new definitions aimed to improve the SI without changing the value of any units, ensuring continuity with existing measurements.[99][100]

This resolution was accepted by the conference,[101] and, in addition, the CGPM moved the date of the 25th meeting forward from 2015 to 2014.[102][103] At the 25th meeting on 18 to 20 November 2014, it was found that "despite [progress in the necessary requirements] the data do not yet appear to be sufficiently robust for the CGPM to adopt the revised SI at its 25th meeting",[104] thus postponing the revision to the next meeting in 2018.

Measurements accurate enough to meet the conditions were available in 2017 and the revision[105] was adopted at the 26th CGPM (13–16 November 2018), with the changes finally coming into force in 2019, creating a system of definitions which is intended to be stable for the long term.The Holden Commodore is a series of automobiles that were sold by now-defunct Australian manufacturer Holden from 1978 until 2020. They were manufactured from 1978 to 2017 in Australia and from 1979 to 1990 in New Zealand, with production of the locally manufactured versions in Australia ending on 20 October 2017.[2]

The first three generations of Holden produced Commodores (1978–2006) were based on the Opel designed V-body rear-wheel drive automotive platform, which was the basis of GM's largest European models, but were structurally strengthened, mechanically modified, and, in time, enlarged by Holden for Australian road conditions, production needs, and market demands. The styling of these cars was generally similar to that of the Opel Commodore C, and later, the Opel Omega A/B and their Vauxhall sister models the Vauxhall Carlton and Omega.

The fourth generation Holden Commodore models VE and VF, manufactured by Holden from 2006 until 2017, were entirely designed in-house and based on the Holden-developed, rear-wheel drive Zeta platform.

Between 2018 and 2020, a rebadged, front-wheel drive Opel Insignia, built by Opel in Germany, was sold in Australia as the Holden Commodore (ZB). All sales of the last Commodore ended at the end of 2020, coinciding with the complete discontinuation of Holden as a subsidiary company, marque, and nameplate.[3]

The Commodore replaced the long-serving Holden Kingswood and Holden Premier. Initially introduced as a single sedan body style, the range expanded in 1979 to include a station wagon. From 1984, Holden began branding the flagship model as Holden Calais, with the Commodore Berlina introduced in 1984 gaining independent Holden Berlina nomenclature in 1988. Long-wheelbase Statesman/Caprice derivatives and Commodore utility body variants followed in 1990. The third generation architecture spawned the most body styles, with a new Holden utility launched in 2000 (known officially as the Holden Ute), reborn Monaro coupé in 2001, four-door Holden Crewman utility and all-wheel drive (AWD) Holden Adventra crossover in 2003. Holden Special Vehicles (HSV) in 1987 began official modification of high performance variants of the Commodore and its derivatives, under its own nameplate.[citation needed]

Rivalry came predominantly from the Ford Falcon—also locally built. Prior to the second generation Commodore of 1988, the Holden was positioned a full class below the full-size Falcon. To varying degrees, competition also came from mid-size offerings from Toyota Australia as well as Chrysler Australia, which morphed into Mitsubishi Motors Australia. Moreover, between 1989 and 1997, Australian federal government policy saw the launch of the Toyota Lexcen, which was a rebadged version of the second generation Commodore. With the introduction of the third generation in 1997, Holden implemented its largest export programs involving Commodore and its derivatives. In the Middle East and South Africa the Commodore sold as a Chevrolet. High-performance export versions followed in North America, sold as Pontiac and later Chevrolet. HSV also exported to the United Kingdom as Vauxhall, in the Middle East as Chevrolet Special Vehicles (CSV) and in New Zealand and Singapore as HSV.

In December 2013, Holden announced that it would cease its local production by the end of October 2017 committing, however, to use the long-standing Commodore nameplate on its fifth-generation fully imported replacement, moving to a front-wheel drive (FWD)/all-wheel drive (AWD) platform.

On 10 December 2019, Holden announced that the Commodore nameplate would be discontinued in 2020, in what is, according to Holden's interim chairman and managing director Kristian Aquilina, "decisive action to ensure a sharp focus on the largest and most buoyant market segments", focusing on their SUV and Ute range, which had accounted for over 76% of their lineup during 2019. This marks the end of the Commodore nameplate's 41 years.[4] On 17 February 2020, General Motors announced that the Holden marque in its entirety would be retired from sale in Australia and New Zealand by 2021.[5]

Introduced in October 1978,[6] the VB Commodore development covered a period with the effects of the 1973 oil crisis still being felt.[7] Hence, when Holden decided to replace the successful full-size HZ Kingswood with a new model line, they wanted the new car to be smaller and more fuel efficient.[8] Originally, Holden looked at developing a new WA Kingswood, but that project was abandoned.[9] With no replacement in development, Holden looked to Opel to provide the design foundations of the VB, basing it on the four-cylinder Rekord E body shell, with the front grafted on from the Opel Senator A, both constructed using GM's V-body platform.[10] This change was necessitated to accommodate the larger Holden six- and eight-cylinder engines.[11] Holden also adopted the name "Commodore" from Opel, which had been using the name since 1967.[12] Opel went on to use Holden's Rekord-Senator hybrid as a foundation for its new generation Commodore C, slotting in between the two donor models.[13]

During the VB's development, Holden realised that when driven at speed over harsh Australian roads, the Opel Rekord would effectively break apart at the firewall.[14] This forced Holden to re-engineer the entire car for the often harsh Australian road conditions, resulting in only 35 percent commonality with the Rekord. Among other changes, the Rekord's MacPherson strut front suspension was modified,[15] and the recirculating ball steering was replaced with a rack and pinion type.[16] These and other mechanical and structural  modifications massively blew out development costs to a reported A$110 million[17]—a figure then close to the cost of developing an all-new model independently.[18] With such a large sum consumed by the VB development programme, Holden was left with insufficient finances for the development of a station wagon variant.[19] Added that the Commodore architecture was considered an unsuitable base for utility and long-wheelbase models,[20] Holden was left with only a sedan, albeit one in three levels of luxury: a base, SL, and SL/E.[21] Desperate measures forced Holden to shape the Commodore front-end to the rear of the Rekord wagon. As the wagon-specific sheet metal had to be imported from Germany, the wagon, introduced in July 1979, suffered from inevitable component differences from the sedan.[19][22] Although infrequently criticised in the early years, quality problems were evident, with poor trim and panel fit problematic for all first generation Commodores. This coupled with mechanical dilemmas such as water pump failure and steering rack rattle ensured warranty claims were high in the first year.[23] Despite these issues, the VB Commodore was widely praised for its value for money sophistication, especially in regards to its steering, handling, braking, and ride quality.[24] thus securing the Wheels Car of the Year award for 1978.[25]

The VB series retained 96 percent of the preceding HZ Kingswood's interior space but was only 86 percent the HZ's external size, although five percent larger than the Torana.[26] With the Commodore dropping a full class below the Kingswood and its Ford Falcon competitor,[27] the smaller Commodore was predictably more fuel-efficient.[28] This downsizing was first seen as a major disadvantage for Holden, as they had effectively relinquished the potential of selling Commodores to the fleet and taxi industries.[29] These sales losses were thought to be unrecoverable; however, the 1979 energy crisis saw Australian oil prices rise by 140 percent, putting substantial strain on the automotive industry to collectively downsize, a change that Holden had already done.[7]

The most significant change to the VC Commodore of March 1980 was the engine upgrading to "XT5" specification. Now painted blue and thus known as the Blue straight-sixes and Holden V8s, these replaced the Red units fitted to the VB and earlier cars.[30] Changes included a new twelve-port cylinder head, redesigned combustion chambers, inlet and exhaust manifolds, a new two-barrel carburettor .[31] Tweaks and changes to the V8s surrounded the implementation of electronic ignition, revised cylinder head and inlet manifold design and the fitment of a four-barrel carburettor on the 4.2-litre variant. These changes brought improved efficiency, increased outputs and aided driveability.[32] In response to increasing oil prices, a four-cylinder variant was spawned in June 1980.[33] Displacing 1.9-litres, this powerplant known as Starfire was effectively Holden's existing straight-six with two cylinders removed. The four's peak power output of 58 kW (78 hp) and torque rated at 140 N⋅m (103 ft⋅lbf) meant its performance was compromised.[34][35] Reports indicate that the need to push the engine hard to extract performance led to real-world fuel consumption similar to the straight-sixes.[36]

Holden's emphasis on fuel economy extended beyond powertrains, with a fuel consumption vacuum gauge replacing the tachometer throughout the range, although this could be optioned back with the sports instrumentation package.[37] Visual changes were limited: the relocation of the corporate crest to the centre of the redesigned grille, black-coloured trim applied to the tail lamp surrounds on sedans, and the embossment of model badging into the side rubbing strips. The previously undesignated base car, was now the Commodore L, opening up the range for a new unbadged sub-level car.[38] This delete option model, was de-specified and available only to fleet customers.[39] On the premium Commodore SL/E, a resurrected "Shadowtone" exterior paint option became available in a limited range of dark-over-light colour combinations.[40] According to contemporary reviews, changes made to the VC's steering produced a heavier feel and inclined understeer, while the revised suspension gave a softer ride and addressed concerns raised while riding fully laden.[41]

The VH series Commodore introduced in September 1981 brought moderately updated frontal bodywork, with a new bonnet and front guards to facilitate the reshaped headlamps and a horizontally slatted grille.[42][43] These front-end design changes worked to produce a longer, yet wider look. At the rear, sedans featured redesigned tail light clusters, the design of which borrowed from Mercedes-Benz models of the day, using a louvered design.[44] At the same time, the nomenclature of the range was rationalised. The SL superseded the L as the base model, with the old SL level becoming the mid-range SL/X, and the SL/E remaining as the top-of-the-line variant.[45] Wagons were restricted to the SL and SL/X trims.[46] Redesigned pentagonal alloy wheels[47]—replacing the original SL/E type used since 1978[48]—along with a black painted B-pillar, wrap-around chrome rear bumper extensions to the wheel arches,[49] and extended tail lamps that converged with the license plate alcove—distinguished the range-topping SL/E from other variants.[44] The new pentagonal wheels were initially in short supply, such that only Shadowtone option SL/E sedans received them during 1981 production.

Mechanical specifications carried over, except for a new five-speed manual transmission, optional on the 1.9-litre four-cylinder and 2.85-litre six-cylinder versions.[50] In an attempt to improve sales figures of the straight-four engine, Holden spent considerable time improving its performance and efficiency. Modifications were also made to the 2.85-litre six to lift economy, and the powerplants managed to reduce fuel consumption by as much as 12.5 and 14 percent, correspondingly.[44][51] Holden released the sports-oriented Commodore SS sedan in September 1982[52]—reintroducing a nameplate used briefly ten years prior with the HQ series.[53] Provisioned with a choice of 4.2- or optional 5.0-litre V8 engines, both versions of the VH SS were teamed with a four-speed manual transmission.[52] Racing driver Peter Brock's Holden Dealer Team (HDT) high performance outfit produced three upgraded versions, known as Group One, Group Two and Group Three, the latter version available in either 4.2-litre or more commonly 5.0-litre V8 configuration.[54]

By the time of the VH series, Commodore sales were beginning to decline. Holden's six-cylinder engine, which was carried over from the Kingswood, could trace its roots back to 1963 and was no longer competitive.[19] Continual improvements made to Commodore's Ford Falcon rival meant the VH was not significantly more fuel-efficient or better performing despite the smaller size.[19][55] This was curtailed by the absence of any major powertrain revisions by the time of the VH and the lack of visual departure from the original VB.[56] Holden also had to deal with the influx of their own mid-size Camira from 1982, which presented comparable interior volume with lower fuel consumption, and for less than the Commodore pricing point. Camira sales were strong initially, but as fuel prices had stabilised, buyers gravitated away from Camira and Commodore towards the larger Falcon, which overtook the Commodore as Australia's bestselling car for the first time in 1982.[19][57][58]

Representing the first major change since the VB original, the VK model of 1984 introduced a six-window glasshouse, as opposed to the previous four-window design, to make the Commodore appear larger.[59] The revised design helped stimulate sales, which totalled 135,000 in two years. This did not put an end to Holden's monetary woes. Sales of the initially popular Camira slumped due to unforeseen quality issues,[60] while the Holden WB series commercial vehicle range and the Statesman WB luxury models were starting to show their age; their 1971 origins compared unfavourably with Ford's more modern Falcon and Fairlane models.[61]

New names for the trim levels were also introduced, such as Commodore Executive (an SL with air conditioning and automatic transmission), Commodore Berlina (replacing SL/X) and Calais (replacing SL/E).[62] The 3.3-litre Blue straight-six engine was replaced by the Black specification, gaining computer-controlled ignition systems on the carburettor versions and optional electronic fuel injection boosting power output to 106 kW (142 hp).[63] The 5.0-litre V8 engine continued to power high specification variants, but was shrunk from 5,044 cc to 4,987 cc in 1985 due to new Group A racing homologation rules. The new car cut its predecessor's weight by 75 kg (165 lb) and models were fitted with an upgraded braking system. As high oil prices became a thing of the past, Holden decided to drop the 2.85- six and 4.2-litre V8,[59] while the 1.9-litre four-cylinder was limited to New Zealand.[64]

Marking a high point in terms of sales, the last-of-the-series VL Commodore sold in record numbers, finally managing to outsell the Ford Falcon in the private sector.[65] The 1986 VL represented a substantial makeover of the VK and would be the last of the mid-size Commodores for 30 years. Designers distanced the Commodore further away from its Opel origins, by smoothing the lines of the outer body and incorporating a subtle tail spoiler. A thorough redesign of the nose saw the Commodore gain sleek, narrow headlamps and a shallower grille, while the Calais specification employed unique partially concealed headlamps.[66]

By this stage, Holden's 24‑year‑old six-cylinder was thoroughly outmoded and would have been difficult to re-engineer to comply with pending emission standards and the introduction of unleaded fuel. This led Holden to sign a deal with Nissan of Japan to import their RB30E engine.[67] This seemed a good idea in 1983 when the Australian dollar was strong; however by 1986 the once viable prospect became rather expensive.[68] The public quickly accepted what was at first a controversial move, as reports emerged of the improvements in refinement, 33 percent gain in power and 15 percent better economy over the carburettor version of the VK's Black straight-six.[65] An optional turbocharger appeared six months later and lifted power output to 150 kW (201 hp).[69][70] In October 1986, an unleaded edition of Holden's carburettored V8 engine was publicised.[65][71] Holden had originally planned to discontinue the V8 to spare the engineering expense of converting to unleaded. However, public outcry persuaded them to relent. VLs in New Zealand, Indonesia, Singapore and Thailand were also available with the 2.0-litre six-cylinder RB20E engine.[72]

The VL suffered from some common build quality problems, such as poor windshield sealing, that can lead to water leakages and corrosion. Awkward packaging under the low bonnet coupled with Holden's decision to utilise a cross-flow radiator (as opposed to the up-down flow radiator installed to the equivalent Nissan Skyline) meant the six-cylinder engine was especially susceptible to cracked cylinder heads, a problem not displayed on the Nissan Skyline with which it shares the RB30E engine.[73] The Used Car Safety Ratings, published in 2008 by the Monash University Accident Research Centre, found that first generation Commodores (VB–VL), similarly to the Ford Falcons manufactured during the same years, provide a "worse than average" level of occupant safety protection in the event of an accident.[74] It is perhaps noteworthy however, that the Monash University publication includes in its averages, vehicles manufactured as late as 2006.[74] As such, and with reasonable necessity, the 2008 Used Car Safety Ratings include comparison of some non-airbag vehicles with later vehicles fitted with airbags. In 1988, it would still be some years before airbags became available to the public on vehicles manufactured in Australia, and, outside of the very high end luxury market, available in Australia at all. As airbag technology later become more available, the Holden Commodore would become one of the first to offer the option (see VR Commodore below).

The VN Commodore of 1988 and subsequent second generation models took their bodywork from the larger Opel Senator B and new Opel Omega A. However, this time, the floor plan was widened and stretched; now matching the rival Ford Falcon for size. Continuing financial woes at Holden meant the wider VN body was underpinned by narrow, carry-over VL chassis components in a bid to save development costs.[75] In Australia, for the VN and succeeding models, the Commodore Berlina became known simply as the Berlina (but in New Zealand the V6 VN Berlina, assembled locally until the Trentham factory was closed in 1990, was badged Executive. The Berlina nameplate was not launched, as a new entry level grade, with trim and equipment equivalent to the Australian V6 Executive, until the locally built four cylinder model, using the Australian-made, Opel designed, two-litre Family Two fuel injected engine, was added some months after the V6s).[76] The range expanded in 1990 to include a utility variant, given the model designation VG. This was built on a longer-wheelbase platform that it shared with the station wagon and luxury VQ Statesman sedans released earlier in the year.[77] During this time, the rival Ford EA Falcon was plagued with initial quality issues which tarnished its reputation.[78] Buyers embraced the VN Commodore, helping Holden to recover and post an operating profit of A$157.3 million for 1989. The team at Wheels magazine awarded the VN Car of the Year in 1988: the second Commodore model to receive this award.[77]

Changes in the relative values of the Australian dollar and Japanese yen made it financially impractical to continue with the well-regarded Nissan engine of the VL. Instead, Holden manufactured their own 3.8-litre V6 engine based on a Buick design, adapted from FWD to RWD.[60] The 5.0-litre V8 remained optional and received a power boost to 165 kW (221 hp) courtesy of multi-point fuel injection.[79] Although not known for its refinement, the new V6 was nevertheless praised for its performance and fuel efficiency at the time.[80] The 2.0-litre Family II engine offered in New Zealand was also offered in some other export markets including Singapore  where the model also was badged Berlina.[60] Accompanying the changes to engines, the VL's four-speed automatic transmission was replaced by the Turbo-Hydramatic and a Borg-Warner five-speed manual.[81] A Series II update of the VN appeared in September 1989, featuring a revised V6 engine known internally as the EV6.[79] With the update came a power hike of rising to 127 kW (170 hp) from 125 kW (168 hp).[79]

Under an unsuccessful model sharing arrangement that was part of the Hawke Labor government reforms in 1989, which saw the formation of the United Australian Automobile Industries alliance between Holden and Toyota Australia, the latter began selling badge engineered versions of the VN Commodore manufactured by Holden.[82] The rebadged Commodores were sold as the Toyota Lexcen, named after Ben Lexcen, who was the designer of the Australia II yacht that won the 1983 America's Cup. The original (VN) T1 Lexcen offered sedan and station wagon body forms in three levels of trim: base, GL and GLX. Moreover, they were only available with a 3.8-litre V6 engine and automatic transmission combination.[83]

The VP update of 1991 featured cosmetic and mechanical changes, while carrying over the 3.8-litre V6 and 5.0-litre V8 engines from the VN. The 2.0-litre straight-four engine previously available in New Zealand was discontinued.[84] Exterior cosmetic changes included a translucent acrylic grille on the base level Executive[85] and Berlina, with a colour-coded grille for the S and SS, and a chrome grille for Calais. Updated tail lights and boot garnishes were also a part of the changes, which were different for each model, with the Berlina having grey stripes and the Calais chrome stripes. semi-trailing arm independent rear suspension became standard on the Calais and SS, but was made an option on lower-end models in lieu of the live rear axle, improving ride and handling.[84]

A new wider front track was introduced to address issues with the previous carried-over VL chassis components.[86] In August 1992, anti-lock brakes were introduced as an option on the Calais and SS trim levels, later becoming optional on all Series II variants. This January 1993 update also included a colour-coded grille for the Executive and alloy wheels for the Commodore S.[85]

Toyota's pattern of updating their Lexcen model tended to follow Commodore's model cycle. The T2 (VP) Lexcen from 1991 pioneered new specification designations: CSi, VXi and Newport. All future updates T3 (VR), T4 (VS), and T5 (VS II) Lexcens made use of the new naming system until 1997, when the badge engineering scheme ceased. To give further differentiation to the Lexcen from the Commodore, the Lexcens from the VP model onwards had unique front-end styling treatments.[87]

The 1993 VR Commodore represented a major facelift of the second generation architecture leaving only the doors and roof untouched.[88] Approximately 80 percent of car was new in comparison to the preceding model. Exterior changes brought an overall smoother body, semicircular wheel arches and the "twin-kidney" grille—a Commodore styling trait which remained until the VX and VU Commodore model ended production in 2003.[89]

The rear-end treatment saw raised tail lights, implemented for safety reasons, and a driver's side airbag was introduced as an option: a first for an Australian-built car.[90] Other safety features such as anti-lock brakes and independent rear suspension were only available with the new electronic GM 4L60-E automatic transmission.[88] Along with a driver's airbag and cruise control, these features were packaged into a new Acclaim specification level: a family-oriented safety spec above the entry-level Executive.[91] Holden's strong focus on safety can be seen in the Used Car Safety Ratings. The findings show that in an accident, VN/VP Commodores provide a "worse than average" level of occupant protection. However, the updated VR/VS models were found to provide a "better than average" level of safety protection.[74] Holden issued a Series II revision in September 1994 bringing audible warning chimes for the handbrake and fuel level among other changes.[88]

The latest revision of the Buick 3.8-litre V6 engine was fitted to the VR Commodore, featuring rolling-element bearings in the valve rocker arms and increased compression ratios.[92] These changes combined to deliver an increase in power to 130 kW (174 hp) and further improvement in noise, vibration, and harshness levels.[89] Wheels magazine awarded the VR Commodore Car of the Year in 1993.[93]

The 1995 VS Commodore served as a mechanical update of the VR, destined to maintain sales momentum before the arrival of an all-new VT model. The extent of exterior changes amounted to little more than a redesigned Holden logo and wheel trims.[94] An overhauled Ecotec (Emissions and Consumption Optimisation through TEChnology) version of the Buick V6 engine coincided with changes to the engine in the United States. The Ecotec engine packed 13 percent more power for a total of 147 kW (197 hp), cut fuel consumption by 5 percent, increased the compression ratio from 9.0:1 to 9.4:1 and improved on the engine's previous rough characteristics. Holden mated the new engine with a modified version of the GM 4L60-E automatic transmission, improving throttle response and smoothing gear changes.[94] The Series II update of June 1996 included elliptical side turn signals, interior tweaks and the introduction of a supercharged V6 engine for selected trim levels, and the introduction of a new Getrag manual transmission.[94] The new supercharged engine slotted between the existing V6 and V8 engines in the lineup and was officially rated at 165 kW (221 hp), just 3 kW (4.0 hp) below the V8.[95]

The VS Commodore was the last to be sold as a Toyota Lexcen, as Holden and Toyota ended their model-sharing scheme.[96] The last Lexcens were built during 1997.[97] This model was also sold as the VS Commodore Royale in New Zealand. Similar in specification to the Calais also sold in New Zealand, the Royale featured a standard VS Commodore body with the front end from the VS Caprice and an Opel 2.6-litre 54-Degree V6 engine. The Royale was also sold between 1995 and 1997 in small numbers to Malaysia and Singapore as the Opel Calais.[98][99]

With the VT Commodore of 1997, Holden looked again to Opel in Germany for a donor platform. The proposal was to take the Opel Omega B and broaden the vehicle's width and mechanical setup for local conditions. In the early days, Holden considered adopting the Omega as is, save for the engines and transmissions, and even investigated reskinning the existing VR/VS architecture.[100] Later on, the VT bodywork spawned a new generation of Statesman and Caprice (again based on the long-wheelbase wagons),[60] and even went as far as resurrecting the iconic Monaro coupé of the 1960s and 1970s[101] via a prototype presented at the 1998 Sydney Motor Show.

The VT heralded the fitment of semi-trailing arm independent rear suspension as standard across the range, a significant selling point over the rival Falcon,[102] along with increased electronics such as Traction Control. However, in terms of suspension, the original Opel design was simplified by removing the toe control links[60] that was standard equipment on the European Omega since 1987.[60] Consequently, this afflicted the VT with excessive tyre wear due to distortions to the suspension camber angle and toe under heavy load, such as heavy towing or when travelling over undulated surfaces.

Notably, Holden's performance arm HSV re-added the toe control link on the flagship GTS 300 model.[60] The 1999 Series II update replaced the venerable Holden 5.0-litre V8 engine with a new 5.7-litre Generation III V8 sourced from the United States.[60] The V8 was detuned to 220 kW (295 hp) from the original US version, but would receive incremental power upgrades to 250 kW (335 hp) throughout its time in the Commodore,[103] before finally being replaced by the related Generation 4 in the VZ.[104] The supercharged V6 was uprated to 171 kW (229 hp) from the VS.[105] Safety wise, side airbags became an option for the Acclaim and higher models, a first for Holden.[106]

From the onset, parent company General Motors was interested in incorporating a left-hand drive Commodore in its Buick lineup, as manifested by the unveiling of the Buick XP2000 concept car in 1996.[107] Although this idea was ultimately abandoned (due to pressures by the North American automotive trade unions to retain local production), the GM-funded project allowed Holden to enter into a range of left-hand export markets.[108] Thus began the Commodore's rapid expansion into parts of Indochina, the Middle East and South Africa badged as the Chevrolet Lumina and Brazil as the Chevrolet Omega 3.8 V6.[109] In its home market, the VT series was awarded the 1997 Wheels Car of the Year award, the fourth such award in Commodore's history.[110] It found ready acceptance in the market as many buyers steered away from the slow selling Ford AU Falcon, becoming the best selling Commodore to date and cementing its place as number one in Australian sales.[60]

The sedan and wagon range comprised: Commodore Executive (base and fleet package); Commodore Acclaim (family and safety package); Berlina (luxury package) and Calais (sedan-only sport luxury package). Limited editions included a "Sydney 2000" Olympic version and Holden 50th Anniversary based on better equipped Executive models (e.g. Berlina alloy wheels on the former but no climate control).

The VX update from 2000 featured a revised headlamp design.[111] The VT's rear tail lamp panel was replaced by two separate light assemblies. Conversely, the luxury-oriented Berlina and Calais sedans continued using a full-width boot-lid panel incorporating the registration plate and tail lamps.[112]

The VX series also formed the basis for a new Holden Ute, designated the VU-series. Earlier utility models were instead entitled "Commodore utility".[113] An updated Series II was launched in early 2002, featuring revised rear suspension system now equipped with toe control links to address the VT's issues.[114] The VX series also spawned the production version of the re-launched Holden Monaro (allowing Holden to commence exports to the United States, with this coupé sold as the Pontiac GTO).[109]

Safety played a substantial role in the development of the VX model. Bosch 5.3 anti-lock brakes were made standard on all variants, a first for an Australian manufactured car; and traction control was made available on vehicles equipped with manual transmission. Extensive research was undertaken to reduce the effects from a side-impact collision through modification of the B-pillars. The risk presented by a side-impact collision in a VX fitted without side airbags is reduced by 50 percent when compared to a similarly specified VT model.[115]

The A$250 million VY mid-cycle update of 2002 represented the first major styling shift since the 1997 VT. Designers discarded the rounded front and rear styling of the VT and VX models, adopting more aggressive, angular lines.[116] The same approach was applied to the interior, whereby the curvaceous dashboard design was orphaned in favour of an angular, symmetrical design. Satin chrome plastic now dominated the façade of the centre console stack, and high-end models received fold-out cup holders borrowed from fellow GM subsidiary Saab.[117] Leaving Eurovox behind, Holden turned towards German electronics manufacturer Blaupunkt to source audio systems, an arrangement that remained in place until the end of the Holden brand.[118]

Engineering wise, Holden kept the changes low key. A revised steering system and tweaked suspension tuning were among some of the changes to sharpen handling precision. Further improvements were made to the Generation III V8 engine to produce peak power of 235 kW (315 hp) for sports variants.[119] In a bid to recapture the market for low-cost, high-performance cars, Holden created a new SV8 specification level. Based on the entry-level Executive, the SV8 inherited the V8 mechanical package from the SS but made do without the luxury appointments and was sold at a correspondingly lower price.[120] Holden also experimented by releasing a limited edition wagon version of its high-performance SS variant, of which only 850 were built.[121] The Series II update added a front strut bar as standard to the SS, which was claimed to increase rigidity and hence handling. As became the trend, the update raised V8 power, now up 10 kW (13 hp).[122] Amendments in the remaining models were confined to new wheels, trims and decals, however, the Calais has taken on a sports-luxury persona as opposed to the discrete luxury character seen in previous models. This repositioning in turn affected the Berlina's standing. The once second-tier model now became the sole luxury model, only overshadowed by the more expensive Calais.[123] Coinciding with the VY II models was the first four-door utility model dubbed the Holden Crewman. Crewman's underpinnings and body structure while somewhat unusual, shared a fair amount in common with the Statesman/Caprice, One tonner and the two-door Ute.[124]

In 2003, Holden launched an AWD system that it developed for the VY platform dubbed Cross Trac, at a cost of A$125 million.[125] Unveiled after the Series II updates, the first application of this electronically controlled system was the Holden Adventra, a raised VY wagon crossover. The system was only available in combination with the V8 and automatic transmission. Holden chose not to spend extra engineering resources on adapting the AWD system to the 3.8-litre V6, due to be replaced in the upcoming VZ model. Unfortunately for Holden, the Adventra fell well short of expected sales, despite modest targets.[126]

The final chapter of the third generation series was the VZ Commodore. Debuting in 2004 with a new series of V6 engines known as the Alloytec V6, both 175 kW (235 hp) and 190 kW (255 hp) versions of the 3.6-litre engine were offered.[127] These were later upgraded to 180 and 195 kW (241 and 261 hp) respectively in the VE model.[128] When compared to the previous Ecotec engines, the Alloytec benefits from increased power output, responsiveness and fuel efficiency.[127] The new engines were mated to a new five-speed 5L40E automatic transmission on the luxury V6 variants, and a new six-speed Aisin AY6 manual transmission on the six-cylinder SV6 sports variant.[129] However, the long serving four-speed automatic carried on in other variants, albeit with further tweaks in an attempt to address complaints about refinement. A new 6.0-litre Generation 4 V8 engine was added to the range in January 2006 to comply with Euro III emission standards. Compared to the American version, both Active Fuel Management and variable valve timing were removed.[104] The Alloytec V6 was also affected by the new standards, which saw the peak output reduced to 172 kW (231 hp).[130]

Along with the new powertrain, Holden also introduced new safety features such as electronic stability control and brake assist.[129] The Used Car Safety Ratings evaluation found that VT/VX Commodores provide a "better than average" level of occupant protection in the event of an accident, with VY/VZ models uprated to "significantly better than average".[74] ANCAP crash test results rate the fourth generation VE lower in the offset frontal impact test than the third generation VY/VZ Commodore. The overall crash score was marginally higher than the outgoing model due to improved side impact protection.[131][132]

Launched in 2006 after GM's 2003 abandonment of their last European rear-drive sedan, the Opel Omega, the VE is the first Commodore model designed entirely in Australia, as opposed to being based on an adapted Opel-sourced platform.[133] Given this and high public expectations of quality, the budget in developing the car reportedly exceeded A$1 billion.[134] Underpinned by the new Holden developed GM Zeta platform, the VE features more sophisticated independent suspension all round and near-even 50:50 weight distribution, leading to improved handling.[135] Engines and transmissions are largely carried over from the previous VZ model.[136] However, a new six-speed GM 6L80-E automatic transmission was introduced for V8 variants, replacing the original four-speed automatic now relegated to base models.[137] The design of this new model included innovative features to help minimise export costs, such as a symmetrical centre console that houses a flush-fitting hand brake lever to facilitate its conversion to left-hand drive.[138] Internationally, the Commodore is again badge engineered as the Chevrolet Lumina and Chevrolet Omega, along with its new export market in the United States as the Pontiac G8 (discontinued as of 2010 along with the Pontiac brand).[139]

Variants by Holden's performance arm, HSV, were released soon after the sedan's debut, followed by the long-wheelbase WM Statesman/Caprice models.[140] The VE Ute did not enter production until 2007 whilst the Sportwagon began production in July 2008.[141][142] A VE V8 Calais was awarded Wheels Car of the Year, being the fifth Commodore/Calais model to do so.[143]

In late 2008 Holden made changes to the VE Commodore, including the addition of a passenger seatbelt-reminder system. The rollout of such modifications allowed the VE range to be upgraded in stages (dependent on model) to the five-star ANCAP safety rating during 2008 and 2009.

The September 2009 MY10 update to the VE Commodore platform introduces a new standard engine–a 3.0-litre Spark Ignition Direct Injection (SIDI) V6 on the Omega and Berlina, with a 3.6-litre version of the same reserved for all other V6 variants.[144] The standard transmission is now a six-speed GM 6L50 automatic, replacing the four-speed in Omega and Berlina models and the five-speed in higher luxury levels. A six-speed manual is still available in sport models.[145] Holden claims the newer powertrains would provide better fuel economy than some smaller four-cylinder cars; the 3.0-litre version is rated at 9.3 L/100 km (25 mpg‑US; 30 mpg‑imp).[146] The 3.0L produces 190 kW (255 hp), more than the earlier 3.6L and more than the old 5.0L Holden V8. The new 3.6 produces a fraction more at 210 kW (282 hp) although the difference is negligible in real world driving.

In mid-2010 Holden released the VE Series 2 (VEII). The major difference saw the introduction of the Holden iQ system, a centre-mounted LCD display that provides navigation, Bluetooth, and controls to the stereo. There were also small alterations to the styling and a number of other changes.

The VF Commodore, a major overhaul of the VE, was officially revealed on 10 February 2013 in Melbourne.

The body shell, suspension and electrics of the GM Zeta platform were thoroughly reworked to reduce weight, improving handling and fuel efficiency. Changes to the model line-up saw the deletion of the Berlina nameplate (which was merged with the standard Calais variant, represented the smallest share of sales in Commodore's line-up) and the base model renamed from Omega to Evoke.

Standard features across the Commodore range includes front and rear parking sensors, reverse camera and auto park assist, whereas high specifications models such as the Calais-V and SS-V redline models also feature, as standard, forward and reverse collision alert system and a colour heads-up display - all possible thanks to the VF's electronics now being compatible with those of more developed GM cars, resulting in the new Commodore being cheaper to manufacture. Indeed, the recommended retail pricing was substantially reduced across the range, from A$5,000 for the base model and up to A$10,000 for the Calais V V8 and SS V Redline.[147]

A day after the Australian range reveal and in the lead up to the Daytona 500 weekend, a more powerful and better equipped export version of the VF Commodore SS also made its debut in Daytona, Florida, as the MY14 Chevrolet SS.[148] To maximise the SS's profile in the United States, GM also replaced in NASCAR the Chevrolet Impala with the SS, which raced in NASCAR's premier series through 2017, when it was replaced by the Chevrolet Camaro ZL1 for the 2018 season.

A Series II update (VF II) was launched in late 2015, introducing minor styling revisions at the front, while the biggest change was the arrival of a 304 kW (408 hp) LS3 across the entire V8 range. In addition, the  V8's final drive ratio and the Redline's suspension tune were also revised.[149]

In 2017, Holden announced that the Commodore would end production in Australia and confirmed that the Commodore badge would be inherited by its replacement – now fully imported. That decision was made on the basis of a survey revealing that a majority of customers were in favour of retaining the long-standing Australian badge, introduced in 1978.

In October 2016, Holden provided selected journalists an opportunity to test drive early prototypes of the 2018 Commodore.

The ZB Commodore was revealed on 6 December 2016, as a rebadged version of the Opel Insignia B. The ZB Commodore was offered with four or six-cylinder engine options, as well as front-wheel drive (FWD) or all-wheel drive. This was a major departure from the V8 and rear-wheel drive variants available on the previous generation Commodore.

There was significant controversy regarding Holden's decision to retain the Commodore name for the 2018 model, despite it being considerably smaller than its predecessor and lacking both the traditional V8 engine and rear-wheel drive configuration. The decision was considered to be a safe choice in order to preserve sales, but the retention of the long-respected Commodore name was criticised for missing the opportunity to re-brand the sedan range and push the vehicle into the more lucrative semi-premium segment of the market.

As of April 2018[update], the ZB Commodore had the lowest resale value, as a proportion of its new price, of any car on the Australian market.[150]

Due to slow sales and Holden's interest in other vehicle segments, it was announced on 10 December 2019 that the ZB Commodore would be discontinued in early 2020, shortly before GM's decision to retire the Holden brand entirely.[151][152]

In the late 1990s Commodores were exported from Australia, branded as the Chevrolet Lumina in the Middle East until 2011 and South Africa until 2013, and as the Chevrolet Omega in Brazil until 2008 and, then again, in 2010. Versions were also previously exported in the mid-1990s to Southeast Asia, as the Opel Calais, and to North America from 2008 to 2009 as the Pontiac G8. The sport version from HSV was sold in the United Kingdom as the Vauxhall VXR8 from 2007 to 2017. From 2014 to 2017, the VF Commodore was sold in the United States as the Chevrolet SS.[citation needed]

The Commodore was sold as the Chevrolet Lumina in the Middle East and South Africa, and previously in South East Asia.

A coupe version based on the Holden Monaro was also sold in the Middle East as the Chevrolet Lumina Coupe. In Arabia, the Lumina was offered in four different trims: LS (Omega), LTZ (Berlina), S (SV6) and SS (SS). The LTZ and S came standard with a 3.6 L Alloytec V6 and a six-speed automatic transmission for the S and four-speed for the LTZ, while the SS came standard with a 6.0 L Alloytec V8 with the option of active fuel management. A six-speed manual was standard with the option of a six-speed automatic on the SS. The LTZ was the luxury model, while the S and SS models focused on sportiness. Exports to the Middle East ceased in 2011.[153]

Lumina models sold in South Africa dropped the V6 engine in favour of 6.0 litre V8 engine mated to a six-speed manual or automatic. Fuel injection and a 10.4:1 compression ratio help contribute to a max power output of 270 kW (362 hp) at 5,700 rpm, and a max torque of 530 N⋅m (391 lb⋅ft) at 4,400 rpm for vehicles equipped with the manual. Automatic cars make 260 kW (349 hp) at 5,800 rpm and 517 N⋅m (381 lb⋅ft) at 4,400 rpm. All cars were equipped with Brembo brake calipers and a ZF limited-slip differentials. The car received updates for the 2011 model year. These changes were introduced to coincide with the release of the VE Series II Commodore. Changes included revised bumpers, there is a chrome moulding above the number plate on the boot lid, refreshed alloy wheel designs and the Holden IQ system. Also the SSV model was introduced.[154] The Holden Ute was sold as the Lumina Ute in South Africa and has the same equipment as the sedan.

Australian production of the first Commodore launched in 1978 was initially spread between Holden's Pagewood (New South Wales) and Dandenong (Victoria) plants. In August 1978, Holden announced a $6.7 million program to enable assembly of the Commodore range at the Elizabeth (South Australia) plant, which resulted in the closure of the Pagewood plant a year later.[155] The Australian production of the Commodore was consolidated at Elizabeth in 1988, coinciding with the launch of the then new VN Commodore.

The Commodore and its derivatives have been the basis of modified variants by companies separate to Holden. Officially, Holden's performance partner is HSV, although other prominent high performance brands include HDT Special Vehicles, Corsa Specialized Vehicles (CSV) and Walkinshaw Performance (WP), since the first, third and fourth generation Commodore, respectively.

In December 2013, Holden announced that it would cease production of the Commodore in Australia in 2017.[156] This was followed, in December 2015, by "Project Erich" involving Belgian entrepreneur Guido Dumarey. His plans involve buying the Holden production facilities, with a view to continue producing in Australia a rebadged range of RWD and AWD premium vehicles based on the GM Zeta platform, for local and export sales. Dumarey's company, Punch Powerglide, already supplies automatic transmissions for Holden's V6-powered models made in Australia.[157] The last Commodore - the last Holden vehicle to be manufactured in Australia - rolled off the line at the Elizabeth plant on 20 October 2017.[2]

Books

MagazinesThe Ice Hockey World Championships are an annual international men's ice hockey tournament organized by the International Ice Hockey Federation (IIHF), first officially held at the 1920 Summer Olympics. The IIHF was created in 1908 while the European Championships, the precursor to the World Championships, were first held in 1910. The tournament held at the 1920 Summer Olympics is recognized as the first Ice Hockey World Championship. From 1920 to 1968, the Olympic hockey tournament was also considered the World Championship for that year.

The first World Championship that was held as an individual event was in 1930 in which twelve nations participated. In 1931, ten teams played a series of round-robin format qualifying rounds to determine which nations participated in the medal round. Medals were awarded based on the final standings of the teams in the medal round. In 1951, thirteen nations took part and were split into two groups. The top seven teams (Pool A) played for the World Championship. The other six (Pool B) played for ranking purposes. This basic format would be used until 1992 (although small variations were made). During a congress in 1990, the IIHF introduced a playoff system. As the IIHF grew, more teams began to participate at the World Championships, so more pools (later renamed divisions) were introduced.

The modern format for the World Championship features 16 teams in the championship group, 12 teams in Division I, 12 teams in Division II and 12 teams in Division III. If there are more than 52 teams, the rest compete in Division IV. The teams in the championship play a preliminary round, then the top eight teams play in the playoff medal round and the winning team is crowned World Champion. Over the years, the tournament has gone through several rule changes. In 1969 body-checking in all three zones in a rink was allowed, helmets and goaltender masks became mandatory in the early 1970s and in 1992 the IIHF began using the shootout. The current IIHF rules differ slightly from the rules used in the NHL. From the 1920 Olympics until the 1976 World Championships, only athletes designated as "amateur" were allowed to compete in the tournament. Because of this, players from the National Hockey League (NHL) and its senior minor-league teams were not allowed to compete, while the Soviet Union was allowed to use permanent full-time players who were positioned as regular workers of an aircraft industry or tractor industry employer that sponsored what would be presented as an after-hours amateur social sports society team for their workers. In 1970, after an agreement to allow just a small number of its professionals to participate was rescinded by the IIHF, Canada withdrew from the tournament.[1] Starting in 1977, professional athletes were allowed to compete in the tournament and Canada re-entered.[2] The IIHF requires that players are citizens of the country they represent and allow players to switch national teams provided that they play in their new nation for a certain period of time.

Canada was the tournament's first dominant team, winning the tournament 12 times from 1930 to 1952. The United States, Czechoslovakia, Sweden, Great Britain and Switzerland were also competitive during this period. The Soviet Union first participated in 1954 and soon became rivals with Canada. From 1963 until the nation's breakup in 1991, the Soviet Union was the dominant team, winning 20 championships out of 26. During that period, only three other nations won medals: Canada, Czechoslovakia and Sweden. Russia first participated in 1992 and the Czech Republic and Slovakia began competing in 1993. In the 2000s, the competition became more open as the "Big Six" teams[3] – Canada, the Czech Republic, Finland, Russia, Sweden, and the United States – have become more evenly matched.

As this tournament takes place during the same period as the later stages of the NHL's Stanley Cup playoffs, many of that league's top players are not available to participate for their national teams or have only become available after their NHL teams have been eliminated, after playing 90+ games. North American teams,[4] and especially the United States, have been criticized for not taking this tournament seriously. For example, USA Hockey often sent teams made up of younger NHL players alongside college players, not using top level stars even when they are available.

The 2024 World Championship, held in Prague and Ostrava, Czech Republic, was the most successful to date in terms of overall attendance; it was visited by 797,727 people and average attendance was at 12,464.

The International Ice Hockey Federation (IIHF), the sport's governing body, was created on 15 May 1908 under the name Ligue Internationale de Hockey sur Glace (LIHG).[5] In 1908, organized ice hockey was still relatively new; the first organized indoor ice hockey game took place on 3 March 1875 at Montreal's Victoria Skating Rink.[6] In 1887, four clubs from Montreal formed the Amateur Hockey Association of Canada (AHAC) and developed a structured schedule. Lord Stanley donated the Stanley Cup and the trustees decided to award it to either the best team in the AHAC, or to any pre-approved team that won it in a challenge.[7] The Eastern Canada Amateur Hockey Association (ECAHA) was formed in 1905,[8] which mixed paid and amateur players in its rosters. The ECAHA eventually folded and as a result of the dissolution, the National Hockey Association (NHA) formed.[9]

The Ice Hockey European Championships, first held in Les Avants, Switzerland in January 1910, was the first official hockey tournament for national teams. Participating nations in the inaugural event were Great Britain, Germany, Belgium and Switzerland.[10] In North America, professional hockey was continuing to grow, the National Hockey League (NHL), the largest professional hockey league in the world, was formed in 1917.[11] The European Championships were held for five consecutive years but were not held from 1915 to 1920 due to World War I.[12] The LIHG Championships, held between 1911 and 1914, is also considered a precursor to the World Championships, though the competition did not gain as much importance at the time.[13]

The IIHF considers the ice hockey tournament held at the 1920 Summer Olympics to be the first Ice Hockey World Championship.[14] It was organized by a committee that included future IIHF president Paul Loicq. The tournament was played from 23 to 29 April. Seven teams participated: Canada, Czechoslovakia, the United States, Switzerland, Sweden, France and Belgium.[15] Canada, represented by the Winnipeg Falcons, won the gold medal, outscoring opponents 27–1.[16] The United States and Czechoslovakia won the silver and bronze medals respectively.[17] Following the 1921 Olympic Congress in Lausanne, the first Winter Olympics were held in 1924 in Chamonix, France, though they were only officially recognized by the International Olympic Committee (IOC) as such in the following year.[18]

Subsequently, every Olympic tournament up to and including the 1968 Winter Olympics is counted as the World Championship. Canada won the gold medal at both the 1924 and 1928 Winter Olympics.[19][20] In 1928, the Swedish and Swiss teams won their first medals–silver and bronze, respectively–and a German team participated for the first time, finishing ninth.[21]

The first World Championship that was held as an individual event was in 1930. It was held in Chamonix, France; Vienna, Austria; and Berlin, Germany. Canada, represented by the Toronto CCMs, defeated Germany in the gold medal game, and Switzerland won the bronze.[22][23] Canada, represented by the Manitoba Grads, won the following year,[24] and the Winnipeg Winnipegs won Gold for Canada at the 1932 Winter Olympics.[25][26] At the 1933 World Championships in Prague, Czechoslovakia, the United States won the gold medal, becoming the first non-Canadian team to win the competition. As of 2023, it is the only gold medal the United States has won at a non-Olympic tournament.[27]
Two days before the 1936 Winter Olympics in Germany, Canadian officials protested that two players on the British team—James Foster and Alex Archer—had played in Canada but transferred without permission to play for clubs in the English National League. The IIHF agreed with Canada, but Britain threatened to withdraw if the two could not compete. Canada withdrew the protest before the games started. Britain became the first non-Canadian team to win Olympic gold, with the United States taking bronze.[28] Canada won the remainder of the World Championship tournaments held in the 1930s. The 1939 World Championships marked the first time that a team from Finland competed in the tournament.[29] World War II forced the cancellation of the 1940 and 1944 Winter Olympics and the World Championships from 1941 to 1946.[22][30]

Following World War II, Czechoslovakia's team was quickly improving. They won the 1947 World Championships, although a Canadian team had not participated in the event. In 1949, they became the third nation to win a World Championship tournament that Canada participated in.[14] During the run-up to the 1948 Winter Olympics in St. Moritz, Switzerland, a conflict broke out with the two American hockey bodies: the American Hockey Association (AHA, a forerunner to USA Hockey) and the Amateur Athletic Union (AAU). The AAU refused to support the AHA's team because they believed that AHA players were "openly paid salaries" and at the time, the Olympics were strictly for amateur players.[31] A compromise was reached that the AHA team would be allowed to compete but would be considered unofficial and unable to win a medal. By the end of the tournament, the AHA team finished fourth in the standings.[31][32] Both Czechoslovakia and the RCAF Flyers of Canada won seven games and tied when they played each other. The gold medal winner was determined by goal average: Canada won the gold because they had an average of 13.8 compared to Czechoslovakia's average of 4.3.[33]

At the 1952 Winter Olympics in Oslo, Norway, the Edmonton Mercurys won Canada's second consecutive Olympic gold medal and their 15th World Championship in 19 competitions. It was the last time that a Canadian team would win an Olympic gold medal in hockey for 50 years.[34] At the 1953 tournament, reigning champion Canada did not attend, while the team from Czechoslovakia withdrew because of the death of the General Secretary of the Communist Party of Czechoslovakia, leaving only Sweden, West Germany, and Switzerland competing in the top division. Sweden finished the tournament undefeated and won their first World Championship.[35]

The 1954 World Championships has been described by the IIHF as "the start of the modern era of international hockey."[36] The tournament saw the first participation of the Soviet Union in international competition. The Soviet Union had organized its first ice hockey league in 1946, having previously focused on bandy.[36] Led by coach Arkady Chernyshev, the Soviet national team finished their first six games undefeated. Canada, represented by the East York Lyndhursts, was also undefeated and, in the final game of the tournament, the two teams met for the first time in international competition. The Soviet Union won the game 7–2, becoming the fifth team to win a World Championship tournament.[36] The 1955 World Championship was held in West Germany, and the two teams again met in the final game of the tournament. The game was so high profile in Canada that announcer Foster Hewitt flew to West Germany to provide play-by-play coverage. Both teams were undefeated and Canada, represented by the Penticton Vees, defeated the Soviets 5–0 to reclaim the World Championship.[37] At the 1956 Winter Olympics in Cortina d'Ampezzo, Italy, Canada's Kitchener-Waterloo Dutchmen lost to both the Soviets and the United States in the medal round and won the bronze. The Soviets went undefeated and won their first Olympic ice hockey gold medal.[38] It would be seven years until the Soviet Union won another World Championship.[14]

The 1957 World Championships were held in Moscow. Canada and the United States did not participate in protest of the Soviet occupation of Hungary. Most of the games were held in the Luzhniki Sports Palace, but the Soviet officials decided to hold the final game in a nearby outdoor soccer stadium. The game was attended by at least 55,000 people, which stood as a World Championship attendance record until 2010. In the final game, Sweden edged the Soviet Union to finish with six wins and one tie (the Soviet Union had five wins and two ties) and won the gold medal.[39] Canada returned to the World Championship in 1958 and won two consecutive titles, with the Soviets winning silver both times.[14] At the 1960 Winter Olympics in Squaw Valley, California, Canada, the Soviet Union, Czechoslovakia and Sweden were the top four teams heading into the Games. All four were defeated by the American team, which won all seven games en route to its first Olympic gold medal.[40]

In 1961, Czechoslovakia defeated the Soviet Union and tied Canada to make it a three-way race for gold. In the final game, Canada defeated the Soviets 5–1 to win their nineteenth gold medal. The Trail Smoke Eaters became the final club team to represent Canada. The following year, Canada implemented a national team program, led by Father David Bauer. Canada would not win another world championship gold until 1994.[41] In 1962, the World Championships were held in North America for the first time. The tournament was held in Denver, United States, and was boycotted by the Soviet and Czechoslovak teams. Sweden defeated Canada for the first time in the history of the competition and won their third gold medal.[35]

At the 1963 World Championships in Stockholm, the Soviet Union won the gold medal, beginning a streak of nine consecutive World Championship golds. The 1964 Winter Olympics in Innsbruck, Austria marked the first time that Canada failed to win an Olympic medal in hockey. The Soviet Union won all seven of their games and the gold medal, but Canada finished the tournament with five wins and two losses, putting them in a three-way tie for second place with Sweden and Czechoslovakia. Prior to 1964, the tie-breaking procedure was based on goal difference from games against teams in the medal round and under that system, Canada would have placed third ahead of the Czechoslovaks. The procedure had been changed to count all games and that meant the Canadians finished fourth.[43] However, the Olympics also counted as the World Championships, and under IIHF rules, Canada should have won a World Championship bronze.[44] In April 2005, the IIHF admitted that a mistake had occurred and announced that they had reviewed the decision and would award the 1964 Canadian team a World Championship bronze medal.[45] However, two months later, the IIHF over-turned their decision and rejected an appeal in September.[46][47]

The Soviets dominated the remainder of the decade. Following 1963, the team went undefeated in Olympic and World Championship competition for four years. Their streak was broken by Czechoslovakia at the 1968 Winter Olympics. Despite the loss, the Soviets still won gold.[48][49] It was the last time that the Olympics were also counted as the World Championships.[50] In 1969, the Soviet Union and Czechoslovakia played "the most emotionally charged games in the history of international hockey."[51] The rights to host the tournament had originally been awarded to Czechoslovakia but they were forced to decline the rights following the Soviet-led Warsaw Pact invasion of the nation in August 1968.[51] The tournament was held in Stockholm, Sweden, and with these international tensions, the Czechoslovak team was determined to defeat the Soviets. They won both of their games 2–0 and 4–3 but despite these wins, the Czechoslovaks lost both of their games to Sweden and won bronze.[51]

With European teams using their best players who are de facto professionals, the Canadian Amateur Hockey Association (CAHA) felt their amateur players could no longer be competitive and pushed for the ability to use players from professional leagues. At the IIHF Congress in 1969, the IIHF voted to allow Canada to use nine non-NHL professional players[53] at the 1970 World Championships. The rights to host the tournament were awarded to Canada for the first time–in Montreal and Winnipeg.[54] However, the decision to allow the use of professionals was reversed in January 1970. IOC president Avery Brundage was opposed to the idea of amateur and professional players competing together and said that ice hockey's status as an Olympic sport would be in jeopardy if the change was made. In response, Canada withdrew from International ice hockey competition.[53][55] Canada's ice hockey team did not participate in the 1972 and 1976 Winter Olympics.[53] Canada also waived their rights to host the 1970 World Championship, so it was held in Stockholm, Sweden instead.[54]

Led by goaltender Vladislav Tretiak and forwards Valeri Kharlamov, Alexander Yakushev, Vladimir Petrov and Boris Mikhailov, the Soviet Union won gold at the 1970 and 1971 World Championships and the 1972 Winter Olympics.[56] 1972 marked the first time that both the Olympics and World Championships were held in the same year as separate events. At the World Championships in Prague, the Czechoslovak team ended the Soviet team's streak and won their first gold since 1949.[50] The Soviet team quickly returned to their winning ways, winning 1973 and 1974 World Championships. However, during the latter tournament, the Czechoslovak team defeated the Soviets 7–2. It was one of the biggest margins the Soviet team had ever lost by in an official game.[56] The 1976 World Championships were held in Katowice, Poland. On the opening day of the tournament, Poland defeated the Soviet Union 6–4 thanks to a hat-trick from forward Wieslaw Jobczyk and the goaltending of Andrzej Tkacz. It was one of the biggest upsets in international hockey history; two months earlier at the 1976 Winter Olympics, Poland had lost 16–1 to the Soviets. The Soviets lost two more games and won the silver, and Czechoslovakia won gold. Poland finished seventh and was relegated to Pool B, the division in which teams play for ranking purposes and not the championship (now known as Division I).[57]

Günther Sabetzki became president of the IIHF in 1975 and helped to resolve the dispute with the CAHA. The IIHF agreed to allow "open competition" of all players in the World Championships, and moved the competition to later in the season so players not involved in the NHL playoffs could participate. However, NHL players were still not allowed to play in the Olympics, because of both the unwillingness of the NHL to take a break mid-season and the IOC's strict amateur-only policy. The IIHF also agreed to endorse the Canada Cup, a competition meant to bring together the best players from the top hockey-playing countries.[58]

The 1976 World Ice Hockey Championships in Katowice were the first to feature professionals although in the end only the United States made use of the new rule, recalling eight pros from the NHL's Minnesota North Stars and the WHA's Minnesota Fighting Saints. The first fully open World Championship was held in 1977 in Vienna, Austria, and saw the first participation of active Canadian NHL players, including two-time NHL MVP Phil Esposito. Sweden and Finland also augmented their rosters with a few NHL and WHA players. Many of the players on the Canadian team were not prepared for the tournament and were unfamiliar with the international game. The team finished fourth, losing both games to the Soviet Union by a combined score of 19–2. Czechoslovakia won gold, becoming the third team (after Canada and the Soviet Union) to win consecutive championships.[59]

As a result of these events, full world championship status was given to the IIHF World Under-20 Championship, which had been held annually since 1974 as an unofficial invitational tournament. Colloquially known as the World Junior Ice Hockey Championship, the event was structured after the World Championships, but limited to players under the age of 20.[60] The World Under-18 Championship was established in 1999 and typically held in April. It usually does not involve some of the top North American-based players because they are involved in junior league playoffs at the time.[61]

Starting in 1978, the Soviet team won five consecutive World Championships, and had an unbeaten streak that
lasted from 1981 through the 1984 Winter Olympics and until 1985.[62] During that period, Canada remained competitive, winning three bronze medals. World Championship tournaments were
not held in 1980, 1984 or 1988–the Olympic years.[14]

The 1987 World Championships in Vienna were over-shadowed by several controversies. At the beginning of the tournament, the roster of the West German team included Miroslav Sikora, a Polish-German forward who had previously played for Poland at the 1977 World Under-20 Championship. Sikora became a naturalized citizen of West Germany and played in the first three games, scoring a goal in a 3–1 win over Finland. Following the game, Finland launched a protest, demanding that the result be over-turned because the West Germans had used an
ineligible player. At the time, players were not allowed to switch nationalities under any circumstances and the IIHF agreed to overturn the result and award the two points to Finland. This angered West German officials, who filed a protest in an Austrian court. The court agreed with the West Germans, overturning the IIHF decision and allowing them to keep their points. The result affected the final standings because had the IIHF's decision stood, Finland would have advanced to the medal round instead of Sweden.[63] However, the Finns finished out of the medal round, and Sweden won their first gold medal since 1962. The tournament format also became controversial because the Soviet Union finished undefeated in the preliminary round but the Swedish team, which had lost three games in the preliminary round, won on goal differential because of a 9–0 win over Canada in the medal round.[64]

Before 1989, players that lived in the Soviet Union, Czechoslovakia, and other nations behind the Iron Curtain were not allowed to leave and play in the NHL.[66] In March 1989, Sergei Pryakhin became the first member of the Soviet national team who was permitted to play for a non-Soviet team.[67] Several Soviet players, including Igor Larionov and Viacheslav Fetisov, wanted to leave and play in the NHL. Soviet officials agreed to allow players to leave if they played one final tournament with the national team. Players agreed to this, and the Soviet Union won its 21st World Championship.[65] Shortly after, Soviet players began to flood into the NHL.[68] Many of the Soviet Union's top players left, including the entire "Green Unit"–Larionov, Fetisov, Vladimir Krutov, Sergei Makarov and Alexei Kasatonov.[69] The following year, the Soviet team won their final title at the 1990 World Championships. In 1991, Swedish forward Mats Sundin–the first European player to be drafted first overall in the NHL–led his team to the gold medal. The Soviets won bronze–the last medal the team would ever win.[70]

The Soviet Union dissolved in December 1991. Nine former Soviet republics became part of the IIHF and began competing in international competitions, including Belarus, Kazakhstan, Latvia (which returned after a 52-year-long absence due to having been occupied by the Soviet Union) and Ukraine; the largest, Russia, succeeded the USSR. With this flood of new teams, the IIHF expanded the number of spots from eight to twelve.[71] From 1963 to 1991, only four teams won a World Championship medal: the Soviet Union, Czechoslovakia (failing to win a medal only three times), Sweden and Canada. The Soviets won a medal in every tournament they participated in (1954 to 1991).[14] At the 1992 World Championships, Sweden won their second consecutive gold. Finland won the silver medal, the nation's first ever World Championship medal (the Finnish team had previously won a silver at the 1988 Winter Olympics).[72]

Czechoslovakia split into the Czech Republic and Slovakia in January 1993. The IIHF recognized the Czech Republic's team as the successor to Czechoslovakia, and it retained its position in the top division while Slovakia's team began in the lowest division (Pool C) in 1994 and was forced to work its way up.[73][74][75][76] Following this, the next decade was dominated by the so-called "Big Six"–Canada, the Czech Republic, Finland, Russia, Sweden and the United States.[77] From 1992 to 1996, five different teams won the World Championship. At the 1993 World Championships, Russia won its first title as an independent nation and the Czech Republic won its first medal (bronze).[14] In 1994, the Canadian team finished the preliminary round undefeated and defeated Finland in the final to win their first World Championship since 1961.[78] The following year in Sweden, the Finnish team won its first ever World Championship. Led by their top line of Saku Koivu, Ville Peltonen and Jere Lehtinen, the Finns defeated rival Sweden in the gold medal game.[79] At the 1995 Pool B championships, Slovakia, led by Peter Šťastný won Pool B and was promoted to the top division, where it has remained ever since.[80] In 1996, the Czech Republic won its first World Championship as a separate country. During this period, the United States was the only one of the "Big Six" not to win the World Championship,[14] although they did win the 1996 World Cup of Hockey[81] and their bronze at that year's World Championship was their first medal since 1962. In the mid-1990s, several new teams such as Slovakia, Latvia, Belarus, Kazakhstan and Ukraine were quickly improving and older nations such as Austria, France, Italy, Norway and Switzerland were at risk of being relegated to Pool B. The IIHF feared that it would lose advertising revenue if that happened, so the number of teams was increased to 16 starting in 1998.[82]

From 1996 to 2001, the Czech Republic won six consecutive World Championship medals, including World Championship gold from 1999 to 2001, as well as gold at the 1998 Winter Olympics.[83][84] In 2002, the Czechs were favoured to win, but were upset in the quarter final by Russia. In the gold medal game between Russia and Slovakia, Slovakian Peter Bondra scored in the final two minutes of the game and the nation won its first ever World Championship.[85] At the 2003 World Championships, Sweden made one of the biggest comebacks in tournament history, rallying from a 5–1 deficit in their quarterfinal game against Finland to win 6–5.[86] The gold medal game between Canada and Sweden went into overtime. Canada's Anson Carter scored the winning goal 13 minutes into play, but the goal had to be reviewed for ten minutes to determine if the puck had crossed the line.[87] In a rematch of the two nations the following year, Canada won and repeated as champions.[88]

The 2004–05 NHL season was locked out, and eventually cancelled, because of a labour dispute between the league and the players.[89] The 2005 World Championships, which featured more top players than normal, was won by the Czech Republic.[90] At the 2006 Winter Olympics, Sweden won the gold medal over Finland. Three months later, Sweden defeated the Czech Republic and won the 2006 World Championships. They became the first team to win Olympic gold and a separate World Championship tournament in the same year.[91] At the 2007 World Championship in Moscow, Canada defeated Finland to win the gold medal.[92] The following year, the tournament was held in Canada for the first time. Russia defeated the home team to win their first gold medal since 1993.[93] The Russian team successfully defended their title with a 2–1 win over Canada in 2009.[94]
In 2009, NHL Players' Association director Paul Kelly suggested that the World Championships be held every other year and that the NHL go on break to allow full player participation. IIHF president René Fasel responded that the tournament has television contracts and hosting commitments and that a large change would be difficult to put in place.[95]

The 2010 tournament took place in Germany. The first game, between Germany and the United States, was played at Veltins-Arena in Gelsenkirchen and was attended by 77,803 people, setting a new record for the most attended game in hockey history.[96] The tournament was noted for having several surprising preliminary round results, including: Switzerland beating Canada for the first time in World Championship play;[97] Norway defeating eventual champions the Czech Republic;[98] and Denmark upsetting Finland and the United States en route to their first ever quarterfinal appearance.[99] The German team, which had finished 15th in 2009 and only avoided relegation to Division I because they were set to host the 2010 tournament, advanced to the semi-finals for the first time since the new playoff format was adapted.[100] They finished fourth, losing to Sweden in the bronze medal game. In the gold medal game, the Czech Republic defeated the Russian team, winning gold.[101]

The 2011 tournament was held in independent Slovakia for the first time. Finland won its second world championship with a 6–1 victory over Sweden. The Czech Republic won the bronze medal over Russia.[102]

The 2012 tournament was held in Sweden and Finland. Russia beat Slovakia in the final, while the Czech Republic beat Finland in the bronze medal game.[103]

In 2013, Switzerland finished the preliminary round undefeated before losing the gold medal game 5–1 to co-hosts Sweden. Switzerland's silver medal was the first for the nation since 1953. Sweden's gold made them the first team to win the tournament at home since the Soviet Union in 1986.[104]

The 2014 tournament was held for the first time in independent Belarus in spite of concerns of the human rights abuses perpetrated by the authoritarian government.[105] The tournament saw more upsets by the less prominent ice-hockey nations. France had beaten Canada for the second time in the modern history and made it to the quarterfinals. Eventual finalist Finland lost to Latvia and made it to the quarterfinals only due to a shootout win over Switzerland. The tournament was won by Russia (which had a stacked NHL squad compared to other teams who sent in younger players after the 2014 Winter Olympics), Finland won silver and Sweden won bronze defeating the Czech Republic.[106]

The 2015 tournament was held in Prague and Ostrava, it was the most attended championship in history.[107] It was to be the last appearance of Jaromír Jágr on the Czech national hockey team, and the home crowd had great expectations for its national team, who had failed to win a gold medal since 2010, matching its longest run without a win since the break-up of Czechoslovakia. However, the tournament was dominated by an excellent Canadian team, which went undefeated and beat Russia 6–1 in the gold medal match. Its captain, Sidney Crosby joined the Triple Gold Club, becoming the first player to achieve that honour as captain of each winning team. The bronze was won by the United States, leaving the Czechs with a second consecutive fourth place.[108]

The 2020 tournament was cancelled due to the COVID-19 pandemic.[109]

The first World Championship to be held as an individual event was in 1930. Twelve different nations participated. Canada's team was given a bye to the gold medal game, and the rest of the nations played an elimination tournament to determine which nation would also play for the gold.[110]

In 1931, the World Championships switched to a similar format to what was used at the Olympics. Ten teams played series of round-robin format qualifying rounds were played to determine which nations participated in the medal round. Medals were awarded based on the final standings of the teams in the medal round.[110] The format was changed several times in the 1930s, in some years there was a gold medal game, while in others the gold medal was awarded based on points.[110]

In 1937, the tournament format was again switched to being similar to the version used at the Olympics. A preliminary round involving 11 teams was played, then the top four advanced to the medal round and medals were awarded based on points; no gold medal game was played. A gold medal game was played in 1938; it was the last gold medal game played in the World Championships until 1992.[110]

In 1951, thirteen nations took part and were split into two groups. The top seven teams (Pool A) played for the World Championship.[110] The other six (Pool B) played for ranking purposes. Generally eight teams played in the top-level Championship, although the number varied over the years, going as low as three (in 1953) and as high as twelve (in 1959). The same format was used until 1992.[110] The format was criticized because often the gold medal winner was decided before the final game was played, such as at the 1988 Winter Olympics.

During a congress in 1990, the IIHF introduced a playoff system.[22][111]

As the IIHF grew, more teams began to participate at the World Championships, so more pools were introduced. Pool C games were first played in 1961 and Pool D was introduced in 1987.[112] In 2001, the pools were renamed: Pool B became Division I, Pool C became Division II and Pool D became Division III.[113][114]

The modern format for the World Championship features a minimum of 52 teams: 16 teams in the main championship group, 12 teams in Division I, 12 teams in Division II and 12 teams in Division III. If there are more than 52 teams, the rest compete in Division IV.

From 2000 to 2011, the teams were divided into four groups and played each other in a round robin format preliminary round, and the top 3 teams in each group advance into the qualifying round. The qualifying round is another round of group play with two groups of six, with the top four teams in each group advancing into the knockout playoff stage. The bottom four teams in the preliminary round played in another group as well; this group determined relegation. After a round-robin format, the bottom two teams were usually relegated to play in Division I the following year.[115]

From 1998 and 2004, the IIHF held a "Far East" qualifying tournament for Asian teams with an automatic berth in the championship division on the line. Japan always won this tournament, but finished last at every World Championship except in 2004, when they finished 15th. The IIHF discontinued the qualifying tournament following the 2004 tournament, and Japan was relegated to compete in Division I.[116]

The main group features 16 teams. The 16 teams are split into two groups based on their world ranking. The ranking is based on the standings of the last Winter Olympics and the last four World Championships. The results of more recent tournaments have a higher weight in the ranking. The last World Championship has 100% value, the tournament before 75% and so on. The Olympic tournament has the same value as the World Championship the same year.[117]

Beginning with the 2012 tournament, the qualifying round was eliminated, and the 16 teams divided into two groups of eight, with each team playing seven games in the preliminary round.

The top four teams from these groups advanced to the knockout playoff stage. In the quarterfinals, the first place team from one group played the fourth place team from the opposite group, and the second place team from one group played the third place team from the opposite group. The winners advanced to the semi-finals. In cases where the quarter-final venues were deemed too far apart to allow easy travel between them, the teams stayed within their groups for the quarters. The winners of the quarter-finals advanced to the semi-finals, with the winners of the semi-finals advancing to the gold medal game and the losers advancing to the bronze medal game.[115]

Also starting in 2012, the relegation round was eliminated. Instead, the eighth-place team in each group was relegated to Division I.[115]

Division I is split into two groups of six, both groups play in round robin tournaments independent of each other and the championship division. Previously the top team from both groups was promoted to the championship, while the bottom team was relegated to Division II. Beginning in 2012, the top two teams from the 'A' group were promoted to the championship, the bottom team was exchanged with the group 'B' winner, and that group's last place team was sent to Division II.

Division II works similarly to Division I, with two six-team groups where group 'A' promotes one team to Division I and group 'B' exchanges its last placed team with Division III. Division III is now composed of one group of six, and if more than six nations register for this, the lowest level, then a qualification tournament will be held.[118][119]

The IIHF introduced Division IV, intended to debut in the 2020 tournament, but cancelled due to COVID-19. Kyrgyzstan hosted the inaugural Division IV Championship in 2022.[120]

At the first tournament in 1920, there were many differences from the modern game: games were played outdoors on natural ice, forward passes were not allowed,[121] the rink was 56x18 metres (the current International standard is 61x30 metres) and two twenty-minute periods were played.[15] Each side had seven players on the ice, the extra position being the rover.[22] Following the tournament, the IIHF held a congress and decided to adopt the "Canadian rules"–six men per side and three periods of play.[121]

At an IIHF congress in 1969, officials voted to allow body-checking in all three zones in a rink similar to the NHL. Prior to that, body-checking was only allowed in the defending zone in international hockey. The IIHF later described the rule change as "arguably the most substantial and dramatic rule changes in the history of international hockey" because it allowed for a more aggressive game.[122] The rule, which was first applied at the 1970 World Championships, was controversial: IIHF president Bunny Ahearne feared it would make ice hockey "a sport for goons."[122] Several other rule changes were implemented in the early 1970s: players were required to wear helmets starting in 1970 and goaltender masks became mandatory in 1972.[22] In 1992, the IIHF switched to using a playoff system to determine medalists and decided that tie games in the medal round would be decided in a shootout.[123] The IIHF decided to test a new rule in 1997 that would allow two-line passes. Prior to that, the neutral zone trap had slowed the game down and reduced scoring. At the 1997 World Championships, teams were allowed to decide if they wanted to test the rule. Although no team accepted the offer, the rule was adopted. The IIHF described it as "the most revolutionary rule change since allowing body-checking in all three zones in 1969. [...] The new rule almost immediately changed the game for the better. The 1999 IIHF World Championship in Norway was a stark contrast to the finals the year before with many more goals scored and with end-to-end action – not defence – dominating play."[124]

The current IIHF rules differ slightly from the rules used in the NHL.[125] One difference between NHL and IIHF rules is rink dimensions: the NHL rink is narrower, measuring 61x26 metres (200x85 feet), instead of the international size of 61x30.5metres (200x100feet).[126] Another rule difference between the NHL and the IIHF rules concerns how icings are called. As of the 2013–14 regular NHL season, a linesman stops play due to icing using the hybrid icing method,[127] instead of the former method, where a defending player (other than the goaltender) touched the puck before an attacking player was able to,[128] in contrast to the IIHF rules that use "no-touch" icing, where play is stopped the moment the puck crosses the goal line. The NHL and IIHF differ also in penalty rules. The NHL, in addition to the minor and double minor penalties called in IIHF games, calls major penalties which are more dangerous infractions of the rules, such as fighting, and have a duration of five minutes.[129] This is in contrast to the IIHF rule, in which players who fight are ejected from the game.[130]

Since the 2005–06 season, the NHL instituted several new rules. Some of them were already used by the IIHF, such as the shootout and making the two-line pass legal.[131] Others which were not picked up by the IIHF, such as requiring smaller goaltender equipment and the addition of the goaltender trapezoid to the rink.[132] However, the IIHF did agree to follow the NHL's league's zero-tolerance policy on obstruction and required referees to call more hooking, holding and interference penalties.[133][134] In 2006, the IIHF voted to eliminate tie games and institute a three-point system: wins in regulation time would be worth three points, overtime wins would be two points and over-time losses would be worth one point. The system was first used at the 2007 World Championships.[135]

Since 2019, the World Championships banned the shootout for the Gold Medal Game. Multiple 20-minute golden goal overtime periods of 3-on-3 are played until whoever scores, which wins the game.

The World Championships have been open to all players, both professional and amateur, since 1977.[59] The IIHF lists the following requirements for a player to be eligible to play:[136][137]

If a player who has never played in an IIHF competition changes their citizenship, they must participate in national competitions in their new country for at least two consecutive years and have an international transfer card (ITC).[136] If a player who has previously played in an IIHF tournament wishes to change their national team, they must have played in their new country for four years. A player can only do this once.[136]

As this tournament takes place during the same time period as the NHL's Stanley Cup playoffs, NHL players generally only become available if their respective NHL team missed the playoffs, or once they have been eliminated from Stanley Cup contention. It is therefore common for several NHL players to join the World Championships while the tournament is already in progress.

As of 2020, the IIHF World Championships are split into five different divisions. This is the alignment of the divisions, accurate as of the 2018 IIHF World Ranking. Teams that are not ranked are not included here, for a full list of IIHF members, see List of members of the International Ice Hockey Federation.

Keys:

E.G. ; 1953 – this means that the team was relegated to that division for the 1953 competition, and have been there ever since.

The Championship division comprises the top sixteen hockey nations in the world. The 87th championship was held from 10 to 26 May 2024, in Prague, and Ostrava, Czech Republic.[138]

Teams for the 2024 IIHF World Championship are:

^ A. The IIHF recognizes Bohemia, which joined in 1908, and Czechoslovakia as the predecessors to the Czech Republic, which officially became a member in 1993.[141]
^ B. The IIHF recognizes the Soviet Union, which joined in 1952, as the predecessor to Russia, which officially became a member in 1992.[156]
^ C. On 28 February 2022, IIHF decided to expel Russia and Belarus from the tournament due to the Russian invasion of Ukraine.[157]

Division I comprises twelve teams. Group A teams compete for promotion to the Elite Division with the loser being relegated to Division I Group B. Group B teams compete for promotion to Division I Group A while the loser is relegated to Division II Group A. In 2024, Group A games will be played in Bolzano, Italy from 28 April to 4 May, and Group B games will be played in Vilnius, Lithuania from 27 April to 3 May.

Table updated 17 December 2023

Division II comprises twelve teams. Group A teams compete for promotion to Division I Group B with the loser being relegated to Division II Group B. Group B teams compete for promotion to Division II Group A while the loser is relegated to Division III. In 2024, Group A games will be played in Belgrade, Serbia from 21 to 27 April and Group B games will be played in Sofia, Bulgaria from 15 to 21 April.

Table updated 10 December 2023

^ D. The IIHF recognizes Yugoslavia, which joined in 1939, and Serbia and Montenegro as the predecessors to Serbia, which officially became a member in 2007.[174][182]

Division III comprises twelve teams. Group A teams compete for promotion to Division II Group B with the loser being relegated to Division III Group B. Group B teams compete for promotion to Division III Group A while the loser is relegated to Division IV. In 2024, Group A games will be played in Bishkek, Kyrgyzstan from 10 to 16 March and Group B games were played in Sarajevo, Bosnia and Herzegovina from 23 to 29 February.

Table updated 10 December 2023

Division IV comprises four teams. Teams compete for promotion to Division III Group B. In 2024 games will be played in Kuwait City, Kuwait from 16 to 19 April.

Table updated 10 December 2023

87 championships (as of 2024); 67 teamsKey:   Current division (if no window is coloured, country doesn't play in any competition in the current year)

Since 1954, the IIHF has given awards for play during the World Championship tournament. Voted on by the tournament directorate, the first awards recognised the top goaltender, forward and defenceman.[199] In 1999, an award for the most valuable player was added. There is also an all-star team voted on by members of the media. In 2004, Canadian Dany Heatley became the first player to lead in scoring, win the MVP award, win the best forward award and be named to the all-star team in the same year.[200] He repeated the feat in 2008.[201]

The highest total attendance at a championship was 797,727 spectators in 2024, and the second highest was 741,690 spectators in 2015. Both of these tournaments were held in the Czech Republic. The 2024 edition also had the highest number of spectators per game at 12,464.[202]

All records are according to the IIHF and as of 2025.[202][52]The International System of Units, internationally known by the abbreviation SI (from French Système international d'unités), is the modern form of the metric system and the world's most widely used system of measurement. It is the only system of measurement with official status in nearly every country in the world, employed in science, technology, industry, and everyday commerce. The SI system is coordinated by the International Bureau of Weights and Measures, which is abbreviated BIPM from French: Bureau international des poids et mesures.

The SI comprises a coherent system of units of measurement starting with seven base units, which are the second (symbol s, the unit of time), metre (m, length), kilogram (kg, mass), ampere (A, electric current), kelvin (K, thermodynamic temperature), mole (mol, amount of substance), and candela (cd, luminous intensity). The system can accommodate coherent units for an unlimited number of additional quantities. These are called coherent derived units, which can always be represented as products of powers of the base units. Twenty-two coherent derived units have been provided with special names and symbols.

The seven base units and the 22 coherent derived units with special names and symbols may be used in combination to express other coherent derived units. Since the sizes of coherent units will be convenient for only some applications and not for others, the SI provides twenty-four prefixes which, when added to the name and symbol of a coherent unit produce twenty-four additional (non-coherent) SI units for the same quantity; these non-coherent units are always decimal (i.e. power-of-ten) multiples and sub-multiples of the coherent unit.

The current way of defining the SI is a result of a decades-long move towards increasingly abstract and idealised formulation in which the realisations of the units are separated conceptually from the definitions. A consequence is that as science and technologies develop, new and superior realisations may be introduced without the need to redefine the unit. One problem with artefacts is that they can be lost, damaged, or changed; another is that they introduce uncertainties that cannot be reduced by advancements in science and technology.

The original motivation for the development of the SI was the diversity of units that had sprung up within the centimetre–gram–second (CGS) systems (specifically the inconsistency between the systems of electrostatic units and electromagnetic units) and the lack of coordination between the various disciplines that used them. The General Conference on Weights and Measures (French: Conférence générale des poids et mesures – CGPM), which was established by the Metre Convention of 1875, brought together many international organisations to establish the definitions and standards of a new system and to standardise the rules for writing and presenting measurements. The system was published in 1960 as a result of an initiative that began in 1948, and is based on the metre–kilogram–second system of units (MKS) combined with ideas from the development of the CGS system.

The International System of Units consists of a set of seven defining constants with seven corresponding base units, derived units, and a set of decimal-based multipliers that are used as prefixes.[1]: 125 

The seven defining constants are the most fundamental feature of the definition of the system of units.[1]: 125 
The magnitudes of all SI units are defined by declaring that seven constants have certain exact numerical values when expressed in terms of their SI units. These defining constants are the speed of light in vacuum c, the hyperfine transition frequency of caesium ΔνCs, the Planck constant h, the elementary charge e, the Boltzmann constant k, the Avogadro constant NA, and the luminous efficacy Kcd. The nature of the defining constants ranges from fundamental constants of nature such as c to the purely technical constant Kcd. The values assigned to these constants were fixed to ensure continuity with previous definitions of the base units.[1]: 128 

The SI selects seven units to serve as base units, corresponding to seven base physical quantities. They are the second, with the symbol s, which is the SI unit of the physical quantity of time; the metre, symbol m, the SI unit of length; kilogram (kg, the unit of mass); ampere (A, electric current); kelvin (K, thermodynamic temperature); mole (mol, amount of substance); and candela (cd, luminous intensity).[1]
The base units are defined in terms of the defining constants.  For example, the kilogram is defined by taking the Planck constant h to be 6.62607015×10−34 J⋅s, giving the expression in terms of the defining constants[1]: 131 

All units in the SI can be expressed in terms of the base units, and the base units serve as a preferred set for expressing or analysing the relationships between units. The choice of which and even how many quantities to use as base quantities is not fundamental or even unique – it is a matter of convention.[1]: 126 

The system allows for an unlimited number of additional units, called derived units, which can always be represented as products of powers of the base units, possibly with a nontrivial numeric multiplier. When that multiplier is one, the unit is called a coherent derived unit. For example, the coherent derived SI unit of velocity is the metre per second, with the symbol m/s.[1]: 139  The base and coherent derived units of the SI together form a coherent system of units (the set of coherent SI units). A useful property of a coherent system is that when the numerical values of physical quantities are expressed in terms of the units of the system, then the equations between the numerical values have exactly the same form, including numerical factors, as the corresponding equations between the physical quantities.[3]: 6 

Twenty-two coherent derived units have been provided with special names and symbols as shown in the table below. The radian and steradian have no base units but are treated as derived units for historical reasons.[1]: 137 

The derived units in the SI are formed by powers, products, or quotients of the base units and are unlimited in number.[1]: 138 [4]: 14, 16 

Derived units apply to some derived quantities, which may by definition be expressed in terms of base quantities, and thus are not independent; for example, electrical conductance is the inverse of electrical resistance, with the consequence that the siemens is the inverse of the ohm, and similarly, the ohm and siemens can be replaced with a ratio of an ampere and a volt, because those quantities bear a defined relationship to each other.[b] Other useful derived quantities can be specified in terms of the SI base and derived units that have no named units in the SI, such as acceleration, which has the SI unit m/s2.[1]: 139 

A combination of base and derived units may be used to express a derived unit. For example, the SI unit of force is the newton (N), the SI unit of pressure is the pascal (Pa) – and the pascal can be defined as one newton per square metre (N/m2).[5]

Like all metric systems, the SI uses metric prefixes to systematically construct, for the same physical quantity, a set of units that are decimal multiples of each other over a wide range. For example, driving distances are normally given in kilometres (symbol km) rather than in metres. Here the metric prefix 'kilo-' (symbol 'k') stands for a factor of 1000; thus, 1 km = 1000 m.

The SI provides twenty-four metric prefixes that signify decimal powers ranging from 10−30 to 1030, the most recent being adopted in 2022.[1]: 143–144 [6][7][8] Most prefixes correspond to integer powers of 1000; the only ones that do not are those for 10, 1/10, 100, and 1/100.
The conversion between different SI units for one and the same physical quantity is always through a power of ten. This is why the SI (and metric systems more generally) are called decimal systems of measurement units.[9]

The grouping formed by a prefix symbol attached to a unit symbol (e.g. 'km', 'cm') constitutes a new inseparable unit symbol. This new symbol can be raised to a positive or negative power. It can also be combined with other unit symbols to form compound unit symbols.[1]: 143  For example, g/cm3 is an SI unit of density, where cm3 is to be interpreted as (cm)3.

Prefixes are added to unit names to produce multiples and submultiples of the original unit. All of these are integer powers of ten, and above a hundred or below a hundredth all are integer powers of a thousand. For example, kilo- denotes a multiple of a thousand and milli- denotes a multiple of a thousandth, so there are one thousand millimetres to the metre and one thousand metres to the kilometre. The prefixes are never combined, so for example a millionth of a metre is a micrometre, not a millimillimetre. Multiples of the kilogram are named as if the gram were the base unit, so a millionth of a kilogram is a milligram, not a microkilogram.[10]: 122 [11]: 14 

The BIPM specifies 24 prefixes for the International System of Units (SI):

The base units and the derived units formed as the product of powers of the base units with a numerical factor of one form a coherent system of units. Every physical quantity has exactly one coherent SI unit. For example, 1 m/s = (1 m) / (1 s) is the coherent derived unit for velocity.[1]: 139  With the exception of the kilogram (for which the prefix kilo- is required for a coherent unit), when prefixes are used with the coherent SI units, the resulting units are no longer coherent, because the prefix introduces a numerical factor other than one.[1]: 137  For example, the metre, kilometre, centimetre, nanometre, etc. are all SI units of length, though only the metre is a coherent SI unit. The complete set of SI units consists of both the coherent set and the multiples and sub-multiples of coherent units formed by using the SI prefixes.[1]: 138 

The kilogram is the only coherent SI unit whose name and symbol include a prefix. For historical reasons, the names and symbols for multiples and sub-multiples of the unit of mass are formed as if the gram were the base unit. Prefix names and symbols are attached to the unit name gram and the unit symbol g respectively. For example, 10−6 kg is written milligram and mg, not microkilogram and μkg.[1]: 144 

Several different quantities may share the same coherent SI unit. For example, the joule per kelvin (symbol J/K) is the coherent SI unit for two distinct quantities: heat capacity and entropy; another example is the ampere, which is the coherent SI unit for both electric current and magnetomotive force. This illustrates why it is important not to use the unit alone to specify the quantity. As the SI Brochure states,[1]: 140  "this applies not only to technical texts, but also, for example, to measuring instruments (i.e. the instrument read-out needs to indicate both the unit and the quantity measured)".

Furthermore, the same coherent SI unit may be a base unit in one context, but a coherent derived unit in another. For example, the ampere is a base unit when it is a unit of electric current, but a coherent derived unit when it is a unit of magnetomotive force.[1]: 140 

According to the SI Brochure,[1]: 148  unit names should be treated as common nouns of the context language. This means that they should be typeset in the same character set as other common nouns (e.g. Latin alphabet in English, Cyrillic script in Russian, etc.), following the usual grammatical and orthographical rules of the context language. For example, in English and French, even when the unit is named after a person and its symbol begins with a capital letter, the unit name in running text should start with a lowercase letter (e.g., newton, hertz, pascal) and is capitalised only at the beginning of a sentence and in headings and publication titles. As a nontrivial application of this rule, the SI Brochure notes[1]: 148  that the name of the unit with the symbol °C is correctly spelled as 'degree Celsius': the first letter of the name of the unit, 'd', is in lowercase, while the modifier 'Celsius' is capitalised because it is a proper name.[1]: 148 

The English spelling and even names for certain SI units, prefixes and non-SI units depend on the variety of English used. US English uses the spelling deka-, meter, and liter, and International English uses deca-, metre, and litre. The name of the unit whose symbol is t and which is defined by 1 t = 103 kg is 'metric ton' in US English and 'tonne' in International English.[4]: iii 

Symbols of SI units are intended to be unique and universal, independent of the context language.[10]: 130–135  The SI Brochure has specific rules for writing them.[10]: 130–135 

In addition, the SI Brochure provides style conventions for among other aspects of displaying quantities units: the quantity symbols, formatting of numbers and the decimal marker, expressing measurement uncertainty, multiplication and division of quantity symbols, and the use of pure numbers and various angles.[1]: 147 

In the United States, the guideline produced by the National Institute of Standards and Technology (NIST)[11]: 37  clarifies language-specific details for American English that were left unclear by the SI Brochure, but is otherwise identical to the SI Brochure.[14] For example, since 1979, the litre may exceptionally be written using either an uppercase "L" or a lowercase "l", a decision prompted by the similarity of the lowercase letter "l" to the numeral "1", especially with certain typefaces or English-style handwriting. NIST recommends that within the United States, "L" be used rather than "l".[11]

Metrologists carefully distinguish between the definition of a unit and its realisation. The SI units are defined by declaring that seven defining constants[1]: 125–129  have certain exact numerical values when expressed in terms of their SI units. The realisation of the definition of a unit is the procedure by which the definition may be used to establish the value and associated uncertainty of a quantity of the same kind as the unit.[1]: 135 

For each base unit the BIPM publishes a mises en pratique, (French for 'putting into practice; implementation',[16]) describing the current best practical realisations of the unit.[17] The separation of the defining constants from the definitions of units means that improved measurements can be developed leading to changes in the mises en pratique as science and technology develop, without having to revise the definitions.

The published mise en pratique is not the only way in which a base unit can be determined: the SI Brochure states that "any method consistent with the laws of physics could be used to realise any SI unit".[10]: 111  Various consultative committees of the CIPM decided in 2016 that more than one mise en pratique would be developed for determining the value of each unit.[18] These methods include the following:

The International System of Units, or SI,[1]: 123  is a decimal and metric system of units established in 1960 and periodically updated since then. The SI has an official status in most countries, including the United States, Canada, and the United Kingdom, although these three countries are among the handful of nations that, to various degrees, also continue to use their customary systems. Nevertheless, with this nearly universal level of acceptance, the SI "has been used around the world as the preferred system of units, the basic language for science, technology, industry, and trade."[1]: 123, 126 

The only other types of measurement system that still have widespread use across the world are the imperial and US customary measurement systems. The international yard and pound are defined in terms of the SI.[22]

The quantities and equations that provide the context in which the SI units are defined are now referred to as the International System of Quantities (ISQ).
The ISQ is based on the quantities underlying each of the seven base units of the SI. Other quantities, such as area, pressure, and electrical resistance, are derived from these base quantities by clear, non-contradictory equations. The ISQ defines the quantities that are measured with the SI units.[23] The ISQ is formalised, in part, in the international standard ISO/IEC 80000, which was completed in 2009 with the publication of ISO 80000-1,[24] and has largely been revised in 2019–2020.[25]

The SI is regulated and continually developed by three international organisations that were established in 1875 under the terms of the Metre Convention. They are the General Conference on Weights and Measures (CGPM[c]),[26] the International Committee for Weights and Measures (CIPM[d]), and the International Bureau of Weights and Measures (BIPM[e]).
All the decisions and recommendations concerning units are collected in a brochure called The International System of Units (SI),[1] which is published in French and English by the BIPM and periodically updated. The writing and maintenance of the brochure is carried out by one of the committees of the CIPM. The definitions of the terms "quantity", "unit", "dimension", etc. that are used in the SI Brochure are those given in the international vocabulary of metrology.[27] The brochure leaves some scope for local variations, particularly regarding unit names and terms in different languages. For example, the United States' National Institute of Standards and Technology (NIST) has produced a version of the CGPM document (NIST SP 330), which clarifies usage for English-language publications that use American English.[4]

The concept of a system of units emerged a hundred years before the SI.
In the 1860s, James Clerk Maxwell, William Thomson (later Lord Kelvin), and others working under the auspices of the British Association for the Advancement of Science, building on previous work of Carl Gauss, developed the centimetre–gram–second system of units or cgs system in 1874. The systems formalised the concept of a collection of related units called a coherent system of units. In a coherent system, base units combine to define derived units without extra factors.[4]: 2  For example, using metre per second is coherent in a system that uses metre for length and second for time, but kilometre per hour is not coherent. The principle of coherence was successfully used to define a number of units of measure based on the CGS, including the erg for energy, the dyne for force, the barye for pressure, the poise for dynamic viscosity and the stokes for kinematic viscosity.[29]

A French-inspired initiative for international cooperation in metrology led to the signing in 1875 of the Metre Convention, also called Treaty of the Metre, by 17 nations.[f][30]: 353–354  The General Conference on Weights and Measures (French: Conférence générale des poids et mesures – CGPM), which was established by the Metre Convention,[29] brought together many international organisations to establish the definitions and standards of a new system and to standardise the rules for writing and presenting measurements.[31]: 37 [32] Initially the convention only covered standards for the metre and the kilogram. This became the foundation of the MKS system of units.[4]: 2 

At the close of the 19th century three different systems of units of measure existed for electrical measurements: a CGS-based system for electrostatic units, also known as the Gaussian or ESU system, a CGS-based system for electromechanical units (EMU), and an International system based on units defined by the Metre Convention[33] for electrical distribution systems. Attempts to resolve the electrical units in terms of length, mass, and time using dimensional analysis was beset with difficulties – the dimensions depended on whether one used the ESU or EMU systems.[34] This anomaly was resolved in 1901 when Giovanni Giorgi published a paper in which he advocated using a fourth base unit alongside the existing three base units. The fourth unit could be chosen to be electric current, voltage, or electrical resistance.[35]

Electric current with named unit 'ampere' was chosen as the base unit, and the other electrical quantities derived from it according to the laws of physics. When combined with the MKS the new system, known as MKSA, was approved in 1946.[4]

In 1948, the 9th CGPM commissioned a study to assess the measurement needs of the scientific, technical, and educational communities and "to make recommendations for a single practical system of units of measurement, suitable for adoption by all countries adhering to the Metre Convention".[36] This working document was Practical system of units of measurement. Based on this study, the 10th CGPM in 1954 defined an international system derived six base units: the metre, kilogram, second, ampere, degree Kelvin, and candela.

The 9th CGPM also approved the first formal recommendation for the writing of symbols in the metric system when the basis of the rules as they are now known was laid down.[37] These rules were subsequently extended and now cover unit symbols and names, prefix symbols and names, how quantity symbols should be written and used, and how the values of quantities should be expressed.[10]: 104, 130 

The 10th CGPM in 1954 resolved to create an international system of units[31]: 41  and in 1960, the 11th CGPM adopted the International System of Units, abbreviated SI from the French name Le Système international d'unités, which included a specification for units of measurement.[10]: 110 

The International Bureau of Weights and Measures (BIPM) has described SI as "the modern form of metric system".[10]: 95  In 1971 the mole became the seventh base unit of the SI.[4]: 2 

After the metre was redefined in 1960, the International Prototype of the Kilogram (IPK) was the only physical artefact upon which base units (directly the kilogram and indirectly the ampere, mole and candela) depended for their definition, making these units subject to periodic comparisons of national standard kilograms with the IPK.[38] During the 2nd and 3rd Periodic Verification of National Prototypes of the Kilogram, a significant divergence had occurred between the mass of the IPK and all of its official copies stored around the world: the copies had all noticeably increased in mass with respect to the IPK. During extraordinary verifications carried out in 2014 preparatory to redefinition of metric standards, continuing divergence was not confirmed. Nonetheless, the residual and irreducible instability of a physical IPK undermined the reliability of the entire metric system to precision measurement from small (atomic) to large (astrophysical) scales.[39]
By avoiding the use of an artefact to define units, all issues with the loss, damage, and change of the artefact are avoided.[1]: 125 

A proposal was made that:[40]

The new definitions were adopted at the 26th CGPM on 16 November 2018, and came into effect on 20 May 2019.[41] The change was adopted by the European Union through Directive (EU) 2019/1258.[42]

Prior to its redefinition in 2019, the SI was defined through the seven base units from which the derived units were constructed as products of powers of the base units. After the redefinition, the SI is defined by fixing the numerical values of seven defining constants. This has the effect that the distinction between the base units and derived units is, in principle, not needed, since all units, base as well as derived, may be constructed directly from the defining constants. Nevertheless, the distinction is retained because "it is useful and historically well established", and also because the ISO/IEC 80000 series of standards, which define the International System of Quantities (ISQ), specifies base and derived quantities that necessarily have the corresponding SI units.[1]: 129 

Many non-SI units continue to be used in the scientific, technical, and commercial literature. Some units are deeply embedded in history and culture, and their use has not been entirely replaced by their SI alternatives. The CIPM recognised and acknowledged such traditions by compiling a list of non-SI units accepted for use with SI,[10] including the hour, minute, degree of angle, litre, and decibel.

This is a list of units that are not defined as part of the International System of Units (SI) but are otherwise mentioned in the SI Brochure,[43] listed as being accepted for use alongside SI units, or for explanatory purposes.

The SI prefixes can be used with several of these units, but not, for example, with the non-SI units of time. 
Others, in order to be converted to the corresponding SI unit, require conversion factors that are not powers of ten. Some common examples of such units are the customary units of time, namely the minute (conversion factor of 60 s/min, since 1 min = 60 s), the hour (3600 s), and the day (86400 s); the degree (for measuring plane angles, 1° = (π /180) rad); and the electronvolt (a unit of energy, 1 eV = 1.602176634×10−19 J).[43]

Although the term metric system is often used as an informal alternative name for the International System of Units,[46] other metric systems exist, some of which were in widespread use in the past or are even still used in particular areas. There are also individual metric units such as the sverdrup and the darcy that exist outside of any system of units. Most of the units of the other metric systems are not recognised by the SI.

Sometimes, SI unit name variations are introduced, mixing information about the corresponding physical quantity or the conditions of its measurement; however, this practice is unacceptable with the SI. "Unacceptability of mixing information with units: When one gives the value of a quantity, any information concerning the quantity or its conditions of measurement must be presented in such a way as not to be associated with the unit."[10]
Instances include: "watt-peak" and "watt RMS"; "geopotential metre" and "vertical metre"; "standard cubic metre"; "atomic second", "ephemeris second", and "sidereal second".

Organisations

Standards and conventions

[1] This article incorporates text from this source, which is available under the CC BY 3.0 license.Iron Maiden are an English heavy metal band formed in Leyton, East London, in 1975 by bassist and primary songwriter Steve Harris. Although fluid in the early years of the band, the line-up for most of the band's history has consisted of Harris, lead vocalist Bruce Dickinson, drummer Nicko McBrain, and guitarists Dave Murray, Adrian Smith and Janick Gers. As pioneers of the new wave of British heavy metal movement, Iron Maiden released a series of UK and US Platinum and Gold albums, including 1980's debut album, 1981's Killers, and 1982's The Number of the Beast – its first album with Dickinson, who in 1981 replaced Paul Di'Anno as lead singer. The addition of Dickinson was a turning point in their career, establishing them as one of heavy metal's most important bands. The Number of the Beast is among the most popular heavy metal albums of all time, having sold almost 20 million copies worldwide.

After some turbulence in the 1990s, the return of lead vocalist Bruce Dickinson and guitarist Adrian Smith in 1999 saw the band undergo a resurgence in popularity, with a series of new albums and highly successful tours. Their three most recent albums — The Final Frontier (2010), The Book of Souls (2015), and Senjutsu (2021) — have all reached number 1 in more than 25 countries. Iron Maiden have sold over 130 million copies of their albums worldwide and have obtained over 600 certifications. The band is considered to be one of the most influential and revered heavy metal bands of all time. They have received multiple industry awards, including the Grammy and Brit Awards.

The band have released 41 albums, including 17 studio albums, 13 live albums, four EPs and seven compilations. They have also released 47 singles and 20 video albums, and two video games. Iron Maiden's lyrics cover such topics as history, literature, war, mythology, dark fantasy, science fiction, society and religion. As of October 2019[update], the band have played 2,500 live shows. For over 40 years the band have featured their signature mascot, "Eddie", on the covers of almost all of their releases.

Iron Maiden were formed on Christmas Day, 25 December 1975, by bassist Steve Harris shortly after he left his previous group, Smiler. Harris attributed the band's name to a film adaptation of The Man in the Iron Mask from the novel by Alexandre Dumas, as the title reminded him of the iron maiden torture device.[2] They originally used the name Ash Mountain, but most of the band members preferred the name Iron Maiden. After months of rehearsal, Iron Maiden made their debut at St. Nicks Hall in Poplar on 1 May 1976,[3] before taking up a semi-residency at the Cart and Horses Pub in Maryland, Stratford.[4] The original line-up was short-lived, with vocalist Paul Mario Day being the first to go as, according to Harris, he lacked "energy or charisma on stage".[5] He was replaced by Dennis Wilcock, a Kiss fan who used makeup and fake blood during live performances[5] and had earlier played with Harris and Doug Sampson in the band Smiler.[6] Wilcock's friend, guitarist Dave Murray, was invited to join, much to the dismay of the band's guitarists Dave Sullivan and Terry Rance.[7] Their frustration led Harris to temporarily disband Iron Maiden in 1976,[7] though the group reformed soon after with Murray as the sole guitarist. Harris and Murray remain the band's longest-serving members and have performed on all of their releases.

Iron Maiden recruited another guitarist in 1977, Bob Sawyer, who was sacked for embarrassing the band on stage by pretending to play guitar with his teeth.[8] Tension ensued again, causing a rift between Murray and Wilcock, who convinced Harris to fire Murray,[9] as well as original drummer Ron Matthews.[3] A new line-up was put together, including future Cutting Crew member Tony Moore on keyboards, Terry Wapram on guitar and drummer Barry Purkis (better known today as Thunderstick). After a single gig with the band in January 1978, Moore was asked to leave as Harris decided keyboards did not suit the band's sound.[10] Dave Murray rejoined in late March 1978, and when Terry Wapram disapproved he was sacked. A few weeks later, Dennis Wilcock decided to leave Iron Maiden to form his own band, V1, with Wapram, and drummer Barry Purkis also left. Former Smiler drummer Doug Sampson was at Dennis' and Thunderstick's last gig, and joined the band afterwards.

Harris, Murray and Sampson spent the summer and autumn of 1978 rehearsing while they searched for a singer to complete the band's new line-up.[11] A chance meeting at the Red Lion, a pub in Leytonstone, in November 1978 evolved into a successful audition for vocalist Paul Di'Anno.[12] Steve Harris said, "There's sort of a quality in Paul's voice, a raspiness in his voice, or whatever you want to call it, that just gave it this great edge".[13] At this time, Murray would typically act as their sole guitarist, with Harris commenting, "Davey was so good he could do a lot of it on his own. The plan was always to get a second guitarist in, but finding one that could match Davey was really difficult".[14]

On New Year's Eve, 1978, Iron Maiden recorded a four-song demo at Spaceward Studios in Cambridge.[15] Hoping the recording would help them secure more gigs,[15] the band gave a copy to Neal Kay, who, at the time, was managing a heavy metal club called "Bandwagon Heavy Metal Soundhouse".[16] After hearing the tape, Kay began playing the demo regularly at the Bandwagon, and one of the songs, "Prowler", eventually went to number 1 in the Soundhouse charts, which were published weekly in Sounds magazine.[17] A copy was also acquired by Rod Smallwood, who soon became the band's manager.[18] As Iron Maiden's popularity increased, they released the demo on their own record label as The Soundhouse Tapes, named after the club.[19] Featuring only three tracks (one song, "Strange World", was excluded as the band were unsatisfied with its production),[20] all 5,000 copies sold out within weeks.[17]

In December 1979, the band secured a major record deal with EMI[21] and asked Dave Murray's childhood friend, Adrian Smith of Urchin, to join the group as their second guitarist.[22] Busy with his own band, Smith declined and Dennis Stratton was hired instead.[23] Shortly after, Doug Sampson left due to health issues and was replaced by ex-Samson drummer Clive Burr at Stratton's suggestion on 26 December 1979.[24] Iron Maiden's first appearance on an EMI album was on the Metal for Muthas compilation (released on 15 February 1980) with two early versions of "Sanctuary" and "Wrathchild".[25] The release led to a tour including several other bands linked with the new wave of British heavy metal movement.[26]

Iron Maiden released their self-titled album in 1980, which debuted at number 4 in the UK Albums Chart.[27] In addition to the title track, the album included other early favourites such as "Running Free", "Transylvania", "Phantom of the Opera" and "Sanctuary" – which was not on the original UK release, but appeared on the US version and subsequent remasters. The band embarked on a headline tour of the UK, before opening for Kiss on their 1980 Unmasked Tour's European leg as well as supporting Judas Priest on select dates. After the Kiss tour, Dennis Stratton was dismissed from the band as a result of creative and personal differences,[28] and was replaced by Smith in October 1980. In December, the band played at the Rainbow Theatre in London, where their first live video was filmed. Live at the Rainbow was released in May 1981, and "Iron Maiden" and "Wrathchild" from this video received heavy rotation on MTV during its first hours on the air as the first metal videos ever.[29][30][31]

In 1981, Iron Maiden released their second studio album, Killers. Although many tracks were written prior to their debut release, it had two new songs: "Prodigal Son" and "Murders in the Rue Morgue"[32] (the latter's title was taken from the short story by Edgar Allan Poe).[33] Unsatisfied with the production on their debut album,[34] the band hired veteran producer Martin Birch,[35] who would continue to work with Iron Maiden until his retirement in 1992.[36] The record was followed by the band's first world tour with their debut performance in the United States opening for Judas Priest at the Aladdin Theatre in Paradise, Nevada. Killers marked the band's USA album charts debut, reaching number 78 on the Billboard 200,[37] and they booked 132 shows to promote the album, including their first concert in Belgrade, Yugoslavia.[38] During the summer, Iron Maiden played several festivals in Europe, including at the Golden Summernights 1981 festivals at Zeppelinfeld in Nuremberg in front of 100,000 people.[30]

By 1981, Paul Di'Anno was demonstrating increasingly erratic behaviour, particularly due to his drug usage,[3] about which Di'Anno comments, "It wasn't just that I was snorting a bit of coke, though; I was just going for it non-stop, 24 hours a day, every day ... the band had commitments piling up that went on for months, years, and I just couldn't see my way to the end of it. I knew I'd never last the whole tour. It was too much".[39] Di'Anno was dismissed following the Killer World Tour[40] with the band already having selected his replacement.[41] After a meeting with Rod Smallwood at the Reading Festival,[42] Bruce Dickinson, formerly of Samson, auditioned for Iron Maiden in September 1981 and was immediately hired.[41] The following month, Dickinson went out on the road with the band on a small headlining tour in Italy and a one-off show at the Rainbow Theatre in the UK.[40] For the last show, and in anticipation of their forthcoming album, the band played "Children of the Damned" and "22 Acacia Avenue", introducing fans to their new material.[43]

In 1982, Iron Maiden released their third studio album, The Number of the Beast, which became the band's first number 1 record on the UK Albums Chart,[44] was a Top 10 hit in many other countries, and reached number 33 on the Billboard 200.[37][45] At the time, Dickinson was in the midst of legal difficulties with Samson's management and was not permitted to add his name to any of the songwriting credits, although he still made what he described as a "moral contribution" to "Children of the Damned", "The Prisoner", and "Run to the Hills".[46] The band embarked on a world tour, dubbed The Beast on the Road, with shows in North America, Japan, Australia and Europe, including a headline appearance for 40,000 people at the Reading Festival. Iron Maiden played 188 shows in 10 months.[30] The Beast on the Road's US leg proved controversial when an American conservative political lobbying group claimed Iron Maiden were Satanic because of the new album's title track and "demonic" cover art,[45] and a group of Christian activists destroyed Iron Maiden records in protest.[47] Dickinson later said the band treated this as "silliness"[48] and the demonstrations in fact gave them "loads of publicity".[3] The Number of the Beast sold 2.5 million copies in its first year, 14 million by 2010, and 20 million by 2022.[49][50][51][52]

In December 1982, drummer Clive Burr was fired from the band and replaced by Nicko McBrain, who previously played for Trust.[53] Although Harris said the dismissal took place because his live performances were affected by offstage activities,[54] Burr later claimed he was unfairly ousted from the band.[55] The band then recorded the first of three consecutive albums at Compass Point Studios in the Bahamas.[56] In 1983, they released their fourth studio album, Piece of Mind, which reached the number 3 spot in the UK[57] and number 14 on the Billboard 200.[37] Piece of Mind features the singles "The Trooper" and "Flight of Icarus", the latter being one of the band's few songs to gain substantial airplay in the US.[58] Iron Maiden played 147 concerts in Europe and North America as a part of the World Piece Tour. This was also their first major North American tour as headliners, selling out Madison Square Garden with a crowd of 20,000.[30][59]

After the success of Piece of Mind and its supporting tour, the band released their fifth studio album, Powerslave, on 9 September 1984. The album features the singles "2 Minutes to Midnight" and "Aces High", the title track, and "Rime of the Ancient Mariner" (based on Samuel Taylor Coleridge's poem Rime of the Ancient Mariner).[60] Powerslave was another chart success, reaching number 12 on the Billboard 200[37] and eventually number 1 in the UK.[61][62][63] The band's fifth studio album sold over 4 million copies in its first year after the premiere.[64] The tour following the album, called World Slavery Tour, was the band's largest to date with 193 shows in 28 countries over 13 months,[60] playing to an estimated 3,500,000 people.[65][66] Many shows were played back to back in the same city, such as in Long Beach, California, where the band played four consecutive concerts at Long Beach Arena for a combined audience of 54,000 fans.[67] Iron Maiden also made their debut appearance in South America, where they co-headlined the Rock in Rio festival with Queen for an audience estimated at 350,000–500,000 people.[68][69] The tour started in August 1984 with five shows in Poland. Iron Maiden were the first Western artists to bring full-scale production behind the Iron Curtain. The band's third official video, entitled Behind the Iron Curtain, was released in October 1984. The World Slavery Tour documentary brought footage of the band touring Eastern Europe in 1984, performing shows in the countries visited, Behind the Iron Curtain was the first documentary ever published by a Western artist that showed them touring the countries of Eastern Bloc. The documentary movie was broadcast by MTV and local TV stations around the world.[70]

The tour was physically gruelling for the band, who demanded six months off when it ended (although this was later reduced to four months).[71] This was the first substantial touring break in the group's history, including the cancellation of a proposed supporting tour for the new live album,[72] with Bruce Dickinson threatening to quit unless the tour ended.[73] In October 1985, Iron Maiden released the double live album and home video, Live After Death. A critical and commercial success, it peaked at number 19 on the Billboard 200[37] and number 2 in the UK.[74] The album was recorded at Long Beach Arena and also features additional tracks from four nights at London's Hammersmith Apollo.[75][76] In November 1985, Iron Maiden were named the best rock and metal band in the world and awarded at Public Choice International.[77]

Returning from their time off, the band added different musical elements to their 1986 studio album, Somewhere in Time. These focused on synthesised bass and guitars to add textures and layers to the sound.[78] The release performed well across the world, particularly the single "Wasted Years", but included no writing credits from Dickinson, whose material was rejected by the rest of the band.[79] The album was the band's biggest American chart success to date, reaching number 11 on the Billboard 200[37] and number 2 in the UK charts.[74][80] The Somewhere on Tour was also a success. The band played 157 shows for over two and a half million fans, including eighty-one shows in North America. Once again, Iron Maiden visited Poland, Hungary and Yugoslavia to play for tens of thousands of fans in each country.[81] The experimentation evident on Somewhere in Time continued on their next album, Seventh Son of a Seventh Son, which was released in 1988. A concept album recorded at Musicland Studios in Munich[82] and based on the 1987 novel Seventh Son by Orson Scott Card,[83] it was the band's first record to include keyboards, which were performed by Harris and Smith.[83] Dickinson's enthusiasm was also renewed as his ideas were accepted for this album.[84] Another popular release, it became Iron Maiden's third album to hit number 1 in the UK charts[85] and reached number 12 on the Billboard 200.[37]

During the following tour, the band headlined the Monsters of Rock festival at Donington Park on 20 August 1988, playing to the largest crowd in the festival's history (107,000).[86] The tour concluded with several headline shows in the UK in November and December 1988, with the concerts at the NEC Arena, Birmingham, recorded for a live video, entitled Maiden England. The video debuted at top spots of worldwide music videos charts.[87] In May, the group set out on a supporting tour, which saw them perform 103 shows to well over two million people worldwide over seven months.[88] To recreate the album's keyboards onstage throughout the tour, the group recruited Michael Kenney, Steve Harris' bass technician; Kenney has served as the band's live keyboard player ever since, also performing on the band's four following albums.[89]

During a break in 1989, guitarist Adrian Smith released a solo album with his band ASAP, entitled Silver and Gold.[90] Vocalist Bruce Dickinson also began work on a solo album with former Gillan guitarist Janick Gers, releasing Tattooed Millionaire in 1990,[91] followed by a tour.[92] At the same time, to mark the band's 10-year recording anniversary, Iron Maiden released a compilation collection, The First Ten Years, a series of 10 CDs and double 12-inch singles. Between 24 February and 28 April 1990, the individual parts were released one by one, each containing two of Iron Maiden's singles, including the original B-sides.[30]

Iron Maiden then began work on a new studio record. During the pre-production stages, Adrian Smith left the band due to differences with Steve Harris regarding the direction the band should be taking. Smith disagreed with the "stripped down" style they were leaning towards.[93] Janick Gers, having worked on Dickinson's solo project, was chosen to replace Smith and became the band's first new member in seven years.[92] The album No Prayer for the Dying was released in October 1990.[94] It contained the hit singles "Holy Smoke" and "Bring Your Daughter... to the Slaughter", the band's first – and, to date, only – UK Singles Chart number 1, originally recorded by Dickinson's solo project for the soundtrack of A Nightmare on Elm Street 5: The Dream Child.[95] Iron Maiden's eighth studio album debuted at number 2 on the UK albums chart[95] and number 17 on the Billboard 200.[37] No Prayer for the Dying was a return to their musical roots, especially in the simplicity of composition.[30][93] The No Prayer on the Road tour was booked for 120 shows in Europe, North America, and Japan. Thirty-three shows in continental Europe were sold out with a reported 530,000 fans attending.[96] In total, Iron Maiden played for some two million fans.[97][96]

After another break, the band recorded their next studio album, Fear of the Dark, which was released in 1992. The title track became a regular part of the band's concert setlists. Achieving their fourth number 1 on the UK albums chart and number 12 on the Billboard 200,[37][98] the release also included the number 2 single "Be Quick or Be Dead", the number 21 single "From Here to Eternity", and the softer "Wasting Love".[99] The album featured the first songwriting by Gers, and no collaboration between Harris and Dickinson on songs. The extensive worldwide tour that followed included their first-ever Latin American leg, although Christian organisations prevented Iron Maiden from performing in Chile and accused them of being "emissaries of satanic propaganda",[100] and headlining the Monsters of Rock festivals in seven European countries.[101] Iron Maiden's second performance at Donington Park, for a sold-out audience of 75,000,[102][103] was filmed for the audio and video release Live at Donington and featured a guest appearance by Adrian Smith, who joined the band to perform "Running Free".[103] The tour also saw conflicts between Bruce Dickinson and rest of the band.[30][104]

In 1993, Dickinson left the band to pursue his solo career, but agreed to remain for a farewell tour and two live albums (later re-released in one package).[105] The first, A Real Live One, was released in March 1993 and featured songs from 1986 to 1992, and the second, A Real Dead One, was released after Dickinson left the band and featured songs from 1980 to 1984. The tour did not go well, with Steve Harris claiming Dickinson would only perform properly for high-profile shows, and that at several concerts, he would only mumble into the microphone.[106] Dickinson denied he was under-performing, saying it was impossible to "make like Mr. Happy Face if the vibe wasn't right", and that news of his exit from the band had prevented any chance of a good atmosphere during the tour.[107] Dickinson played his farewell show with Iron Maiden on 28 August 1993. The show was filmed, broadcast by the BBC, MTV and released on video under the name Raising Hell.[108]

In 1994, the title track from the Fear of the Dark album received a Grammy Awards nomination for "Best Metal Performance", a first for Iron Maiden.[109] The band listened to the thousands of tapes sent in by vocalists before convincing Blaze Bayley, formerly of the band Wolfsbane, who had supported Iron Maiden in 1990, to audition for them.[110] Harris' preferred choice from the outset,[111] Bayley had a different vocal style from his predecessor and ultimately received a mixed reception among fans.[112]

After a three-year hiatus – a record for the band at the time – Iron Maiden released their next studio album, The X Factor. The band had their lowest chart position since 1981 for an album in the UK, debuting at number 8; however, the album went on to win "Album of the Year" awards in France, Spain and Germany.[113] The record included the 11-minute epic "Sign of the Cross", the band's longest song since "Rime of the Ancient Mariner", as well as the singles "Man on the Edge" (based on the film Falling Down)[114] and "Lord of the Flies", based on the novel Lord of the Flies.[115] The release is notable for its "dark" tone, inspired by Steve Harris' divorce.[116] The band toured for the rest of 1995 and 1996, playing their first shows in Israel and South Africa as well as Malta, Bulgaria and Romania in Europe, before concluding in the Americas. The biggest show of the whole tour was a headline appearance for 60,000 people at the Monsters of Rock festival in São Paulo, Brazil. The X Factor sold 1.3 million copies, the lowest sales result since 1981.[117] After the tour, Iron Maiden released a compilation album, Best of the Beast. The band's first compilation, it included a new single, "Virus", in which the lyrics attack critics who had recently written off the band.[118]

In 1998, Iron Maiden released Virtual XI, whose chart scores were the band's lowest to date.[119][120] The album peaked at number 16 in the UK, the band's lowest for a new studio record.[121] At the same time, Steve Harris assisted in remastering the band's entire discography, up to and including Live at Donington.[122] Bayley's tenure in Iron Maiden ended in January 1999 when he was asked to leave during a band meeting.[123] The dismissal took place due to issues Bayley had experienced with his voice during the Virtual XI World Tour,[124] although Janick Gers said this was partly the band's fault for forcing him to perform songs pitched outside the natural range of his voice.[125]

The band entered into talks with Dickinson, who agreed to rejoin during a meeting in Brighton in January 1999,[126] along with guitarist Adrian Smith, who was telephoned a few hours later.[127] With Gers remaining, Iron Maiden now had a three-guitar line-up (nicknamed "The Three Amigos"), and embarked on a hugely successful reunion tour.[128] Dubbed The Ed Hunter Tour, it tied in with the band's newly released greatest hits collection, Ed Hunter, whose track listing was decided by a poll on the group's website, and also contained a computer game starring Eddie, the band's mascot.[129]

Not satisfied with the results from Harris' Barnyard Studios, located on his property in Essex,[130] which had been used for the last four Iron Maiden studio albums, the band recorded the new release, Brave New World, at Guillaume Tell Studios in Paris, France in November 1999 with producer Kevin Shirley.[131] Iron Maiden continued to find inspiration in movies and books, as shown in songs like "The Wicker Man" – based on the 1973 British cult film The Wicker Man – and "Brave New World" – a title taken from the Aldous Huxley novel Brave New World.[132] The album revisited the more progressive and melodic sound featured in some earlier recordings, along with elaborate song structures and keyboard orchestration.[132] The album was a commercial and artistic success.[133][30]

The reunion world tour that followed had over 100 dates (including 31 shows of the 1999 tour), and culminated on 19 January 2001 in a show at the Rock in Rio festival in Brazil, where Iron Maiden played to an audience of over 250,000.[134] While the performance was being produced for a CD and DVD release in March 2002, under the name Rock in Rio,[135] the band took a year off from touring, although they played three consecutive shows at Brixton Academy to raise funds for former drummer Clive Burr, who had recently announced that he had been diagnosed with multiple sclerosis.[136] The band performed two further concerts for Burr's MS Trust Fund charity in 2005,[137] and 2007,[138] before his death in 2013.[139] During the 2000–2002 tour, Iron Maiden played 91 shows for over two million people in 33 countries.[140] In addition to their touring success, the band was nominated twice for the annual Grammy Awards[141] and received the International Achievement Award at the 2001 Ivor Novello Awards.[142] In November 2001, a documentary movie about the making of The Number of the Beast album was produced by BBC as a part of the Classic Album series.[143]

Following their summer 2003 Give Me Ed... 'Til I'm Dead Tour, with 57 shows in Europe and North America and headlining large festivals such as Roskilde, Heineken Jammin' Festival, Rock am Ring and Rock im Park (combined attendance of 130,000) and the first Download Festival held at Donington Park; a successor to Monsters of Rock,[144] Iron Maiden released Dance of Death, their thirteenth studio album. It met with worldwide critical and commercial success, reaching number 2 on the UK Albums Chart[145] and number 18 on the Billboard 200.[37] Produced by Kevin Shirley, now the band's regular producer, many critics felt this release reached the standard of their earlier efforts.[146] Historical and literary references were present, with "Montségur" focussing on the Cathar stronghold conquered in 1244, and "Paschendale" relating to the First World War battle.[147]

During the Dance of Death Tour 2003–04, which began in September 2003, Iron Maiden played 53 shows across Europe, North America, Latin America and Japan.[148] The band's performance at Westfalenhalle, in Dortmund, Germany, was recorded and released in August 2005 as a live album and DVD entitled Death on the Road.[149] In 2005, the band announced the Eddie Rips Up the World Tour, which, tying in with their 2004 DVD entitled The History of Iron Maiden – Part 1: The Early Days, only featured material from their first four albums.[150] As part of this celebration of their earlier years, "The Number of the Beast" single was re-released[151] and went straight to number 3 on the UK Chart.[152] The tour featured many headlining stadium and festival dates, including a performance at Ullevi Stadium in Sweden to an audience of almost 60,000.[153] This concert was also broadcast live on satellite television across Europe to approximately 60 million viewers.[154] The band completed the tour by headlining the Reading and Leeds Festivals on 26–28 August,[155] and the RDS Stadium in Ireland on 31 August.[137]

At the end of 2005, Iron Maiden began work on A Matter of Life and Death, their fourteenth studio album, which was released in autumn 2006. War and religion are recurring themes in the lyrics and the cover artwork.[156] The release was a critical and commercial success, marking the band's first top ten on the Billboard 200 and debuting at number one in the album charts of 13 countries.[157][30] The supporting tour saw mixed critical reception,[158] but included the band's first performance in Dubai at the Dubai Desert Rock Festival for 25,000 people,[159] followed by a concert in Bangalore Palace Grounds, the first of any heavy metal band in India.[160][159] The band then played a string of European dates, including an appearance at Download Festival, their fourth headline performance at Donington Park,[161] to approximately 80,000 people.[162]

On 5 September 2007, the band announced their Somewhere Back in Time World Tour, tying in with the DVD release of their Live After Death album.[163] The setlist for the tour consisted of songs from the 1980s.[163] They played their first concerts in Costa Rica and Colombia and their first shows in Australia and Puerto Rico since 1992.[164] The tour led to the release of a new compilation album, entitled Somewhere Back in Time, which included a selection of tracks from their 1980 eponymous debut to 1988's Seventh Son of a Seventh Son, as well as several live versions from Live After Death.[165] In 2008–09 in Latin America the band played 27 concerts for about a million people in total, a record for a heavy rock performer.[30][166] The tour continued with two legs in the US and Europe in the summer of 2008.[167] The sole UK concert took place at Twickenham Stadium, marking the first time the band would headline a stadium in their own country.[168] The 2008 tour was the second highest-grossing tour of the year for a British artist.[169] The final leg included the band's first appearances in Peru and Ecuador, as well as their return to Venezuela and New Zealand after 17 years.[170] The band also played another show in India at the Rock in India festival to a crowd of 20,000. At their concert in São Paulo on 15 March, Dickinson announced on stage that it was the largest non-festival show of their career, with an overall attendance of 100,000 people.[171][172] The final leg ended in Florida on 2 April after which the band took a break. Overall, the tour reportedly had an attendance of over two and a half million people worldwide over both years.[173] At the 2009 Brit Awards, Iron Maiden won the award for best British live act.[174]

On 20 January 2009, the band announced they were planning to release a full-length documentary film in select cinemas on 21 April 2009. Entitled Iron Maiden: Flight 666, it documented the first part of the Somewhere Back in Time World Tour (between February and March 2008).[175] Flight 666 was co-produced by Banger Productions and was distributed in cinemas by Arts Alliance Media and EMI, with D&E Entertainment sub-distributing in the US.[176] The film went on to have a Blu-ray, DVD, and CD release in May and June,[173] topping the music DVD charts in 25 countries.[172] In most of them the release went Gold, Platinum or Multi-Platinum.[30]

The band had begun composing new material and booked studio time in early 2010 with Kevin Shirley producing,[177] and The Final Frontier was announced on 4 March and featured three singles "The Final Frontier", "El Dorado" and "Coming Home", as well as epic, progressive opuses "Isle of Avalon", "The Talisman" and "When The Wild Wind Blows".[178] The album, the band's fifteenth, was released on 16 August[179] to critical acclaim.[180] It was also the band's greatest commercial success to that point, reaching number 1 in twenty-eight countries worldwide,[181] including a debut at number 4 on Billboard 200.

The album's supporting tour saw the band perform 101 shows across the globe to an estimated audience of over two and a half million,[182] including their first visits to Singapore, Indonesia, and South Korea.[181] "El Dorado" won the Best Metal Performance award at the 2011 Grammy Awards, the band's first win after two previous nominations.[183][184] On 15 March, a new compilation to accompany 2009's Somewhere Back in Time was announced.[185] The double disc set covers the period 1990–2010 (the band's most recent eight studio albums).[185] In 2012, the band announced a new live album and DVD release entitled En Vivo!, based on footage from the Chile concert. The DVD topped the music video charts around the world.[186][187] In addition to the concert footage, the video release includes an 88-minute tour documentary, entitled Behind The Beast, containing interviews with the band and their crew.[188] In December 2012, one song from the release ("Blood Brothers") was nominated for a Grammy Award for Best Hard Rock/Metal Performance at the 2013 Grammy Awards.[189]

On 15 February, the band announced their third retrospective Maiden England World Tour 2012–14, which was based around the video Maiden England.[190] The tour commenced in North America in the summer of 2012 and was followed by further dates in 2013 and 2014, and included the band's fifth headline performance at Donington Park with 100,000 fans in attendance.[191][186][192] Iron Maiden closed the tour in July 2014 at Sonisphere Festival, Knebworth, having undertaken 100 shows in 32 countries before an estimated audience of more than 2.7 million people.[193][194][30]

The band's 2015 album, The Book of Souls, was released on 4 September.[195] The band's first original studio album not to be issued by EMI outside North America, following Parlophone's acquisition by Warner Music Group in 2013,[196] it was a critical and commercial success, becoming the band's fifth UK number 1 album[197] and hit number 4 on Billboard 200 in the US. The new release reached the number one position in the album charts of 43 countries.[198] The new record was recorded at Guillaume Tell Studios in late summer 2014;[199] its closing song, "Empire of the Clouds", penned by Dickinson, surpassed "Rime of the Ancient Mariner" (from 1984's Powerslave) as Iron Maiden's longest song, at 18 minutes in length.[195]

In February 2016, the band embarked on The Book of Souls World Tour, with shows in 35 countries across six continents, including their first performances in China, El Salvador, and Lithuania. It was the band's biggest album tour since 1996.[200] In total, Iron Maiden played 117 shows on six continents for well over two and a half million people.[201][30] The band then launched the Legacy of the Beast World Tour in Europe in 2018,[202] with North and South American shows following in 2019. The tour was inspired by the band's new mobile game and comic series released in 2017, entitled Legacy of the Beast.[203] The tour was received very positively by fans and critics,[204] spanning up to three years with 140 shows, performing to over 3.5 million fans.[205]

The COVID-19 pandemic forced the rescheduling of nearly one million tickets from 2020, first to 2021, and then to 2022.[30][206] In October 2020, the band announced they would release a live album from the Legacy of the Beast World Tour called Nights of the Dead, Legacy of the Beast: Live in Mexico City. The double concert album was recorded during three sold-out concerts in Mexico City's Palacio de los Deportes for a combined audience of over 70,000 people.[207]

On 15 July 2021, Iron Maiden released a video for their first song in six years, "The Writing on the Wall", which was directed by Nicos Livesey.[208] Four days later, the band announced their seventeenth studio album, Senjutsu, would be released on 3 September 2021.[209][210] Senjutsu eventually reached the top of the best-seller lists in 27 countries,[211][212][213] but it was the band's first album in fifteen years not to reach number one on the UK charts, although it did top the UK Rock & Metal Singles and Albums Charts. In total, Senjutsu reached the top three in 55 countries and the top five in 63 countries.[211][212][213][214][215]

On 1 February 2023, the band received their second nomination for the Rock and Roll Hall of Fame.[216][217] The band started their 25th global tour, The Future Past World Tour, with a concert in Ljubljana in May 2023.[218] On 6 October, the band performed at the Power Trip festival which drew nearly 100,000 people.[219] Throughout the 2023–24 world tour, Iron Maiden performed 81 shows for almost two million fans.[220] Following the conclusion of the tour in São Paulo in December 2024, McBrain retired from touring,[221] but would still remain a member of Iron Maiden and be involved with various upcoming band-related projects in the studio.[222] The following day, it was announced that British Lion drummer Simon Dawson would be his touring replacement for any further tours.[223][224]

On 19 September 2024, the band announced their 26th global tour, Run For Your Lives World Tour, which is set to start in May 2025, to celebrate the band's 50th anniversary and is set to be focused on the band's first nine studio albums.[225]

Iron Maiden have received numerous nominations, honours and awards including Grammy Awards[226] and equivalents awards in many countries,[227][228] Brit Awards,[229] Ivor Novello Awards,[230] and Juno Awards.[231] They have ranked highly in many polls of the greatest metal artists of all time.[232][233][234][30] In 2012 The Number of the Beast was voted as Best British Album Ever by the British public as part of Elizabeth II's Diamond Jubilee celebrations.[235] Iron Maiden have an exhibit at the Rock and Roll Hall of Fame,[236] and Rock in Rio Wall of Fame.[237]

Iron Maiden were inducted into Hollywood RockWalk, BPI Hall of Fame and Kerrang! Hall of Fame.[238] Band's mascot Eddie the Head is a part of the British Music Experience permanent exhibition.[239] In April 2021, the band's former members (Paul Di'Anno, Blaze Bayley, and illustrator Derek Riggs) were inducted into the Metal Hall of Fame.[240] They have twice been nominated for the Rock and Roll Hall of Fame.[241][216]  In January 2023 Iron Maiden were honoured by Royal Mail UK with dedicated postal stamps and cards.[242][243]

Iron Maiden have sold over 130 million copies of their albums worldwide,[244][243] despite little radio or television support.[245] According to many sources all audio-visual catalogue of the band have sold in over 200 million copies worldwide, including regular albums, singles, compilations, and videos.[246][247][248][249][250] Their third studio album, The Number of the Beast, is among the most popular heavy metal albums of all time and the most commercially successful release of the band, having sold almost 20 million copies worldwide.[251][252][253][254][255] As of 2022 their releases have been certified silver, gold and platinum around 600 times worldwide.[256]

In 1979–1980, visual artist Derek Riggs created the macabre mascot named Eddie The Head. Since then, Eddie has been an integral part of the stage and media image of the group.[257] Originally a papier-mâché mask which would squirt fake blood during their live shows,[258] the character featured on the band's debut album cover, also done by Derek Riggs.[259] Eddie was painted exclusively by Riggs until 1992, at which point the band began using artwork from other artists, including Melvyn Grant.[98]

A large puppet version of Eddie has appeared many times during carnival celebrations in Rio de Janeiro and other South American cities.[260] During the Cavalcade of Magi 2021 in the Spanish city of Cadiz, next to dolls representing characters known from the world of pop culture, there was a huge, inflatable mummy inspired by the image of the Iron Maiden mascot from 1985.[261][262][263]

Iron Maiden's distinct logo has adorned all of the band's releases since their debut, 1979's The Soundhouse Tapes EP. The typeface originates with Vic Fair's poster design for the 1976 science fiction film, The Man Who Fell to Earth,[264] also used by Gordon Giltrap, although Steve Harris claims he designed it himself, using his training as an architectural draughtsman.[265]

Iron Maiden have influenced numerous artists and bands representing different genres of rock and metal music. Kiss co-founder Paul Stanley said Iron Maiden "have helped spawn an entire genre of music" and influenced literally thousands of other artists.[266][267] According to Guitar World, Iron Maiden's music has "influenced generations of newer metal acts, from legends like Metallica to current stars like Avenged Sevenfold."[268] Metallica members Lars Ulrich, Kirk Hammett, and Jason Newsted have cited Iron Maiden as a major influence on their work.[269][270][271][272] Other bands and artists directly influenced by Iron Maiden include Ghost,[273][274] Avenged Sevenfold,[275] In Flames,[276] Anthrax,[277][278] Exodus,[279] and Alice in Chains.[280]

Journalist Geoff Barton says the band's music constituted an important passage between the classic heavy rock school during the 1960s and 1970s, based on rhythm and blues, and contemporary heavy metal, characterised by sub-genre diversification and stylistic eclecticism.[281] Music journalist Götz Kühnemund said "Iron Maiden were (and still are) the inspiration for all the heavy metal bands we know today because they're an intrinsically heavy metal group. They're equally important for those who play power metal, speed, thrash, death, black, nu metal, metal core and hard rock – almost every genre." The journalist stated the band introduced a DIY approach to all rock music.[282] According to Rock 'n Roll Fantasy Camp the style and attitude of Iron Maiden drummer Nicko McBrain has inspired generations of heavy-metal drummers that followed.[283] Music writer, heavy metal expert and radio broadcaster Scott Penfold stated band's "influence on the genre is immeasurable, as they not only inspired subsequent generations of metal bands but also revolutionized live shows with their elaborate stage productions, further cementing their status as pioneers of heavy metal."[284]

Music journalist and the writer Neil Daniels said Iron Maiden "redefined the whole genre blending classic heavy rock influence with punky vibe, twin guitars attack and progressive approach which finally have created the new quality. [The] Band's influence on generations of rock and metal bands cannot be overstated. They elevated metal to an art form, proving that academic and musical inspirations can coexist."[257] The band's profile by the Rock and Roll Hall of Fame says "in the 1980s, Iron Maiden released seven high-octane albums that cemented them as one of the greatest rock bands – creating a blueprint for how heavy metal bands should look, sound and tour."[285] According to Metal Hammer Iron Maiden is the second band to Black Sabbath, which has had the most significant impact on metal and heavy rock music.[286]

The first heavy metal videos broadcast by MTV were the live versions of "Iron Maiden" and "Wrathchild" taken from the official VHS Live at the Rainbow (Iron Maiden).[287][288] In 1989, Iron Maiden took part in the Rock Aid Armenia project (also known as Live Aid Armenia) - a humanitarian project by the British music industry. The project aimed to raise funds to help people affected by the earthquake in Armenia in 1988.[289]

The number of releases in tribute to the British band can be estimated in the hundreds, with an extremely wide range of stylistic variants.[290][291] In 2008, Kerrang! released Maiden Heaven: A Tribute to Iron Maiden, an album composed of Iron Maiden cover songs performed by Metallica, Machine Head, Dream Theater, Trivium, Coheed and Cambria, Avenged Sevenfold, and other groups influenced by the band.[269] In 2010, Maiden uniteD, an acoustic tribute band consisting of members of Ayreon, Threshold and Within Temptation, released Mind the Acoustic Pieces, a re-interpretation of the entire Piece of Mind album.[292] As of 2021 nearly 200 Iron Maiden cover audio-visual releases exist (each featuring various artists), including piano,[293] electro,[294] string quartet[295] and hip-hop tributes.[296]

In March 2025, Netflix will release Run to the Hills, a documentary celebrating Iron Maiden's 50th anniversary. Directed by David Morgan, it will explore the band's rise from their early days in London to global heavy metal icons. The exact release date is yet to be confirmed.[297]

The 1982 release of The Number of the Beast created some controversy for the band. The artwork and title track led to Christian groups in the United States branding the band as Satanists, encouraging people to destroy copies of the release.[47] The band's manager, Rod Smallwood, later said the groups initially burnt the records, but later decided to destroy them with hammers due to fear of breathing in the melting vinyl's fumes.[298] The protests were not restricted to the US, with Christian organisations preventing Iron Maiden from performing in Chile in 1992.[100]

The band have always denied the notion they are Satanists, with lead vocalist, Bruce Dickinson, doing so on-stage in the Live After Death concert video.[69] Steve Harris has since commented that, "It was mad. They completely got the wrong end of the stick. They obviously hadn't read the lyrics. They just wanted to believe all that rubbish about us being Satanists."[45] Harris has also said that "The Number of the Beast" song was inspired by a nightmare he had after watching Damien: Omen II,[299] and also influenced by Robert Burns' "Tam o' Shanter".[48] The band's drummer, Nicko McBrain, has been a born-again Christian since 1999.[300]

For their Somewhere Back in Time World Tour in 2008 and 2009, Iron Maiden commissioned an Astraeus Airlines Boeing 757 as transport.[301] The aeroplane was converted into a combi configuration, which enabled it to carry the band, their crew and stage production, allowing the group to perform in countries which were previously deemed unreachable logistically.[302] It was also repainted with a special Iron Maiden livery,[302] which the airline decided to retain after receiving positive feedback from customers.[303]

The aircraft, named "Ed Force One" after a competition on the band's website,[304] was flown by Dickinson until 2022,[305] as he was also a commercial airline pilot for Astraeus; the plane also appears in the documentary[306] Iron Maiden: Flight 666.[175] For The Book of Souls World Tour in 2016, the band upgraded to an ex-Air France Boeing 747-400 jumbo jet[307] which allows for more space without the aircraft having to undergo a significant conversion to carry their equipment.[308]

In 2025, the band announced that the 747-400 they used during 2016's The Book of Souls World Tour would be scrapped, and keychains constructed from the plane's parts could be collected by fans. "She's been scrapped but bits of her will live on," Dickinson said. The keychains were sold for €66.66, which is a reference to the band's album The Number of the Beast.[309]

Steve Harris, Iron Maiden's bassist and primary songwriter,[310] has said his influences include Black Sabbath, Deep Purple, Led Zeppelin, Uriah Heep, Pink Floyd, Genesis, Yes, Jethro Tull, Thin Lizzy, UFO, Queen, Wishbone Ash.[311] and Golden Earring.[312] Iron Maiden covered a song by Golden Earring called Kill Me Ce Soir on the B-side of the single "Holy Smoke". In 2010 Harris said, "I think if anyone wants to understand Maiden's early thing, in particular the harmony guitars, all they have to do is listen to Wishbone Ash's Argus album. Thin Lizzy too, but not as much. And then we wanted to have a bit of a prog thing thrown in as well, because I was really into bands like Genesis and Jethro Tull. So you combine all that with the heavy riffs and the speed, and you've got it."[268] In 2004, Harris explained the band's "heaviness" was inspired by "Black Sabbath and Deep Purple with a bit of Zeppelin thrown in."[313] Harris also developed his own playing style, which guitarist Janick Gers describes as "more like a rhythm guitar."[314] Harris's bass technique is responsible for the band's galloping style,[315] heard in such songs as "The Trooper"[316] and "Run to the Hills".[317]

The band's guitarists, Dave Murray, Adrian Smith, and Janick Gers, each have their own individual influences and playing styles. Dave Murray is known for his legato technique which, he says, "evolved naturally. I'd heard Jimi Hendrix using legato when I was growing up, and I liked that style of playing."[318] Stating that he "was inspired by blues rock rather than metal," Adrian Smith was influenced by Johnny Winter and Pat Travers, leading to him becoming a "melodic player."[319] Janick Gers prefers a more improvised style, largely inspired by Ritchie Blackmore,[320] which he says is in contrast to Smith's "rhythmic" sound.[321]

Singer Bruce Dickinson, who typically works in collaboration with guitarist Adrian Smith,[322] has an operatic vocal style, inspired by Arthur Brown, Peter Hammill, Ian Anderson and Ian Gillan,[323] and is often considered to be one of the best heavy metal vocalists of all time.[324] Although Nicko McBrain has only received one writing credit, on the Dance of Death album,[325] Harris often relies on him while developing songs. Adrian Smith commented, "Steve loves playing with him. [They] used to work for hours going over these bass and drum patterns."[326]

Throughout their career, the band's style has remained largely unchanged, although the addition of guitar synthesisers on 1986's Somewhere in Time,[94] keyboards on 1988's Seventh Son of a Seventh Son,[84] and an attempt to return to the "stripped down" production of their earlier material on 1990's No Prayer for the Dying marked some experimentation.[93] In recent years, however, the band have begun using more progressive elements in their songs,[327] which Steve Harris describes as not progressive "in the modern sense, but like Dream Theater, more in a 70s way".[328] Greg Prato of Ultimate-Guitar wrote, "By and large, Iron Maiden's long and lengthy career can be categorized into two separate eras: "punk Maiden" and "prog Maiden".[329] According to Harris, Seventh Son of a Seventh Son was the band's first album which was "more progressive",[330] and they would return to this style in 1995's The X Factor, which he states is "like an extension of Seventh Son..., in the sense of the progressive element to it".[116] The development contrasts with the band's raw-sounding earlier material,[268] which AllMusic states was "clearly drawing from elements of punk rock",[331] although Harris firmly denies this.[332]

Current lineupJames Francis Cagney Jr. (/ˈkæɡni/;[1] July 17, 1899 – March 30, 1986)[2] was an American actor and dancer. On stage and in film, he was known for his consistently energetic performances, distinctive vocal style, and deadpan comic timing. He won acclaim and major awards for a wide variety of performances.[3]

Cagney is remembered for playing multifaceted tough guys in films such as The Public Enemy (1931), Taxi! (1932), Angels with Dirty Faces (1938), The Roaring Twenties (1939), City for Conquest (1940) and White Heat (1949), finding himself typecast or limited by this reputation earlier in his career.[4] He was able to negotiate dancing opportunities in his films and ended up winning the Academy Award for his role in the musical Yankee Doodle Dandy (1942). In 1999 the American Film Institute ranked him eighth on its list of greatest male stars of the Golden Age of Hollywood.[5] Orson Welles described him as "maybe the greatest actor who ever appeared in front of a camera".[6]

In his first professional acting performance in 1919, Cagney was costumed as a woman when he danced in the chorus line of the revue Every Sailor. He spent several years in vaudeville as a dancer and comedian, until he got his first major acting part in 1925. He secured several other roles, receiving good notices, before landing the lead in the 1929 play Penny Arcade. Al Jolson saw him in the play and bought the movie rights, before selling them to Warner Bros. with the proviso that James Cagney and Joan Blondell be able to reprise their stage roles in the movie. After rave reviews, Warner Bros. signed him for an initial $400-a-week, three-week contract; when the executives at the studio saw the first dailies for the film, Cagney's contract was immediately extended.

Cagney's fifth film, The Public Enemy, became one of the most influential gangster movies of the period. Notable for a famous scene in which Cagney pushes half a grapefruit against Mae Clarke's face, the film thrust him into the spotlight. He became one of Hollywood's leading stars and one of Warner Bros.' biggest contracts at the time. In 1938 he received his first Academy Award nomination for Best Actor for his subtle portrayal of the tough guy/man-child Rocky Sullivan in Angels with Dirty Faces. In 1942 Cagney won the Oscar for his energetic portrayal of George M. Cohan in Yankee Doodle Dandy.[7] He was nominated a third time in 1955 for Love Me or Leave Me with Doris Day. Cagney retired from acting and dancing in 1961.  He came out of retirement 20 years later for a part in the movie Ragtime (1981), mainly to aid his recovery from a stroke.[8]

Cagney walked out on Warner Bros. twice over the course of his career, each time returning on much improved personal and artistic terms. In 1935 he sued Warner for breach of contract and signed with Edward L. Alperson's independent company Grand National Pictures. In 1942 he established his own production company, Cagney Productions, before returning to Warner seven years later. In reference to Cagney's refusal to be pushed around, Jack L. Warner called him "the Professional Againster".[9] Cagney also made numerous USO troop tours before and during World War II and served as president of the Screen Actors Guild for two years.[10]

James Francis "Jimmy" Cagney Jr. was born in 1899 on the Lower East Side of Manhattan in New York City. His biographers disagree as to the actual location: either on the corner of Avenue D and 8th Street,[2] or in a top-floor apartment at 391 East 8th Street, the address that is on his birth certificate.[11] His father, James Francis Cagney Sr. (1875–1918), was of Irish descent. At the time of his son's birth, he was a bartender[12] and amateur boxer, although on Cagney's birth certificate, he is listed as a telegraphist.[11] His mother was Carolyn Elizabeth (née Nelson; 1877–1945); her father was a Norwegian ship's captain,[3] and her mother was Irish.[13]

Cagney was the second of seven children, two of whom died within months of their births. He was sickly as an infant—so much so that his mother feared he would die before he could be baptized. He later attributed his sickly health to the poverty his family endured.[12][14] The family moved twice while he was still young, first to East 79th Street, and then to East 96th Street.[15] He was confirmed at St. Francis de Sales Roman Catholic Church in Manhattan; his funeral service would eventually be held in the same church.[16]

The red-haired, blue-eyed Cagney graduated from Stuyvesant High School in New York City in 1918, and attended Columbia College,[17] where he intended to major in Art.[18] He also took German and joined the Student Army Training Corps,[19] but he dropped out after one semester, returning home upon the death of his father during the 1918 flu pandemic.[18]

Cagney held a variety of jobs early in his life: junior architect, copy boy for the New York Sun, book custodian at the New York Public Library, bellhop, draughtsman, and night doorkeeper.[20] He gave all his earnings to his family. While Cagney was working for the New York Public Library, he met Florence James, who helped him into an acting career.[21] Cagney believed in hard work, later stating, "It was good for me. I feel sorry for the kid who has too cushy a time of it. Suddenly he has to come face-to-face with the realities of life without any mama or papa to do his thinking for him."[20]

He started tap dance as a boy (a skill that eventually contributed to his Academy Award) and was nicknamed "Cellar-Door Cagney" after his habit of dancing on slanted cellar doors.[20] He was a good street fighter, defending his older brother Harry, a medical student, when necessary.[12][22] He engaged in amateur boxing, and was a runner-up for the New York state lightweight title. His coaches encouraged him to turn professional, but his mother would not allow it.[23] He also played semi-professional baseball for a local team,[20] and entertained dreams of playing in the Major Leagues.[24]

His introduction to films was unusual. When visiting an aunt who lived in Brooklyn, opposite Vitagraph Studios, Cagney would climb over the fence to watch the filming of John Bunny movies.[20] He became involved in amateur dramatics, starting as a scenery boy for a Chinese pantomime at Lenox Hill Neighborhood House (one of the first settlement houses in the nation) where his brother Harry performed and Florence James directed.[21] He was initially content working behind the scenes and had no interest in performing. One night, however, Harry became ill, and although Cagney was not an understudy, his photographic memory of rehearsals enabled him to stand in for his brother without making a single mistake.[25]

In 1919, while Cagney was working at Wanamaker's Department Store, a colleague saw him dance and informed him about a role in the upcoming production, Every Sailor. It was a wartime play in which the chorus was made up of servicemen dressed as women that was originally titled Ever Sailor. Cagney auditioned for the chorus, although considering it a waste of time, as he knew only one dance step, the complicated Peabody, but he knew it perfectly.[26] This was enough to convince the producers that he could dance, and he copied the other dancers' moves and added them to his repertoire while waiting to go on.[27] He did not find it odd to play a woman, nor was he embarrassed. He later recalled how he was able to shed his own naturally shy persona when he stepped onto the stage: "For there I am not myself. I am not that fellow, Jim Cagney, at all. I certainly lost all consciousness of him when I put on skirts, wig, paint, powder, feathers and spangles."[28]

Had Cagney's mother had her way, his stage career would have ended when he quit Every Sailor after two months; proud as she was of his performance, she preferred that he get an education.[29] Cagney appreciated the $35 a week he was paid, which he later remembered as "a mountain of money for me in those worrisome days."[26][27] In deference to his mother's concerns, he got a job as a brokerage house runner.[27] This did not stop him from looking for more stage work, however, and he went on to audition successfully for a chorus part in the William B. Friedlander musical Pitter Patter,[3][28] for which he earned $55 a week. (He sent $40 to his mother each week.[30]) So strong was his habit of holding down more than one job at a time, that he also worked as a dresser for one of the leads, portered the casts' luggage, and understudied for the lead.[30] Among the chorus line performers was 20-year-old Frances Willard "Billie" Vernon; they married in 1922.[3][28]

The show began Cagney's 10-year association with vaudeville and Broadway. The Cagneys were among the early residents of Free Acres, a social experiment established by Bolton Hall in Berkeley Heights, New Jersey.[31]

Pitter Patter was not hugely successful, but it did well enough to run for 32 weeks, making it possible for Cagney to join the vaudeville circuit. He and Vernon toured separately with a number of different troupes, reuniting as "Vernon and Nye" to do simple comedy routines and musical numbers. "Nye" was a rearrangement of the last syllable of Cagney's surname.[32][33] One of the troupes Cagney joined was Parker, Rand, and Leach, taking over the spot vacated when Archie Leach—who later changed his name to Cary Grant—left.[34][35]

In 1924, after years of touring and struggling to make money, Cagney and Vernon moved to Hawthorne, California, partly for Cagney to meet his new mother-in-law, who had just moved there from Chicago, and partly to investigate breaking into the movies. Their train fares were paid for by a friend, the press officer of Pitter Patter, who was also desperate to act.[36] They were not successful at first; the dance studio Cagney set up had few clients and folded; Vernon and he toured the studios, but there was no interest. Eventually, they borrowed some money and headed back to New York via Chicago and Milwaukee, enduring failure along the way when they attempted to make money on the stage.[36]

Cagney secured his first significant nondancing role in 1925. He played a young tough guy in the three-act play Outside Looking In by Maxwell Anderson, earning $200 a week. As with Pitter Patter, Cagney went to the audition with little confidence he would get the part. At this point, he had had no experience with drama.[37] Cagney felt that he only got the role because his hair was redder than that of Alan Bunce, the only other red-headed performer in New York.[37][38] Both the play and Cagney received good reviews; Life magazine wrote, "Mr. Cagney, in a less spectacular role [than his co-star] makes a few minutes silence during his mock-trial scene something that many a more established actor might watch with profit." Burns Mantle wrote that it "...contained the most honest acting now to be seen in New York."[39]

Following the four-month run of Outside Looking In, the Cagneys were financially secure enough for Cagney to return to vaudeville over the next few years, achieving various success. During this period, he met George M. Cohan, whom he later portrayed in Yankee Doodle Dandy, though they never spoke.[40]

Cagney secured the lead role in the 1926–27 season West End production of Broadway by George Abbott. The show's management insisted that he copy Broadway lead Lee Tracy's performance, despite Cagney's discomfort in doing so, but the day before the show sailed for England, they decided to replace him.[40][41] This was a devastating turn of events for Cagney apart from the logistical difficulties this presented – the couple's luggage was in the hold of the ship and they had given up their apartment. He almost quit show business. As Vernon recalled, "Jimmy said that it was all over. He made up his mind that he would get a job doing something else."[42]

The Cagneys had run-of-the-play contracts, which lasted as long as the play did. Vernon was in the chorus line of the show, and with help from the Actors' Equity Association, Cagney understudied Tracy on the Broadway show, providing them with a desperately needed steady income. Cagney also established a dance school for professionals, and then landed a part in the play Women Go On Forever, directed by John Cromwell, which ran for four months. By the end of the run, Cagney was exhausted from acting and running the dance school.[43]

Cagney had built a reputation as an innovative teacher; when he was cast as the lead in Grand Street Follies of 1928, he was also appointed choreographer. The show received rave reviews[44] and was followed by Grand Street Follies of 1929. These roles led to a part in George Kelly's Maggie the Magnificent, a play the critics disliked, though they liked Cagney's performance. Cagney saw this role (and Women Go on Forever) as significant because of the talented directors he met. He learned "...what a director was for and what a director could do. They were directors who could play all the parts in the play better than the actors cast for them."[45]

Playing opposite Cagney in Maggie the Magnificent was Joan Blondell, who starred again with him a few months later in Marie Baumer's new play, Penny Arcade.[46] While the critics panned Penny Arcade, they praised Cagney and Blondell. Al Jolson, sensing film potential, bought the rights for $20,000. He then sold the play to Warner Bros., with the stipulation that they cast Cagney and Blondell in the film version. Retitled Sinners' Holiday, the film was released in 1930, starring Grant Withers and Evalyn Knapp.[46] Joan Blondell recalled that when they were casting the film, studio head Jack Warner believed that she and Cagney had no future, and that Withers and Knapp were destined for stardom.[47] Cagney was given a $500-a-week, three-week contract with Warner Bros.[48]

In the film, he portrayed Harry Delano, a tough guy who becomes a killer but generates sympathy because of his unfortunate upbringing. This role of the sympathetic "bad" guy was to become a recurring character type for Cagney throughout his career.[49] During filming of Sinners' Holiday, he also demonstrated the stubbornness that characterized his attitude toward the work. He later recalled an argument he had with director John Adolfi about a line: "There was a line in the show where I was supposed to be crying on my mother's breast... [The line] was 'I'm your baby, ain't I?' I refused to say it. Adolfi said 'I'm going to tell Zanuck.' I said 'I don't give a shit what you tell him, I'm not going to say that line.'" They took the line out.[50]

Despite this outburst, the studio liked him, and before his three-week contract was up—while the film was still shooting[51]—they gave Cagney a three-week extension, which was followed by a full seven-year contract at $400 a week.[50] However, the contract allowed Warners to drop him at the end of any 40-week period, effectively guaranteeing him only 40 weeks’ income at a time. As he did when he was growing up, Cagney shared his income with his family.[50] Cagney received good reviews, and immediately played another colorful gangster supporting role in The Doorway to Hell (1930) starring Lew Ayres. The film was a financial hit, and helped to cement Cagney's growing reputation.[52] He made four more movies before his breakthrough role.

Warner Brothers' succession of gangster movie hits, in particular Little Caesar with Edward G. Robinson,[53] culminated in the 1931 film The Public Enemy. Due to the strong reviews he had received in his short film career, Cagney was cast as nice-guy Matt Doyle, opposite Edward Woods as Tom Powers. However, after the initial rushes, the actors switched roles.[53][54] Years later, Joan Blondell recalled that a few days into the filming, director William Wellman turned to Cagney and said "Now you’re the lead, kid!" "Jimmy's charisma was so outstanding", she added.[47] The film cost only $151,000 to make, but it became one of the first low-budget films to gross $1 million.[55]

Cagney received widespread praise for his performance. The New York Herald Tribune described his interpretation as "...the most ruthless, unsentimental appraisal of the meanness of a petty killer the cinema has yet devised."[56] He received top billing after the film,[57] but while he acknowledged the importance of the role to his career, he always disputed the suggestion that it changed the way heroes and leading men were portrayed. He cited Clark Gable's slapping of Barbara Stanwyck six months earlier (in Night Nurse) as more important.[58] Night Nurse was actually released three months after The Public Enemy. Gable's character punched Stanwyck's, knocking the nurse unconscious.

Many critics view the scene in which Cagney pushes half a grapefruit into Mae Clarke's face as one of the most famous moments in movie history.[17][54][59][60] The scene itself was a late addition, and the origin of the idea is a matter of debate: producer Darryl Zanuck claimed he thought of it in a script conference, Wellman said the idea came to him when he saw the grapefruit on the table during the shoot, and writers Glasmon and Bright claimed it was based on the real life of gangster Hymie Weiss, who threw an omelette into his girlfriend's face. Joan Blondell recalled that the change was made when Cagney decided the omelette wouldn't work.[47] Cagney himself usually cited the writers' version, but the fruit's victim, Clarke, agreed that it was Wellman's idea, saying, "I'm sorry I ever agreed to do the grapefruit bit. I never dreamed it would be shown in the movie. Director Bill Wellman thought of the idea suddenly. It wasn't even written into the script."[61] However, according to Turner Classic Movies (TCM), the grapefruit scene was a practical joke that Cagney and costar Mae Clarke decided to play on the crew while the cameras were rolling. Wellman liked it so much that he left it in. TCM also notes that the scene made Clarke's ex-husband, Lew Brice, very happy. "He saw the film repeatedly just to see that scene, and was often shushed by angry patrons when his delighted laughter got too loud."[62]

Cagney's stubbornness became well known behind the scenes, especially after he refused to join in a 100% participation-free charity drive[63] pushed by Douglas Fairbanks Jr. Cagney did not object to donating money to charity, but he did object to being forced to give. Already he had acquired the nickname "The Professional Againster".[64][65]

Warner Bros. was quick to team its two rising gangster stars, Edward G. Robinson and Cagney, for the 1931 film Smart Money. Eager to follow the success of Robinson's Little Caesar, the studio filmed Smart Money concurrently with The Public Enemy.[66]

With the introduction of the Motion Picture Production Code of 1930 that placed limits upon on-screen violence, Warner Bros. allowed Cagney a change of pace, casting him in the comedy Blonde Crazy, again opposite Blondell.[67]

The Public Enemy was an enormous box-office success, and Cagney began to compare his pay with that of his peers, believing that his contract allowed for salary adjustments based on the success of his films. However, Warner Bros. refused to allow him a pay raise. The studio heads also insisted that Cagney continue promoting their films, even those in which he did not appear, despite his opposition. Cagney returned to New York, leaving his brother Bill to look after his apartment.[68]

While Cagney was in New York, his brother, who had effectively become his agent, sought a substantial pay raise and more personal freedom for him. Following the success of The Public Enemy and Blonde Crazy, Warner Bros. offered Cagney a contract for $1,000 per week.[69] Cagney's first film upon returning from New York was Taxi! (1932), a critical success in which Cagney danced for the first time on screen. It also marked the last time that he permitted live ammunition to be shot at him, a relatively common occurrence at the time, as blank cartridges and squibs were rare and expensive. During filming for Taxi!, he was almost hit by gunfire.[70] In the film's opening scene, Cagney speaks fluent Yiddish, a language that he had learned during childhood in New York City.[16][70]

"I never said, 'Mmm, you dirty rat!' What I actually did say was 'Judy, Judy, Judy.'"

Blonde Crazy and Taxi! contain lines that became the basis of many misquoted celebrity impersonations of Cagney. He never said "Mmm, you dirty rat!" on film; in Blonde Crazy, he says: "That dirty, double-crossin' rat!"[67] and in Taxi!, he says: "Come out and take it, you dirty, yellow-bellied rat, or I'll give it to you through the door!" The quote from Blonde Crazy was nominated for the American Film Institute's 2005 AFI's 100 Years...100 Movie Quotes list.[71]

The film was swiftly followed by The Crowd Roars and Winner Take All.[citation needed]

Despite his success, Cagney remained dissatisfied with his contract. He wanted more money for his successful films, but he also offered to take a smaller salary should his star wane.[72][73] Warner Bros. refused, so Cagney once again walked out. He held out for $4000 a week,[72] the same salary as Edward G. Robinson, Douglas Fairbanks Jr., and Kay Francis.[73] Warner Bros. refused to cave in this time, and suspended him. Cagney announced that he would do his next three pictures for free if they canceled the five years remaining on his contract. He also threatened to quit Hollywood and go back to Columbia University to follow his brothers into medicine. After six months of suspension, Frank Capra brokered a deal that increased Cagney's salary to around $3000 a week, and guaranteed top billing and no more than four films a year.[74]

Having learned about the block-booking studio system that virtually guaranteed the studios huge profits, Cagney was determined to spread the wealth.[75][76] He regularly sent money and goods to old friends from his neighborhood, though he did not generally make this known.[77] His insistence on no more than four films a year was based on his having witnessed actors—even teenagers—regularly being worked 100 hours a week to turn out more films. This experience was an integral reason for his involvement in forming the Screen Actors Guild in 1933.[citation needed]

Cagney returned to the studio and made Hard to Handle (1933). This was followed by a steady stream of crowd-pleasing films, including the highly regarded Footlight Parade,[78] which gave Cagney the chance to return to his song-and-dance roots. The film includes show-stopping scenes with Busby Berkeley-choreographed routines.[79] In 1934, Here Comes the Navy paired him with Pat O'Brien for the first of nine films together. The two would have an enduring friendship.[80] Also in 1934, Cagney made his first of two raucous comedies with Bette Davis, Jimmy the Gent, for which he had himself heavily made up with thick eyebrows and procured an odd haircut for the period without the studio's permission, shaved on the back and sides. Cagney initially had the make-up department put prominent scars on the back of his head for a close-up but the studio demanded that he remove them. Cagney's and Davis's fast-paced scenes together were particularly energetic.

In 1935 Cagney was listed as one of the Top Ten Moneymakers in Hollywood for the first time,[81] and was cast more frequently in non-gangster roles; he played a lawyer who joins the FBI in G-Men, and he also took on his first, and only, Shakespearean role, as top-billed Nick Bottom in A Midsummer Night's Dream alongside Joe E. Brown as Francis Flute and Mickey Rooney as Puck.

Cagney's last movie in 1935 was Ceiling Zero, his third film with Pat O'Brien. O'Brien received top billing, which was a clear breach of Cagney's contract. This, combined with the fact that Cagney had made five movies in 1934, again against his contract terms, caused him to bring legal proceedings against Warner Bros. for breach of contract.[82][83] The dispute dragged on for several months. Cagney received calls from David Selznick and Sam Goldwyn, but neither felt in a position to offer him work while the dispute went on.[82] Meanwhile, while being represented by his brother William in court, Cagney went back to New York to search for a country property where he could indulge his passion for farming.[82]

Cagney spent most of the next year on his farm, and went back to work only when Edward L. Alperson of Grand National Pictures, a newly established, independent studio, approached him to make movies for $100,000 a film and 10% of the profits.[84][85] Cagney made two features for Grand National: the crime drama Great Guy (1936) with Cagney as a federal inspector, and the musical Something to Sing About (1937) with Cagney as a bandleader and dancer. He received good reviews for both.[86][87]

Cagney might have continued with Grand National but the studio, having spent lavishly on the Cagney films, couldn't recoup the production costs. Grand National usually made low-budget features for small, neighborhood theaters, and the Cagney films proved too expensive for the intended market. Grand National had acquired a promising story property from author Rowland Brown, Angels with Dirty Faces, for $30,000.[88] Cagney was slated to star in the film version but, with the studio in financial trouble, the project went no further. Cagney took the script to Warner Bros., which bought it from Grand National and filmed it in 1938.

Cagney also became involved in political causes, and in 1936, agreed to sponsor the Hollywood Anti-Nazi League.[89] Unknown to Cagney, the League was in fact a front organization for the Communist International (Comintern), which sought to enlist support for the Soviet Union and its foreign policies.[89][90]

The courts eventually decided the Warner Bros. lawsuit in Cagney's favor. He had done what many thought unthinkable: taking on the studios and winning.[91] Not only did he win, but Warner Bros. also knew that he was still their foremost box office draw and invited him back for a five-year, $150,000-a-film deal, with no more than two pictures a year. Cagney also had full say over what films he did and did not make.[92] Additionally, William Cagney was guaranteed the position of assistant producer for the movies in which his brother starred.[93]

Cagney had demonstrated the power of the walkout in holding the studios to their word. He later explained his reasons, saying, "I walked out because I depended on the studio heads to keep their word on this, that, or other promise, and when the promise was not kept, my only recourse was to deprive them of my services."[94] Cagney himself acknowledged the importance of the walkout for other actors in breaking the dominance of the studio system. Normally, when a star walked out, the time he or she was absent was added onto the end of an already long contract, as happened with Olivia de Havilland and Bette Davis.[76] Cagney, however, walked out and came back to a better contract. Many in Hollywood watched the case closely for hints of how future contracts might be handled.[95]

Cagney's two films of 1938, Boy Meets Girl and Angels with Dirty Faces, both costarred Pat O'Brien. The former was a fast-paced farce with a Hollywood theme, with Cagney and O'Brien playing for laughs, and received mixed reviews. Warner Bros. had allowed Cagney his change of pace,[96] but was anxious to get him back to playing tough guys, which was more lucrative.

Cagney starred as Rocky Sullivan, a gangster fresh out of jail and looking for his former associate, played by Humphrey Bogart, who owes him money. While revisiting his old haunts, he runs into his old friend Jerry Connolly, played by O'Brien, who is now a priest concerned about the Dead End Kids' futures, particularly as they idolize Rocky. After a messy shootout, Sullivan is eventually captured by the police and sentenced to death in the electric chair. Connolly pleads with Rocky to "turn yellow" on his way to the chair so the Kids will lose their admiration for him, and hopefully avoid turning to crime. Sullivan refuses, but on his way to his execution, he breaks down and begs for his life. It is unclear whether this cowardice is real or just feigned for the Kids' benefit. Cagney himself refused to say, insisting he liked the ambiguity.[97] The film is regarded by many as one of Cagney's finest,[98] and garnered him an Academy Award for Best Actor nomination for 1938. He lost to Spencer Tracy in Boys Town. Cagney had been considered for the role, but lost out on it due to his typecasting.[99] (He also lost the role of Notre Dame football coach Knute Rockne in Knute Rockne, All American to his friend Pat O'Brien for the same reason.[99]) Cagney did, however, win that year's New York Film Critics Circle Award for Best Actor.

His earlier insistence on not filming with live ammunition proved to be a good decision. Having been told while filming Angels with Dirty Faces that he would be doing a scene with real machine gun bullets (a common practice in the Hollywood of the time), Cagney refused and insisted the shots be added afterwards. As it turned out, a ricocheting bullet passed through exactly where his head would have been.[100][101]

During his first year back at Warner Bros., Cagney became the studio's highest earner, making $324,000.[102] He starred with George Raft in the smash hit Each Dawn I Die, an extremely entertaining prison movie that was so successful at the box office that it prompted the studio to offer Raft an important contract in the wake of his departure from Paramount. In addition, Cagney made The Oklahoma Kid, a memorable Western with Humphrey Bogart as the black-clad villain. Cagney completed his first decade of movie-making in 1939 with The Roaring Twenties, his first film with Raoul Walsh and his last with Bogart. After The Roaring Twenties, it would be a decade before Cagney made another gangster film. Cagney again received good reviews; Graham Greene stated, "Mr. Cagney, of the bull-calf brow, is as always a superb and witty actor".[103] The Roaring Twenties was the last film in which Cagney's character's violence was explained by poor upbringing, or his environment, as was the case in The Public Enemy. From that point on, violence was attached to mania, as in White Heat.[103] In 1939 Cagney was second to only Gary Cooper in the national acting wage stakes, earning $368,333.[104]

In 1940, Cagney portrayed a boxer in the epic thriller City for Conquest with Ann Sheridan as Cagney's leading lady, Arthur Kennedy in his first screen role as Cagney's younger brother attempting to compose musical symphonies, Anthony Quinn as a brutish dancer, and Elia Kazan as a flamboyantly dressed young gangster originally from the local neighborhood. The well-received film with its shocking plot twists features one of Cagney's most moving performances. 
Later the same year, Cagney and Sheridan reunited with Pat O'Brien in Torrid Zone, a turbulent comedy set in a Central American country in which a labor organizer is turning the workers against O'Brien's character's banana company, with Cagney's "Nick Butler" intervening. The supporting cast features Andy Devine and George Reeves.

Cagney's third film in 1940 was The Fighting 69th, a World War I film about a real-life unit with Cagney playing a fictional private, alongside Pat O'Brien as Father Francis P. Duffy, George Brent as future OSS leader Maj. "Wild Bill" Donovan, and Jeffrey Lynn as famous young poet Sgt. Joyce Kilmer. Alan Hale Sr., Frank McHugh and Dick Foran also appear. In 1941, Cagney and Bette Davis reunited for a comedy set in the contemporary West titled The Bride Came C.O.D., followed by a change of pace with the gentle turn-of-the-century romantic comedy The Strawberry Blonde (1941) featuring songs of the period and also starring Olivia de Havilland and rising young phenomenon Rita Hayworth, along with Alan Hale Sr. and Jack Carson.

"Smart, alert, hard-headed, Cagney is as typically American as Cohan himself... It was a remarkable performance, probably Cagney's best, and it makes Yankee Doodle a dandy"

In 1942, Cagney portrayed George M. Cohan in Yankee Doodle Dandy, a film Cagney "took great pride in"[106] and considered his best.[107] Producer Hal Wallis said that having seen Cohan in I'd Rather Be Right, he never considered anyone other than Cagney for the part.[108] Cagney, though, insisted that Fred Astaire had been the first choice, but turned it down.[108][109] Many critics of the time and since have declared it Cagney's best film, drawing parallels between Cohan and Cagney; they both began their careers in vaudeville, struggled for years before reaching the peak of their profession, were surrounded with family and married early, and both had a wife who was happy to sit back while he went on to stardom.[110][111] The film was nominated for eight Academy Awards and won three, including Cagney's for Best Actor. In his acceptance speech, Cagney said, "I've always maintained that in this business, you're only as good as the other fellow thinks you are. It's nice to know that you people thought I did a good job. And don't forget that it was a good part, too."[112]

Filming began the day after the attack on Pearl Harbor, and the cast and crew worked in a "patriotic frenzy"[108] as the United States' involvement in World War II gave the workers a feeling that "they might be sending the last message from the free world", according to actress Rosemary DeCamp.[113] Cohan was given a private showing of the film shortly before his death, and thanked Cagney "for a wonderful job,"[114] exclaiming, "My God, what an act to follow!"[115] A paid première, with seats ranging from $25 to $25,000, raised $5,750,000 for war bonds for the US treasury.[116][117]

Cagney announced in March 1942 that his brother William and he were setting up Cagney Productions to release films through United Artists.[84][118] Free of Warner Bros. again, Cagney spent some time relaxing on his farm in Martha's Vineyard before volunteering to join the USO. He spent several weeks touring the US, entertaining troops with vaudeville routines and scenes from Yankee Doodle Dandy.[119] In September 1942, he was elected president of the Screen Actors Guild.

Almost a year after its creation, Cagney Productions produced its first film, Johnny Come Lately, in 1943. While the major studios were producing patriotic war movies, Cagney was determined to continue dispelling his tough-guy image,[120] so he produced a movie that was a "complete and exhilarating exposition of the Cagney 'alter-ego' on film".[121] According to Cagney, the film "made money but it was no great winner", and reviews varied from excellent (Time) to poor (New York's PM).[122]

"I'm here to dance a few jigs, sing a few songs, say hello to the boys, and that's all."

Following the film's completion, Cagney went back to the USO and toured US military bases in the UK. He refused to give interviews to the British press, preferring to concentrate on rehearsals and performances. He gave several performances a day for the Army Signal Corps of The American Cavalcade of Dance, which consisted of a history of American dance, from the earliest days to Fred Astaire, and culminated with dances from Yankee Doodle Dandy.

The second movie Cagney's company produced was Blood on the Sun. Insisting on doing his own stunts, Cagney required judo training from expert Ken Kuniyuki and from Jack Halloran, a former policeman.[124] He continued to study judo for some time after the film was finished.[125] His use of actual judo throws and holds in the movie has been noted as the first appearance of eastern martial arts in Western film.[citation needed] The Cagneys had hoped that an action film would appeal more to audiences, but it fared worse at the box office than Johnny Come Lately. At this time, Cagney heard of young war hero Audie Murphy, who had appeared on the cover of Life magazine.[126] Cagney thought that Murphy had the looks to be a movie star, and suggested that he come to Hollywood. Cagney felt, however, that Murphy could not act, and his contract was loaned out and then sold.[127]

While negotiating the rights for his third independent film, Cagney starred in 20th Century Fox's 13 Rue Madeleine for $300,000 for two months of work.[128] The wartime spy film was a success, and Cagney was keen to begin production of his new project, an adaptation of William Saroyan's Broadway play The Time of Your Life. Saroyan himself loved the film, but it was a commercial disaster, costing the company half a million dollars to make;[129] audiences again struggled to accept Cagney in a nontough-guy role.[129][130]

Cagney Productions was in serious trouble; poor returns from the produced films, and a legal dispute with Sam Goldwyn Studio over a rental agreement[129][130] forced Cagney back to Warner Bros. He signed a distribution-production deal with the studio for the film White Heat,[130] effectively making Cagney Productions a unit of Warner Bros.[93]

Cagney's portrayal of Cody Jarrett in the 1949 film White Heat is one of his most memorable.[131][132] Cinema had changed in the 10 years since Walsh last directed Cagney (in The Strawberry Blonde), and the actor's portrayal of gangsters had also changed. Unlike Tom Powers in The Public Enemy, Jarrett was portrayed as a raging lunatic with few if any sympathetic qualities.[133] In the 18 intervening years, Cagney's hair had begun to gray, and he developed a paunch for the first time. He was no longer a dashing romantic commodity in precisely the same way he obviously was before, and this was reflected in his performance.[133] Cagney himself had the idea of playing Jarrett as psychotic; he later stated, "it was essentially a cheapie one-two-three-four kind of thing, so I suggested we make him nuts. It was agreed so we put in all those fits and headaches."[134]

Cagney's final lines in the film – "Made it, Ma! Top of the world!" – was voted the 18th-greatest movie line by the American Film Institute. Likewise, Jarrett's explosion of rage in prison on being told of his mother's death is widely hailed as one of Cagney's most memorable performances.[132][135] Some of the extras on set actually became terrified of the actor because of his violent portrayal.[132] Cagney attributed the performance to his father's alcoholic rages, which he had witnessed as a child, as well as someone that he had seen on a visit to a mental hospital.[132]

"[A] homicidal paranoiac with a mother fixation"

The film was a critical success, though some critics wondered about the social impact of a character that they saw as sympathetic.[136] Cagney was still struggling against his gangster typecasting. He said to a journalist, "It's what the people want me to do. Some day, though, I'd like to make another movie that kids could go and see."[137] However, Warner Bros., perhaps searching for another Yankee Doodle Dandy,[137] assigned Cagney a musical for his next picture, 1950's The West Point Story with Doris Day, an actress he admired.[138]

His next film, Kiss Tomorrow Goodbye, was another gangster movie, which was the first by Cagney Productions since its acquisition. While compared unfavorably to White Heat by critics, it was fairly successful at the box office, with $500,000 going straight to Cagney Productions' bankers to pay off their losses.[139] Cagney Productions was not a great success, however, and in 1953, after William Cagney produced his last film, A Lion Is in the Streets, a drama loosely based on flamboyant politician Huey Long, the company came to an end.[84]

Cagney's next notable role was the 1955 film Love Me or Leave Me, his third with Doris Day, who was top-billed above Cagney for this picture, the first movie for which he'd accepted second billing since Smart Money in 1931. Cagney played Martin "Moe the Gimp" Snyder, a lame Jewish-American gangster from Chicago, a part Spencer Tracy had turned down.[140] Cagney described the script as "that extremely rare thing, the perfect script".[140][141] When the film was released, Snyder reportedly asked how Cagney had so accurately copied his limp, but Cagney himself insisted he had not, having based it on personal observation of other people when they limped: "What I did was very simple. I just slapped my foot down as I turned it out while walking. That's all".[140][141]

His performance earned him another Best Actor Academy Award nomination, 17 years after his first.[7] Reviews were strong, and the film is considered one of the best of his later career. In Day, he found a co-star with whom he could build a rapport, such as he had had with Blondell at the start of his career.[142] Day herself was full of praise for Cagney, stating that he was "the most professional actor I've ever known. He was always 'real'. I simply forgot we were making a picture. His eyes would actually fill up when we were working on a tender scene. And you never needed drops to make your eyes shine when Jimmy was on the set."[142]

Cagney's next film was Mister Roberts, directed by John Ford and slated to star Spencer Tracy. Tracy's involvement ensured that Cagney accepted a supporting role in his close friend's movie, although in the end, Tracy did not take part and Henry Fonda played the titular role instead.[143] Cagney enjoyed working with the film's superb cast despite the absence of Tracy. Major film star William Powell played a rare supporting role as "Doc" in the film, his final picture before retirement from a stellar career that had spanned 33 years, since his first appearance in Sherlock Holmes with John Barrymore in 1922. Cagney had worked with Ford on What Price Glory? three years earlier, and they had gotten along fairly well. However, as soon as Ford had met Cagney at the airport for that film, the director warned him that they would eventually "tangle asses", which caught Cagney by surprise. He later said, "I would have kicked his brains out. He was so goddamned mean to everybody. He was truly a nasty old man."[144] The next day, Cagney was slightly late on set, incensing Ford. Cagney cut short his imminent tirade, saying "When I started this picture, you said that we would tangle asses before this was over. I'm ready now – are you?" Ford walked away, and they had no more problems, though Cagney never particularly liked Ford.[144]

Cagney's skill at noticing tiny details in other actors' performances became apparent during the shooting of Mister Roberts. While watching the Kraft Music Hall anthology television show some months before, Cagney had noticed Jack Lemmon performing left-handed, doing practically everything with his left hand. The first thing that Cagney asked Lemmon when they met was if he was still using his left hand. Lemmon was shocked; he had done it on a whim, and thought no one else had noticed. He said of his co-star, "his powers of observation must be absolutely incredible, in addition to the fact that he remembered it. I was very flattered."[143]

The film was a success, securing three Oscar nominations, including Best Picture, Best Sound Recording and Best Supporting Actor for Lemmon, who won. While Cagney was not nominated, he had thoroughly enjoyed the production. Filming on Midway Island and in a more minor role meant that he had time to relax and engage in his hobby of painting. He also drew caricatures of the cast and crew.[145]

In 1955 Cagney replaced Spencer Tracy on the Western film Tribute to a Bad Man for Metro-Goldwyn-Mayer. He received praise for his performance, and the studio liked his work enough to offer him These Wilder Years with Barbara Stanwyck. The two stars got on well; they had both previously worked in vaudeville, and they entertained the cast and crew off-screen by singing and dancing.[146]

In 1956 Cagney undertook one of his very rare television roles, starring in Robert Montgomery's Soldiers From the War Returning. This was a favor to Montgomery, who needed a strong fall season opener to stop the network from dropping his series. Cagney's appearance ensured that it was a success. The actor made it clear to reporters afterwards that television was not his medium: "I do enough work in movies. This is a high-tension business. I have tremendous admiration for the people who go through this sort of thing every week, but it's not for me."[147]

The following year, Cagney appeared in Man of a Thousand Faces, in which he played a fictionalized version of Lon Chaney. He received excellent reviews, with the New York Journal American rating it one of his best performances, and the film, made for Universal, was a box office hit. Cagney's skill at mimicry, combined with a physical similarity to Chaney, helped him generate empathy for his character.[148][149]

Later in 1957, Cagney ventured behind the camera for the first and only time to direct Short Cut to Hell, a remake of the 1941 Alan Ladd film This Gun for Hire, which in turn was based on the Graham Greene novel A Gun for Sale. Cagney had long been told by friends that he would make an excellent director,[149] so when he was approached by his friend, producer A. C. Lyles, he instinctively said yes. He refused all offers of payment, saying he was an actor, not a director. The film was low budget, and shot quickly. As Cagney recalled, "We shot it in twenty days, and that was long enough for me. I find directing a bore, I have no desire to tell other people their business".[150]

In 1959 Cagney played a labor leader in what proved to be his final musical, Never Steal Anything Small, which featured a comical song and dance duet with Cara Williams, who played his girlfriend.

For Cagney's next film, he traveled to Ireland for Shake Hands with the Devil, directed by Michael Anderson. Cagney had hoped to spend some time tracing his Irish ancestry, but time constraints and poor weather meant that he was unable to do so. The overriding message of violence inevitably leading to more violence attracted Cagney to the role of an Irish Republican Army commander, and resulted in what some critics would regard as the finest performance of his final years.[151]

Cagney's career began winding down, and he made only one film in 1960, the critically acclaimed The Gallant Hours, in which he played Admiral William F. "Bull" Halsey. The film, although set during the Guadalcanal Campaign in the Pacific Theater during World War II, was not a war film, but instead focused on the impact of command. Cagney Productions, which shared the production credit with Robert Montgomery's company, made a brief return, though in name only. The film was a success, and The New York Times's Bosley Crowther singled out its star for praise: "It is Mr. Cagney's performance, controlled to the last detail, that gives life and strong, heroic stature to the principal figure in the film. There is no braggadocio in it, no straining for bold or sharp effects. It is one of the quietest, most reflective, subtlest jobs that Mr. Cagney has ever done."[152][153]

Cagney's penultimate film was a comedy. He was hand-picked by Billy Wilder to play a hard-driving Coca-Cola executive in the film One, Two, Three.[154] Cagney had concerns with the script, remembering back 23 years to Boy Meets Girl, in which scenes were reshot to try to make them funnier by speeding up the pacing, with the opposite effect. Cagney received assurances from Wilder that the script was balanced. Filming did not go well, though, with one scene requiring 50 takes, something to which Cagney was unaccustomed.[155] In fact, it was one of the worst experiences of his long career. Cagney noted, "I never had the slightest difficulty with a fellow actor. Not until One, Two, Three. In that picture, Horst Buchholz tried all sorts of scene-stealing didoes. I came close to knocking him on his ass."[152] For the first time, Cagney considered walking out of a film. He felt he had worked too many years inside studios, and combined with a visit to Dachau concentration camp during filming, he decided that he had had enough, and retired afterward.[156] One of the few positive aspects was his friendship with Pamela Tiffin, to whom he gave acting guidance, including the secret that he had learned over his career: "You walk in, plant yourself squarely on both feet, look the other fella in the eye, and tell the truth."[157]

Cagney remained in retirement for 20 years, conjuring up images of Jack L. Warner every time he was tempted to return, which soon dispelled the notion. After he had turned down an offer to play Alfred Doolittle in My Fair Lady,[158][159] he found it easier to rebuff others, including a part in The Godfather Part II.[159] He made few public appearances, preferring to spend winters in Los Angeles, and summers either at his Martha's Vineyard farm or at Verney Farms in New York. When in New York, Billie Vernon and he held numerous parties at the Silver Horn restaurant, where they got to know Marge Zimmermann, the proprietress.[160]

Cagney was diagnosed with glaucoma and began taking eye drops, but continued to have vision problems. On Zimmermann's recommendation, he visited a different doctor, who determined that glaucoma had been a misdiagnosis, and that Cagney was actually diabetic. Zimmermann then took it upon herself to look after Cagney, preparing his meals to reduce his blood triglycerides, which had reached alarming levels. Such was her success that, by the time Cagney made a rare public appearance at his American Film Institute Life Achievement Award ceremony in 1974, he had lost 20 pounds (9.1 kg) and his vision had improved.[161] Charlton Heston opened the ceremony, and Frank Sinatra introduced Cagney. So many Hollywood stars attended—said to be more than for any event in history—that one columnist wrote at the time that a bomb in the dining room would have ended the movie industry. In his acceptance speech, Cagney lightly chastised the impressionist Frank Gorshin, saying, "Oh, Frankie, just in passing, I never said 'MMMMmmmm, you dirty rat!' What I actually did say was 'Judy, Judy, Judy!'"—a joking reference to a similar misquotation attributed to Cary Grant.[162]

"I think he's some kind of genius. His instinct, it's just unbelievable. I could just stay at home. One of the qualities of a brilliant actor is that things look better on the screen than the set. Jimmy has that quality."

While at Coldwater Canyon in 1977, Cagney had a minor stroke. After he spent two weeks in the hospital, Zimmermann became his full-time caregiver, traveling with Billie Vernon and him wherever they went.[164] After the stroke, Cagney was no longer able to undertake many of his favorite pastimes, including horseback riding and dancing, and as he became more depressed, he even gave up painting. Encouraged by his wife and Zimmermann, Cagney accepted an offer from the director Miloš Forman to star in a small but pivotal role in the film Ragtime (1981).[165]

This film was shot mainly at Shepperton Studios in Surrey, England, and on his arrival at Southampton aboard the Queen Elizabeth 2, Cagney was mobbed by hundreds of fans. Cunard Line officials, who were responsible for security at the dock, said they had never seen anything like it, although they had experienced past visits by Marlon Brando and Robert Redford.[citation needed]

Despite the fact that Ragtime was his first film in 20 years, Cagney was immediately at ease: Flubbed lines and miscues were committed by his co-stars, often simply through sheer awe. Howard Rollins, who received a Best Supporting Actor Oscar nomination for his performance, said, "I was frightened to meet Mr. Cagney. I asked him how to die in front of the camera. He said 'Just die!' It worked. Who would know more about dying than him?" Cagney also repeated the advice he had given to Pamela Tiffin, Joan Leslie, and Lemmon. As filming progressed, Cagney's sciatica worsened, but he finished the nine-week filming, and reportedly stayed on the set after completing his scenes to help the other actors with their dialogue.[citation needed]

Cagney's frequent co-star, Pat O'Brien, appeared with him on the British chat show Parkinson in the early 1980s and they both made a surprise appearance at the Queen Mother's command birthday performance at the London Palladium in 1980.[166] His appearance onstage prompted the Queen Mother to rise to her feet, the only time she did so during the whole show, and she later broke protocol to go backstage to speak with Cagney directly.[163]

Cagney made a rare TV appearance in the lead role of the movie Terrible Joe Moran in 1984. This was his last role. Cagney's health was fragile and more strokes had confined him to a wheelchair, but the producers worked his real-life mobility problem into the story. They also decided to dub his impaired speech, using the impersonator Rich Little.[167] The film made use of fight clips from Cagney's boxing movie Winner Take All (1932).

In 1920, Cagney was a member of the chorus for the show Pitter Patter, where he met Frances Willard "Billie" Vernon. They married on September 28, 1922, and the marriage lasted until his death in 1986. Frances Cagney died in 1994.[168] In 1940 they adopted a son whom they named James Francis Cagney III, and later a daughter, Cathleen "Casey" Cagney.[169][170] Cagney was a very private man, and while he was willing to give the press opportunities for photographs, he generally spent his personal time out of the public eye.[171]

Cagney's son died from a heart attack on January 27, 1984, in Washington, D.C., two years before his father's death.[172][173] James III had become estranged from him, and they had not seen or talked to one another since 1982.[174][172] Cagney's daughter Cathleen was also estranged from her father during the final years of his life. She died on August 11, 2004.[175]

As a young man, Cagney became interested in farming – sparked by a soil conservation lecture he had attended[18] – to the extent that during his first walkout from Warner Bros., he helped to found a 100-acre (0.40 km2) farm in Martha's Vineyard.[176][177] Cagney loved that no paved roads surrounded the property, only dirt tracks. The house was rather run-down and ramshackle, and Billie was initially reluctant to move in, but soon came to love the place as well. After being inundated by movie fans, Cagney sent out a rumor that he had hired a gunman for security. The ruse proved so successful that when Spencer Tracy came to visit, his taxi driver refused to drive up to the house, saying, "I hear they shoot!" Tracy had to go the rest of the way on foot.[85]

In 1955, having shot three films, Cagney bought a 120-acre (0.49 km2) farm in Stanfordville, Dutchess County, New York, for $100,000, from show-business and Army veteran Lanny Ross.[178] Cagney named it Verney Farm, taking the first syllable from Billie's maiden name and the second from his own surname. He turned it into a working farm, selling some of the dairy cattle and replacing them with beef cattle.[179][180] He expanded it over the years to 750 acres (3.0 km2). Such was Cagney's enthusiasm for agriculture and farming that his diligence and efforts were rewarded by an honorary degree from Florida's Rollins College. Rather than just "turning up with Ava Gardner on my arm" to accept his honorary degree, Cagney turned the tables upon the college's faculty by writing and submitting a paper on soil conservation.[179]

Cagney was born in 1899 (prior to the widespread use of automobiles) and loved horses from childhood. As a child, he often sat on the horses of local deliverymen and rode in horse-drawn streetcars with his mother. As an adult, well after horses were replaced by automobiles as the primary mode of transportation, Cagney raised horses on his farms, specializing in Morgans, a breed of which he was particularly fond.[181]

Cagney was a keen sailor and owned boats that were harbored on both coasts of the U.S.,[182] including the Swift of Ipswich.[183] His joy in sailing, however, did not protect him from occasional seasickness—becoming ill, sometimes, on a calm day while weathering rougher, heavier seas[184] at other times. Cagney greatly enjoyed painting,[185] and claimed in his autobiography that he might have been happier, if somewhat poorer, as a painter than a movie star.[186] The renowned painter Sergei Bongart taught Cagney in his later life and owned two of Cagney's works. Cagney often gave away his work but refused to sell his paintings, considering himself an amateur. He signed and sold only one painting, purchased by Johnny Carson to benefit a charity.[185]

In his autobiography, Cagney said that as a young man, he had no political views, since he was more concerned with from where his next meal was coming.[187] However, the emerging labor movement of the 1920s and 1930s soon forced him to take sides. The first version of the National Labor Relations Act was passed in 1935, and growing tensions between labor and management fueled the movement. Fanzines in the 1930s, however, described his politics as "radical".[188]

This somewhat exaggerated view was enhanced by his public contractual wranglings with Warner Bros. at the time, his joining of the Screen Actors Guild in 1933, and his involvement in the revolt against the so-called "Merriam tax". The "Merriam tax" was an underhanded method of funnelling studio funds to politicians; during the 1934 Californian gubernatorial campaign, the studio executives would "tax" their actors, automatically taking a day's pay from their biggest earners, ultimately sending nearly half a million dollars to the gubernatorial campaign of Frank Merriam. Cagney (as well as Jean Harlow) publicly refused to pay[189][190] and Cagney even threatened that, if the studios took a day's pay for Merriam's campaign, he would give a week's pay to Upton Sinclair, Merriam's opponent in the race.[191]

He supported political activist and labor leader Thomas Mooney's defense fund, but was repelled by the behavior of some of Mooney's supporters at a rally.[187] Around the same time, he gave money for a Spanish Republican Army ambulance during the Spanish Civil War, which he put down to being "a soft touch". This donation enhanced his liberal reputation. He also became involved in a "liberal group...with a leftist slant," along with Ronald Reagan. However, when he and Reagan saw the direction in which the group was heading, they resigned on the same night.[192]

Cagney was accused of being a communist sympathizer in 1934, and again in 1940. The accusation in 1934 stemmed from a letter police found from a local Communist official that alleged that Cagney would bring other Hollywood stars to meetings. Cagney denied this, and Lincoln Steffens, husband of the letter's writer, backed up this denial, asserting that the accusation stemmed solely from Cagney's donation to striking cotton workers in the San Joaquin Valley. William Cagney claimed this donation was the root of the charges in 1940.[193] Cagney was cleared by U.S. Representative Martin Dies Jr. on the House Un-American Activities Committee.[194]

Cagney became president of the Screen Actors Guild in 1942 for a two-year term. He took a role in the Guild's fight against the Mafia and the Chicago Outfit, which had been using the threat of strike action by a mob-controlled labor union to extort protection money from Hollywood studios. His wife, Billie Vernon, once received a phone call telling her that Cagney had died in an automobile accident.[195] According to Cagney, having failed to scare off the Guild and him, the Chicago Outfit allegedly sent a hitman to kill him by dropping a heavy light onto his head. Upon hearing of the rumor of a hit, George Raft made a call, and the contract was supposedly canceled.[195][196]

During World War II, Cagney raised money for war bonds by taking part in racing exhibitions at the Roosevelt Raceway and selling seats for the premiere of Yankee Doodle Dandy.[116][105] He also let the United States Army practice maneuvers at his Martha's Vineyard estate.[197]

After the war, Cagney's politics started to change. He had worked on Democrat Franklin D. Roosevelt's presidential campaigns, including the 1940 presidential election against Wendell Willkie. By the time of the 1948 election, he had become disillusioned with Harry S. Truman, and he voted for Thomas E. Dewey, his first non-Democratic vote.[198] He would also support Ronald Reagan in the 1966 California gubernatorial election.[199]

By 1980, Cagney was contributing financially to the Republican Party, supporting his friend Ronald Reagan's bid for the presidency in the 1980 election.[200] As he got older, Cagney even began referring to himself in his autobiography as "arch-conservative". He regarded his move away from Marxism as "a totally natural reaction once I began to see undisciplined elements in our country stimulating a breakdown of our system... Those functionless creatures, the hippies ... just didn't appear out of a vacuum".[201]

Cagney died of a heart attack at his Dutchess County farm in Stanford, New York, on Easter Sunday 1986, aged 86.[17] A funeral Mass was held at St. Francis de Sales Roman Catholic Church in Manhattan.[16][202] The eulogy was delivered by his close friend, Ronald Reagan, who was also the President of the United States at the time.[16] His pallbearers included boxer Floyd Patterson, dancer Mikhail Baryshnikov (who had hoped to play Cagney on Broadway), actor Ralph Bellamy, and director Miloš Forman. Governor Mario M. Cuomo and Mayor Edward I. Koch were also in attendance at the service.[203]

Cagney was interred in a crypt in the Garden Mausoleum at Cemetery of the Gate of Heaven in Hawthorne, New York.[204]

Cagney won the Academy Award in 1943 for his performance as George M. Cohan in Yankee Doodle Dandy.[205]

For his contributions to the film industry, Cagney was inducted into the Hollywood Walk of Fame in 1960 with a motion pictures star located at 6504 Hollywood Boulevard.[206][207]

In 1974, Cagney received the American Film Institute's Life Achievement Award. Charlton Heston, in announcing that Cagney was to be honored, called him "one of the most significant figures of a generation when American film was dominant, Cagney, that most American of actors, somehow communicated eloquently to audiences all over the world ... and to actors as well."[208]

He received the Kennedy Center Honors in 1980, and a Career Achievement Award from the U.S. National Board of Review in 1981.[209] In 1984, Ronald Reagan awarded him the Presidential Medal of Freedom.[210]

In 1999, the United States Postal Service issued a 33-cent stamp honoring Cagney.[211]

Cagney was among the most favored actors for director Stanley Kubrick and actor Marlon Brando,[212] and was considered by Orson Welles to be "maybe the greatest actor to ever appear in front of a camera."[6] Warner Bros. arranged private screenings of Cagney films for Winston Churchill.[131]

On May 19, 2015, a new musical celebrating Cagney, and dramatizing his relationship with Warner Bros., opened off-Broadway in New York City at the York Theatre.[213] Cagney, The Musical then moved to the Westside Theatre until May 28, 2017.[214][215]Japan Airlines (JAL) is the flag carrier airline of Japan. JAL is headquartered in Shinagawa, Tokyo. Its main hubs are Tokyo's Narita and Haneda airports, as well as secondary hubs in Osaka's Kansai and Itami airports. The JAL group, which includes Japan Airlines, also comprises J-Air, Japan Air Commuter, Japan Transocean Air, Hokkaido Air System, and Ryukyu Air Commuter for domestic feeder services, and JAL Cargo for cargo and mail services.

JAL group operations include scheduled and non-scheduled international and domestic passenger and cargo services to 220 destinations in 35 countries worldwide, including codeshares. The group has a fleet of 279 aircraft. In the fiscal year ended 31 March 2009, the airline group carried over 52 million passengers and over 1.1 million tons of cargo and mail. Japan Airlines, J-Air, JAL Express, and Japan Transocean Air are members of the Oneworld airline alliance network.

JAL was established in 1951 as a government-owned business and became the national airline of Japan in 1953.[6] After over three decades of service and expansion, the airline was fully privatised in 1987. In 2002, the airline merged with Japan Air System (JAS), Japan's third-largest airline, and became the sixth-largest airline in the world by passengers carried. Japan Airlines is currently an official sponsor of Shimizu S-Pulse and Consadole Sapporo & Liverpool.

The original Japan Airlines Co. was established on 1 August 1951, with the government of Japan recognising the need for a reliable air transportation system to help Japan grow in the aftermath of World War II. The airline was founded with an initial capital of ¥100 million; its headquarters were located in Ginza, Chūō, Tokyo. Between 27 and 29 of August, the airline operated invitational flights on a Douglas DC-3 Kinsei, leased from Philippine Airlines. On 25 October, Japan's first post-war domestic airline service was inaugurated, using a Martin 2-0-2 aircraft, named Mokusei, and crew leased from Northwest Orient Airlines subsidiary TALOA.[7]

On 1 August 1953, the National Diet passed the Japan Airlines Company Act (日本航空株式会社法, Nihon Kōkū Kabushiki-gaisha Hō) forming a new state-owned Japan Airlines on 1 October, which assumed all assets and liabilities of its private predecessor.[8][9][10][11] By 1953, the JAL network extended northward from Tokyo to Sapporo and Misawa, and westward to Nagoya, Osaka, Iwakuni, and Fukuoka.[12]

On 2 February 1954, the airline began international flights, carrying 18 passengers from Tokyo to San Francisco on a Douglas DC-6B City of Tokyo via Wake Island and Honolulu.[8][11][13][14] The flights between Tokyo and San Francisco are still Flights 1 and 2, to commemorate its first international service.[15] The early flights were advertised as being operated by American crews and serviced by United Airlines in San Francisco.[16]

The airline, in addition to the Douglas DC-3, Douglas DC-6B, and Martin 2-0-2s, operated Douglas DC-4s and Douglas DC-7Cs during the 1950s.[8] JAL flew to Hong Kong via Okinawa by 1955, having pared down its domestic network to Tokyo, Osaka, Fukuoka, and Sapporo.[17] By 1958, the Hong Kong route had been extended to Bangkok and Singapore.[18] With DC-7Cs, JAL was able to fly nonstop between Seattle and Tokyo in 1959.[19]

In 1960, the airline took delivery of its first jet, a Douglas DC-8 named Fuji, introducing jet service on the Tokyo-Honolulu-San Francisco route. JAL went on to operate a fleet of 51 DC-8s, retiring the last of the type in 1987. Fuji flew until 1974 and was then used as a maintenance training platform until 1989; its nose section was stored at Haneda Airport and eventually put on public display at the JAL Sky Museum in March 2014.[20]

JAL also began flying to Seattle and Hong Kong in 1960. At the end of 1961, JAL had transpolar flights from Tokyo to Seattle, Copenhagen, London, and Paris via Anchorage, Alaska, and to Los Angeles and San Francisco via Honolulu, Hawaii.[21]

During the 1960s, JAL flew to many new cities, including Moscow, New York, and Busan.[8][22][23] DC-8 flights to Europe via Anchorage started in 1961; flights to Europe via India started in 1962, initially with Convair 880s. Under government pressure, Boeing 727s were acquired for domestic services in 1965 to allow the Japan Civil Aviation Bureau to issue an import license for All Nippon Airways' (ANA) own fleet of 727s.[24]

By 1965, Japan Airlines was headquartered in the Tokyo Building in Marunouchi, Chiyoda, Tokyo.[25] Around this time, over half of JAL's revenue was generated on transpacific routes to the United States, and the airline was lobbying the United States for fifth freedom rights to fly transatlantic routes from the East Coast.[26] The transpacific route was extended east from San Francisco to New York in November 1966 and to London in 1967; flights between San Francisco and London ended in December 1972.

Between 1967 and 1969, JAL had an agreement with Aeroflot to operate a joint service between Tokyo and Moscow using a Soviet Tupolev Tu-114.[27] The flight crew included one JAL member, and the cabin crew had five members each from Aeroflot and JAL. The weekly flight started in April 1967.[28]

In 1972, under the 45/47 system (45/47体制, yongo-yonnana taisei), the so-called "aviation constitution" enacted by the Japanese government, JAL was granted flag carrier status to operate international routes. The airline was also designated to operate domestic trunk routes in competition with ANA and Toa Domestic Airlines.[23]

The signing of a civil air transport agreement between China and Japan on 20 April 1974 caused the suspension of air routes between Taiwan and Japan on 21 April. A new subsidiary, Japan Asia Airways, was established on 8 August 1975, and air services between the two countries were restored on 15 September. During the 1970s, the airline bought the Boeing 747 and McDonnell Douglas DC-10 for its growing routes within Japan and to other countries.[22][29]

In the 1980s the airline performed special flights for the Crown Prince Akihito and Crown Princess Michiko of Japan, Pope John Paul II, and Japanese prime ministers. Until the introduction of dedicated government aircraft two Boeing 747-400s operated as Japanese Air Force One and Japanese Air Force Two. During that decade, the airline introduced new Boeing 747-100SR, Boeing 747-SUD, and Boeing 767 jets to the fleet and retired the Boeing 727s and Douglas DC-8s.[30]

In 1978, JAL started flights to São Paulo and Rio de Janeiro via Anchorage and San Juan;[31] The stopover was changed to Los Angeles in 1982 and to New York's John F. Kennedy International Airport in 1999.[32] Until 2009, the airline operated fifth-freedom flights between New York and São Paulo and between Vancouver and Mexico City.[33]

Japan began considering airline deregulation in the late 1970s, with the government announcing the abandoning of the 45/47 system in 1985.[34] In 1987, Japan Airlines was completely privatised, and the other two airlines in Japan, All Nippon Airways and Japan Air System, were permitted to compete with JAL on domestic and international routes. The increased competition resulted in changes to the airline's corporate structure, and it was reorganized into three divisions: international passenger service, domestic passenger service, and cargo (including mail) service.[23][30]

Japan Airlines began the 1990s with flights to evacuate Japanese citizens from Iraq before the start of the Gulf War. In October 1990, Japan Air Charter was established,[30] and in September 1996, an agreement with the Walt Disney Company made Japan Airlines the official airline of Tokyo Disneyland. JAL Express was established in April 1997, with Boeing 737 aircraft.[35] In the 1990s, the airline experienced economic difficulties that stemmed from recessions in the United States and the United Kingdom, as well as a domestic downturn. Despite years of profits since 1986, the airline began to report operating losses in 1992. Cost-cutting, including the formation of the low-cost JAL Express domestic subsidiary and the transfer of tourist operations to JALways (the successor to Japan Air Charter), helped return the airline to profitability in 1999.[23]

In 1997, the airline flew Japanese Prime Minister Ryutaro Hashimoto to Peru to help negotiate in the Japanese embassy hostage crisis. Japan Airlines placed orders for Boeing 777s during the 1990s, allowing for fleet renewal. It was one of eight airlines participating in the Boeing 777 design process, shaping the design to their specifications.[36]

In 2001, Japan Air System and Japan Airlines agreed to merge; and on 2 October 2002, they established a new holding company called Japan Airlines System (日本航空システム, Nihon Kōkū Shisutemu), forming a new core of the JAL Group. Aircraft liveries were changed to match the design of the new JAL Group. At that time, the merged group of airlines was the sixth largest in the world by passengers carried.[37]

On 1 April 2004, JAL changed its name to Japan Airlines International and JAS changed its name to Japan Airlines Domestic. JAS flight codes were changed to JAL flight codes, JAS check-in desks were refitted in JAL livery, and JAS aircraft were gradually repainted. On 26 June 2004, the parent company Japan Airlines System was renamed to Japan Airlines Corporation.[38][39]

Following the merger, two companies operated under the JAL brand: Japan Airlines International (日本航空インターナショナル, Nihon Kōkū Intānashonaru) and Japan Airlines Domestic (日本航空ジャパン, Nihon Kōkū Japan). Japan Airlines Domestic had primary responsibility for JAL's large network of intra-Japan flights, while JAL International operated both international and trunk domestic flights. On 1 October 2006, Japan Airlines International and Japan Airlines Domestic merged into a single brand, Japan Airlines International.[38][40]

The airline applied to join Oneworld on 25 October 2005. Japan Airlines claimed that its Oneworld membership would be in the best interests of the airline's plans to further develop the airline group and its strong commitment to providing the very best to its customers.[41] Japan Airlines, together with Malév and Royal Jordanian, joined the alliance on 1 April 2007.[42]

On 1 April 2008, JAL merged the operations of its subsidiary Japan Asia Airways (JAA) into JAL mainline operations. JAA had operated all JAL group flights between Japan and Taiwan between 1975 and 2008 as a separate entity due to the special political status of Taiwan.[43]

In 2009, Japan Airlines suffered steep financial losses, despite remaining Asia's largest airline by revenue.[44] As a result, the airline embarked on staff cuts and route cutbacks in an effort to reduce costs.[44][45] The carrier also received ¥100 billion through capital injection and credit from the Japanese government as part of the proposed bankruptcy.[46] In September 2009, Japan's Ministry of Land, Infrastructure, Transport and Tourism formed a task force aimed at aiding a corporate turnaround at JAL, which examined various cost-cutting and strategic partnership proposals.[47] Haruka Nishimatsu, the President and CEO of JAL, already known for eschewing many executive perks, cut his salary to the same amount that JAL pilots were earning during the financial crisis.[48]

One proposal considered was to merge JAL with ANA, which would create a single larger international airline and replace Japan Airlines International; however, media reports suggested that ANA would oppose this proposal given its comparatively better financial performance as an independent carrier.[47] The task force also examined possible partnerships with foreign carriers.[47]

After weeks of speculation, JAL applied for protection under the Corporate Rehabilitation Law (the Japanese equivalent of Administration in the United Kingdom or a Chapter 11 bankruptcy filing in the United States) on 19 January 2010. JAL would receive a ¥300 billion cash injection and have debts worth ¥730 billion waived, in exchange for which it will cut its capital to zero, cut unprofitable routes and reduce its workforce by 15,700 employees—a third of its 47,000 total.[46] JAL's main creditors (Mizuho Corporate Bank, Bank of Tokyo-Mitsubishi UFJ and Sumitomo Mitsui Banking Corporation) originally objected to the bankruptcy declaration, but changed their positions after the Enterprise Turnaround Initiative Corporation of Japan recommended court protection, according to a senior bank official.[49][50] Shares of JAL were delisted from the Tokyo Stock Exchange on 20 February 2010.[51][52] At a time, its stock was considered one of "bluest of blue chips" of Japan.[46] At the time, the bankruptcy was the largest Japanese bankruptcy involving a non-financial company and the fourth largest in Japan's history.[46]

Kazuo Inamori, founder of Kyocera and KDDI, took over as CEO of JAL. Transport minister Seiji Maehara personally visited Kyocera headquarters in late 2009 to persuade Inamori to accept the position; task force leader Shinjiro Takagi believed that appointing a proven entrepreneur CEO was necessary to fix the various problems at JAL.[53] Japan Air Commuter president Masaru Onishi was promoted to president of JAL.[54]

In May, JAL began to see an increase in its passenger numbers by 1.1% year-on-year.[55] In August, it was reported that JAL would cut 19,133 jobs from its workforce of 47,000 by the end of March 2015 – whilst also increasing capacity – in an attempt to make the business viable.[56]

Although JAL ultimately exited bankruptcy while remaining in the Oneworld alliance, JAL was seriously considering accepting a strategic investment from Delta Air Lines and joining the SkyTeam alliance during the period between September 2009 and February 2010.[57] JAL also had talks with Skyteam members Air France-KLM and Korean Air regarding their potential involvement.[44][58]

The Delta deal was favored by the Ministry of Land, Infrastructure, Transport and Tourism because Delta had an extensive global network and had the largest Japanese operation of any foreign airline, which it had inherited through its merger with Northwest Airlines.[59] MLITT also supported a transaction with Air France-KLM because it was a "healthier company" than American.[60]

American planned to team up with Oneworld alliance members British Airways and Qantas to make a joint offer to recapitalise JAL.[61] British Airways said that it was attempting to persuade JAL to remain part of Oneworld rather than aligning itself with Delta and SkyTeam,[62] while American CEO Gerard Arpey said that American and Oneworld remained committed to a partnership with Japan Airlines, as long as it remained a major international carrier,[63] and reiterated his encouragement for JAL to stay with Oneworld during ceremonies to welcome Mexicana into the alliance.[64]

In an interview with the Asahi Shimbun on 1 January 2010, JAL president Haruka Nishimatsu stated his preference in forming a partnership with Delta over American,[65] and the Yomiuri Shimbun reported shortly thereafter that JAL and the Japanese government-backed Enterprise Turnaround Initiative Corporation would likely choose to form a business and capital tie-up with Delta, as part of which JAL would enter SkyTeam and reduce its international flight operations in favor of code-share agreements with Delta, and that American Airlines had begun procedures to end negotiations with JAL.[66] Both JAL and American denied the report.[67][68] The Wall Street Journal then reported that American Airlines raised its JAL investment offer by $300 million, to $1.4 billion, and in separate comments to the press, Delta president Ed Bastian said that Delta was "willing and able to raise additional capital through third-party resources."[69]

After JAL filed for bankruptcy, there were further media reports that JAL would leave Oneworld in favour of SkyTeam,[70] but JAL president Masaru Onishi said on 1 February that the new JAL leadership was "seriously reviewing the issue from scratch, without being influenced by previous discussions," and its decision on an alliance partner would be made soon.[71]

On 7 February, several news outlets reported that JAL would decide to keep its alliance with American Airlines and end talks with Delta. Inamori and ETIC officials, according to the reports, decided that switching alliances from Oneworld to Skyteam would be too risky and could hinder JAL's ability to turn around quickly.[72] Two days later, JAL officially announced that it would strengthen its partnership with American, including a joint application for antitrust immunity on transpacific routes. The airline would also fortify its relationship with other partners in the Oneworld alliance.[73]

JAL emerged from bankruptcy protection in March 2011. In July, ETIC selected Nomura Holdings, Daiwa Securities, MUFG Bank, Morgan Stanley, Mizuho Securities, SMBC, and Nikko Securities to underwrite the sale of its equity stake in JAL, without specifying amounts or dates.[74] On 6 January 2012, JAL announced its intent to relist its shares on the Tokyo Stock Exchange in an initial public offering (IPO) of up to ¥1 trillion, which would be the largest offering in Japan in more than a year.[75] The airline completed its IPO on the first section of the Tokyo Stock Exchange (TYO: 9201) on 19 September 2012. The Enterprise Turnaround Initiative Corporation of Japan sold all its holdings (96.5%) in JAL for ¥650 billion, greater than its ¥350 billion investment in 2010.

Following its exit from bankruptcy protection, JAL began several new partnerships within the Oneworld alliance. The transpacific joint venture between JAL and American commenced in April 2011.[76] JAL formed Jetstar Japan, a low-cost carrier joint venture with Qantas subsidiary Jetstar Airways, in July.[77] In 2012, JAL and British Airways parent company International Airlines Group (IAG) submitted applications to the Japanese government and European Union respectively in seeking a joint venture business operation for flights between Japan and Europe.[78] Finnair applied to join the JV with IAG in July 2013, in conjunction with JAL starting new nonstop service to Helsinki.[79]

Between 2020 and 2021, Japan Airlines incorporated numerous safety measures to prevent the spread of COVID-19 pandemic. Japan Airlines undertakes JAL FlySafe hygiene measures to prevent the spread of the virus, to provide all Japan Airlines guests with a safe and secure travel experience. Measures taken by Japan Airlines to protect guests and keep them safe from infection include face masks and face guards worn by airport staff, disinfecting areas around seats, including tables, armrests, screens, and controllers, and sanitizing frequently touched surfaces, such as lavatory doorknobs and faucet handles.[80]

On June 18, 2021, Japan airlines announced it had conducted the first flight with loading 2 different types of Sustainable Aviation Fuel produced domestically in Japan.[81] The flight was directed from Tokyo (Haneda) to Sapporo (Shin-Chitose) and used 3,132 litters (9.1% mixing ratio) of SAF sourced from wood chips and from microalgae.[82] It was the first flight in the world to use biofuel derived from gasified wood chips[83] and to mix two different types of biofuels.[82]

In May 2023, JAL announced its intention to reintroduce dedicated cargo service using Boeing 767-300BCF aircraft converted from its passenger fleet. This service is planned to begin in early 2024.[84] The company retired its previous fleet of Boeing 747 freighters in 2011.[85]

In January 2024, the company announced that Mitsuko Tottori would succeed Yuji Akasaka as president. As of 1 April 2024, Tottori became the first female president in the history of the company. She began her career as a flight attendant at Toa Domestic Airlines in 1985 after graduating from a two-year college. With over 30 years of experience as a flight attendant, Tottori was the senior managing director in charge of cabin safety and passenger service. Akasaka replaced Yoshiharu Ueki as chairman.[87]

The key trends of Japan Airlines are (as at the financial year ending March 31):[88][89]

In addition to its operations under the JAL name, the airline owns five domestic airlines which feed or supplement mainline JAL flights:[91]

Former subsidiaries:

JALUX Inc., established in 1962, is the airline's procurement business which handles various work for the company, including the JAL SELECTION merchandise and in-flight meals and refreshments, supplies for Blue Sky and JAL-DFS shops, aircraft fuel components, cabin services, and in-flight duty-free. JALUX merged with JAS Trading on 1 January 2004, to unify support operations for the JAL group.[92][93][94]

JAL Cargo is the brand of the airline group's freight service and is a member of the WOW cargo alliance with these products: J Speed, General Cargo, and Dangerous Goods.[95] In the fiscal year ended 31 March 2009, the cargo division carried 500,779 tonnes of freight domestically and 627,213 tonnes of freight internationally.[96]

On 1 April 2011, the airline changed its trade name from Japan Airlines International Co., Ltd (株式会社日本航空インターナショナル, Kabushiki-gaisha Nihon Kōkū Intānashonaru) to Japan Airlines Co., Ltd (日本航空株式会社, Nihon Kōkū Kabushiki-gaisha).[97][98] in the first quarter of 2019, JAL launches its low-cost carrier, Zipair Tokyo, which will focus on medium to long-haul destinations. It is estimated to commence operation in summer 2020.

The headquarters, the Nomura Fudosan Tennozu Building (野村不動産天王洲ビル, Nomura Fudōsan Tennōzu Biru), is located on Tennōzu Isle in Higashi Shinagawa, Shinagawa, Tokyo.[99][100] The 26-floor building was a project of the Kajima Corporation.[101] The building, which also has two underground levels, has a land area of 11,670.4 square metres (125,619 sq ft) and a floor area of 82,602.11 square metres (889,121.7 sq ft).[102]

Several divisions of JAL, including JALPAK,[103] JAL Aero-Consulting,[104] and JAL Hotels are located in the building.[105] The building also houses the Japan office of American Airlines.[106] It is also known as the JAL Building (ＪＡＬビルディング, JAL Birudingu), the Japan Airlines Headquarters, and the Shinagawa Kyodo Building.

When JAL was originally established in 1951, its headquarters were in Ginza, Chuo, Tokyo.[107] By 1965, Japan Airlines was headquartered in the Tokyo Building in Marunouchi, Chiyoda, Tokyo.[25][108] The Yomiuri Shimbun stated that because Japan Airlines worked closely with the Japanese government, people mockingly referred to the Tokyo Building as "a branch office of the transport ministry."[109]

On 28 June 1996, construction was completed on the JAL Building. On 27 July 1996, JAL moved its headquarters into the JAL Building. The Flight Operation Center at the JAL Building began on 20 September 1996.[110] A holding company for JAL and Japan Airlines System, a carrier merging into JAL, was established on 2 October 2002; the head office of that company, Japan Airlines System (JALS) (日本航空システム, Nihon Kōkū Shisutemu), was in 2-15-1 Kōnan in Shinagawa Intercity, Minato, Tokyo. On 11 August 2003, the headquarters of JAS moved from Haneda Maintenance Centre 1 to the JAL Building. On 25 November 2003, the JALS headquarters moved to the JAL Building.[111][112] Originally the JAL Building was co-owned by Japan Airlines and Mitsubishi Trading Co.; they co-owned a subsidiary that owned the JAL Building. In 2004, the building was to be sold to Nomura Real Estate for 65 billion yen. The contract date was 1 December 2004, and the handover date was 18 March 2005.[102]

The JAL Subsidiary JALUX Inc. at one time had its headquarters in the JAL Building.[113] One group of employees moved out of the building on 26 July 2010, and one moved out on 2 August 2010.[114]

The JAL livery is called the tsurumaru (鶴丸) or "crane circle." It is an image of a Japanese red-crown crane with its wings extended in full flight. The Tsurumaru JAL logo was created in 1958 by Jerry Huff, the creative director at Botsford, Constantine and Gardner of San Francisco, which had been the advertising agency for Japan Airlines from its earliest days. JAL had used several logos up until 1958. When the airline arranged to buy new Douglas DC-8s, it decided to create a new official logo for the inauguration of its jet service worldwide.

In the creation of the logo, Huff was inspired by the personal crests of Samurai families. In a book he'd been given, We Japanese, he found pages of crests, including the crane. On his choice of the crane, he writes: "I had faith that it was the perfect symbol for Japan Airlines. I found that the Crane myth was all positive—it mates for life (loyalty), and flies high for miles without tiring (strength)".[115]

The tsurumaru livery was in use until 2002 when it was replaced by a livery called the "Arc of the Sun." The livery featured the motif of a rising sun on a creamy parchment-coloured background.[116]

JAL is a strong supporter of UNICEF and expresses its support by having a "We Support UNICEF" logo on each aircraft.[117]

Following its corporate restructuring, Japan Airlines returned to the classic tsurumaru logo starting on 1 April 2011.[118] A Boeing 767-300 (JA8299) was the last remaining aircraft that had the "Arc of the Sun" livery until it was retired in January 2016.

JAL is known for adopting special liveries. A Boeing 747 (JA8908) carried a World Cup soccer livery in 2002.[119] Another Boeing 747 (JA8907) was the Matsui Jet, featuring the famous Japanese baseball player Hideki Matsui in 2003.[120]   One of the airline's Boeing 767-300 (JA8253) was the Expo 2005 aircraft.

Various aircraft in the JAL fleet also carry a Yokoso Japan logo supporting the Visit Japan campaign, in various forms. In late 2005, Japan Airlines began using a Boeing 777 (JA8941), featuring Japanese actor Shingo Katori on one side, and the television series Saiyuki, along with its main character "Goku" on the other side.

JAL has also been known for its liveries featuring Tokyo Disneyland and Tokyo DisneySea, as it is the official airline of the Tokyo Disney Resort. It sponsors the attraction Star Jets (not related to past Star Jets fleet with the old tsurumaru livery), which features a variation of the current livery on the ride vehicles. At one time there were more than six widebody aircraft painted with the special liveries.[121]

Some Boeing 747s of JAL had also been painted with tropical-influenced liveries along with Reso'cha titles.[122] These aircraft were typically used by JALways on charter flights to holiday destinations in the Pacific,[23] such as Hawaii. Reso'cha was a marketing abbreviation for Resort Charter and were formerly known as JAL Super Resort Express.

In April 2007, JAL debuted a Boeing 777-300 (JA8941, since moved to JA752J) with a special Oneworld livery to promote the airlines's entry to the global airline alliance.[123] Previously this aircraft carried the Shingo Katori and the Saiyuki television livery.[124]

JAL repainted a Boeing 777-200 (JA8984) in 2008 and a Boeing 777-300ER (JA731J)[citation needed] in 2009 to have a green rather than red arc on its tail, along with a green origami airplane on the fuselage, and named them the Eco Jet, to highlight the company's efforts to reduce the environmental impact of commercial aviation.[125] Following the brand image change to the third Tsurumaru livery, JAL redesigned the 2 Eco Jet liveries. JA8984's livery was removed in April 2019 prior to its retirement in December 2019[citation needed] while JA731J's livery was moved onto JA734J (another JAL 777-300ER), which continues flying the Eco Jet livery as of March 2020.[citation needed]

In 2009, JAL repainted JA8941 again, as well as a JTA 737-400 (JA8933) to promote Kobukuro and their new album Calling as well as a live concert tour in Okinawa and around Japan. This livery was released officially on 30 July 2009.[126][127][128]

On 4 September 2010, in conjunction with the Boku no Miteiru Fūkei album, JAL and Arashi (one of their songs, "Movin' On", is used for a commercial) introduced a new livery on one of its Boeing 777-200 JA8982 featuring the five members of Arashi in the aircraft; the first flight was on the next day, 5 September.[129] In May 2019, JAL also painted one of its Boeing 787-9 JA873J the ARASHI HAWAII JET livery, and in November, painted an Airbus A350-900 JA04XJ the 20th ARASHI THANKS JET livery to celebrate the band's 20th anniversary.[130]

On 3 August 2017, JAL announced a new livery on board an Embraer 190 of subsidiary J-Air, in commemoration of the new Despicable Me: Minion Mayhem ride in Universal Studios Japan.[citation needed]

Starting from April 2019, JAL introduced the 'Tokyo 2020, Fly For it!' series of special liveries, in commemoration of the upcoming Tokyo 2020 Olympics and featuring the two mascots of the 2020 Olympics. Two jetliners in the JAL fleet have been painted so far,  JA773J (a Boeing 777-200, painted April 2019)[131] and JA601J (a Boeing 767-300ER, painted July 2019)[132]

In December 2022, JAL and The Walt Disney Company Japan introduced a special livery on its Boeing 767-300ER JA615J in commemoration of the upcoming "Disney 100 Years of Wonder" celebration for Walt Disney Company's 100th anniversary in 2023.[133]

In 1959, Japan Airlines adopted their logo, which is a crane known as the 'Tsuru' crane, along with a livery featuring a white top with the text "JAPAN AIR LINES" in capital italic letters, an exposed-metal bottom, and red and dark blue pinstripes separating the two.[134]

Landor Associates created JAL's 1989 brand identity, along with a livery that featured a new stylized JAL initialism with a red square and grey band on the front of fuselage, and the name "Japan Airlines" featured in small black text. The 1989 livery retained the 'Tsuru' crane logo on the tail but with the same stylised JAL lettering incorporated within it.[134]

After Japan Airlines and Japan Air System merged in 2004, the Tokyo office of Landor and JAL worked together again to create a new brand identity. Landor devised a livery referred to as the "Arc of the Sun". The 2000s rebranding began in April 2002 and was completed in April 2004. The brand identity firm designed 300,000 specific items for JAL.[135] The JAL acronym remained, but it was changed to include a curved bar, which replaced the simple red square and gray rectangle used from 1989. The curved bar was likened to a samurai sword. The tail now featured a quarter sun outlined in silver.[134]

JAL changed its branding again on 1 April 2011 as part of their post-bankruptcy restructuring.[136][137][134] The new livery was reminiscent of the original 1959 one, featured the tsurumaru back on the vertical stabilizer and the full name in capital italic letters above the windows, but did not include the pinstripes or exposed metal bottom, and retained the two-word "JAPAN AIRLINES" spelling over the original "JAPAN AIR LINES".[134] Repainting was completed in January 2016.

Japan Airlines serves 60 domestic destinations and 39 international destinations in Asia, the Americas, Europe and Oceania, excluding codeshare agreements.[138][139] The airline's international hubs are Tokyo's Narita International Airport and Haneda Airport, Osaka's Kansai International Airport and Itami Airport. Japan Airlines and its subsidiaries currently operate over 4,500 international flights and 26,000 domestic flights monthly.[138][140]

In the fiscal year ended 31 March 2009, the airline introduced or increased services on ten international routes, including between Tokyo (Narita) and New York City, and between Osaka (Kansai) and Shanghai; and it ceased operations on four international routes, including between Tokyo (Narita) and Xi'an, and between Osaka (Kansai) and Qingdao. Domestically, JAL suspended 14 routes, including between Sapporo and Okinawa. Additionally, the airline expanded codesharing alliance with fellow Oneworld partners like American Airlines, British Airways, Cathay Pacific and Finnair, and other airlines, including Air France, China Eastern and Jetstar.[141]

In the early years, Narita International Airport had been the main hub of international and freight flights. Nowadays, Haneda Airport is becoming a major international hub due to its close proximity to the Tokyo metropolis, and the heavy expansion occurring there.

Japan Airlines has codeshare agreements with the following airlines:[142][143]

In addition to the above codeshares, Japan Airlines has entered into joint ventures with the following airlines:

In January 2024, Japan Airlines debuted new First, Business, Premium Economy, and Economy class cabins on their A350-1000 fleet of aircraft. These cabins include enclosed suites, manufactured by Safran GB, and in-seat audio in the First and Business class cabins. The A350-1000 includes 6 First class seats in a 1-1-1 layout, 54 Business class seats in a 1-2-1 layout, 24 Premium Economy seats in a 2-4-2 layout, and 155 Economy seats in a 3-3-3 layout.[168]

JAL introduced new international First and Executive Class seats: the JAL Suite for First Class, featured a seat 20 percent roomier than the Skysleeper Solo in a 1-2-1 configuration; and the JAL Shell Flat Neo Seat for Executive Class Seasons, a slightly revised version of the original Shell Flat Seat, with a wider seat; expanded center console; and the world's first in-flight photo art exhibit, Sky Gallery. These seats, along with the Premium Economy seats, debuted on Japan Airlines Flights 5 and 6, operated on the Tokyo–New York route on 1 August 2008. It expanded to the Tokyo–San Francisco route on 13 September 2008, and the Tokyo–Chicago and Los Angeles in 2009.[169] Between 2010 and 2017, the new cabin was also flown on flights from Narita to Jakarta, the only Asian destination for which the new cabin was used during that period.[170]

In 2013, JAL debuted new versions of its economy and premium economy seats called Sky Premium and Sky Wider Economy respectively. The Sky Premium seats, found on selected 777-300s and soon 787s, feature the same width as the Sky Shell seats but with a 4" larger seat pitch of 42" and a 3" larger recline of up to 10" compared to a 38" pitch and 7" recline on the Sky Shell seats. The Sky Wider Economy seats, found on select 767s and select 777-300s, feature slimmer seats with 4" more legroom, and another inch of width totaling up to 35" of seat pitch, and a 19" width compared to the 31" pitch and 18" width of standard economy seats, plus a larger PTV screen of up to 11". The newer 787s will feature a new version of the Sky Wider seats called Sky Wider II, which will feature 5" more legroom and 2" more width totaling up to 36" of seat pitch and a 20" width in a less dense 2-4-2 setup instead of the 3-3-3 setup commonly used in a 787's economy cabin.[171]

In premium cabins, JAL introduced fully lie-flat seats, branded as Sky Suite in Business Class cabin and enhanced First Suite seats in First Class cabin. The Sky Suite is in a staggered 2-3-2 setup that offer direct aisle access to all business class passengers. These can be found on all 13 of JAL's Boeing 777-300ER aircraft (named SS7), 10 of JAL's 787-8 aircraft (named SS8) and eight of JAL's 787-9 aircraft (named SS9). Later in 2015, JAL introduced a new version of Sky Suite, called Sky Suite II, in order to fit lie-flat seats on its new international 767-300ER fleet (named SS6), in a 1-2-1 setup. Since the seats are less wide than the original Sky Suite, SS6 aircraft are often seen on shorter international routes, like inter-Asian routes and Hawaiian routes.[172] In 2016, as JAL was upgrading its Boeing 777-200ER fleet used on selected inter-Asian and Hawaiian flights, JAL introduced a third version of Sky Suite, called Sky Suite III, which is a lie-flat reverse-herringbone arranged seat. This seat is equipped on JAL's most Boeing 777-200ER fleet (named SS2) and five of JAL's Boeing 787-9 fleet (named SS9 II) in a 1-2-1 setup. Like SS6 aircraft, SS2 and SS9 II aircraft are operating on shorter international routes.[173]

The airline's international services with existing cabins feature the fully reclining JAL First Class JAL Suite; JAL Business Class JAL Sky Suite, JAL Sky Suite II, JAL Sky Suite III, JAL Shell Flat NEO, JAL Skyluxe Seat or JAL Skyrecliner; JAL Premium Economy JAL Sky Premium; and JAL Economy Class JAL Economy Class Seat or JAL Sky Wider.[174] The First Class Skysleeper Solo reclines fully and features genuine leather upholstery from Poltrona Frau of Italy.[175] The Executive Class Seasons Shell Flat Seat is a lie-flat design with the ability to lower armrests to the same height as the seat when reclined.[176] It features a shell-shaped seat that allows passengers to recline by sliding their seat forward, without having the seat in front intrude when reclining.[177]

On Japan domestic services, the airline offers First Class, Executive (Business) Class Class J and Economy Class.[178] The First Class seat is made from premium genuine leather with a seat width of about 53 cm (21 in) and a seat pitch of about 130 cm (51 in).[179] Class J features ergonomically designed reclining seats that promote relaxation by allowing passengers to move naturally and maintain a balanced posture.[180]

MAGIC, JAL's in-flight entertainment system, supported by the JAL Mooove! (formerly Entertainment Network),[181] features the latest hit movies and videos, games and audio programs. There are six generations of the MAGIC system: MAGIC-I, MAGIC-II, MAGIC-III, MAGIC-IV, MAGIC-V and the new MAGIC-VI. Introduced on 1 December 2007, the MAGIC-III system provides Audio/Video on Demand (AVOD) entertainment to all passengers. The number of movie, music, video and game channels on MAGIC-III was doubled from 57 to 130 by 2008; and it is installed on all seats on Boeing 767-300ER, 777-200ER and 777-300ER aircraft.[182] Aircraft with MAGIC-I and MAGIC-II have movies that automatically start when the AVOD system is turned on—once the aircraft reaches cruise level—and economy class passengers can tune in to watch the movie in progress; and all movies restart upon completion. Executive and First Class passengers have full AVOD control. MAGIC systems also have JAL's duty-free shopping catalogue, including flight crew recommendations and a video of specials available on the flight.[183] MAGIC-V will feature mostly the same entertainment as MAGIC-III, but with a touch screen controller, along with a handset. There will be USB ports for iPod connectivity, and an easier to control handset. (introduced on selected Boeing 767-300ER routes).
The MAGIC-III system is used on internationally configured Boeing 767-300 with Skyluxe Seat, older internationally configured Boeing 767-300ER with Skyluxe Seat, all Boeing 777-200ER, older Boeing 777-300ER with Skysleeper Solo/Suite first class and Shell Flat Seat/Neo Business class. The MAGIC-IV is used on internationally configured Boeing 737-800s, along with a newer look of Skyluxe Seat. It uses 9-inch touchscreens by Panasonic SFX. The MAGIC-V system is deploying across the fleet, with selected Boeing 767-300ERs (Skyrecliner seat) and B787-8 (Shell Flat Neo seat) getting the IFE. Followed by refurbished Boeing 777-300ERs (all aircraft) and selected Boeing 767-300ER aircraft (including those with Skyluxe seat) will get the MAGIC-V along with new seats in all classes. The MAGIC-VI is installed on selected Boeing 787-8s and 777-300s.[184]

On most JAL international flights, on-plane cameras are available, either on the wings, the belly or on the tail. When the aircraft is in the pushback; taxi; take-off; ascent; descent; stacking; landing; and docking phases of flight, all TV's in the cabin automatically tune into the video camera outside the aircraft to provide "Pilot Vision" to the passengers. However, new entertainment systems do not have them anymore (except the airline's new A350, which does have cameras).[185]

Skyward, the airline group's inflight magazine, reflects the company motto of "Dream Skyward". Before the merger with JAS, JAL's inflight magazine was called Winds.[186] All of the JAL Group magazines are provided by JALUX.[187]


In a promotion, between 1 June and 31 August 2006, all Executive and First Class passengers would be offered the use of Nintendo DS Lites specially manufactured for air travel, with the wireless capabilities removed to conform with airline safety standards.[188] 
After a trial run at Haneda airport, JAL announced it will offer selected passengers a VR experience using the Samsung Gear VR and the Samsung S8. Passengers will be able to experience specially curated programming in Germany, Argentina, the Nagoya fireworks and more programs at a later date.[189]

Japan Airlines offers meals on intercontinental routes, depending on the cabin class, destination and flight length. Western and Japanese menu selections are typically offered, including seasonal menu selections varied by destination.[190][191] The airline has worked with high-profile chefs, including Fumiko Kono, Shinichi Sato, Koji Shimomura, Naoki Uchiyama, Chikara Yamada, Seiji Yamammoto and Hiroki Yoshitake in the creation of its menus and in 2016, launched a children's menu created by Kono, Yamada, Yamammoto, and Yosuke Suga.[192][193]

Sakura Lounge, named after the Japanese word for cherry blossom, is Japan Airlines' signature lounge. In addition, the airline also operates the following international, including First Class Lounge, Sakura Lounge annex and JAL Lounge; and domestic lounges, including Diamond Premier Lounge and JAL Lounge. Access to the lounges depend on the class of travel or the membership status in the JAL Mileage Bank or JAL Global Club.[194][195][196]

Circa the 1990s, JAL previously operated buses from Frankfurt Airport to Düsseldorf to serve customers in that German city,[197] as well as buses from John F. Kennedy Airport in New York City to Fort Lee, New Jersey.[198]

On 25 June 2024 through a press release, Japan Airlines announced its multi-year partnership with Liverpool Football Club, becoming the latter's official airline partner which will benefit the global presence of both entities.[199]

On 24 June 2024, Japan Airlines was voted 2024 World's Best Premium Economy Class by Skytrax.[200]

Japan Airlines has been the focus of several television programs in Japan over the years, most being dramas revolving around cabin attendants. Attention Please was a drama in 1970 that followed the story of a young girl who joined JAL to be a cabin attendant while overcoming many difficulties. This show was remade in 2006 again as Attention Please starring Aya Ueto who joins a class of cabin attendant nominees and later graduates. Most of the action of the story of the 2006 series takes place at JAL's Haneda flight operations headquarters. The series has had two specials since the original, marking the main character's transition into JAL's international operations.[201]

During the 1980s, JAL was also the focus of another drama entitled Stewardess Monogatari which featured another young girl during training to be a JAL cabin attendant.[202] During the 1990s, JAL featured several commercials with celebrities, including Janet Jackson who danced and sang to a backdrop of JAL 747s on rotation at LAX.[203]

In Animal Crossing: New Horizons, Dodo Airlines is a parody of Japan Airlines.Jay Park (Korean name: Park Jae-beom (Korean: 박재범); born April 25, 1987) is an American rapper, singer-songwriter and dancer based in South Korea.[4] He is a member of the Seattle-based b-boy crew Art of Movement (AOM), and founder and former CEO of the independent hip hop record labels AOMG and H1ghr Music, as well as the founder of the record label More Vision.[5] Park returned to South Korea in June 2010 for the filming of Hype Nation,[6] and in July, Park signed a contract with SidusHQ, one of the largest entertainment agencies in South Korea.[7] Rebranding and re-debuting as both a solo singer and a rapper, Park has participated in the underground hip hop culture scene in South Korea, a rarity for both active and former K-Pop idols.[8]

Known for his charismatic performances and stage presence,[9][10] Park has been described as a "born entertainer" by Korean pop singer Patti Kim,[11] and The New York Times quoted the president of digital music distributor DFSB Kollective illustrating Park as "not just an artist, but also his own PR agent, fan club president, and TV network."[12] An influential figure in the Korean hip hop scene, Park has been described as the "scene stalwart" of Korean R&B,[13] and has been credited as one of the main figures responsible for the increased commercial acceptance and mainstream popularization of K-hip hop in South Korea.[14]

Born in Edmonds, Washington, in the Seattle metropolitan area, Park showed great interest in hip hop music and breakdancing at a young age. Park attended Edmonds-Woodway, where he spent most of his break and lunch times practicing dance with friends.[15][16] Park started listening to hip hop and rap music in his early teens, and spent time learning and writing raps himself during high school.[17]  In 2003, Park became one of the first members of the Seattle-based b-boy crew Art of Movement (AOM). Often skipping classes to participate in b-boy competitions, Park would have continuous clashes with his mother regarding his lack of interest in academics and potential higher education. In 2004, Park's mother, seeing how her son spent more time breakdancing than studying, suggested he try out for a locally advertised talent audition, which was organized by South Korean conglomerate JYP Entertainment.[18] With his family financially struggling at the time, Park auditioned for the program, believing it to be a contest where the winner would receive a monetary prize reward.[19] Unbeknownst to Park, the success of his audition would eventually lead to him being officially contacted and selected by JYP Entertainment to be part of a Korean boy band as an idol. In January 2005, Park was brought to South Korea to receive training in dancing, rapping, singing, and the Korean language under the strict supervision of JYPE.[20] Park eventually completed his university education at Dankook University.[21]

Park first appeared through Mnet's Hot Blood Men, a documentary-style reality program that showed the future members of One Day, split as idol groups 2AM and 2PM, in training.  Park, finishing at the top spot with the most fan votes, became the leader of 2PM.[22] On September 4, 2008, 2PM debuted with the song "10 Out of 10" (Korean: 10점 만점에 10점; Revised Romanization: Ship Jeom Manjeome Ship Jeom) on the music program M Countdown after the release of their first mini-album, Hottest Time of the Day, a few days prior.[23]
Aside from 2PM's music activities, Park created the song "Jeong" (정) with Yeeun of the Wonder Girls for the original soundtrack of the television drama Conspiracy in the Court, and featured on V.O.S's "To Luv...".[24] He also participated in special stage performances, such as Navi's "Heart Damage" (마음이 다쳐서; Revised Romanization: Maeumi Dachyeoseo) on May 3, 2009,[25] and K.Will's "One Drop per Second" (Korean: 1초에 한방울; Revised Romanization: Il Choe Hanbangul) on June 20, 2009.[26] In addition to Idol Show and Wild Bunny with fellow 2PM members, he also became a regular cast member in several variety programs, including Star King and Introducing a Star's Friend.[27] In August 2009, he and Kara band member, Nicole Jung, became the new hosts for a cultural variety show called Nodaji.[28]

On September 4, 2009, unfavorable comments about Korea were found on Park's personal Myspace account from 2005. The comments, written in English to a friend, were translated by Korean media and quickly spread across hundreds of news articles. Park expressed deep remorse and shame over his forgotten words and issued an official apology, explained the unhappiness that he experienced during his early days as a trainee in an unfamiliar country where he lacked family, the ability to easily communicate, and an understanding of the culture.[29] Outraged protesters demanded that Park should be removed from 2PM, but JYP CEO Park Jin-young on September 7, 2009, said Jay Park would continue as a part of the group.[30] The following day, Jay Park announced on his official fancafe that he would be leaving the group to calm the situation and return to his hometown of Seattle. He also apologized to the other 2PM members and promised to "come back a better person".[31] Park Jin-young then confirmed that 2PM would continue as a six-member group.[32] Due to the sensitive topic of Jay Park's departure, all 2PM members were withdrawn from their regular appearances on variety shows,[33] and the final episode of their reality show, Wild Bunny, was postponed indefinitely.[34] Park's vocals would not be removed from the older songs he promoted with 2PM, but he would be absent from the newer tracks. The remaining six members of 2PM re-filmed their music video for "Heartbeat" without Park on October 31, 2009.[35]

The title of 2PM's first official album 1:59PM symbolized Park's absence, according to the six remaining members.[36] Through their acceptance speeches at year-end awards shows for "Again & Again", the members thanked Jay Park and reiterated their wish for his return. At the Mnet Asian Music Awards, the group paid homage to him during their performance of "Again & Again" with a spotlight shown over his usual position in the dance formation and his lines left unsung.[37][38]

Soon after Park returned to the United States, the South Korean public changed their perspective on the matter when they realized that his Myspace messages had been severely mistranslated and taken out of context [citation needed], in addition to strong fan support for Park's return.[39] Park was seen at b-boy battles with fellow Art of Movement members during his time in Seattle. However, on February 25, with Park's comeback looking more and more likely, JYP suddenly announced that Jay Park's contract with them had been terminated, citing a separate "personal mistake" that Park had made in 2009.[40] JYP would make reference to this unknown event several times in 2010, but would never elaborate on any details. This termination had been agreed by all six members of 2PM and led to fan boycotts on 2PM-endorsed products. Protests for Park's return to the group began to take place, not only in South Korea, but internationally as well.[41] Various Jay Park-dedicated forums and fansites all over the world organized silent protests and flash dance mobs.[39] Fans also hired a plane with a banner showing "J, what time is it now?" to fly over Seattle, and was broadcast on Seattle-based radio stations.[42][43]

On several occasions, Park was the number one trending topic on Twitter, even topping the Oscars on March 8.[44] Park's fans revealed plans to release a self-produced album in his honor on March 27 to commemorate the 200th day anniversary of his departure from Korea.[45] However, because the album had been in preparation since January and public opinion on 2PM had since changed, the fans decided it would be in the best interest to not release the CD, and instead, mailed 10–20 copies to Seattle, Park's hometown.[46]

Park created his own YouTube channel on March 15, "jayparkaom", with the first upload being his own version of "Nothin' on You", which went viral and reached over 2,000,000 views in less than 24 hours.[47] In Korea, the original song by B.o.B and Bruno Mars topped the Cyworld music chart in a matter of hours upon the video's release.[48] "Nothin' on You" earned $300,000 in sales through the effect of Park's video.[49] On June 15, 2010, B.o.B released "Nothin' on You" featuring Park, in South Korea, where he replaces Bruno Mars' vocals. His YouTube cover helped contribute to much of the song's success in Korea, with more than 5 million copies sold.[50] Park subsequently thanked his fans for their support and continued to urge them not to hate remaining 2PM members.[51]

He appeared with fellow Art of Movement members at an annual Korean-American festival event called Project Korea III: KSA Cinderella Story at Rutgers University, New Jersey, on April 3, along with Ailee and Clara C.[52] Videos of the event were uploaded onto internet portal sites, where footage of Park acting as an MC and dancing to Beyoncé's "Single Ladies (Put a Ring on It)" on stage drew much positive interest.[53] On April 24, Dumbfoundead released a free collaboration track featuring Park and Clara Chung on his website, titled "Clouds".[54]

Los Angeles-based entertainment attorney Ned Sherman, CEO of Digital Media Wire, announced on May 28 that he was representing Park as his legal representative.[55][56] Sherman and his wife Tinzar reached out to Park, after seeing Park's story and feeling bad about what happened to him during the MySpace controversy.[57][58] The Shermans and Park worked on a lot of projects together, including his movie deal for Hype Nation, an endorsement deal with dENiZEN, Levi Strauss & Co.'s new brand, and others.[59][60]

Park returned to Korea on June 18 at Incheon International Airport, to the biggest crowd ever seen at the airport, for the filming of Hype Nation.[6] "Park Jaebeom has returned" became the biggest headline in Korea that day, and "JayIsBack" shot up immediately on the trending topics on Twitter on June 18 at 9:30 AM GMT.[61] Pictures of Park in Hype Nation were released on July 2,[62] and Park was able to meet with the Korean media for interviews for the first time, talking about his current activities.[63] It was also revealed that his single "Demon" would be included in Hype Nation's original soundtrack.[64]

It was reported on July 8 that Park would sign a contract with SidusHQ for his domestic Korean activities in terms of acting and singing;[65] his management stated that he planned to redebut as a rookie artist,[7] and the contract with SidusHQ was finalized on July 16.[66] Park released an EP titled Count on Me (믿어줄래; Revised Romanization: Mideojullae) containing three tracks, including a rearranged Korean version of "Nothin' on You", on July 13.[67] The English and Korean lyrics were written by Park himself.[68] The EP sold 21,989 physical copies on the first day of release, coming in at number one in sales and number seven in the overall ranking of albums released from January to July 13. Without any promotion on music shows, more than 41,316 copies sold, and the EP placed at number 32 on Gaon's year end chart,[69] earning Park approximately 700 million.[70]

Park began working with singer, producer, and fellow AOM member, Cha Cha Malone, releasing "Bestie" in both Korean and English, and a duet titled "Speechless".[71] Also in 2010, Park began collaborations and forming close ties with rappers Dok2 and The Quiett of Illionaire Records, titling the partnership "AOM & 1llionaire".
On September 5, Park participated in the 3rd International Secret Agents Los Angeles concert held in Cerritos, California, together with well-known YouTube celebrities such as Ryan Higa, KevJumba, AJ Rafael, Alyssa Bernal, Far East Movement, and America's Best Dance Crew Season 5 champions, Poreotix. Park's performance drew many positive responses.[72][73]

Park was cast for the 2011 Korean movie Mr. Idol, starring alongside friend and fellow SidusHQ actor, Kim Soo-ro.[74] Park was also one of the performers, alongside SE7EN, Taeyang and Musiq Soulchild, at the Seoul Soul Festival held at the War Memorial of Korea on October 10. Park and Musiq Soulchild also performed the latter's song "Love" at the festival.[75] Park held a charity concert in December called the "White Love Party Concert", with Supreme Team and Dok2 making appearances.[76] Park and Art of Movement performed at "Fever Seoul Live" alongside Dumbfoundead, David Choi, and several international b-boys.[77]

In December, Park was named as Naver's most searched solo singer of the year.[78]

On January 6, Park was announced as the winner in "Best Web Video" Category around the globe for his song "Nothin' on You" at the Mashable Awards, and was also nominated in the "Must Follow Personality" category, finishing second.[79] At the beginning of February, Park was featured in two music videos of girl group, 5dolls, called "Lips Stains" and "It's You". Park was chosen to be the sole opener for Ne-Yo in his first concert in Seoul for the "2011 Hyundai Mall 40th Anniversary Concert" held on March 29. He also cooperated with Ne-Yo for a charity event for children dreaming to become musicians on March 28 at the Dream Academy in Seoul.[80] On March 28, Park was a finalist and received the most votes in the special category "Connecting People Award", a joint venture between Shorty Awards and Nokia. Park also was a finalist in the "Celebrity" category at the same event.[81]

Park released his first Korean mini-album, Take a Deeper Look, in April, with the lead single "Abandoned".[82] Take a Deeper Look debuted at number three in the Billboard World Album Charts and ranked number 26 on the Billboard Heatseekers Albums Chart.[83] Park also made his debut as a solo artist on Korean music shows such as KBS' Music Bank, Mnet's M Countdown, MBC's Music Core and SBS' Inkigayo, returning for the first time in two years. Park made history as the first artist crowned winner at a debut stage, as he won Music Bank on May 6 and won again a week later on May 13.[84][85]

On August 6, Park participated in his first KBS Immortal Songs 2 episode, performing "Candy" by Korean boy band H.O.T. with its leader Moon Hee-joon.[86] Park followed up with his own R&B rendition of "Aemo" by Kim Soo-hee on the next episode, which was well received.[87] On his third episode of Immortal Songs 2, Park performed "Tell me the Truth" by Jinusean with Solbi, Jang Hyuk and Kim Soo-ro, and was the victor for that episode, allowing him to choose the order for the next episode.[88] For his fourth episode, Park again made his own R&B version of "Feel Good Day" by Kim Wan-sun.[89] On his fifth episode, Park performed a remixed version of "Look Back at Me" by Deux with a b-boy dance break.[90] On his sixth episode and final appearance on the show, he performed "Dear, Do Not Change" by Nam Jin and won the trophy for his last performance,[91] before leaving the show to focus on his next album's preparations.[92] It was revealed that Park would return to Immortal Songs 2 for a special episode "King of Kings" with other singers that have claimed the number one spot. This special episode was recorded on November 7 at the KBS Open Hall and aired on November 19.[93]

The single "Demon" was released on September 5 through various Korean digital music websites.[94] The music video was also released on the same day, and Park apologized for its low quality, saying he had no control over the release of both the music video and the song.[95] "Demon" peaked at number 14 on the Gaon Chart[96] and at number 8 on the iTunes R&B/Soul Chart.[97]

Park returned to the US on October 1 to perform at the International Secret Agents Los Angeles concert.[98] On October 29, Park delivered a performance to over 25,000 fans as he headlined the MTV EXIT (End Exploitation and Trafficking) Live in Manila Concert held at the SM Mall of Asia. The concert also featured Californian alternative rock band, Evaline, and international singer-songwriter, Jason Mraz.[99]

Park released his single "Girlfriend", along with the music video via his YouTube channel on November 3.[100] Park performed his single on the same day at the 2011 Style Icon Awards, following a recording of tvN's Taxi.[101] The movie Mr. Idol in which Park starred in was also released on November 3.[102] The single peaked at number 28 on the Gaon chart.[103] Park released Part 1 of his first full-length Korean album, New Breed, with the lead single "Star" (별; Revised Romanization: Byeol) on December 28.[104] "Star" peaked at number one on various sites like Bugs, Olleh Music, and Soribada while being on the top of other charts of music sites.[105] The first part of New Breed also reached the top of charts of various music sites including Olleh Music, Bugs, Soribada and Daum.[104] "Star" peaked at number 17 on the Gaon Chart.[106]

Park received the Disk Bonsang for his mini album Take a Deeper Look on January 11, on the first day of the 26th Golden Disk Awards, which was held in Osaka. He was the only solo artist to receive the Disk Bonsang among the other Bonsang winners.[107] On January 18, he received the Popular Artist Award of the Asia Model Awards, which was held in Seoul.[108][109][110]

Several songs composed and written by Park for other artists were released in 2012. Jay Park produced and composed a song with Cha Cha Malone for Korean-American singer Brian Joo, titled "Can't Stop".[111][112][113] Park wrote the Korean version of the song, while Joo wrote the English Version.[112][113] Park co-wrote the song "4U" for the idol group U-Kiss at the request of main vocalist Soohyun, a long-time friend and fellow former JYP Entertainment trainee, and it was included in U-Kiss's sixth EP DoraDora.[114][115] Park also produced and composed a song for the girl group Tiny-G, entitled "Polaris"; the lyrics were written by actress Lee Si-young. They worked together through MBC's Music and Lyrics.[116][117] On July 3, Korean pop singer Younha released her fourth Korean album, Supersonic, including the track "Driver" written by Park. He also featured in the song, providing rap verses.[118]

He released his first full-length Korean album, New Breed, in February, with the lead single "Know Your Name".[119] The album reached number 1 twice on the Gaon Weekly album chart in South Korea,[120][121] and also reached number 1 on the Hanteo Weekly chart by selling more than 80,000 copies offline in 10 days.[122][123] New Breed debuted at number 4 on the Billboard World Album Charts and ranked number 16 on the Billboard Heatseekers Albums Chart.[124] Park made his comeback stage on Korean music shows on February 16. He won Music Bank on February 24, one week after his comeback.[125] On April 27, Park began Asian promotions for the Asian version release of his album New Breed, and visited 5 countries: Singapore, the Philippines, Indonesia, Malaysia and Japan. On April 28, Jay performed at the 2012 Star Awards in Singapore.[126] On June 13, he released the Japan editions of his albums Take a Deeper Look and New Breed through Universal J. The regular edition of New Breed includes the English version of "I Love You" and the acoustic version of "Know Your Name" as bonus tracks.[127]

Jay Park held his first solo concert, New Breed Live in Seoul, on March 3 at the Olympic Hall of Seoul Olympic Park.[128] Park successfully held a second concert New Breed Asia Tour in Seoul in Seoul on August 18 at the Olympic Hall of Seoul Olympic Park.[129] Park was selected as the 2012 R-16 Korea ambassador by the Korea Tourism Organization on April 23.[126]

He headlined the APAHM tour organized by Verizon in the US in May, including concerts in Washington, D.C., New York City, San Francisco and Los Angeles.[130] On May 16, Park released his first mixtape, Fresh Air: Breathe It,[131] containing songs he performed for the tour.[132] The mixtape was certified Gold by Datpiff within a month since its release, meaning it was downloaded over 100,000 times. Jay Park became the first artist of Asian origin to achieve this result on Datpiff.[133]

Park was the closing act at the MTV World Stage Live in Malaysia, along with Canadian pop singer Justin Bieber, Korean girl band Kara, and Malaysian singer Mizz Nina, on July 14. Park also performed his song "Carefree",[134] which was included on the New Breed Red Edition repackaged album, and also showed his self-choreographed dance break to "Dirty Bass" by Far East Movement featuring Tyga,[9] which he later shared on YouTube.[135] Park also held his own concerts in Sydney and Melbourne at the end of September 2012.[136] Also in July, Park started a web series, Jay Park TV, through his YouTube channel, filmed and edited by Hep, Park's friend and fellow Art of Movement member.[137] The web series shows Park's daily life with his friends, behind-the-scenes of concerts and filming, and the fun and games that he gets up to.

In August, Jay Park released a music video for "New Breed", from the album of the same name, which was recorded during the album's photo shoot with Park featuring Rick Genest-inspired make-up.[138] Park also uploaded a practice video to "I Love You", showing the complex choreography that he and Prepix members were practising for his upcoming concert.[139] On August 22, Park was announced to be a fixed panel member on MBC's Come to Play, along with Kim Eung-soo and Kwon Oh-joong.[140] He joined as a part of a new corner on the show, titled the "Trueman Show".[141] MBC suddenly axed the show after several months, with no prior warning to the cast nor producers.

Later in September, Park performed for the first time in Australia, successfully holding concerts in Sydney and Melbourne.[142] Park returned to Immortal Songs 2 for a "King of Kings" special in October, where he performed "The Woman in Rain" by Shin Jung-hyeon. In a flying visit to Los Angeles, Jay Park made a last-minute appearance in a YouTube sketch by David So, which parodies Wong Fu Productions' The Last.[143]

In November, he featured on "If You Love Me", a digital single by NS Yoon-G. Although Park does not make an appearance in the official music video, he performed on stage with Yoon-G on music shows, with the first being Mnet's M Countdown, and also makes an appearance in the behind-the-scenes music video, showing NS Yoon-G and Park in the studio recording the song.[144] A practice video of the choreography was released soon after.[145]
Park was the host of the December 1 episode of Saturday Night Live Korea, garnering attention for his R-rated skits, acting, and a parody music video of Eminem's "Love the Way You Lie", bringing in the highest ratings of the season for the show.[146]

On February 6, 2013, it was announced that Jay Park would be joining the cast of Saturday Night Live Korea, having previously hosted an episode in late 2012.[147] On Valentine's Day, February 14, Park released the song "Appetizer" alongside a music video. The track was  produced by friend and fellow Art of Movement member, Cha Cha Malone.[148] On the first episode of Saturday Night Live Korea, Park garnered attention for his skit parodying the film Holiday with Korean actor and host of the episode, Choi Min-soo, and for his ad lib during a skit with Shin Dong-yup and Kim Seul-gi.[149][150] on February 25, Mizz Nina's single "Around the World" featuring Jay Park was officially released. The song had been in the works since Park and Mizz Nina had performed at MTV World Stage Live in Malaysia 2012.[151][152] On February 28, Park received the "Musician of the Year" award at the 2013 Korean Music Awards, and his album New Breed was nominated in the "Best R&B and Soul album" category.[153][154]

Park featured on the cover of the March edition of Men's Health Korea magazine, and the cover was subsequently chosen by staff as the best in the magazine's 7-year history.[155] On March 28, it was revealed that Park would be rejoining the cast of KBS' Immortal Songs 2 for a third time.[156]

Following on from the success of the Jay Park's March edition of Men's Health Korea, the same cover was used for the April edition of Men's Health China, which was also the magazine's 10th anniversary edition, and included large promotional posters across the country.[157][158] In collaboration with Ustream Korea, Park held an official livestreamed event titled "Fan & Music Live" on April 2,[159] and revealed the titles of the 3 songs from his upcoming single release; "Joah" (Korean: 좋아; Revised Romanization: Joha), "1 Hunnit" (사실이야; Revised Romanization: Sasiriya), and "Welcome". The title track, "Joah", was revealed to be a mellow feel-good song, and was produced by Cha Cha Malone, with instrumentals provided by Saturday Night Live Korea in-session band Common Ground, "1 Hunnit" was a  rap track featuring Dok2, and "Welcome" was provocative R&B track.[160] On April 10, Park released the single album "Joah", along with the music video for the title track.[161] The music video was filmed in Park's hometown of Seattle, and starred Korean actress Clara Lee.[161][162] On April 16, Park appeared on SBS' Hwasin – Controller of the Heart, the sequel of Strong Heart. On Park's 26th birthday, April 25, Park released a music video for the song "Welcome". On the April 27 episode of Immortal Songs 2, Park performed Lee Moon-sae's "Sunset Glow" with R&B singer and friend Crush.[163] Park's skit on the April 27 episode of Saturday Night Live with Gayoon of 4Minute gained attention for Park's character mistakenly saying inappropriate and sexual words in Korean, and featured a return of Park's in-character ad lib as the camera's failed to cut away at the end of the skit.[164]

On May 3, it was revealed that Park would be participating on the soundtrack of M. Night Shyamalan's 2013 American science-fiction thriller film, After Earth, having been personally contacted by Will Smith and Jaden Smith.[165] Produced by Cha Cha Malone, the song "I Like 2 Party" was the ending theme for the Korean version of the film and was written and composed by Park himself.[166][167] On May 7, Park attended the After Earth red carpet premiere in Korea alongside Will and Jaden Smith.[168] On May 11, Immortal Songs 2's 100th episode, Park performed "Everyday Day With You" by Deulgukhwa, remixed with Park's latest single at the time, "Joah".[169] Later that month on May 30, a song titled "Rude Girl" by Kim Seul-gi and Jay Park for was released for Korean drama She is Wow.

In early July, Jay Park renewed his contract with SidusHQ after much public speculation of him signing with other record labels. He was to perform at the Hennessy Artistry event early in July in Guangzhou, China, alongside B.o.B, but the event was cancelled at the last moment due to technical difficulties.[170] On July 10, Park released a four-track EP, I Like 2 Party, alongside the music video for the title track, which was filmed in Los Angeles. On November 30, Park served as the emcee for the Red Bull BC One.

On September 1, 2014, Jay Park released his second full-length album, Evolution.[171] With features and production from many of his AOMG labelmates such as Simon Dominic and Gray, the album presented tributes to Michael Jackson and R&B throwback ballads in its 17 tracks.[172] In 2015, Jay Park participated as a judge on Mnet's rap competition program Show Me the Money 4. During the same year, Park released an extensive list of songs including "Lotto", "All I Got Time For", "On It", "Want It", "Mommae" and "Sex Trip". Park released "My Last" featuring Loco and Gray on July 17, 2015, alongside a music video.[173] On November 5, 2015, Jay Park released his third official studio album, Worldwide. The album was released alongside lead single "You Know" (뻔하잖아), which featured Okasian and a music video. Directed by the production company Tiger Cave, the music video starred 4Minute's HyunA and featured cameos from other Korean talents such as rappers Reddy and Ugly Duck, among others. The hip hop project contained a total of 18 tracks and contained some of the songs Park had previously released in the year, such as "Mommae". The album also debuted at number five on Billboard's World Albums chart. Soon after the album's release, Park released music videos for the tracks "Bo$$", "Worldwide x Want It" and "In this B*tch x My".

On October 20, 2016, Park released his fourth solo album Everything You Wanted. The bilingual project contained 19 tracks and featured R&B songs in both English and Korean.[174] Upon release the album debuted at number three on Billboard's World Albums chart. On October 21, 2016, Far East Movement released their album Identity including the track "SXWME", which features MNEK and Jay Park.[175] On July 20, 2017, it was announced that Jay-Z's Roc Nation had signed Park for his activities in the US, marking the first ever Asian American artist on the record label.[176] Later that same month, it was announced that Park would be replacing Melanie C and Vanness Wu as one of the judges on the second season of Asia's Got Talent, alongside Anggun and David Foster.[177] In 2017, Jay Park founded record label H1ghr Music with producer Cha Cha Malone, with an aim to bring both Seattle and South Korean talent to the hip hop forefront.[178] Currently, the label houses American and South Korean artists such as GroovyRoom,  pH-1, Sik-K, Woogie, Phe Reds, Raz Simone, and Ted Park.[179]

In May 2018, Park released his first official single under Roc Nation, "SOJU" featuring 2 Chainz.[180] On July 20, 2018, Park released his debut American project, an English-language EP named Ask Bout Me. Along with "SOJU", the seven track EP featured several notable American rappers such as Rich the Kid and Vic Mensa, as well as in-house production from AOMG and H1ghr Music artists such as Cha Cha Malone and GroovyRoom.[181] On June 7, 2019, Park released his fifth solo studio album, The Road Less Traveled. The album contained features from Jay Electronica and Higher Brothers.[182] In November 2019, Jay Park released a collaborative EP with American producer Hit-Boy, called This Wasn't Supposed to Happen. Named after the surprise nature of the duo's collaboration, This Wasn't Supposed to Happen marked Park's second release under Roc Nation, and was preceded by their single "K-Town".[183] In the month after, Park collabed with Latin artist and fellow Roc Nation signee Mozart La Para for the single "Son Malas".[184] Both singles contained music videos.

Park featured as the first-ever non-Chinese guest judge in the fourth season of The Rap of China, first airing on August 14, 2020. His appearance on the TV rap competition show was met with mixed reception, drawing backlash from some Chinese audiences who resented the appearance of a non-Chinese on the show, and from Korean audiences who resented the fact that Park chose to appear on the Chinese show and not the similar South Korean TV series Show Me The Money.[185][186] On December 31, 2021, Park stepped down as CEO of both AOMG and H1gher Music, but remained as adviser of both companies.[187]

On March 3, 2022, Park established a new label called More Vision.[188] In July, Park will release Need To Know on July 12, 2022.[189] Later that month, it was announced that  Park would be attending the Hip-Hop Playa Festival 2022 taking place September 17–18.[190] On August 16, it was announced that Park has released a teaser image for their new song "Bite", which will be released on August 18.[191] In November 2022, Park was named one of GQ Korea's Men of the Year.[192]

Park's music takes its roots in R&B and hip hop, more specifically from the 90s.[193] He is influenced by the work of American artists he grew up listening to, such as Usher, Michael Jackson, Ne-Yo, Chris Brown, Ginuwine, Musiq Soulchild, Justin Timberlake, Eminem, Jay-Z, Nas, Tupac, Dr. Dre, and Canibus.[57][80][194][195] Park credits Usher as his idol and as one of his biggest influences, who he started listening to in the 6th grade.[196][197] He also cites Michael Jackson as his role model, as "he is the best".[194] He cites both Usher and Michael Jackson as singers and dancers who influence him vocally and in dance, as he attempts to mimic them, looking at their music videos.[197] In 2012, Park also cited Chris Brown as an inspiration, and wanted "to be a singer who can be good at both singing and rapping like [him]".[195] Park began listening to hip hop music when he was in second grade, after one of his cousins let him listen to Warren G's "Regulate".[57][198] In general, Park stated he "doesn't really listen to music to get inspiration, I listen to music because I listen to music, but I do get inspired when I listen to music, it definitely helps".[199]

For his dance, Park highlights Art of Movement, Skill Methods, and Massive Monkees as b-boy crews that influence him.[200][201] He also cites Andrew Baterina from SoReal Cru; Movement Lifestyle's members Keone Madrid, Lyle Beniga, and Shaun Evaristo; Ian Eastwood; and Twitch.[200][201] Park is also inspired by Taiwanese-American NBA player Jeremy Lin, saying that Lin "was off the radar and now he's playing with the best of the best. People can't hate on him even though they want to because he's so good. That's how a K-pop star has to be over in America if they want to succeed. They have to be so good in every single way that even if people hate, they can't really say anything".[202]

Jay Park's music is generally contemporary R&B and hip hop, but he also incorporates pop, dance, soul, electronic and acoustic into his songs.[203] Prior to debuting with 2PM, Park was coached vocally by former SOLID member and renowned R&B artist Kim Jo-han, also known to be the vocal coach of many idols in the industry.[204][205] Kim went on to say that "[Park] has an appealing and unique voice" and "definitely stood out amongst the rest".[204] Park began to write rap lyrics in eighth grade.[206] Since becoming a solo artist, he has complete creative control over his music : he writes and composes his own songs, produces his albums, chooses the people he wants to work with, and participates in the mastering and mixing of his songs, something almost unheard-of for a singer in the Korean music industry.[57][206][207] More than making hit songs, Park says he wants people to hear his style of music, to make them listen to "Jay Park-sounding" music.[207][208] Being fluent in both English and Korean, Park writes songs in these two languages.[47] He admits that he is more at ease when writing songs in English than in Korean, as he started learning Korean in his late teens.[206] In 2011, Park stated that the music he currently creates is "the music that I wanted to do" prior to his departure from 2PM.[209] His musical process starts by listening to a lot of beats sent by various producers, while thinking of what kind of song he wants to write, and eventually short-listing a selection of potential beats to work with.[206] Then, Park works on the melody of the song, along with ideas and lyrics. He writes the hook of the song first, and finally the verses.[210][211] Park is inspired by everything when writing songs: "just listening to good songs, seeing a good live performance, having good conversations with people. Just everyday life".[212] Some songs Park wrote are influenced by songs from other artists, for example, his song "Turn Off Your Phone" (전화기를 꺼놔; Revised Romanization: Jeonhwagireur Kkeonwa) from his album New Breed was inspired by Leessang's hit single "Turn Off the TV ..." (Korean: TV를 껐네…; Revised Romanization: TV Reurkkeotne) from their 7th album Asura Balbalta.[206]

In July 2010, Park was announced to be endorsing Levi Strauss & Co.'s new brand, dENiZEN. He had previously revealed on YouTube in June 2010, before coming back to Korea, that he was at a photoshoot without revealing it was for dENiZEN.[213] He continued to endorse the brand until the end of 2011 with female model Kwon Ri-se.[214] Park and Kwon recorded a remix duet song of dENiZEN's theme song, "Manifesto".[215] In early 2012, Park was chosen by KT Tech to be the official model of Take HD, a new smartphone by the brand.[216] Park released the maxi-single "Take HD Special Maxi Album" on February 7, 2012, produced by KT Tech to promote the smartphone.[217] The maxi-single includes remixes of Park's lead single from his album New Breed, "Know Your Name", released on the same day.

Park appears with fellow SidusHQ celebrities Jang Hyuk, Kim Soo-ro, Kim Shin-young and Jang Hee-jin in the MMORPG Lineage 2 as supporting characters.[218] On behalf of Naver's first "Fashion Collaboration" event, Park also with fashion designer 275C for creation of a joint T-shirt titled Live Free, representing Park's free mind and lifestyle, and displaying both the Space Needle from Seattle, his hometown, and the Namsan Tower from Seoul, where he currently lives. The back of the T-shirt shows the names of his b-boy crews Art of Movement and Korean Assassins, and dance collaboration team, Project Prepix Asia.[219]

Park was also chosen to endorse and model for casual clothing brand Googims [ko] for their 2012 collections; after the announcement was made, it was stated that "the brand's home page server was down every five minutes".[220] A behind the scenes video of Park performing an impromptu dance to "Gangnam Style" by Psy at a Googims photo shoot was uploaded to YouTube on September 2, 2012, capturing the public's attention for his unique take of the choreography.[221][222] Park also has been selected as the new model for outdoor clothing brand, Williamsburg, participating in TV commercials and fan-signing events.[223]

In March 2013, Park, a long time fan of Nike, participated at a Nike event in Seoul, Korea, titled "Nike on Air", which was streamed live online via Ustream. Park is currently sponsored by Nike Korea, and has modeled various times for the apparel company. In 2016, Park, alongside Jessica Jung, Tian Yo, Charlene, and Pakho Chau modeled for Adidas's Celebration of Sportswear campaign.  In March 2016, Park modeled for Umbro Korea's 'Reborn to Heritage' campaign; and did a photoshoot for Police Sunglasses, which was featured in the April issue of Dazed Korea magazine.[citation needed]

On March 8, 2020, a police report was filed against Brian Ortega for allegedly slapping Park, who is a good friend of UFC featherweight fighter Chan Sung Jung (also known as "Korean Zombie"), and serves as his translator. The slap allegedly happened while Jung went to the bathroom at UFC 248.[224][225]

Park is bilingual, fluent in both English and Korean, and can speak basic Mandarin.[226]

On March 8, 2022, Park donated ₩50 million to the Hope Bridge Disaster Relief Association to help the victims of the massive wildfires that started in Uljin, Gyeongbuk, and also spread to Samcheok, Gangwon.[227] On August 12, 2022, Park donated ₩100 million to help those affected by the 2022 South Korean floods through the Hope Bridge Korea Disaster Relief Association.[228]

Headlining

SupportingHeterodox

John Maynard Keynes, 1st Baron Keynes[6] CB, FBA (/keɪnz/ KAYNZ; 5 June 1883 – 21 April 1946), was an English economist and philosopher whose ideas fundamentally changed the theory and practice of macroeconomics and the economic policies of governments. Originally trained in mathematics, he built on and greatly refined earlier work on the causes of business cycles.[7] One of the most influential economists of the 20th century,[8][9][10] he produced writings that are the basis for the school of thought known as Keynesian economics, and its various offshoots.[11] His ideas, reformulated as New Keynesianism, are fundamental to mainstream macroeconomics. He is known as the "father of macroeconomics".[12]

During the Great Depression of the 1930s, Keynes spearheaded a revolution in economic thinking, challenging the ideas of neoclassical economics that held that free markets would, in the short to medium term, automatically provide full employment, as long as workers were flexible in their wage demands. He argued that aggregate demand (total spending in the economy) determined the overall level of economic activity, and that inadequate aggregate demand could lead to prolonged periods of high unemployment, and since wages and labour costs are rigid downwards the economy will not automatically rebound to full employment.[13] Keynes advocated the use of fiscal and monetary policies to mitigate the adverse effects of economic recessions and depressions. After the 1929 crisis, Keynes also turned away from a fundamental pillar of neoclassical economics: free trade. He criticized Ricardian comparative advantage theory (the foundation of free trade), considering the theory's initial assumptions unrealistic, and became definitively protectionist.[14][15][16] He detailed these ideas in his magnum opus, The General Theory of Employment, Interest and Money, published in early 1936. By the late 1930s, leading Western economies had begun adopting Keynes's policy recommendations. Almost all capitalist governments had done so by the end of the two decades following Keynes's death in 1946. As a leader of the British delegation, Keynes participated in the design of the international economic institutions established after the end of World War II but was overruled by the American delegation on several aspects.

Keynes's influence started to wane in the 1970s, partly as a result of the stagflation that plagued the British and American economies during that decade, and partly because of criticism of Keynesian policies by Milton Friedman and other monetarists,[17] who disputed the ability of government to favourably regulate the business cycle with fiscal policy.[18] The 2008 financial crisis sparked the 2008–2009 Keynesian resurgence. Keynesian economics provided the theoretical underpinning for economic policies undertaken in response to the 2008 financial crisis by President Barack Obama of the United States, Prime Minister Gordon Brown of the United Kingdom, and other heads of governments.[19]

When Time magazine included Keynes among its Most Important People of the Century in 1999, it reported that "his radical idea that governments should spend money they don't have may have saved capitalism".[20] The Economist has described Keynes as "Britain's most famous 20th-century economist".[21] In addition to being an economist, Keynes was also a civil servant, a director of the Bank of England, and a part of the Bloomsbury Group of intellectuals.[22]

John Maynard Keynes was born in Cambridge, England, in June 1883 to an upper-middle-class family. His father, John Neville Keynes, was an economist and a lecturer in moral sciences at the University of Cambridge and his mother, Florence Ada Keynes, a local social reformer. Keynes was the firstborn and was followed by two more children – Margaret Neville Keynes in 1885 and Geoffrey Keynes in 1887. Geoffrey became a surgeon and Margaret married the Nobel Prize-winning physiologist Archibald Hill.

According to the reminiscences of his brother Geoffrey, their parents were loving and attentive. They attended a Congregational Church[23][24]: 14  and remained in the same house throughout their lives, where the children were always welcome to return. Keynes received considerable support from his father, including expert coaching to help him pass his scholarship exams and financial help both as a young man and when his assets were nearly wiped out at the onset of Great Depression in 1929. Keynes's mother made her children's interests her own, and according to Skidelsky, "because she could grow up with her children, they never outgrew home".[24]: 14 

In January 1889, at the age of five and a half, Keynes started at the kindergarten of the Perse School for Girls for five mornings a week. He quickly showed a talent for arithmetic, but his health was poor leading to several long absences. He was tutored at home by a governess, Beatrice Mackintosh, and his mother. In January 1892, at eight and a half, he started as a day pupil at St Faith's preparatory school. By 1894, Keynes was top of his class and excelling at mathematics. In 1896, St Faith's headmaster, Ralph Goodchild, wrote that Keynes was "head and shoulders above all the other boys in the school" and was confident that Keynes could get a scholarship to Eton.[25]

In 1897, Keynes won a King's Scholarship to Eton College, where he displayed talent in a wide range of subjects, particularly mathematics, classics and history: in 1901, he was awarded the Tomline Prize for mathematics. At Eton, Keynes experienced the first "love of his life" in Dan Macmillan, older brother of the future Prime Minister Harold Macmillan.[26]: 27  Despite his middle-class background, Keynes mixed easily with upper-class pupils.

In 1902, Keynes left Eton for King's College, Cambridge, after receiving a scholarship for this also, to read mathematics. Alfred Marshall begged Keynes to become an economist,[27] although Keynes's own inclinations drew him towards philosophy, especially the ethical system of G. E. Moore. Keynes was elected to the University Pitt Club[28]: 52–81  and was an active member of the semi-secretive Cambridge Apostles society, a debating club largely reserved for the brightest students. Like many members, Keynes retained a bond to the club after graduating and continued to attend occasional meetings throughout his life. Before leaving Cambridge, Keynes became the president of the Cambridge Union Society and Cambridge University Liberal Club. He was said to be an atheist.[29][30]

In May 1904, he received a first-class BA in mathematics. Aside from a few months spent on holidays with family and friends, Keynes continued to involve himself with the university over the next two years. He took part in debates, further studied philosophy and attended economics lectures informally as a graduate student for one term, which constituted his only formal education in the subject. He took civil service exams in 1906.

The economist Harry Johnson wrote that the optimism imparted by Keynes's early life is a key to understanding his later thinking.[31]
Keynes was always confident he could find a solution to whatever problem he turned his attention to and retained a lasting faith in the ability of government officials to do good.[32]
Keynes's optimism was also cultural, in two senses: he was of the last generation raised by an empire still at the height of its power and was also of the last generation who felt entitled to govern by culture, rather than by expertise. According to Skidelsky, the sense of cultural unity current in Britain from the 19th century to the end of World War I provided a framework with which the well-educated could set various spheres of knowledge in relation to each other and life, enabling them to confidently draw from different fields when addressing practical problems.[24]: 146–147 

In October 1906 Keynes began his Civil Service career as a clerk in the India Office.[33] He enjoyed his work at first, but by 1908 had become bored and resigned his position to return to Cambridge and work on probability theory, through a lectureship in economics at first funded personally by economists Alfred Marshall and Arthur Pigou; he became a fellow of King's College in 1909.[34]

By 1909 Keynes had also published his first professional economics article in The Economic Journal, about the effect of a recent global economic downturn on India.[35] He founded the Political Economy Club, a weekly discussion group. Keynes's earnings rose further as he began to take on pupils for private tuition.

In 1911 Keynes was made the editor of The Economic Journal. By 1913 he had published his first book, Indian Currency and Finance.[36] He was then appointed to the Royal Commission on Indian Currency and Finance[37] – the same topic as his book – where Keynes showed considerable talent at applying economic theory to practical problems. His written work was published under the name "J M Keynes", though to his family and friends he was known as Maynard. (His father, John Neville Keynes, was also always known by his middle name).[38]

The British Government called on Keynes's expertise during the First World War. While he did not formally rejoin the civil service in 1914, Keynes travelled to London at the government's request a few days before hostilities started. Bankers had been pushing for the suspension of specie payments – the gold equivalent of banknotes – but with Keynes's help, the Chancellor of the Exchequer (then Lloyd George) was persuaded that this would be a bad idea, as it would hurt the future reputation of the city if payments were suspended before it was necessary.

In January 1915 Keynes took up an official government position at the Treasury. Among his responsibilities were the design of terms of credit between Britain and its continental allies during the war and the acquisition of scarce currencies. According to economist Robert Lekachman, Keynes's "nerve and mastery became legendary" because of his performance of these duties, as in the case where he managed to assemble a supply of Spanish pesetas.

The secretary of the Treasury was delighted to hear Keynes had amassed enough to provide a temporary solution for the British Government. But Keynes did not hand the pesetas over, choosing instead to sell them all to break the market: his boldness paid off, as pesetas then became much less scarce and expensive.[39]

On the introduction of military conscription in 1916, he applied for exemption as a conscientious objector, which was effectively granted conditional upon continuing his government work.

In the 1917 King's Birthday Honours, Keynes was appointed Companion of the Order of the Bath for his wartime work,[40] and his success led to the appointment that had a huge effect on Keynes's life and career; Keynes was appointed financial representative for the Treasury to the 1919 Versailles peace conference. He was also appointed Officer of the Belgian Order of Leopold.[41]

Keynes's experience at Versailles was influential in shaping his future outlook, yet it was not a successful one. Keynes's main interest had been in trying to prevent Germany's compensation payments being set so high it would traumatise innocent German people, damage the nation's ability to pay and sharply limit its ability to buy exports from other countries – thus hurting not just Germany's economy but that of the wider world.

Unfortunately for Keynes, conservative powers in the coalition that emerged from the 1918 coupon election were able to ensure that both Keynes himself and the Treasury were largely excluded from formal high-level talks concerning reparations. Their place was taken by the Heavenly Twins – the judge Lord Sumner and the banker Lord Cunliffe, whose nickname derived from the "astronomically" high war compensation they wanted to demand from Germany. Keynes was forced to try to exert influence mostly from behind the scenes.

The three principal players at the Paris conferences were Britain's Lloyd George, France's Georges Clemenceau and America's President Woodrow Wilson.[42]
It was only Lloyd George to whom Keynes had much direct access; until the 1918 election he had some sympathy with Keynes's view but while campaigning had found his speeches were well received by the public only if he promised to harshly punish Germany, and had therefore committed his delegation to extracting high payments.

Lloyd George did, however, win some loyalty from Keynes with his actions at the Paris conference by intervening against the French to ensure the dispatch of much-needed food supplies to German civilians. Clemenceau also pushed for substantial reparations, though not as high as those proposed by the British, while on security grounds, France argued for an even more severe settlement than Britain.

Wilson initially favoured relatively lenient treatment of Germany – he feared too harsh conditions could foment the rise of extremism and wanted Germany to be left sufficient capital to pay for imports. To Keynes's dismay, Lloyd George and Clemenceau were able to pressure Wilson to agree to include pensions in the reparations bill.

Towards the end of the conference, Keynes came up with a plan that he argued would not only help Germany and other impoverished central European powers but also be good for the world economy as a whole. It involved the radical writing down of war debts, which would have had the possible effect of increasing international trade all round, but at the same time thrown over two-thirds of the cost of European reconstruction on the United States.[24]: 225–232 [a]

Lloyd George agreed it might be acceptable to the British electorate. However, America was against the plan; the US was then the largest creditor, and by this time Wilson had started to believe in the merits of a harsh peace and thought that his country had already made excessive sacrifices. Hence despite his best efforts, the result of the conference was a treaty which disgusted Keynes both on moral and economic grounds and led to his resignation from the Treasury.[24]: 231–235 

In June 1919 he turned down an offer to become chairman of the British Bank of Northern Commerce, a job that promised a salary of £2,000 in return for a morning per week of work.[24]: 234 

Keynes's analysis on the predicted damaging effects of the treaty appeared in the highly influential book, The Economic Consequences of the Peace, published in 1919.[44] This work has been described as Keynes's best book, where he was able to bring all his gifts to bear – his passion as well as his skill as an economist. In addition to economic analysis, the book contained appeals to the reader's sense of compassion:[45]: 209 

I cannot leave this subject as though its just treatment wholly depended either on our pledges or on economic facts. The policy of reducing Germany to servitude for a generation, of degrading the lives of millions of human beings, and of depriving a whole nation of happiness should be abhorrent and detestable, – abhorrent and detestable, even if it was possible, even if it enriched ourselves, even if it did not sow the decay of the whole civilized life of Europe.
Also present was striking imagery such as "year by year Germany must be kept impoverished and her children starved and crippled" along with bold predictions which were later justified by events:[45]: 251 

If we aim deliberately at the impoverishment of Central Europe, vengeance, I dare predict, will not limp. Nothing can then delay for very long that final war between the forces of Reaction and the despairing convulsions of Revolution, before which the horrors of the late German war will fade into nothing.
Keynes's followers assert that his predictions of disaster were borne out when the German economy suffered the hyperinflation of 1923, and again by the collapse of the Weimar Republic and the outbreak of the Second World War. However, historian Ruth Henig claims that "most historians of the Paris peace conference now take the view that, in economic terms, the treaty was not unduly harsh on Germany and that, while obligations and damages were inevitably much stressed in the debates at Paris to satisfy electors reading the daily newspapers, the intention was quietly to give Germany substantial help towards paying her bills, and to meet many of the German objections by amendments to the way the reparations schedule was in practice carried out".[46][47][b]

Only a small fraction of reparations was ever paid. In fact, historian Stephen A. Schuker demonstrates in American 'Reparations' to Germany, 1919–33, that the capital inflow from American loans substantially exceeded German out payments so that, on a net basis, Germany received support equal to four times the amount of the post-Second World War Marshall Plan.[48]

Schuker also shows that, in the years after Versailles, Keynes became an informal reparations adviser to the German government, wrote one of the major German reparation notes, and supported hyperinflation on political grounds. Nevertheless, The Economic Consequences of the Peace gained Keynes international fame, even though it also caused him to be regarded as anti-establishment – it was not until after the outbreak of the Second World War that Keynes was offered a directorship of a major British Bank, or an acceptable offer to return to government with a formal job. However, Keynes was still able to influence government policy-making through his network of contacts, his published works and by serving on government committees; this included attending high-level policy meetings as a consultant.[24]: 593–598 

Keynes had completed his A Treatise on Probability before the war but published it in 1921.[24]: 283  The work was a notable contribution to the philosophical and mathematical underpinnings of probability theory, championing the important view that probabilities were no more or less than truth values intermediate between simple truth and falsity. Keynes developed the first upper-lower probabilistic interval approach to probability in chapters 15 and 17 of this book, as well as having developed the first decision weight approach with his conventional coefficient of risk and weight, c, in chapter 26. In addition to his academic work, the 1920s saw Keynes active as a journalist selling his work internationally and working in London as a financial consultant. In 1924 Keynes wrote an obituary for his former tutor Alfred Marshall which Joseph Schumpeter called "the most brilliant life of a man of science I have ever read".[49] Mary Paley Marshall was "entranced" by the memorial, while Lytton Strachey rated it as one of Keynes's "best works".[24]: 342 

In 1922 Keynes continued to advocate reduction of German reparations with A Revision of the Treaty.[24]: 245  He attacked the post-World War I deflation policies with A Tract on Monetary Reform in 1923[24]: 329  – a trenchant argument that countries should target stability of domestic prices, avoiding deflation even at the cost of allowing their currency to depreciate. Britain suffered from high unemployment through most of the 1920s, leading Keynes to recommend the depreciation of sterling to boost jobs by making British exports more affordable. From 1924 he was also advocating a fiscal response, where the government could create jobs by spending on public works.[24]: 343–344  During the 1920s Keynes's pro-stimulus views had only limited effect on policymakers and mainstream academic opinion – according to Hyman Minsky one reason was that at this time his theoretical justification was "muddled".[35] The Tract had also called for an end to the gold standard. Keynes advised it was no longer a net benefit for countries such as Britain to participate in the gold standard, as it ran counter to the need for domestic policy autonomy. It could force countries to pursue deflationary policies at exactly the time when expansionary measures were called for to address rising unemployment. The Treasury and Bank of England were still in favour of the gold standard and in 1925 they were able to convince the then Chancellor Winston Churchill to re-establish it, which had a depressing effect on British industry. Keynes responded by writing The Economic Consequences of Mr. Churchill and continued to argue against the gold standard until Britain finally abandoned it in 1931.[24]: 352–355 

Keynes had begun a theoretical work to examine the relationship between unemployment, money and prices back in the 1920s.[50] The work, Treatise on Money, was published in 1930 in two volumes. A central idea of the work was that if the amount of money being saved exceeds the amount being invested – which can happen if interest rates are too high – then unemployment will rise. This is in part a result of people not wanting to spend too high a proportion of what employers pay out, making it difficult, in aggregate, for employers to make a profit. Another key theme of the book is the unreliability of financial indices for representing an accurate – or indeed meaningful – indication of general shifts in purchasing power of currencies over time. In particular, he criticised the justification of Britain's return to the gold standard in 1925 at pre-war valuation by reference to the wholesale price index. He argued that the index understated the effects of changes in the costs of services and labour.

Keynes was deeply critical of the British government's austerity measures during the Great Depression. He believed that budget deficits during recessions were a good thing and a natural product of an economic slump. He wrote, "For Government borrowing of one kind or another is nature's remedy, so to speak, for preventing business losses from being, in so severe a slump as the present one, so great as to bring production altogether to a standstill."[51]

At the height of the Great Depression, in 1933, Keynes published The Means to Prosperity, which contained specific policy recommendations for tackling unemployment in a global recession, chiefly counter-cyclical public spending. The Means to Prosperity contains one of the first mentions of the multiplier effect. While it was addressed chiefly to the British Government, it also contained advice for other nations affected by the global recession. A copy was sent to the newly elected President Franklin D. Roosevelt and other world leaders. The work was taken seriously by both the American and British governments, and according to Robert Skidelsky, helped pave the way for the later acceptance of Keynesian ideas, though it had little immediate practical influence. In the 1933 London Economic Conference opinions remained too diverse for a unified course of action to be agreed upon.[24]: 490–500 

Keynesian-like policies were adopted by Sweden and Germany, but Sweden was seen as too small to command much attention, and Keynes was deliberately silent about the successful efforts of Germany as he was dismayed by its imperialist ambitions and its treatment of Jews.[24]: 500–504  Apart from Great Britain, Keynes's attention was primarily focused on the United States. In 1931, he received considerable support for his views on counter-cyclical public spending in Chicago, then America's foremost center for economic views alternative to the mainstream.[35][24]: 507  However, orthodox economic opinion remained generally hostile regarding fiscal intervention to mitigate the depression, until just before the outbreak of war.[35] In late 1933 Keynes was persuaded by Felix Frankfurter to address President Roosevelt directly, which he did by letters and face-to-face in 1934, after which the two men spoke highly of each other.[24]: 506–509  However, according to Skidelsky, the consensus is that Keynes's efforts began to have a more than marginal influence on US economic policy only after 1939.[24]

Keynes's magnum opus, The General Theory of Employment, Interest and Money was published in 1936.[13]It was researched and indexed by one of Keynes's favourite students, and later economist, David Bensusan-Butt.[52] The work served as a theoretical justification for the interventionist policies Keynes favoured for tackling a recession. Although Keynes stated in his preface that his General Theory was only secondarily concerned with the "applications of this theory to practice", the circumstances of its publication were such that his suggestions shaped the course of the 1930s.[53] In addition, Keynes introduced the world to a new interpretation of taxation: since the legal tender is now defined by the state, inflation becomes "taxation by currency depreciation". This hidden tax meant a) that the standard of value should be governed by deliberate decision; and (b) that it was possible to maintain a middle course between deflation and inflation.[54] This novel interpretation was inspired by the desperate search for control over the economy which permeated the academic world after the Depression. The General Theory challenged the earlier neoclassical economic paradigm, which had held that provided it was unfettered by government interference, the market would naturally establish full employment equilibrium. In doing so Keynes was partly setting himself against his former teachers Marshall and Pigou. Keynes believed the classical theory was a "special case" that applied only to the particular conditions present in the 19th century, his theory being the general one. Classical economists had believed in Say's law, which, simply put, states that "supply creates its demand", and that in a free-market workers would always be willing to lower their wages to a level where employers could profitably offer them jobs.[55]

An innovation from Keynes was the concept of price stickiness – the recognition that in reality workers often refuse to lower their wage demands even in cases where a classical economist might argue that it is rational for them to do so. Due in part to price stickiness, it was established that the interaction of "aggregate demand" and "aggregate supply" may lead to stable unemployment equilibria – and in those cases, it is on the state, not the market, that economies must depend for their salvation. In contrast, Keynes argued that demand is what creates supply and not the other way around. He questioned Say's Law by asking what would happen if the money that is being given to individuals is not finding its way back into the economy and is saved instead. He suggested the result would be a recession. To tackle the fear of a recession Say's Law suggests government intervention. This government intervention can be used to prevent any further increase in savings in the form of a decreased interest rate. Decreasing the interest rate will encourage people to start spending and investing again, or so it is stated by Say's Law. The reason behind this is that when there is little investing, savings start to accumulate and reach a stopping point in the flow of money. During the normal economic activity, it would be justified to have savings because they can be given out as loans but in this case, there is little demand for them, so they are doing no good for the economy. The supply of savings then exceeds the demand for loans and the result is lower prices or lower interest rates. Thus, the idea is that the money that was once saved is now re-invested or spent, assuming lower interest rates appeal to consumers. To Keynes, however, this was not always the case, and it couldn't be assumed that lower interest rates would automatically encourage investment and spending again since there is no proven link between the two.[55]

The General Theory argues that demand, not supply, is the key variable governing the overall level of economic activity. Aggregate demand, which equals total un-hoarded income in a society, is defined by the sum of consumption and investment. In a state of unemployment and unused production capacity, one can enhance employment and total income only by first increasing expenditures for either consumption or investment. Without government intervention to increase expenditure, an economy can remain trapped in a low-employment equilibrium. The demonstration of this possibility has been described as the revolutionary formal achievement of the work.[24]: 528–538 
The book advocated activist economic policy by government to stimulate demand in times of high unemployment, for example by spending on public works. "Let us be up and doing, using our idle resources to increase our wealth," he wrote in 1928. "With men and plants unemployed, it is ridiculous to say that we cannot afford these new developments. It is precisely with these plants and these men that we shall afford them."[51]

The General Theory is often viewed as the foundation of modern macroeconomics. Few senior American economists agreed with Keynes through most of the 1930s.[56]
Yet his ideas were soon to achieve widespread acceptance, with eminent American professors such as Alvin Hansen agreeing with the General Theory before the outbreak of World War II.[57][58][59]

Keynes himself had only limited participation in the theoretical debates that followed the publication of the General Theory as he suffered a heart attack in 1937, requiring him to take long periods of rest. Among others, Hyman Minsky and Post-Keynesian economists have argued that as a result, Keynes's ideas were diluted by those keen to compromise with classical economists or to render his concepts with mathematical models like the IS–LM model (which, they argue, distort Keynes's ideas).[35][59] Keynes began to recover in 1939, but for the rest of his life his professional energies were directed largely towards the practical side of economics: the problems of ensuring optimum allocation of resources for the war efforts, post-war negotiations with America, and the new international financial order that was presented at the Bretton Woods Conference.

In the General Theory and later, Keynes responded to the socialists who argued, especially during the Great Depression of the 1930s, that capitalism caused war. He argued that if capitalism were managed domestically and internationally (with coordinated international Keynesian policies, an international monetary system that did not pit the interests of countries against one another, and a high degree of freedom of trade), then this system of managed capitalism could promote peace rather than conflict between countries. His plans during World War II for post-war international economic institutions and policies (which contributed to the creation at Bretton Woods of the International Monetary Fund and the World Bank, and later to the creation of the General Agreement on Tariffs and Trade and eventually the World Trade Organization) were aimed to give effect to this vision.[60]

Although Keynes has been widely criticised – especially by members of the Chicago school of economics – for advocating irresponsible government spending financed by borrowing, in fact he was a firm believer in balanced budgets and regarded the proposals for programmes of public works during the Great Depression as an exceptional measure to meet the needs of exceptional circumstances.[61]

During the Second World War, Keynes argued in How to Pay for the War, published in 1940, that the war effort should be largely financed by higher taxation and especially by compulsory saving (essentially workers lending money to the government), rather than deficit spending, to avoid inflation. Compulsory saving would act to dampen domestic demand, assist in channelling additional output towards the war efforts, would be fairer than punitive taxation and would have the advantage of helping to avoid a post-war slump by boosting demand once workers were allowed to withdraw their savings. In September 1941 he was proposed to fill a vacancy in the Court of Directors of the Bank of England, and subsequently carried out a full term from the following April.[62] In June 1942, Keynes was rewarded for his service with a hereditary peerage in the King's Birthday Honours.[63] On 7 July his title was gazetted as "Baron Keynes, of Tilton, in the County of Sussex" and he took his seat in the House of Lords on the Liberal Party benches.[64]

As the Allied victory began to look certain, Keynes was heavily involved, as leader of the British delegation and chairman of the World Bank commission, in the mid-1944 negotiations that established the Bretton Woods system. The Keynes plan, concerning an international clearing-union, argued for a radical system for the management of currencies. He proposed the creation of a common world unit of currency, the bancor and new global institutions – a world central bank and the International Clearing Union. Keynes envisaged these institutions as managing an international trade and payments system with strong incentives for countries to avoid substantial trade deficits or surpluses.[65] The USA's greater negotiating strength, however, meant that the outcomes accorded more closely to the more conservative plans of Harry Dexter White. According to US economist J. Bradford DeLong, on almost every point where he was overruled by the Americans, Keynes was later proven correct by events.[66]

The two new institutions, later known as the World Bank and the International Monetary Fund (IMF), were founded as a compromise that primarily reflected the American vision. There would be no incentives for states to avoid a large trade surplus; instead, the burden for correcting a trade imbalance would continue to fall only on the deficit countries, which Keynes had argued were least able to address the problem without inflicting economic hardship on their populations. Yet, Keynes was still pleased when accepting the final agreement, saying that if the institutions stayed true to their founding principles, "the brotherhood of man will have become more than a phrase."[67][68]

After the war, Keynes continued to represent the United Kingdom in international negotiations despite his deteriorating health. He succeeded in obtaining preferential terms from the United States for new and outstanding debts to facilitate the rebuilding of the British economy.[69]: §1945 to 1946 

Just before his death in 1946, Keynes told Henry Clay, a professor of social economics and advisor to the Bank of England,[70] of his hopes that Adam Smith's "invisible hand" could help Britain out of the economic hole it was in: "I find myself more and more relying for a solution of our problems on the invisible hand which I tried to eject from economic thinking twenty years ago."[69]: §1945 to 1946 

According to Keynesian theory, trade deficits are harmful.The countries that import more than they export weaken their economies. When the trade deficit increases, unemployment rises and gross domestic product (GDP) slows down. Furthermore, surplus countries exert a "negative externality" on their trading partners. They get richer at the expense of others and destroy the output of their trading partners. John Maynard Keynes believed that the products of surplus countries should be taxed to avoid trade imbalances.[71]

At the beginning of his career, Keynes was an economist close to Alfred Marshall, deeply convinced of the benefits of free trade. From the crisis of 1929 onwards, noting the commitment of the British authorities to defend the gold parity of the pound sterling and the rigidity of nominal wages, he gradually adhered to protectionist measures.[14]

On 5 November 1929, when heard by the Macmillan Committee to bring the British economy out of the crisis, Keynes indicated that the introduction of tariffs on imports would help to rebalance the trade balance. The committee's report states in a section entitled "import control and export aid", that in an economy where there is not full employment, the introduction of tariffs can improve production and employment. Thus, the reduction of the trade deficit favours the country's growth.[14]

In January 1930, in the Economic Advisory Council, Keynes proposed the introduction of a system of protection to reduce imports. In the autumn of 1930, he proposed a uniform tariff of 10% on all imports and subsidies of the same rate for all exports.[14] In the Treatise on Money, published in the autumn of 1930, he took up the idea of tariffs or other trade restrictions with the aim of reducing the volume of imports and rebalancing the balance of trade.[14]

On 7 March 1931, in the New Statesman and Nation, he wrote an article entitled Proposal for a Tariff Revenue. He pointed out that the reduction in wages led to a reduction in national demand which constrained markets. Instead, he proposed the idea of an expansionary policy combined with a tariff system to neutralise the effects on the balance of trade. The application of customs tariffs seemed to him "unavoidable, whoever the Chancellor of the Exchequer might be". Thus, for Keynes, an economic recovery policy is only fully effective if the trade deficit is eliminated. He proposed a 15% tax on manufactured and semi-manufactured goods and 5% on certain foodstuffs and raw materials, with others needed for exports exempted (wool, cotton).[14]

In 1932, in an article entitled The Pro- and Anti-Tariffs, published in The Listener, he envisaged the protection of farmers and certain sectors such as the automobile and iron and steel industries, considering them indispensable to Britain.[14]

In the post-crisis situation of 1929, Keynes judged the assumptions of the free trade model unrealistic. He criticised, for example, the neoclassical assumption of wage adjustment.[14][15]

As early as 1930, in a note to the Economic Advisory Council, he doubted the intensity of the gain from specialisation in the case of manufactured goods. While participating in the MacMillan Committee, he admitted that he no longer "believed in a very high degree of national specialisation" and refused to "abandon any industry which is unable, for the moment, to survive". He also criticised the static dimension of the theory of comparative advantage, which, in his view, by fixing comparative advantages definitively, led in practice to a waste of national resources.[14][15]

In the Daily Mail of 13 March 1931, he called the assumption of perfect sectoral labour mobility "nonsense" since it states that a person made unemployed contributes to a reduction in the wage rate until he finds a job. But for Keynes, this change of job may involve costs (job search, training) and is not always possible. Generally speaking, for Keynes, the assumptions of full employment and automatic return to equilibrium discredit the theory of comparative advantage.[14][15]

In July 1933, he published an article in the New Statesman and Nation entitled National Self-Sufficiency, in which he criticised the argument of the specialisation of economies, which is the basis of free trade. He thus proposed the search for a certain degree of self-sufficiency. Instead of the specialisation of economies advocated by the Ricardian theory of comparative advantage, he prefers the maintenance of a diversity of activities for nations.[15] In it he refutes the principle of peacemaking trade. His vision of trade became that of a system where foreign capitalists compete for new markets. He defends the idea of producing on national soil when possible and reasonable and expresses sympathy for the advocates of protectionism.[16]
He notes in National Self-Sufficiency:[16][14]

A considerable degree of international specialization is necessary in a rational world in all cases where it is dictated by wide differences of climate, natural resources, native aptitudes, level of culture and density of population. But over an increasingly wide range of industrial products, and perhaps of agricultural products also, I have become doubtful whether the economic loss of national self-sufficiency is great enough to outweigh the other advantages of gradually bringing the product and the consumer within the ambit of the same national, economic, and financial organization. Experience accumulates to prove that most modern processes of mass production can be performed in most countries and climates with almost equal efficiency.

He also writes in National Self-Sufficiency:[16][14]
I sympathize, therefore, with those who would minimize, rather than with those who would maximize, economic entanglement among nations. Ideas, knowledge, science, hospitality, travel – these are the things which should of their nature be international. But let goods be homespun whenever it is reasonably and conveniently possible, and, above all, let finance be primarily national.
Later, Keynes had a written correspondence with James Meade centred on the issue of import restrictions. Keynes and Meade discussed the best choice between quota and tariff. In March 1944 Keynes began a discussion with Marcus Fleming after the latter had written an article entitled Quotas versus depreciation. On this occasion, we see that he has definitely taken a protectionist stance after the Great Depression. He considered that quotas could be more effective than currency depreciation in dealing with external imbalances. Thus, for Keynes, currency depreciation was no longer sufficient and protectionist measures became necessary to avoid trade deficits. To avoid the return of crises due to a self-regulating economic system, it seemed essential to him to regulate trade and stop free trade (deregulation of foreign trade).[14]

Keynes was the principal author of a proposal – the so-called Keynes Plan – for an International Clearing Union. The two governing principles of the plan were that the problem of settling outstanding balances should be solved by "creating" additional "international money", and that debtor and creditor should be treated almost alike as disturbers of equilibrium. In the event, though, the plans were rejected, in part because "American opinion was naturally reluctant to accept the principle of equality of treatment so novel in debtor-creditor relationships".[72]: 326–329 

The new system is not founded on free-trade (liberalisation[73] of foreign trade[74]) but rather on the regulation of international trade, to eliminate trade imbalances: the nations with a surplus would have an incentive to reduce it, and in doing so they would automatically clear other nations deficits.[75] He proposed a global bank that would issue its currency – the bancor – which was exchangeable with national currencies at fixed rates of exchange and would become the unit of account between nations, which means it would be used to measure a country's trade deficit or trade surplus. Every country would have an overdraft facility in its bancor account at the International Clearing Union. He pointed out that surpluses lead to weak global aggregate demand – countries running surpluses exert a "negative externality" on trading partners, and pose, far more than those in deficit, a threat to global prosperity.[71]

In his 1933 Yale Review article "National Self-Sufficiency",[16][76] he already highlighted the problems created by free trade. His view, supported by many economists and commentators at the time, was that creditor nations may be just as responsible as debtor nations for disequilibrium in exchanges and that both should be under an obligation to bring trade back into a state of balance. Failure for them to do so could have serious consequences. In the words of Geoffrey Crowther, then editor of The Economist, "If the economic relationships between nations are not, by one means or another, brought fairly close to balance, then there is no set of financial arrangements that can rescue the world from the impoverishing results of chaos."[72]: 336 

These ideas were informed by events prior to the Great Depression when – in the opinion of Keynes and others – international lending, primarily by the US, exceeded the capacity of sound investment and so got diverted into non-productive and speculative uses, which in turn invited default and a sudden stop to the process of lending.[72]: 368–372 

Influenced by Keynes, economics texts in the immediate post-war period put a significant emphasis on balance in trade. For example, the second edition of the popular introductory textbook, An Outline of Money,[72] devoted the last three of its ten chapters to questions of foreign exchange management and in particular the "problem of balance". However, in more recent years, since the end of the Bretton Woods system in 1971, with the increasing influence of monetarist schools of thought in the 1980s, and particularly in the face of large sustained trade imbalances, these concerns – and particularly concerns about the destabilising effects of large trade surpluses – have largely disappeared from mainstream economics discourse[77][page needed] and Keynes's insights have slipped from view.[78][page needed] They received some attention again after the 2008 financial crisis.[79]


Keynes has been characterised as being indifferent or even positive about mild inflation.[45]: 220–232  He had expressed a preference for inflation over deflation, saying that if one has to choose between the two evils, it is "better to disappoint the rentier" than to inflict pain on working-class families.[80] Keynes was also aware of the dangers of inflation.[59][page range too broad] In The Economic Consequences of the Peace, he wrote:[45]: 220  
Lenin is said to have declared that the best way to destroy the Capitalist System was to debauch the currency. By a continuing process of inflation, governments can confiscate, secretly and unobserved, an important part of the wealth of their citizens. There is no subtler, no surer means of overturning the existing basis of society than to debauch the currency. The process engages all the hidden forces of economic law on the side of destruction, and does it in a manner which not one man in a million is able to diagnose.
Views on alienation

Keynes was not deeply concerned with alienation and did not explicitly use the term in his writings. While Keynes was pro-business and viewed capitalism as essential for economic growth, he acknowledged that it created economic insecurity and dissatisfaction, which could alienate workers, yet he believed this was only a temporary problem. In his book the Economic Possibilities for Our Grandchildren (1930), he envisioned a future where society would become so wealthy that the pursuit of money for its own sake would become irrelevant, effectively eliminating alienation.[81] He believed that continuous economic growth would eventually reduce the need for excessive labor and alleviate extreme inequality, allowing people to pursue more fulfilling lives. While Keynes' optimistic vision has not fully materialized, global wealth has grown significantly since his time. Yet, alienation remains a pressing issue, especially during periods of rising inequality, suggesting that economic growth alone may not be enough to eliminate the problem.  

From the end of the Great Depression to the mid-1970s, Keynes provided the main inspiration for economic policymakers in Europe, America and much of the rest of the world.[59] While economists and policymakers had become increasingly won over to Keynes's way of thinking in the mid and late 1930s, it was only after the outbreak of World War II that governments started to borrow money for spending on a scale sufficient to eliminate unemployment. According to the economist John Kenneth Galbraith (then a US government official charged with controlling inflation), in the rebound of the economy from wartime spending, "one could not have had a better demonstration of the Keynesian ideas".[82]

The Keynesian Revolution was associated with the rise of modern liberalism in the West during the post-war period.[83] Keynesian ideas became so popular that some scholars point to Keynes as representing the ideals of modern liberalism, as Adam Smith represented the ideals of classical liberalism.[84] After the war, Winston Churchill attempted to check the rise of Keynesian policy-making in the United Kingdom and used rhetoric critical of the mixed economy in his 1945 election campaign. Despite his popularity as a war hero, Churchill suffered a landslide defeat to Clement Attlee, whose government's economic policy continued to be influenced by Keynes's ideas.[82]

In the late 1930s and 1940s, economists (notably John Hicks, Franco Modigliani and Paul Samuelson) attempted to interpret and formalise Keynes's writings in terms of formal mathematical models. In what had become known as the neoclassical synthesis, they combined Keynesian analysis with neoclassical economics to produce neo-Keynesian economics, which came to dominate mainstream macroeconomic thought for the next 40 years.

By the 1950s, Keynesian policies were adopted by almost the entire developed world and similar measures for a mixed economy were used by many developing nations. By then, Keynes's views on the economy had become mainstream in the world's universities. Throughout the 1950s and 1960s, the developed and emerging free capitalist economies enjoyed exceptionally high growth and low unemployment.[85][86] Professor Gordon Fletcher has written that the 1950s and 1960s, when Keynes's influence was at its peak, appear in retrospect as a golden age of capitalism.[59]

In late 1965 Time magazine ran a cover article with a title comment from Milton Friedman (later echoed by US President Richard Nixon), "We are all Keynesians now". The article described the exceptionally favourable economic conditions then prevailing and reported that "Washington's economic managers scaled these heights by their adherence to Keynes's central theme: the modern capitalist economy does not automatically work at top efficiency, but can be raised to that level by the intervention and influence of the government." The article also states that Keynes was one of the three most important economists who ever lived, and that his General Theory was more influential than the magna opera of other famous economists, such as Adam Smith's The Wealth of Nations.[87]

The concept of the multiplier was first developed by R. F. Kahn[88] in his article "The relation of home investment to unemployment"[89] in The Economic Journal of June 1931. The Kahn multiplier was the employment multiplier; Keynes took the idea from Kahn and formulated the investment multiplier.[90]

Keynesian economics were officially discarded by the British Government in 1979, but forces had begun to gather against Keynes's ideas over 30 years earlier. Friedrich Hayek had formed the Mont Pelerin Society in 1947, with the explicit intention of nurturing intellectual currents to one day displace Keynesianism and other similar influences. Its members included the Austrian School economist Ludwig von Mises along with the then-young Milton Friedman. Initially, the society had little impact on the wider world – according to Hayek it was as if Keynes had been raised to sainthood after his death and that people refused to allow his work to be questioned.[82][91]
Friedman, however, began to emerge as a formidable critic of Keynesian economics from the mid-1950s, and especially after his 1963 publication of A Monetary History of the United States.

On the practical side of economic life, "big government" had appeared to be firmly entrenched in the 1950s, but the balance began to shift towards the power of private interests in the 1960s. Keynes had written against the folly of allowing "decadent and selfish" speculators and financiers the kind of influence they had enjoyed after World War I. For two decades after World War II public opinion was strongly against private speculators, the disparaging label "Gnomes of Zurich" being typical of how they were described during this period. International speculation was severely restricted by the capital controls in place after Bretton Woods. According to the journalists Larry Elliott and Dan Atkinson, 1968 was the pivotal year when power shifted in favour of private agents such as currency speculators. As the key 1968 event Elliott and Atkinson picked out America's suspension of the conversion of the dollar into gold except on request of foreign governments, which they identified as the beginning of the breakdown of the Bretton Woods system.[92]

Criticisms of Keynes's ideas had begun to gain significant acceptance by the early 1970s, as they were then able to make a credible case that Keynesian models no longer reflected economic reality. Keynes himself included few formulas and no explicit mathematical models in his General Theory. For economists such as Hyman Minsky, Keynes's limited use of mathematics was partly the result of his scepticism about whether phenomena as inherently uncertain as economic activity could ever be adequately captured by mathematical models. Nevertheless, many models were developed by Keynesian economists, with a famous example being the Phillips curve which predicted an inverse relationship between unemployment and inflation. It implied that unemployment could be reduced by government stimulus with a calculable cost to inflation. In 1968, Milton Friedman published a paper arguing that the fixed relationship implied by the Philips curve did not exist.[93]
Friedman suggested that sustained Keynesian policies could lead to both unemployment and inflation rising at once – a phenomenon that soon became known as stagflation. In the early 1970s stagflation appeared in both the US and Britain just as Friedman had predicted, with economic conditions deteriorating further after the 1973 oil crisis. Aided by the prestige gained from his successful forecast, Friedman led increasingly successful criticisms against the Keynesian consensus, convincing not only academics and politicians but also much of the general public with his radio and television broadcasts. The academic credibility of Keynesian economics was further undermined by additional criticism from other monetarists trained in the Chicago school of economics, by the Lucas critique and by criticisms from Hayek's Austrian School.[59] So successful were these criticisms that by 1980 Robert Lucas claimed economists would often take offence if described as Keynesians.[94]

Keynesian principles fared increasingly poorly on the practical side of economics – by 1979 they had been displaced by monetarism as the primary influence on Anglo-American economic policy.[59] However, many officials on both sides of the Atlantic retained a preference for Keynes, and in 1984 the Federal Reserve officially discarded monetarism, after which Keynesian principles made a partial comeback as an influence on policy-making.[95]
Not all academics accepted the criticism against Keynes – Minsky has argued that Keynesian economics had been debased by excessive mixing with neoclassical ideas from the 1950s, and that it was unfortunate that this branch of economics had even continued to be called "Keynesian".[35] Writing in The American Prospect, Robert Kuttner argued it was not so much excessive Keynesian activism that caused the economic problems of the 1970s but the breakdown of the Bretton Woods system of capital controls, which allowed capital flight from regulated economies into unregulated economies in a fashion similar to Gresham's law phenomenon (where weak currencies undermine strong ones).[96]
Historian Peter Pugh has stated that a key cause of the economic problems afflicting America in the 1970s was the refusal to raise taxes to finance the Vietnam War, which was against Keynesian advice.[97]

A more typical response was to accept some elements of the criticisms while refining Keynesian economic theories to defend them against arguments that would invalidate the whole Keynesian framework – the resulting body of work largely composing New Keynesian economics. In 1992 Alan Blinder wrote about a "Keynesian Restoration", as work based on Keynes's ideas had to some extent become fashionable once again in academia, though in the mainstream it was highly synthesised with monetarism and other neoclassical thinking. In the world of policy making, free market influences broadly sympathetic to monetarism have remained very strong at government level – in powerful normative institutions like the World Bank, the IMF and US Treasury, and in prominent opinion-forming media such as the Financial Times and The Economist.[98]

The 2008 financial crisis led to public skepticism about the free market consensus even from some on the economic right. In March 2008, Martin Wolf, chief economics commentator at the Financial Times, announced the death of the dream of global free-market capitalism.[100] In the same month macroeconomist James K. Galbraith used the 25th Annual Milton Friedman Distinguished Lecture to launch a sweeping attack against the consensus for monetarist economics and argued that Keynesian economics were far more relevant for tackling the emerging crises.[101]
Economist Robert J. Shiller had begun advocating robust government intervention to tackle the financial crises, specifically citing Keynes.[102][103][104] Nobel laureate Paul Krugman also actively argued the case for vigorous Keynesian intervention in the economy in his columns for The New York Times.[105][106][107]
Other prominent economic commentators who have argued for Keynesian government intervention to mitigate the 2008 financial crisis included George Akerlof,[108] J. Bradford DeLong,[109] Robert Reich[110] and Joseph Stiglitz.[111]
Newspapers and other media have also cited work relating to Keynes by Hyman Minsky,[35] Robert Skidelsky,[24] Donald Markwell[112] and Axel Leijonhufvud.[113]

A series of major bailouts were pursued during the 2008 financial crisis, starting on 7 September 2008 with the announcement that the US Government was to nationalise the two government-sponsored enterprises which oversaw most of the US subprime mortgage market – Fannie Mae and Freddie Mac. In October, Alistair Darling, the British Chancellor of the Exchequer, referred to Keynes as he announced plans for substantial fiscal stimulus to head off the worst effects of recession, in accordance with Keynesian economic thought.[114][115] Similar policies have been adopted by other governments worldwide.[116][117] This was in stark contrast to the action imposed on Indonesia during the 1997 Asian financial crisis, when it was forced by the IMF to close 16 banks at the same time, prompting a bank run.[118]
Much of the post-crisis discussion reflected Keynes's advocacy of international coordination of fiscal or monetary stimulus, and of international economic institutions such as the IMF and the World Bank, which many had argued should be reformed as a "new Bretton Woods", and should have been even before the crises broke out.[119]
The IMF and United Nations economists advocated a coordinated international approach to fiscal stimulus.[120] Donald Markwell argued that in the absence of such an international approach, there would be a risk of worsening international relations and possibly even world war arising from economic factors similar to those present during the depression of the 1930s.[112]

By the end of December 2008, the Financial Times reported that "the sudden resurgence of Keynesian policy is a stunning reversal of the orthodoxy of the past several decades."[121]
In December 2008, Paul Krugman released his book The Return of Depression Economics and the Crisis of 2008, arguing that economic conditions similar to those that existed during the earlier part of the 20th century had returned, making Keynesian policy prescriptions more relevant than ever. In February 2009 Robert J. Shiller and George Akerlof published Animal Spirits, a book where they argue the current US stimulus package is too small as it does not take into account Keynes's insight on the importance of confidence and expectations in determining the future behaviour of businesspeople and other economic agents.

In the March 2009 speech entitled Reform the International Monetary System, Zhou Xiaochuan, the governor of the People's Bank of China, came out in favour of Keynes's idea of a centrally managed global reserve currency. Zhou argued that it was unfortunate that part of the reason for the Bretton Woods system breaking down was the failure to adopt Keynes's bancor. Zhou proposed a gradual move towards increased use of IMF special drawing rights (SDRs).[122][123]
Although Zhou's ideas had not been broadly accepted, leaders meeting in April at the 2009 G-20 London summit agreed to allow $250 billion of special drawing rights to be created by the IMF, to be distributed globally. Stimulus plans were credited for contributing to a better-than-expected economic outlook by both the OECD[124]
and the IMF,[125][126] in reports published in June and July 2009. Both organisations warned global leaders that recovery was likely to be slow, so counter-recessionary measures ought not be rolled back too early.

While the need for stimulus measures was broadly accepted among policymakers, there had been much debate over how to fund the spending. Some leaders and institutions, such as Angela Merkel[127]
and the European Central Bank,[128]
expressed concern over the potential impact on inflation, national debt and the risk that a too-large stimulus will create an unsustainable recovery.

Among professional economists the revival of Keynesian economics has been even more divisive. Although many economists, such as George Akerlof, Paul Krugman, Robert Shiller and Joseph Stiglitz, supported Keynesian stimulus, others did not believe higher government spending would help the United States economy recover from the Great Recession. Some economists, such as Robert Lucas, questioned the theoretical basis for stimulus packages.[129] Others, like Robert Barro and Gary Becker, say that empirical evidence for beneficial effects from Keynesian stimulus does not exist.[130] However, there is a growing academic literature that shows that fiscal expansion helps an economy grow in the near term, and that certain types of fiscal stimulus are particularly effective.[131][132]

New Keynesian economics developed in the 1990s and early 2000s as a response to the critique that macroeconomics lacked microeconomic foundations. New Keynesianism developed models to provide microfoundations for Keynesian economics. It incorporated parts of new classical macroeconomics to develop the new neoclassical synthesis, which forms the basis for mainstream macroeconomics today.[133][134][135][136]

Two main assumptions define the New Keynesian approach to macroeconomics. Like the New Classical approach, New Keynesian macroeconomic analysis usually assumes that households and firms have rational expectations. However, the two schools differ in that New Keynesian analysis usually assumes a variety of market failures. In particular, New Keynesians assume that there is imperfect competition[137] in price and wage setting to help explain why prices and wages can become "sticky", which means they do not adjust instantaneously to changes in economic conditions.

Wage and price stickiness, and the other market failures present in New Keynesian models, imply that the economy may fail to attain full employment. Therefore, New Keynesians argue that macroeconomic stabilisation by the government (using fiscal policy) and the central bank (using monetary policy) can lead to a more efficient macroeconomic outcome than a laissez faire policy would.

On a personal level, Keynes's charm was such that he was generally well received wherever he went – even those who found themselves on the wrong side of his occasionally sharp tongue rarely bore a grudge.[138] Keynes's speech at the closing of the Bretton Woods negotiations was received with a lasting standing ovation, rare in international relations, as the delegates acknowledged the scale of his achievements made despite poor health.[32]

Austrian School economist Friedrich Hayek was Keynes's most prominent contemporary critic, with sharply opposing views on the economy.[24]: 482–485  Yet after Keynes's death, he wrote: "He was the one really great man I ever knew, and for whom I had unbounded admiration. The world will be a very much poorer place without him."[139] Colleague Nicholas Davenport recalled, "There were deep emotional forces about Maynard ... One could sense his humanity. There was nothing of the cold intellectual about him."[140]

Lionel Robbins, former head of the economics department at the London School of Economics, who engaged in many heated debates with Keynes in the 1930s, had this to say after observing Keynes in early negotiations with the Americans while drawing up plans for Bretton Woods:[24]: 760–761 

This went very well indeed. Keynes was in his most lucid and persuasive mood: and the effect was irresistible. At such moments, I often find myself thinking that Keynes must be one of the most remarkable men that have ever lived – the quick logic, the birdlike swoop of intuition, the vivid fancy, the wide vision, above all the incomparable sense of the fitness of words, all combine to make something several degrees beyond the limit of ordinary human achievement. 
Douglas LePan, an official from the Canadian High Commission, wrote:[24]: 789 

I am spellbound. This is the most beautiful creature I have ever listened to. Does he belong to our species? Or is he from some other order? There is something mythic and fabulous about him. I sense in him something massive and sphinx like, and yet also a hint of wings. 
Bertrand Russell named Keynes one of the most intelligent people he had ever known,[141] commenting:[142]

Keynes's intellect was the sharpest and clearest that I have ever known. When I argued with him, I felt that I took my life in my hands, and I seldom emerged without feeling something of a fool.
Keynes's obituary in The Times included the comment: "There is the man himself – radiant, brilliant, effervescent, gay, full of impish jokes ... He was a humane man genuinely devoted to the cause of the common good."[57]

As a man of the centre described by some as having the greatest impact of any 20th-century economist,[50] Keynes attracted considerable criticism from both sides of the political spectrum. In the 1920s, Keynes was seen as anti-establishment and was mainly attacked from the right. In the "red 1930s", many young economists favoured Marxist views, even in Cambridge,[35] and while Keynes was engaging principally with the right to try to persuade them of the merits of more progressive policy, the most vociferous criticism against him came from the left, who saw him as a supporter of capitalism. From the 1950s and onwards, most of the attacks against Keynes have again been from the right.

In 1931, Friedrich Hayek extensively critiqued Keynes's 1930 Treatise on Money.[143] After reading Hayek's The Road to Serfdom, Keynes wrote to Hayek: "Morally and philosophically I find myself in agreement with virtually the whole of it."[144] He concluded the letter with the recommendation:

What we need therefore, in my opinion, is not a change in our economic programmes, which would only lead in practice to disillusion with the results of your philosophy; but perhaps even the contrary, namely, an enlargement of them. Your greatest danger is the probable practical failure of the application of your philosophy in the United States.
On the pressing issue of the time, whether deficit spending could lift a country from depression, Keynes replied to Hayek's criticism[145] in the following way:

I should... conclude rather differently. I should say that what we want is not no planning, or even less planning, indeed I should say we almost certainly want more. But the planning should take place in a community in which as many people as possible, both leaders and followers wholly share your moral position. Moderate planning will be safe enough if those carrying it out are rightly oriented in their minds and hearts to the moral issue.
Asked why Keynes expressed "moral and philosophical" agreement with Hayek's Road to Serfdom, Hayek stated:[146]

Because he believed that he was fundamentally still a classical English liberal and wasn't quite aware of how far he had moved away from it. His basic ideas were still those of individual freedom. He did not think systematically enough to see the conflicts. He was, in a sense, corrupted by political necessity.
According to some observers,[who?] Hayek felt that the post-World War II "Keynesian orthodoxy" gave too much power to the state, and that such policies would lead toward socialism.[147]

While Milton Friedman described The General Theory as "a great book", he argues that its implicit separation of nominal from real magnitudes is neither possible nor desirable. Macroeconomic policy, Friedman argues, can reliably influence only the nominal.[148] He and other monetarists have consequently argued that Keynesian economics can result in stagflation, the combination of low growth and high inflation that developed economies suffered in the early 1970s. More to Friedman's taste was the Tract on Monetary Reform (1923), which he regarded as Keynes's best work because of its focus on maintaining domestic price stability.[148]

Joseph Schumpeter was an economist of the same age as Keynes and one of his main rivals. He was among the first reviewers to argue that Keynes's General Theory was not a general theory, but a special case.[149] He said the work expressed "the attitude of a decaying civilisation". After Keynes's death Schumpeter wrote a brief biographical piece Keynes the Economist – on a personal level he was very positive about Keynes as a man, praising his pleasant nature, courtesy and kindness. He assessed some of Keynes's biographical and editorial work as among the best he'd ever seen. Yet Schumpeter remained critical of Keynes's economics, linking Keynes's childlessness to what Schumpeter saw as an essentially short-term view. He considered Keynes to have a kind of unconscious patriotism that caused him to fail to understand the problems of other nations. For Schumpeter, "Practical Keynesianism is a seedling which cannot be transplanted into foreign soil: it dies there and becomes poisonous as it dies."[150] He "admired and envied Keynes, but when Keynes died in 1946, Schumpeter's obituary gave Keynes this same off-key, perfunctory treatment he would later give Adam Smith in the History of Economic Analysis, the "discredit of not adding a single innovation to the techniques of economic analysis."[151]

Ludwig von Mises, an Austrian economist, describes a Keynesian system as believing it can solve most problems with "more money and credit" which leads to a system of "inflationism" in which "prices (of goods) rise higher and higher."[152] Murray Rothbard wrote that Keynesian-style governmental regulation of money and credit created a "dismal monetary and banking situation," since it allows for the central bankers that have the exclusive ability to print money to be "unchecked and out of control."[153] Rothbard went on to say in an interview that, "There is one good thing about (Karl) Marx: he was not a Keynesian."[154]

President Harry S. Truman was sceptical of Keynesian theorising. He told Leon Keyserling, a Keynesian economist who chaired Truman's Council of Economic Advisers: "Nobody can ever convince me that government can spend a dollar that it's not got."[51]

Some critics have sought to show that Keynes had sympathies towards Nazism, and a number of writers have described him as antisemitic. Keynes's private letters contain portraits and descriptions, some of which can be characterised as antisemitic, while others as philosemitic.[155][156]

Scholars have suggested that these reflect clichés current at the time that he accepted uncritically, rather than any racism.[157] On several occasions Keynes used his influence to help his Jewish friends, most notably when he successfully lobbied for Ludwig Wittgenstein to be allowed residency in the United Kingdom, explicitly to rescue him from being deported to Nazi-occupied Austria. Keynes was a supporter of Zionism, serving on committees supporting the cause.[157]

Allegations that he was racist or had totalitarian beliefs have been rejected by Robert Skidelsky and other biographers.[32] Professor Gordon Fletcher wrote that "the suggestion of a link between Keynes and any support of totalitarianism cannot be sustained".[59] Once the aggressive tendencies of the Nazis towards Jews and other minorities had become apparent, Keynes made clear his loathing of Nazism. As a lifelong pacifist he had initially favoured peaceful containment of Nazi Germany, yet he began to advocate a forceful resolution while many conservatives were still arguing for appeasement. After the war started, he roundly criticised the Left for losing their nerve to confront Adolf Hitler, saying:[24]: 586 

The intelligentsia of the Left were the loudest in demanding that the Nazi aggression should be resisted at all costs. When it comes to a showdown, scarce four weeks have passed before they remember that they are pacifists and write defeatist letters to your columns, leaving the defence of freedom and civilisation to Colonel Blimp and the Old School Tie, for whom Three Cheers.
Keynes's early romantic and sexual relationships were exclusively with men.[158] Keynes had been in relationships while at Eton and Cambridge; significant among these early partners were Dilly Knox and Daniel Macmillan.[26]: 27 [159] Keynes was open about his affairs, and from 1901 to 1915 kept separate diaries in which he tabulated his many sexual encounters.[160][161] Keynes's relationship and later close friendship with Macmillan was to be fortunate, as Macmillan's company first published his tract Economic Consequences of the Peace.[26]: 18 

Attitudes in the Bloomsbury Group, in which Keynes was avidly involved, were relaxed about homosexuality. Keynes, together with writer Lytton Strachey, had reshaped the Victorian attitudes of the Cambridge Apostles: "since [their] time, homosexual relations among the members were for a time common", wrote Bertrand Russell.[162] The artist Duncan Grant has been described as "the supreme male love of Keynes's life", and their sexual relationship lasted from 1908 to 1915.[163] Keynes was also involved with Lytton Strachey,[158] though they were, for the most part, love rivals rather than lovers. Keynes had won the affections of Arthur Hobhouse[164] and, as with Grant, fell out with a jealous Strachey over it.[165] Strachey had previously found himself put off by Keynes, not least because of his manner of "treat[ing] his love affairs statistically".[166]

Political opponents have used Keynes's sexuality to attack his academic work.[167] One line of attack held that he was uninterested in the long-term ramifications of his theories because he had no children.[167] Donald Kagan, in  On the Origins of War, quotes Sally Marks and Stephen A. Schuker to suggest that "[Keynes's] passion for Carl Melchior" distorted his position in favor of Germany.[168]

Keynes's friends in the Bloomsbury Group were initially surprised when, in his later years, he began pursuing affairs with women,[169] demonstrating himself to be bisexual.[170] Ray Costelloe (who later married Oliver Strachey) was an early heterosexual interest of Keynes.[171] In 1906, Keynes had written of this infatuation that, "I seem to have fallen in love with Ray a little bit, but as she isn't male I haven't [been] able to think of any suitable steps to take."[28]: 104 

In 1921, Keynes wrote that he had fallen "very much in love" with Lydia Lopokova, a well-known Russian ballerina and one of the stars of Sergei Diaghilev's Ballets Russes.[28]: 395  In the early years of his courtship, he maintained an affair with a younger man, Sebastian Sprott, in tandem with Lopokova, but eventually chose Lopokova exclusively.[172][173] They were married in 1925, with Keynes's former lover Duncan Grant as best man.[141][158] "What a marriage of beauty and brains, the fair Lopokova and John Maynard Keynes" was said at the time. Keynes later commented to Strachey that beauty and intelligence were rarely found in the same person, and that only in Duncan Grant had he found the combination.[174] The union was happy, with biographer Peter Clarke writing that the marriage gave Keynes "a new focus, a new emotional stability and a sheer delight of which he never wearied".[38][175] The couple hoped to have children but this did not happen.[38]

Among Keynes's Bloomsbury friends, Lopokova was, at least initially, subjected to criticism for her manners, mode of conversation and supposedly humble social origins – the last of the ostensible causes being particularly noted in the letters of Vanessa and Clive Bell, and Virginia Woolf.[176][177] In her novel Mrs Dalloway (1925), Woolf bases the character of Rezia Warren Smith on Lopokova.[178] E. M. Forster later wrote in contrition about "Lydia Keynes, whose every word should be recorded";[179] "How we all used to underestimate her".[176]

Keynes had no children, and his wife outlived him by 35 years, dying in 1981.

Keynes thought that the pursuit of money for its own sake was a pathological condition, and that the proper aim of work is to provide leisure. He wanted shorter working hours and longer holidays for all.[61]

Keynes was interested in literature in general and drama in particular and supported the Cambridge Arts Theatre financially, which allowed the institution to become one of the major British stages outside London.[141]

Keynes's interest in classical opera and dance led him to support the Royal Opera House at Covent Garden and the Ballet Company at Sadler's Wells. During the war, as a member of CEMA (Council for the Encouragement of Music and the Arts), Keynes helped secure government funds to maintain both companies while their venues were shut. Following the war, Keynes was instrumental in establishing the Arts Council of Great Britain and was its founding chairman in 1946. From the start, the two organisations that received the largest grants from the new body were the Royal Opera House and Sadler's Wells.

Keynes built up a substantial collection of fine art, including works by Paul Cézanne, Edgar Degas, Amedeo Modigliani, Georges Braque, Pablo Picasso and Georges Seurat (some of which can now be seen at the Fitzwilliam Museum).[141] He enjoyed collecting books; he collected and protected many of Isaac Newton's papers. In part on the basis of these papers, Keynes wrote of Newton as "the last of the magicians."[181]

Keynes, like other members of the Bloomsbury Group, was greatly influenced by the philosophy of G. E. Moore, which in 1938 he described as "still my religion under the surface".[182] According to Moore, states of mind were the only valuable things in themselves, the most important being "the pleasures of human intercourse and the enjoyment of beautiful objects".[183][184] Virginia Woolf's biographer tells an anecdote of how Virginia Woolf, Keynes and T. S. Eliot discussed religion at a dinner party, in the context of their struggle against Victorian era morality.[185]

Keynes may have been confirmed,[186] but according to Cambridge University he was clearly an agnostic, which he remained until his death.[187] According to one biographer, "he was never able to take religion seriously, regarding it as a strange aberration of the human mind"[186] but also added that he came to "value it for social and moral reasons" later in life.[188] Another biographer writes that he "broke the family faith and became a 'ferocious agnostic'" during his time at Eton.[189] One Cambridge acquaintance remembered him as "an atheist with a devotion to King's chapel".[29] At Cambridge, he was strongly associated with the Cambridge Heretics Society, an avowed atheist group which promoted secularism and humanism.[190]

Keynes was ultimately a successful investor, building up a private fortune. His assets were nearly wiped out following the Wall Street crash of 1929, which he did not foresee, but he soon recouped. At Keynes's death, in 1946, his net worth stood just short of £500,000 – equivalent to £23,000,000 in 2023. The sum had been amassed despite lavish support for various charities and philanthropies and despite his ethical reluctance to sell on a falling market in cases where he saw such behaviour as likely to deepen a slump.[c][24]: 520–521, 563–565 

Keynes managed the endowment of King's College, Cambridge starting in the 1920s, initially with an unsuccessful strategy based on market timing but later shifting to focus in the publicly traded stock of small and medium-sized companies that paid large dividends.[191] This was a controversial decision at the time, as stocks were considered high-risk and the centuries-old endowment had traditionally been invested in agricultural land and fixed income assets like bonds.[192] Keynes was granted permission to invest a small minority of assets in stocks, and his adroit management resulted this portion of the endowment growing to become the majority of the endowment's assets.[192] The active component of his portfolio outperformed a British equity index by an average of 6%[191] to 8% a year over a quarter century, earning him a favourable mention by later investors such as Warren Buffett and George Soros.[193]

Joel Tillinghast of Fidelity Investments describes Keynes as an early practitioner of value investing, a school of thought formalised in the US by Benjamin Graham and David Dodd at Columbia Business School during the 1920s and 1930s.[191] However, Keynes is believed to have developed his ideas independently.[192] Keynes is also regarded as a pioneer of financial diversification as he recognised the importance of holding assets with "opposed risks" as he wrote "since they are likely to move in opposite directions when there are general fluctuations";[194] and also as an early international investor who avoided home country bias by investing substantially in stocks outside the United Kingdom.[195] Ken Fisher characterised Keynes as an exception to the rule that economists usually make horrible investors.[194]

Keynes joined the Board of the National Mutual Life Assurance Society in 1919 and served as chairman from 1921 to 1938. Keynes introduced a policy of active trading of fixed interest stocks, coupled with investment in equities. "Keynes was the first to give [investment trading] the seal of respectability and to apply it to a life assurance fund".[140][196]

Olivier Accominotti and David Chambers have pointed out that Keynes did not make use of currency trading and the carry trade in his investments.[197] Keynes understood the strategy however, but considered that in his time the separation of the interest rates was not sufficient to pay for the costs of transporting the capital as gold, as he explained to the Macmillan Committee of 1930.[198][199]

Keynes was a lifelong member of the Liberal Party, which until the 1920s had been one of the two main political parties in the United Kingdom, and as late as 1916 had often been the dominant power in government. Keynes had helped campaign for the Liberals at elections from about 1906, yet he always refused to run for office himself, despite being asked to do so on three separate occasions in 1920. From 1926, when Lloyd George became leader of the Liberals, Keynes took a major role in defining the party's economic policy, but by then the Liberals had been displaced into third-party status by the growing workers-oriented Labour Party.[24]: 380–384 

In 1939 Keynes had the option to enter Parliament as an independent MP with the University of Cambridge seat. A by-election for the seat was to be held due to the illness of an elderly Tory, and the master of Magdalene College had obtained agreement that none of the major parties would field a candidate if Keynes chose to stand. Keynes declined the invitation as he felt he would wield greater influence on events if he remained a free agent.[38]

Keynes was a proponent of eugenics.[200] He served as director of the British Eugenics Society from 1937 to 1944. As late as 1946, shortly before his death, Keynes declared eugenics to be "the most important, significant and, I would add, genuine branch of sociology which exists."[201]

Keynes once remarked that "the youth had no religion save communism and this was worse than nothing."[185] Marxism "was founded upon nothing better than a misunderstanding of Ricardo", and, given time, he (Keynes) "would deal thoroughly with the Marxists" and other economists to solve the economic problems their theories "threaten to cause".[185] In 1925, Keynes said "the class war will find me on the side of the educated bourgeoisie."[202][203]

In 1931 Keynes had the following to say on Leninism:[43]: 300 

How can I accept a doctrine, which sets up as its bible, above and beyond criticism, an obsolete textbook [Das Kapital] which I know not only to be scientifically erroneous but without interest or application to the modern world? How can I adopt a creed which, preferring the mud to the fish, exalts the boorish proletariat above the bourgeoisie and the intelligentsia who, with whatever faults, are the quality of life and surely carry the seeds of all human advancement? Even if we need a religion, how can we find it in the turbid rubbish of the red bookshop? It is hard for an educated, decent, intelligent son of Western Europe to find his ideals here, unless he has first suffered some strange and horrid process of conversion which has changed all his values.
Keynes was a firm supporter of women's rights and in 1932 became vice-chairman of the Marie Stopes Society which provided birth control education. He also campaigned against job discrimination against women and unequal pay. He was an outspoken campaigner for reform of the laws against homosexuality.[61]



Throughout his life, Keynes worked energetically for the benefit both of the public and his friends; even when his health was poor, he laboured to sort out the finances of his old college.[206] Helping to set up the Bretton Woods system, he worked to institute an international monetary system that would be beneficial for the world economy. In 1946, Keynes suffered a series of heart attacks, which ultimately proved fatal. They began during negotiations for the Anglo-American loan in Savannah, Georgia, where he was trying to secure favourable terms for the United Kingdom from the United States, a process he described as "absolute hell".[50][207] A few weeks after returning from the United States, Keynes died of a heart attack at Tilton, his farmhouse home near Firle, East Sussex, England, on 21 April 1946, at the age of 62.[24]: 832 [208] Against his wishes (he wanted his ashes to be deposited in the crypt at King's), his ashes were scattered on the Downs above Tilton.[209]

Both of Keynes's parents outlived him: his father John Neville Keynes (1852–1949) by three years, and his mother Florence Ada Keynes (1861–1958) by twelve. Keynes's brother Sir Geoffrey Keynes (1887–1982) was a distinguished surgeon, scholar and bibliophile. Keynes's sister, Margaret Hill (1885–1970) was a prominent social reformer. His nephews include Richard Keynes (1919–2010), a physiologist, and Quentin Keynes (1921–2003), an adventurer and bibliophile. His niece, Polly Hill (1914–2005), was an economic anthropologist and emeritus fellow of Clare Hall, Cambridge. 

In John Buchan's novel Island of Sheep (1936) the character of the financier Barralty is based on Keynes.[210]

In the film Wittgenstein (1993), directed by Derek Jarman, Keynes was played by John Quentin.[211]

The docudrama Paris 1919, based around Margaret MacMillan's book, featured Paul Bandey as Keynes.[212]

In the BBC series about the Bloomsbury Group, Life in Squares, Keynes was portrayed by Edmund Kingsley.[213]

The novel Mr Keynes' Revolution (2020) by E. J. Barnes is about Keynes's life in the 1920s.[214]

Love Letters, based on the correspondence of Keynes and Lydia Lopokova, was performed by Tobias Menzies and Helena Bonham-Carter at Charleston in 2021.[215]

(A partial list.)