Troilus[1] (English:  /ˈtrɔɪləs/ or /ˈtroʊələs/; Ancient Greek: Τρωΐλος, romanized: Troïlos; Latin: Troilus) is a legendary character associated with the story of the Trojan War. The first surviving reference to him is in Homer's Iliad, composed in the late 8th century BC.

In Greek mythology, Troilus is a young Trojan prince, one of the sons of King Priam (or Apollo) and Hecuba. Prophecies link Troilus' fate to that of Troy and so he is ambushed and murdered by Achilles. Sophocles was one of the writers to tell this tale. It was also a popular theme among artists of the time. Ancient writers treated Troilus as the epitome of a dead child mourned by his parents. He was also regarded as a paragon of youthful male beauty.

In Western European medieval and Renaissance versions of the legend, Troilus is the youngest of Priam's five legitimate sons by Hecuba. Despite his youth he is one of the main Trojan war leaders. He dies in battle at Achilles' hands. In a popular addition to the story, originating in the 12th century, Troilus falls in love with Cressida, whose father Calchas has defected to the Greeks. Cressida pledges her love to Troilus but she soon switches her affections to the Greek hero Diomedes when sent to her father in a hostage exchange. Chaucer and Shakespeare are among the authors who wrote works telling the story of Troilus and Cressida. Within the medieval tradition, Troilus was regarded as a paragon of the faithful courtly lover and also of the virtuous pagan knight. Once the custom of courtly love had faded, his fate was regarded less sympathetically.

Little attention was paid to the character during the 18th and 19th centuries. However, Troilus has reappeared in 20th and 21st century retellings of the Trojan War by authors who have chosen elements from both the classical and medieval versions of his story.

For the ancient Greeks, the tale of the Trojan War and the surrounding events appeared in its most definitive form in the Epic Cycle of eight narrative poems[2] from the archaic period in Greece (750 BC – 480 BC). The story of Troilus is one of a number of incidents that helped provide structure to a narrative that extended over several decades and 77 books from the beginning of the Cypria to the end of the Telegony. The character's death early in the war and the prophecies surrounding him demonstrated that all Trojan efforts to defend their home would be in vain. His symbolic significance is evidenced by linguistic analysis of his Greek name "Troilos". It can be interpreted as an elision of the names of Tros and Ilos, the legendary founders of Troy, as a diminutive or pet name "little Tros" or as an elision of Troíē (Troy) and lúein (to destroy). These multiple possibilities emphasise the link between the fates of Troilus and of the city where he lived.[3] On another level, Troilus' fate can also be seen as foreshadowing the subsequent deaths of his murderer Achilles, and of his nephew Astyanax and sister Polyxena, who, like Troilus, die at the altar in at least some versions of their stories.[4]

Given this, it is unfortunate that the Cypria—the part of the Epic Cycle that covers the period of the Trojan War of Troilus' death—does not survive. Indeed, no complete narrative of his story remains from archaic times or the subsequent classical period (479–323 BC). Most of the literary sources from before the Hellenistic age (323–30 BC) that even referred to the character are lost or survive only in fragments or summary. The surviving ancient and medieval sources, whether literary or scholarly, contradict each other, and many do not tally with the form of the myth that scholars now believe to have existed in the archaic and classical periods.

Partially compensating for the missing texts are the physical artifacts that remain from the archaic and classical periods. The story of the circumstances around Troilus' death was a popular theme among pottery painters. (The Beazley Archive website lists 108 items of Attic pottery alone from the 6th to 4th centuries BC containing images of the character.[5]) Troilus also features on other works of art and decorated objects from those times. It is a common practice for those writing about the story of Troilus as it existed in ancient times to use both literary sources and artifacts to build up an understanding of what seems to have been the most standard form of the myth and its variants.[6] The brutality of this standard form of the myth is highlighted by commentators such as Alan Sommerstein, an expert on ancient Greek drama, who describes it as "horrific" and "[p]erhaps the most vicious of all the actions traditionally attributed to Achilles."[7]

Troilus is an adolescent boy or young man, the son of Hecuba, queen of Troy. As he is so beautiful, Troilus is taken to be the son of the god Apollo.[8] However, Hecuba's husband, King Priam, treats him as his own much-loved child.

A prophecy says that Troy will not fall if Troilus lives to the age of twenty. So the goddess Athena encourages the Greek warrior Achilles to seek him out early in the Trojan War. Troilus is known to take great delight in his horses. Achilles ambushes him and his sister Polyxena when he has ridden with her for water from a well in the Thymbra – an area outside Troy where there is a temple of Apollo.

The Greek is struck by the beauty of both Trojans and is filled with lust. It is the fleeing Troilus whom swift-footed[9] Achilles catches, dragging him by the hair from his horse. The young prince refuses to yield to Achilles' sexual attentions and somehow escapes, taking refuge in the nearby temple. But the warrior follows him in, and beheads him at the altar before help can arrive. The mourning of the Trojans at Troilus' death afterward is great.

This sacrilege leads to Achilles’ own death, when Apollo avenges himself by helping Paris strike Achilles with the arrow that pierces his heel.

The earliest surviving literary reference to Troilus is in Homer's Iliad, which formed one part of the Epic Cycle. It is believed that Troilus' name was not invented by Homer and that a version of his story was already in existence.[10] Late in the poem, Priam berates his surviving sons, and compares them unfavourably to their dead brothers including Trôïlon hippiocharmên.[11] The interpretation of hippiocharmên is controversial but the root hipp- implies a connection with horses. For the purpose of the version of the myth given above, the word has been taken as meaning "delighting in horses".[12] Sommerstein believes that Homer wishes to imply in this reference that Troilus was killed in battle, but argues that Priam's later description of Achilles as andros paidophonoio ("boy-slaying man")[13] indicates that Homer was aware of the story of Troilus as a murdered child; Sommerstein believes that Homer is playing here on the ambiguity of the root paido- meaning boy in both the sense of a young male and of a son.[14]

Troilus' death was also described in the Cypria, one of the parts of the Epic Cycle that is no longer extant. The poem covered the events preceding the Trojan War and the first part of the war itself up to the events of the Iliad. Although the Cypria does not survive, most of an ancient summary of the contents, thought to be by Eutychius Proclus, remains. Fragment 1 mentions that Achilles killed Troilus, but provides no more detail.[15] However, Sommerstein takes the verb used to describe the killing (phoneuei) as meaning that Achilles murders Troilus.[16]

In Athens, the early tragedians Phrynicus and Sophocles both wrote plays called Troilos and the comic playwright Strattis wrote a parody of the same name. Of the esteemed Nine lyric poets of the archaic and classical periods, Stesichorus may have referred to Troilus' story in his Iliupersis and Ibycus may have written in detail about the character. With the exception of these authors, no other pre-Hellenistic written source is known to have considered Troilus at any length.[17]

Unfortunately, all that remains of these texts are the smallest fragments or summaries and references to them by other authors. What does survive can be in the form of papyrus fragments, plot summaries by later authors or quotations by other authors. In many cases these are just odd words in lexicons or grammar books with an attribution to the original author.[18] Reconstructions of the texts are necessarily speculative and should be viewed with "wary but sympathetic scepticism".[19] In Ibycus' case all that remains is a parchment fragment containing a mere six or seven words of verse accompanied with a few lines of scholia. Troilus is described in the poem as godlike and is killed outside Troy. From the scholia, he is clearly a boy. The scholia also refer to a sister, someone "watching out" and a murder in the sanctuary of Thymbrian Apollo. While acknowledging that these details may have been reports of other later sources, Sommerstein thinks it probable that Ibycus told the full ambush story and is thus the earliest identifiable source for it.[20] Of Phrynicus, one fragment remains considered to refer to Troilus. This speaks of "the light of love glowing on his reddening cheeks".[21]

Of all these fragmentary pre-Hellenistic sources, the most is known of Sophocles Troilos. Even so, only 54 words have been identified as coming from the play.[22] Fragment 619 refers to Troilus as an andropais, a man-boy. Fragment 621 indicates that Troilus was going to a spring with a companion to fetch water or to water his horses.[23] A scholion to the Iliad[24] states that Sophocles has Troilus ambushed by Achilles while exercising his horses in the Thymbra. Fragment 623 indicates that Achilles mutilated Troilus' corpse by a method known as maschalismos. This involved preventing the ghost of a murder victim from returning to haunt their killer by cutting off the corpse's extremities and stringing them under its armpits.[25] Sophocles is thought to have also referred to the maschalismos of Troilus in a fragment taken to be from an earlier play Polyxene.[26]

Sommerstein attempts a reconstruction of the plot of the Troilos, in which the title character is incestuously in love with Polyxena and tries to discourage the interest in marrying her shown by both Achilles and Sarpedon, a Trojan ally and son of Zeus. Sommerstein argues that Troilus is accompanied on his fateful journey to his death, not by Polyxena, but by his tutor, a eunuch Greek slave.[27] Certainly there is a speaking role for a eunuch who reports being castrated by Hecuba[28] and someone reports the loss of their adolescent master.[29] The incestuous love is deduced by Sommerstein from a fragment of Strattis' parody, assumed to partially quote Sophocles, and from his understanding that the Sophocles play intends to contrast barbarian customs, including incest, with Greek ones. Sommerstein also sees this as solving what he considers the need for an explanation of Achilles' treatment of Troilus' corpse, the latter being assumed to have insulted Achilles in the process of warning him off Polyxena.[30] Italian professor of English and expert on Troilus, Piero Boitani, on the other hand, considers Troilus' rejection of Achilles' sexual advances towards him as sufficient motive for the mutilation.[31]

The first surviving text with more than the briefest mention of Troilus is Alexandra, a Hellenistic poem dating from no earlier than the 3rd century BC by the tragedian Lycophron (or a namesake of his). The poem consists of the obscure prophetic ravings of Cassandra:[32]

Ay! me, for thee fair-fostered flower, too, I groan, O lion whelp, sweet darling of thy kindred, who didst smite with fiery charm of shafts the fierce dragon and seize for a little loveless while in unescapable noose him that was smitten, thyself unwounded by thy victim: thou shalt forfeit thy head and stain thy father’s altar-tomb with thy blood.[33]
This passage is explained in the Byzantine writer John Tzetzes' scholia as a reference to Troilus seeking to avoid the unwanted sexual advances of Achilles by taking refuge in his father Apollo's temple. When he refuses to come out, Achilles goes in and kills him on the altar.[34] Lycophron's scholiast also says that Apollo started to plan Achilles' death after the murder.[35] This begins to build up the elements of the version of Troilus' story given above: he is young, much loved and beautiful; he has divine ancestry, is beheaded by his rejected Greek lover and, we know from Homer, had something to do with horses. The reference to Troilus as a "lion whelp" hints at his having the potential to be a great hero, but there is no explicit reference to a prophecy linking the possibility of Troilus reaching adulthood and Troy then surviving.

No other extended passage about Troilus exists from before the Augustan Age by which time other versions of the character's story have emerged. The remaining sources compatible with the standard myth are considered below by theme.

Ancient Greek art, as found in pottery and other remains, frequently depicts scenes associated with Troilus' death: the ambush, the pursuit, the murder itself and the fight over his body.[57] Depictions of Troilus in other contexts are unusual. One such exception, a red-figure vase painting from Apulia c.340BC, shows Troilus as a child with Priam.[58]

In the ambush, Troilus and Polyxena approach a fountain where Achilles lies in wait. This scene was familiar enough in the ancient world for a parody to exist from c.400BC showing a dumpy Troilus leading a mule to the fountain.[59] In most serious depictions of the scene, Troilus rides a horse, normally with a second next to him.[60] He is usually, but not always, portrayed as a beardless youth. He is often shown naked; otherwise he wears a cloak or tunic. Achilles is always armed and armoured. Occasionally, as on the vase picture at  [35], or the fresco from the Tomb of the Bulls shown at the head of this article, either Troilus or Polyxena is absent, indicating how the ambush is linked to each of their stories. In the earliest definitely identified version of this scene, (a Corinthian vase c.580BC), Troilus is bearded and Priam is also present. Both these features are unusual.[61] More common is a bird sitting on the fountain; normally a raven, symbol of Apollo and his prophetic powers and thus a final warning to Troilus of his doom;[62] sometimes a cock, a common love gift suggesting that Achilles attempted to seduce Troilus.[63] In some versions, for example an Attic amphora in the Museum of Fine Arts, Boston dating from c.530BC (seen here [36]) Troilus has a dog running with him. On one Etruscan vase from the 6th century BC, doves are flying from Achilles to Troilus, suggestive of the love gift in Servius.[64] The fountain itself is conventionally decorated with a lion motif.

The earliest identified version of the pursuit or chase is from the third quarter of the 7th century BC.[59] Next chronologically is the best known[65] version on the François Vase by Kleitias.[66] The number of characters shown on pottery scenes varies with the size and shape of the space available.[67]  The François Vase is decorated with several scenes in long narrow strips. This means that the Troilus frieze is heavily populated. In the centre, (which can be seen at the Perseus Project at [37],) is the fleeing Troilus, riding one horse with the reins of the other in his hand. Below them is the vase—which Polyxena (partially missing), who is ahead of him, has dropped. Achilles is largely missing but it is clear that he is armoured. They are running towards Troy [38] where Antenor gestures towards Priam. Hector and Polites, brothers of Troilus, emerge from the city walls in the hope of saving Troilus. Behind Achilles [39] are a number of deities, Athena, Thetis (Achilles' mother), Hermes, and Apollo (just arriving). Two Trojans are also present, the woman gesturing to draw the attention of a youth filling his vase. As the deities appear only in pictorial versions of the scene, their role is subject to interpretation. Boitani sees Athena as urging Achilles on and Thetis as worried by the arrival of Apollo who, as Troilus' protector, represents a future threat to Achilles.[68] He does not indicate what he thinks Hermes may be talking to Thetis about. The classicist and art historian Professor Thomas H. Carpenter sees Hermes as a neutral observer, Athena and Thetis as urging Achilles on, and the arrival of Apollo as the artist's indication of the god's future role in Achilles' death.[61] As Athena is not traditionally a patron of Achilles, Sommerstein sees her presence in this and other portrayals of Troilus' death as evidence of the early standing of the prophetic link between Troilus' death and the fall of Troy, Athena being driven, above all, by her desire for the city's destruction.[69]

The standard elements in the pursuit scene are Troilus, Achilles, Polyxena, the two horses and the fallen vase. On two tripods, an amphora and a cup, Achilles already has Troilus by the hair.[70] A famous vase in the British Museum, which gave the Troilos Painter the name by which he is now known, shows the two Trojans looking back in fear, as the beautiful youth whips his horse on. This vase can be seen at the Perseus Project site [40]. The water spilling from the shattered vase below Troilus' horse, symbolises the blood he is about to shed.[71]

The iconography of the eight legs and hooves of the horses can be used to identify Troilus on pottery where his name does not appear; for example, on a Corinthian vase where Troilus is shooting at his pursuers and on a peaceful scene on a Chalcidian krater where the couples Paris and Helen, Hector and Andromache are labelled, but the youth riding one of a pair of horses is not.[72]

A later Southern Italian interpretation of the story is on vases held respectively at the Boston Museum of Fine Arts and the Hermitage Museum in St Petersburg. On the krater from c.380-70BC at [41] Troilus can be seen with just one horse trying to defend himself with a throwing spear; on the hydria from c.325-320BC at [42], Achilles is pulling down the youth's horse.

The earliest known depictions of the death or murder of Troilus are on shield bands from the turn of the 7th into the 6th century BC found at Olympia. On these, a warrior with a sword is about to stab a naked youth at an altar. On one, Troilus clings to a tree (which Boitani takes for the laurel sacred to Apollo).[74]  A crater contemporary with this shows Achilles at the altar holding the naked Troilus upside down while Hector, Aeneas and an otherwise unknown Trojan Deithynos arrive in the hope of saving the youth. In some depictions Troilus is begging for mercy. On an amphora, Achilles has the struggling Troilus slung over his shoulder as he goes to the altar.[75] Boitani, in his survey of the story of Troilus through the ages, considers it of significance that two artifacts (a vase and a sarcophagus) from different periods link Troilus' and Priam's death by showing them on the two sides of the same item, as if they were the beginning and end of the story of the fall of Troy.[76]  Achilles is the father of Neoptolemus, who slays Priam at the altar during the sack of Troy. Thus the war opens with a father killing a son and closes with a son killing a father.

Some pottery shows Achilles, already having killed Troilus, using his victim's severed head as a weapon as Hector and his companions arrive too late to save him; some includes the watching Athena, occasionally with Hermes. At [43] is one such picture showing Achilles fighting Hector over the altar. Troilus' body is slumped and the boy's head is either flying through the air, or stuck to the end of Achilles' spear. Athena and Hermes look on. Aeneas and Deithynos are behind Hector.

Sometimes details of the closely similar deaths of Troilus and Astyanax are exchanged.[77] [44] shows one such image where it is unclear which murder is portrayed. The age of the victim is often an indicator of which story is being told and the relative small size here might point towards the death of Astyanax, but it is common to show even Troilus  as much smaller than his murderer, (as is the case with the kylix pictured to the above right). Other factors in this case are the presence of Priam (suggesting Astyanax), that of Athena (suggesting Troilus) and the fact that the scene is set outside the walls of Troy (again suggesting Troilus).[78]

A different version of Troilus' death appears on a red-figure cup by Oltos. Troilus is on his knees, still in the process of drawing his sword when Achilles' spear has already stabbed him and Aeneas comes too late to save him. Troilus wears a helmet, but it is pushed up to reveal a beautiful young face. This is the only such depiction of Troilus' death in early figurative art.[79] However, this version of Troilus as a youth defeated in battle appears also in written sources.

This version of the story appears in Virgil's Aeneid,[80] in a passage describing a series of paintings decorating the walls of a temple of Juno. The painting immediately next to the one depicting Troilus shows the death of Rhesus, another character killed because of prophecies linked to the fall of Troy. Other pictures are similarly calamitous.

 In a description whose pathos is heightened by the fact that it is seen through a compatriot's eyes,[81] Troilus is infelix puer ("unlucky boy") who has met Achilles in "unequal" combat. Troilus' horses flee while he, still holding their reins, hangs from the chariot, his head and hair trailing behind while the backward-pointing spear scribbles in the dust. (The First Vatican Mythographer[41] elaborates on this story, explaining that Troilus's body is dragged right to the walls of Troy.[82])

In his commentary on the Aeneid, Servius[48] considers this story as a deliberate departure from the "true" story, bowdlerized to make it more suitable for an epic poem. He interprets it as showing Troilus overpowered in a straight fight. Gantz,[83] however, argues that this might be a variation of the ambush story. For him, Troilus is unarmed because he went out not expecting combat and the backward pointing spear was what Troilus was using as a goad in a manner similar to characters elsewhere in the Aeneid. Sommerstein, on the other hand believes that the spear is Achilles' that has struck Troilus in the back. The youth is alive but mortally wounded as he is being dragged towards Troy.[84]

An issue here is the ambiguity of the word congressus ("met"). It often refers to meeting in a conventional combat but can have reference to other types of meetings too. A similar ambiguity appears in Seneca[85]  and in  Ausonius' 19th epitaph,[86] narrated by Troilus himself. The dead prince tells how he has been dragged by his horses after falling in unequal battle with Achilles. A reference in the epitaph comparing Troilus' death to Hector's suggests that Troilus dies later than in the traditional narrative, something that, according to Boitani,[87] also happens in Virgil.

Quintus of Smyrna, in a passage whose atmosphere Boitani describes as sad and elegiac, retains what for Boitani are the two important issues of the ancient story, that Troilus is doomed by Fate and that his failure to continue his line symbolises Troy's fall.[88] In this case, there is no doubt that Troilus entered battle knowingly, for in the Posthomerica Troilus's armour is one of the funerary gifts after Achilles' own death. Quintus repeatedly emphasises Troilus's youth: he is beardless, virgin of a bride, childlike, beautiful, the most godlike of all Hecuba's children. Yet he was lured by Fate to war when he knew no fear and was struck down by Achilles' spear just as a flower or corn that has borne no seed is killed by the gardener.[89]

In the Ephemeridos belli Trojani (Journal of the Trojan War),[90] supposedly written by Dictys the Cretan during the Trojan War itself, Troilus is again a defeated warrior, but this time captured with his brother Lycaon. Achilles vindictively orders that their throats be slit in public, because he is angry that Priam has failed to advance talks over a possible marriage to Polyxena. Dictys' narrative is free from gods and prophecy but he preserves Troilus' loss as something to be greatly mourned:

The Trojans raised a cry of grief and, mourning loudly, bewailed the fact that Troilus had met so grievous a death, for they remembered how young he was, who being in the early years of his manhood, was the people's favourite, their darling, not only because of his modesty and honesty, but more especially because of his handsome appearance.[91]

In the sources considered so far, Troilus' only narrative function is his death.[92] The treatment of the character changes in two ways in the literature of the medieval and renaissance periods. First, he becomes an important and active protagonist in the pursuit of the Trojan War itself. Second, he becomes an active heterosexual lover, rather than the passive victim of Achilles' pederasty. By the time of John Dryden's neo-classical adaptation of Shakespeare's Troilus and Cressida it is the ultimate failure of his love affair that defines the character.

For medieval writers, the two most influential ancient sources on the Trojan War were the purported eye-witness accounts of Dares the Phrygian, and Dictys the Cretan, which both survive in Latin versions. In Western Europe the Trojan side of the war was favoured and therefore Dares was preferred over Dictys.[93] Although Dictys' account positions Troilus' death later in the war than was traditional, it conforms to antiquity's view of him as a minor warrior if one at all. Dares' De excidio Trojae historia (History of the Fall of Troy)[94] introduces the character as a hero who takes part in events beyond the story of his death.

Authors of the 12th and 13th centuries such as Joseph of Exeter and Albert of Stade continued to tell the legend of the Trojan War in Latin in a form that follows Dares' tale with Troilus remaining one of the most important warriors on the Trojan side. However, it was two of their contemporaries, Benoît de Sainte-Maure in his French verse romance and Guido delle Colonne in his Latin prose history, both also admirers of Dares, who were to define the tale of Troy for the remainder of the medieval period. The details of their narrative of the war were copied, for example, in the Laud and Lydgate Troy Books and also in Raoul Lefevre's Recuyell of the Historyes of Troye. Lefevre, through Caxton's 1474 printed translation, was in turn to become the best known retelling of the Troy story in Renaissance England and influenced Shakespeare among others. The story of Troilus as a lover, invented by Benoît and retold by Guido, generated a second line of influence. It was taken up as a tale that could be told in its own right by Boccaccio and then by Chaucer who established a tradition of retelling and elaborating the story in English-language literature, which was to be followed by Henryson and Shakespeare.

As indicated above, it was through the writings of Dares the Phrygian that the portrayal of Troilus as an important warrior was transmitted to medieval times. However, some authors have argued that the tradition of Troilus as a warrior may be older. The passage from the Iliad described above is read by Boitani[95] as implying that Priam put Troilus on a par with the very best of his warrior sons. The description of him in that passage as hippiocharmên is rendered by some authorities as meaning a warrior charioteer rather than merely someone who delights in horses.[12] The many missing and partial literary sources might include such a hero. Yet only a single known ancient vase shows Troilus as a warrior falling in a conventional battle.[79]

In Dares, Troilus is the youngest of Priam's royal sons, bellicose when peace or truces are suggested and the equal of Hector in bravery, "large and most beautiful... brave and strong for his age, and eager for glory."[97] He slaughters many Greeks, wounds Achilles and Menelaus, routs the Myrmidons more than once before his horse falls and traps him and Achilles takes the opportunity to put an end to his life. Memnon rescues the body, something that didn't happen in many later versions of the tale. Troilus' death comes near the end of the war not at its beginning. He now outlives Hector and succeeds him as the Trojans' great leader in battle. Now it is in reaction to Troilus's death that Hecuba plots Achilles' murder.

As the tradition of Troilus the warrior advances through time, the weaponry and the form of combat change. Already in Dares he is a mounted warrior, not a charioteer or foot warrior, something anachronistic to epic narrative.[98] In later versions he is a knight with armour appropriate to the time of writing who fights against other knights and dukes. His expected conduct, including his romance, conforms to courtly or other values contemporary to the writing.

The medieval texts follow Dares' structuring of the narrative in describing Troilus after his parents and four royal brothers Hector, Paris, Deiphobus and Helenus.

Joseph of Exeter, in his Daretis Phrygii Ilias De bello Troiano (The Iliad of Dares the Phrygian on the Trojan War), describes the character as follows:

The limbs of Troilus expand and fill his space.
In mind a giant, though a boy in years, he yields
to none in daring deeds with strength in all his parts
his greater glory shines throughout his countenance.[99]

Benoît de Sainte-Maure's description in Le Roman de Troie (The Romance of Troy) is too long to quote in full, but influenced the descriptions that follow. Benoît goes into details of character and facial appearance avoided by other writers. He tells that Troilus was "the fairest of the youths of Troy" with:

fair hair, very charming and naturally shining, eyes bright and full of gaiety... He was not insolent or haughty, but light of heart and gay and amorous. Well was he loved, and well did he love...[100]

Guido delle Colonne's Historia destructionis Troiae (History of the Destruction of Troy) says:

The fifth and last was named Troilus, a young man as courageous as possible in war, about whose valour there are many tales which the present history does not omit later on.[101]

The Laud Troy Book:

The youngest doughti Troylus
A doughtier man than he was on
Of hem alle was neuere non,-
Save Ector, that was his brother
There never was goten suche another.[102]

The boy who in the ancient texts was never Achilles' match has now become a young knight, a worthy opponent to the Greeks.

In the medieval and renaissance tradition, Troilus is one of those who argue most for war against the Greeks in Priam's council. In several texts, for example the Laud Troy Book, he says that those who disagree with him are better suited to be priests.[103] Guido, and writers who follow him, have Hector, knowing how headstrong his brother can be, counsel Troilus not to be reckless before the first battle.[104]

In the medieval texts, Troilus is a doughty knight throughout the war, taking over, as in Dares, after Hector's death as the main warrior on the Trojan side. Indeed he is named as a second Hector by Chaucer and Lydgate.[105]  These two poets follow Boccaccio in reporting that Troilus kills thousands of Greeks.[106] However, the comparison with Hector can be seen as acknowledging Troilus' inferiority to his brother through the very need to mention him.[107]

In Joseph, Troilus is greater than Alexander, Hector, Tydeus, Bellona and even Mars, and kills seven Greeks with one blow of his club. He does not strike at opponents' legs because that would demean his victory. He only fights knights and nobles, and disdains facing the common warriors.

Albert of Stade saw Troilus as so important that he is the title character of his version of the Trojan War. He is "the wall of his homeland, Troy's protection, the rose of the military...."[108]

The list of Greek leaders Troilus wounds expands in the various re-tellings of the war from the two in Dares to also include Agamemnon, Diomedes and Menelaus. Guido, in keeping his promise to tell of all Troilus' valorous deeds, describes many incidents. Troilus is usually victorious but is captured in an early battle by Menestheus before his friends rescue him. This incident reappears in the imitators of Guido, such as Lefevre and the Laud and Lydgate Troy Books.[109]

Within the medieval Trojan tradition, Achilles withdraws from fighting in the war because he is to marry Polyxena. Eventually, so many of his followers are killed that he decides to rejoin the battle leading to Troilus' death and, in turn, to Hecuba, Polyxena and Paris plotting Achilles' murder.

Albert and Joseph follow Dares in having Achilles behead Troilus as he tries to rise after his horse falls. In Guido and authors he influenced, Achilles specifically seeks out Troilus to avenge a previous encounter where Troilus has wounded him. He therefore instructs the Myrmidons to find Troilus, surround him and cut him off from rescue.

In the Laud Troy Book, this is because Achilles almost killed Troilus in the previous fight but the Trojan was rescued. Achilles wants to make sure that this does not happen again. This second combat is fought as a straight duel between the two with Achilles, the greater warrior, winning.

In Guido, Lefevre and Lydgate Troilus' killer's behaviour is very different, shorn of any honour. Achilles waits until his men have killed Troilus' horse and cut loose his armour. Only then

And when he sawe how Troilus nakid stod,
Of longe fightyng awaped and amaat
And from his folke alone disolat
 —Lydgate, Troy Book, iv, 2756-8.

does Achilles attack and behead him.

In an echo of the Iliad, Achilles drags the corpse behind his horse. Thus, the comparison with the Homeric Hector is heightened and, at the same time, aspects of the classical Troilus's fate are echoed.

The last aspect of the character of Troilus to develop in the tradition has become the one for which he is best known in modern times. Chaucer's Troilus and Criseyde and Shakespeare's Troilus and Cressida both focus on Troilus in his role as a lover. This theme is first introduced by Benoît de Sainte-Maure in the Roman de Troie and developed by Guido delle Colonne. Boccaccio's Il Filostrato is the first book to take the love-story as its main theme.  Robert Henryson and John Dryden are other authors who dedicate works to it.

The story of Troilus' romance developed within the context of the male-centred conventions of courtly love and thus the focus of sympathy was to be Troilus and not his beloved.[111] As different authors recreated the romance, they would interpret it in ways affected both by the perspectives of their own times and their individual preoccupations. The story as it would later develop through the works of Boccaccio, Chaucer and Shakespeare is summarised below.

Troilus used to mock the foolishness of other young men's love affairs. But one day he sees Cressida in the temple of Athena and falls in love with her. She is a young widow and daughter of the priest Calchas who has defected to the Greek camp.

Embarrassed at having become exactly the sort of person he used to ridicule, Troilus tries to keep his love secret. However, he pines for Cressida and becomes so withdrawn that his friend Pandarus asks why he is unhappy and eventually persuades Troilus to reveal his love.

Pandarus offers to act as a go-between, even though he is Cressida's relative and should be guarding her honour. Pandarus convinces Cressida to admit that she returns Troilus' love and, with Pandarus's help, the two are able to consummate their feelings for each other.

Their happiness together is brought to an end when Calchas persuades Agamemnon to arrange Cressida's return to him as part of a hostage exchange in which the captive Trojan Antenor is freed. The two lovers are distraught and even think of eloping together but they finally cooperate with the exchange. Despite Cressida's initial intention to remain faithful to Troilus, the Greek warrior Diomedes wins her heart. When Troilus learns of this, he seeks revenge on Diomedes and the Greeks and dies in battle. Just as Cressida betrayed Troilus, Antenor was later to betray Troy.

In the Roman de Troie, the daughter of Calchas whom Troilus loves is called Briseis. Their relationship is first mentioned once the hostage exchange has been agreed:

Whoever had joy or gladness, Troilus suffered affliction and grief. That was for the daughter of Calchas, for he loved her deeply. He had set his whole heart on her; so mightily was he possessed by his love that he thought only of her. She had given herself to him, both her body and her love. Most men knew of that.[112]

In Guido, Troilus' and Diomedes' love is now called Briseida. His version (a history) is more moralistic and less touching, removing the psychological complexity of Benoît's (a romance) and the focus in his retelling of the love triangle is firmly shifted to the betrayal of Troilus by Briseida. Although Briseida and Diomedes are most negatively caricatured by Guido's moralising, even Troilus is subject to criticism as a "fatuous youth" prone, as in the following, to youthful faults.[113]

Troilus, however, after he had learned of his father's intention to go ahead and release Briseida and restore her to the Greeks, was overwhelmed and completely wracked by great grief, and almost entirely consumed by tears, anguished sighs, and laments, because he cherished her with the great fervour of youthful love and had been led by the excessive ardour of love into the intense longing of blazing passion. There was no one of his dear ones who could console him.[114]

Briseis, at least for now, is equally affected by the possibility of separation from her lover. Troilus goes to her room and they spend the night together, trying to comfort each other. Troilus is part of the escort to hand her over the next day. Once she is with the Greeks, Diomedes is immediately struck by her beauty. Although she is not hostile, she cannot accept him as her lover. Meanwhile Calchas tells her to accept for herself that the gods have decreed Troy's fall and that she is safer now she is with the Greeks.

A battle soon takes place and Diomedes unseats Troilus from his horse. The Greek sends it as a gift to Briseis/Briseida with an explanation that it had belonged to her old lover.
In Benoît, Briseis complains at Diomedes' seeking to woo her by humbling Troilus, but in Guido all that remains of her long speech in Benoît is that she "cannot hold him in hatred who loves me with such purity of heart."[115]

Diomedes soon does win her heart. In Benoît, it is through his display of love and she gives him her glove as a token. Troilus seeks him out in battle and utterly defeats him. He saves Diomedes' life, only so that he can bring her a message of Troilus' contempt.
In Guido, Briseida's change of heart comes after Troilus wounds Diomedes seriously. Briseida tends Diomedes and then decides to take him as her lover, because she does not know if she will ever meet Troilus again.

In later medieval tellings of the war, the episode of Troilus and Briseida/Cressida is acknowledged and often given as a reason for Diomedes and Troilus to seek each other out in battle. The love story also becomes one that is told separately.

The first major work to take the story of Troilus' failed love as its central theme is Giovanni Boccaccio's Il Filostrato.[116] The title means "the one struck down by love".[117] There is an overt purpose to the text. In the proem, Boccaccio himself is Filostrato and addresses his own love who has rejected him.[118]

Boccaccio introduces a number of features of the story that were to be taken up by Chaucer. Most obvious is that Troilus' love is now called Criseida or Cressida.[119] An innovation in the narrative is the introduction of the go-between Pandarus. Troilus is characterised as a young man who expresses whatever moods he has strongly, weeping when his love is unsuccessful, generous when it is.

Boccaccio fills in the history before the hostage exchange as follows. Troilus mocks the lovelorn glances of other men who put their trust in women before falling victim to love himself when he sees Cressida, here a young widow, in the Palladium, the temple of Athena. Troilus keeps his love secret and is made miserable by it. Pandarus, Troilus' best friend and Cressida's cousin in this version of the story, acts as go-between after persuading Troilus to explain his distress. In accordance with the conventions of courtly love, Troilus' love remains secret from all except Pandarus,[120] until Cassandra eventually divines the reason for Troilus' subsequent distress.

After the hostage exchange is agreed, Troilus suggests elopement, but Cressida argues that he should not abandon Troy and that she should protect her honour. Instead, she promises to meet him within ten days. Troilus spends much of the intervening time on the city walls, sighing in the direction where Cressida has gone. No horses or sleeves, as used by Guido or Benoît, are involved in Troilus' learning of Cressida's change of heart. Instead a dream hints at what has happened, and then the truth is confirmed when a brooch – previously a gift from Troilus to Cressida – is found on Diomedes' looted clothing. In the meantime, Cressida has kept up the pretence in their correspondence that she still loves Troilus. After Cressida's betrayal is confirmed, Troilus becomes ever fiercer in battle.

Geoffrey Chaucer's Troilus and Criseyde[121] reflects a more humorous world-view than Boccaccio's poem. Chaucer does not have his own wounded love to display and therefore allows himself an ironic detachment from events and Criseyde is more sympathetically portrayed.[122] In contrast to Boccaccio's final canto, which returns to the poet's own situation, Chaucer's palinode has Troilus looking down laughing from heaven, finally aware of the meaninglessness of earthly emotions. About a third of the lines of the Troilus are adapted from the much shorter Il Filostrato, leaving room for a more detailed and characterised narrative.[123]

Chaucer's Criseyde is swayed by Diomedes playing on her fear. Pandarus is now her uncle, more worldly-wise and more active in what happens and so Troilus is more passive.[124] This passivity is given comic treatment when Troilus passes out in Criseyde's bedroom and is lifted into her bed by Pandarus. Troilus' repeated emotional paralysis is comparable to that of Hamlet who may have been based on him. It can be seen as driven by loyalty both to Criseyde and to his homeland, but has also been interpreted less kindly.[125]

Another difference in Troilus' characterisation from the Filostrato is that he is no longer misogynistic in the beginning. Instead of mocking lovers because of their putting trust in women, he mocks them because of how love affects them.[126] Troilus' vision of love is stark: total commitment offers total fulfilment; any form of failure means total rejection. He is unable to comprehend the subtleties and complexities that underlie Criseyde's vacillations and Pandarus' manoeuvrings.[127]

In his storytelling Chaucer links the fates of Troy and Troilus, the mutual downturn in fortune following the exchange of Criseyde for the treacherous Antenor being the most significant parallel.[128]
Little has changed in the general sweep of the plot from Boccaccio. Things are just more detailed, with Pandarus, for example, involving Priam's middle son Deiphobus during his attempts to unite Troilus and Cressida. Another scene that Chaucer adds was to be reworked by Shakespeare. In it, Pandarus seeks to persuade Cressida of Troilus' virtues over those of Hector, before uncle and niece witness Troilus returning from battle to public acclaim with much damage to his helmet. Chaucer also includes details from the earlier narratives. So, reference is made not just to Boccaccio's brooch, but to the glove, the captured horse and the battles of the two lovers in Benoît and Guido.

Because of the great success of the Troilus, the love story was popular as a free standing tale to be retold by English-language writers throughout the 15th and 16th centuries and into the 17th century. The theme was treated either seriously or in burlesque. For many authors, true Troilus, false Cresseid and pandering Pandarus became ideal types eventually to be referred to together as such in Shakespeare.[129]

During the same period, English retellings of the broader theme of the Trojan War tended to avoid Boccaccio's and Chaucer's additions to the story, though their authors, including Caxton, commonly acknowledged Chaucer as a respected predecessor. John Lydgate's Troy Book is an exception.[130]  Pandarus is one of the elements from Chaucer's poem that Lydgate incorporates, but Guido provides his overall narrative framework. As with other authors, Lydgate's treatment contrasts Troilus' steadfastness in all things with Cressida's fickleness. The events of the war and the love story are interwoven. Troilus' prowess in battle markedly increases once he becomes aware that Diomedes is beginning to win Cressida's heart, but it is not long after Diomedes final victory in love when Achilles and his Myrmidons treacherously attack and kill Troilus and maltreat his corpse, concluding Lydgate's treatment of the character as an epic hero,[131] who is the purest of all those who appear in the Troy Book.[132]

Of all the treatments of the story of Troilus and, especially, Cressida in the period between Chaucer and Shakespeare, it is Robert Henryson's that receives the most attention from modern critics. His poem The Testament of Cresseid is described by the Middle English expert C. David Benson as the "only fifteenth century poem written in Great Britain that begins to rival the moral and artistic complexity of Chaucer's Troilus".[133]  In the Testament the title-character is abandoned by Diomedes and then afflicted with leprosy so that she becomes unrecognizable to Troilus. He pities the lepers she is with and is generous to her because she reminds him of the idol of her in his mind, but he remains the virtuous pagan knight and does not achieve the redemption that she does. Even so, following Henryson Troilus was seen as a representation of generosity.[134]

Another approach to Troilus' love story in the centuries following Chaucer is to treat Troilus as a fool, something Shakespeare does in allusions to him in plays leading up to Troilus and Cressida.[135] In Shakespeare's "problem play"[136] there are elements of Troilus the fool. However, this can be excused by his age. He is an almost beardless youth, unable to fully understand the workings of his own emotions, in the middle of an adolescent infatuation, more in love with love and his image of Cressida than the real woman herself.[137] He displays a mixture of idealism about eternally faithful lovers and of realism, condemning Hector's "vice of mercy".[138] His concept of love involves both a desire for immediate sexual gratification and a belief in eternal faithfulness.[139] He also displays a mixture of constancy, (in love and supporting the continuation of war) and inconsistency (changing his mind twice in the first scene on whether to go to battle or not). More a Hamlet than a Romeo,[140] by the end of the play his illusions of love shattered and Hector dead, Troilus might show signs of maturing, recognising the nature of the world, rejecting Pandarus and focusing on revenge for his brother's death rather than for a broken heart or a stolen horse.[141]  The novelist and academic Joyce Carol Oates, on the other hand, sees Troilus as beginning and ending the play in frenzies – of love and then hatred. For her, Troilus is unable to achieve the equilibrium of a tragic hero despite his learning experiences, because he remains a human-being who belongs to a banal world where love is compared to food and cooking and sublimity cannot be achieved.[142]

Troilus and Cressida's sources include Chaucer, Lydgate, Caxton and Homer,[143] but there are creations of Shakespeare's own too and his tone is very different. Shakespeare wrote at a time when the traditions of courtly love were dead and when England was undergoing political and social change.[144] Shakespeare's treatment of the theme of Troilus' love is much more cynical than Chaucer's, and the character of Pandarus is now grotesque. Indeed, all the heroes of the Trojan War are degraded and mocked.[145] Troilus' actions are subject to the gaze and commentary of both the venal Pandarus and of the cynical Thersites who tells us:

...That dissembling abominable varlet Diomed has got that same scurvy, doting, foolish knave's sleeve of Troy there in his helm. I would fain see them meet, that that same young Trojan ass, that loves the whore there, might send that Greekish whoremasterly villain with the sleeve back to the dissembling luxurious drab of a sleeveless errand...[146]

The action is compressed and truncated, beginning in medias res with Pandarus already working for Troilus and praising his virtues to Cressida over those of the other knights they see returning from battle, but comically mistaking him for Deiphobus. The Trojan lovers are together only one night before the hostage exchange takes place. They exchange a glove and a sleeve as love tokens, but the next night Ulysses takes Troilus to Calchas' tent, significantly near Menelaus' tent.[147] There they witness Diomedes successfully seducing Cressida after taking Troilus' sleeve from her. The young Trojan struggles with what his eyes and ears tell him, wishing not to believe it. Having previously considered abandoning the senselessness of war in favour of his role of lover and having then sought to reconcile love and knightly conduct, he is now left with war as his only role.[148]

Both the fights between Troilus and Diomedes from the traditional narrative of Benoît and Guido take place the next day in Shakespeare's retelling. Diomedes captures Troilus' horse in the first fight and sends it to Cressida. Then the Trojan triumphs in the second, though Diomedes escapes. But in a deviation from this narrative it is Hector, not Troilus, whom the Myrmidons surround in the climactic battle of the play and whose body is dragged behind Achilles' horse. Troilus himself is left alive vowing revenge for Hector's death and rejecting Pandarus. Troilus' story ends, as it began, in medias res with him and the remaining characters in his love-triangle remaining alive.

Some seventy years after Shakespeare's Troilus was first presented, John Dryden re-worked it as a tragedy, in his view strengthening Troilus' character and indeed the whole play, by removing many of the unresolved threads in the plot and ambiguities in Shakespeare's portrayal of the protagonist as a believable youth rather than a clear-cut and thoroughly sympathetic hero.[149] Dryden described this as "remov[ing] that heap of Rubbish, under which many excellent thoughts lay bury'd."[150] His Troilus is less passive on stage about the hostage exchange, arguing with Hector over the handing over of Cressida, who remains faithful. Her scene with Diomedes that Troilus witnesses is her attempt "to deceive deceivers".[151] She throws herself at her warring lovers' feet to protect Troilus and commits suicide to prove her loyalty. Unable to leave a still living Troilus on the stage, as Shakespeare did, Dryden restores his death at the hands of Achilles and the Myrmidons but only after Troilus has killed Diomedes. According to P. Boitani, Dryden goes to "the opposite extreme of Shakespeare's... solv[ing] all problems and therefore kill[ing] the tragedy".[152]

After Dryden's Shakespeare, Troilus is almost invisible in literature until the 20th century. Keats does refer to Troilus and Cressida in the context of the "sovereign power of love"[153] and Wordsworth translated some of Chaucer but, as a rule, love was portrayed in ways far different from how it is in the Troilus and Cressida story.[154] Boitani sees the two World Wars and the 20th century's engagement "in the recovery of all sorts of past myths"[155] as contributing to a rekindling of interest in Troilus as a human being destroyed by events beyond his control. Similarly Foakes sees the aftermath of one World War and the threat of a second as key elements for the successful revival of Shakespeare's Troilus in two productions in the first half of the 20th century,[156] and one of the authors discussed below names Barbara Tuchman's The March of Folly: From Troy to Vietnam as the trigger for his wish to retell the Trojan war.[157]

Boitani discusses the modern use of the character of Troilus in a chapter entitled Eros and Thanatos.[158] Love and death, the latter either as a tragedy in itself or as an epic symbol of Troy's own destruction, therefore, are the two core elements of the Troilus myth for the editor of the first book-length survey of it from ancient to modern times. He sees the character as incapable of transformation on a heroic scale in the manner of Ulysses and also blocked from the possibility of development as an archetypal figure of troubled youth by Hamlet. Troilus' appeal for the 20th and 21st century is his very humanity.[155]

Belief in the medieval tradition of the Trojan War that followed Dictys and Dares survived the Revival of Learning in the Renaissance and the advent of the first English translation of the Iliad in the form of Chapman's Homer. (Shakespeare used both Homer and Lefevre as sources for his Troilus.) However the two supposedly eye-witness accounts were finally discredited by Jacob Perizonius in the early years of the 18th century.[159] With the chief source for his portrayal as one of the most active warriors of the Trojan War undermined, Troilus has become an optional character in modern Trojan fiction, except for those that retell the love story itself. Lindsay Clarke and Phillip Parotti, for example, omit Troilus altogether. Hilary Bailey includes a character of that name in Cassandra: Princess of Troy but little remains of the classical or medieval versions except that he fights Diomedes. However, some of the over sixty re-tellings of the Trojan War since 1916[160] do feature the character.

One consequence of the reassessment of sources is the reappearance of Troilus in his ancient form of andropais.[161] Troilus takes this form in Giraudoux's The Trojan War Will Not Take Place, his first successful reappearance in the 20th century.[162] Troilus is a fifteen-year-old boy whom Helen has noticed following her around. After turning down the opportunity to kiss her when she offers and when confronted by Paris, he eventually accepts the kiss at the end of the play just as Troy has committed to war. He is thus a symbol of the whole city's fatal fascination with Helen.[163]

Troilus, in one of his ancient manifestations as a boy-soldier overwhelmed, reappears both in works Boitani discusses and those he does not. Christa Wolf in her Kassandra features a seventeen-year-old Troilus, first to die of all the sons of Priam. The novel's treatment of the character's death has features of both medieval and ancient versions.[164] Troilus has just gained his first love, once more called Briseis. It is only after his death that she is to betray him. On the first day of the war, Achilles seeks Troilus out and forces him into battle with the help of the Myrmidons. Troilus tries to fight in the way he has been taught princes should do, but Achilles strikes the boy down and leaps on top of him, before attempting to throttle him. Troilus escapes and runs to the sanctuary of the temple of Apollo where he is helped to take his armour off. Then, in "some of the most powerful and hair-raising" words ever written on Troilus' death,[165] Wolf describes how Achilles enters the temple, caresses then half-throttles the terrified boy, who lies on the altar, before finally beheading him like a sacrificial victim. After his death, the Trojan council propose that Troilus be officially declared to have been twenty in the hope of avoiding the prophecy about him but Priam, in his grief, refuses as this would insult his dead son further. In "exploring the violent underside of sexuality and the sexual underside of violence",[166] Wolf revives a theme suggested by the ancient vases where an "erotic aura seems to pervade representations of a fully armed Achilles pursuing or butchering a naked, boyish Troilus".[167]

Colleen McCullough is another author who incorporates both the medieval Achilles' seeking Troilus out in battle and the ancient butchery at the altar. Her The Song of Troy includes two characters, Troilos and Ilios,[168] who are Priam's youngest children – both with prophecies attached and both specifically named for the city's founders. They are eight and seven respectively when Paris leaves for Greece and somewhere in their late teens when killed. Troilos is made Priam's heir after Hector's death, against the boy's will. Odysseus's spies learn of the prophecy that Troy will not fall if Troilos comes of age. Achilles therefore seeks him out in the next battle and kills him with a spear-cast to his throat. In a reference to the medieval concept of Troilus as the second Hector, Automedon observes that "with a few more years added, he might have made another Hektor."[169] Ilios is the last son of Priam to die, killed at the altar in front of his parents by Neoptolemos.

Marion Zimmer Bradley's The Firebrand features an even younger Troilus, just twelve when he becomes Hector's charioteer. (His brother wants to keep a protective eye on him now he is ready for war.) Troilus helps kill Patroclus. Although he manages to escape the immediate aftermath of Hector's death, he is wounded. After the Trojans witness Achilles' treatment of Hector's body, Troilus insists on rejoining the battle despite his wounds and Hecuba's attempts to stop him. Achilles kills him with an arrow. The mourning Hecuba comments that he did not want to live because he blamed himself for Hector's death.

A feature already present in the treatments of the love story by Chaucer, Henryson, Shakespeare and Dryden is the repeated reinvention of its conclusion. Boitani sees this as a continuing struggle by authors to find a satisfying resolution to the love triangle. The major difficulty is the emotional dissatisfaction resulting from how the tale, as originally invented by Benoît, is embedded into the pre-existing narrative of the Trojan War with its demands for the characters to meet their traditional fates. This narrative has Troilus, the sympathetic protagonist of the love story, killed by Achilles, a character totally disconnected from the love triangle, Diomedes survive to return to Greece victorious, and Cressida disappear from consideration as soon as it is known that she has fallen for the Greek. Modern authors continue to invent their own resolutions.[170]

William Walton's Troilus and Cressida is the best known and most successful of a clutch of 20th-century operas on the subject after the composers of previous eras had ignored the possibility of setting the story.[171] Christopher Hassall's libretto blends elements of Chaucer and Shakespeare with inventions of its own arising from a wish to tighten and compress the plot, the desire to portray Cressida more sympathetically and the search for a satisfactory ending.[172]  Antenor is, as usual, exchanged for Cressida but, in this version of the tale, his capture has taken place while he was on a mission for Troilus. Cressida agrees to marry Diomedes after she has not heard from Troilus. His apparent silence, however, is because his letters to her have been intercepted. Troilus arrives at the Greek camp just before the planned wedding. When faced with her two lovers, Cressida chooses Troilus. He is then killed by Calchas with a knife in the back. Diomedes sends his body back to Priam with Calchas in chains. It is now the Greeks who condemn "false Cressida" and seek to keep her but she commits suicide.

Before Cressida kills herself she sings to Troilus to

...turn on that cold river's brim
beyond the sun's far setting.
Look back from the silent stream
of sleep and long forgetting.
Turn and consider me
and all that was ours;
you shall no desert see
but pale unwithering flowers.[173]

This is one of three references in 20th century literature to Troilus on the banks of the River Styx that Boitani has identified. Louis MacNeice's long poem The Stygian Banks explicitly takes its name from Shakespeare who has Troilus compare himself to "a strange soul upon the Stygian banks" and call upon Pandarus to transport him "to those fields where I may wallow in the lily beds".[174] In MacNeice's poem the flowers have become children, a paradoxical use of the traditionally sterile Troilus[175] who

Patrols the Stygian banks, eager to cross,
But the value is not on the further side of the river,
The value lies in his eagerness. No communion
In sex or elsewhere can be reached and kept
Perfectly for ever. The closed window,
The river of Styx, the wall of limitation
Beyond which the word beyond loses its meaning,
Are the fertilising paradox, the grille
That, severing, joins, the end to make us begin
Again and again, the infinite dark that sanctions
Our growing flowers in the light, our having children...

The third reference to the Styx is in Christopher Morley's The Trojan Horse. A return to the romantic comedy of Chaucer is the solution that Boitani sees to the problem of how the love story can survive Shakespeare's handling of it.[176] Morley gives us such a treatment in a book that revels in its anachronism. Young Lieutenant (soon to be Captain) Troilus lives his life in 1185 BC where he has carefully timetabled everything from praying, to fighting, to examining his own mistakes. He falls for Cressida after seeing her, as ever, in the Temple of Athena where she wears black, as if mourning the defection of her father, the economist Dr Calchas. The flow of the plot follows the traditional story, but the ending is changed once again. Troilus' discovery of Cressida's change of heart happens just before Troy falls. (Morley uses Boccaccio's version of the story of a brooch, or in this case a pin, attached to a piece of Diomedes' armour as the evidence that convinces the Trojan.) Troilus kills Diomedes as he exits the Trojan Horse, stabbing him in the throat where the captured piece of armour should have been. Then Achilles kills Troilus. The book ends with an epilogue. The Trojan and Greek officers exercise together by the River Styx, all enmities forgotten. A new arrival (Cressida) sees Troilus and Diomedes and wonders why they seem familiar to her. What Boitani calls "a rather dull, if pleasant, ataraxic eternity" replaces Chaucer's Christian version of the afterlife.[177]

In Eric Shanower's graphic novel Age of Bronze, currently still being serialised, Troilus is youthful but not the youngest son of Priam and Hecuba. In the first two collected volumes of this version of the Trojan War, Shanower provides a total of six pages of sources covering the story elements of his work alone. These include most of the fictional works discussed above from Guido and Boccaccio down to Morley and Walton. Shanower begins Troilus' love story with the youth making fun of Polyxena's love for Hector and in the process accidentally knocking aside Cressida's veil. He follows the latter into the temple of Athena to gawp at her. Pandarus is the widow Cressida's uncle encouraging him. Cressida rejects Troilus' initial advances not because of wanting to act in a seemly manner, as in Chaucer or Shakespeare, but because she thinks of him as just a boy. However, her uncle persuades her to encourage his affection, in the hope that being close to a son of Priam will protect against the hostility of the Trojans to the family of the traitor Calchas. Troilus' unrequited love is used as comic relief in an otherwise serious retelling of the Trojan War cycle. The character is portrayed as often indecisive and ineffectual as on the second page of this episode sample at the official site [45]. It remains to be seen how Shanower will further develop the story.

Troilus is rewarded a rare happy ending in the early Doctor Who story The Myth Makers.[178] The script was written by Donald Cotton who had previously adapted Greek tales for the BBC Third Programme.[179] The general tone is one of high comedy combined with a "genuine atmosphere of doom, danger and chaos" with the BBC website listing A Funny Thing Happened on the Way to the Forum as an inspiration together with Chaucer, Shakespeare, Homer and Virgil.[180] Troilus is again an andropais "seventeen next birthday"[181] described as "looking too young for the military garb".[182] Both "Cressida" and "Diomede" are the assumed names of the Doctor's companions Vicki and Steven. Thus Troilus' jealousy of Diomede, whom he believes also loves Cressida, is down to confusion about the real situation. In the end "Cressida" decides to leave the Doctor for Troilus and saves the latter from the fall of Troy by finding an excuse to get him away from the city. In a reversal of the usual story, he is able to avenge Hector by killing Achilles: they meet outside Troy and the Greek hero, despite being more than a match for the young Trojan, catches his heel on some vegetation and stumbles. Subsequently the two lovers join up with Aeneas, implying a role in the events of The Aeneid. The story was originally intended to end more conventionally, with "Cressida", despite her love for him, apparently abandoning him for "Diomede", but the producers declined to renew co-star Maureen O'Brien's contract, requiring that her character Vicki be written out.[citation needed]UBS Group AG[nb 1] (stylized simply as UBS) is a multinational investment bank and financial services firm founded and based in Switzerland, with headquarters in both Zurich and Basel.[9] It holds a strong foothold in all major financial centres as the largest Swiss banking institution and the world's largest private bank. UBS manages the largest amount of private wealth in the world,  counting approximately half of The World's Billionaires among its clients, with over US$6 trillion in assets (AUM).[10] Based on international deal flow and political influence, the firm is considered one of the "biggest, most powerful financial institutions in the world".[11][12]  UBS is also a leading market maker and one of the eight global 'Bulge Bracket' banks. Due to its large presence across the Americas, EMEA and Asia–Pacific markets, the Financial Stability Board considers it a global systemically important bank.

UBS investment bankers and private bankers are known for their strict bank–client confidentiality and culture of banking secrecy.[nb 2] Apart from private banking, UBS provides wealth management, asset management and investment banking services for private, corporate and institutional clients with international service. The bank also maintains numerous underground bank vaults, bunkers and storage facilities for gold bars around the Swiss Alps and internationally. UBS acquired rival Credit Suisse in an emergency rescue deal brokered by the Swiss government and its Central bank in 2023, following which UBS' AUM increased to over $5 trillion along with an increased balanced sheet of $1.6 trillion.[19]

In June 2017, its return on invested capital was 11.1%, followed by Goldman Sachs' 9.35%, and JPMorgan Chase's 9.456%.[20] The company's capital strength, security protocols, and reputation for discretion have yielded a substantial market share in banking and a high level of brand loyalty. Alternatively, it receives routine criticism for facilitating tax noncompliance and off-shore financing. Partly due to its banking secrecy, it has also been at the centre of numerous tax avoidance investigations undertaken by U.S., French, German, Israeli and Belgian authorities. UBS operations in Switzerland and the United States were respectively ranked first and second on the 2018 Financial Secrecy Index. UBS is a primary dealer and Forex counterparty of the U.S. Federal Reserve.[21]

UBS was founded in 1862 as the Bank in Winterthur.[22] The Bank in Winterthur came with the movement that founded muiltple Suisse Grossbanken (Swiss big banks) that occurred in the latter sector of the 19th century.[23] The name of the bank was derived from the town of Winterthur, which served as Switzerland's industrial hub in the 19th century.[24] By 1854, six private bankers in Basel founded the Swiss Bank Corporation (SBC) to cater to the increasing credit needs of Swiss railroad and manufacturing companies.[25] It formed a private banking syndicate that expanded, aided by Switzerland's international neutrality.[citation needed]

In 1912, the Bank of Winterthur merged with Toggenburger Bank to form the Union Bank of Switzerland (UBS)[22] and grew rapidly after the Banking Law of 1934 codified Swiss banking secrecy. Following decades of market competition between Union Bank of Switzerland and the Swiss Bank Corporation (SBC), the two merged in 1998 to create a single company known solely as "UBS".[nb 3] The agreement to merge had been made the previous year at a meeting of the Institut International d’Etudes Bancaires, a secretive club for European banking CEOs.[27]

UBS does not stand for Union Bank of Switzerland. The name 'UBS' came from one of its predecessor firms - the Union Bank of Switzerland. However, just like other prominent brands which used to be an abbreviation of a company name, UBS is no longer considered an acronym. In fact, that was one of the more than 370 financial firms that have, since 1862, become part of today's UBS.

During the 2008 financial crisis, UBS managed heavy losses with an asset relief recovery programme.[citation needed] In 2011, the company was hit by the 2011 rogue trader scandal resulting in a US$2 billion trading loss.[28] In 2012, the bank reoriented itself around wealth management advisory services and limited its sell side operations.

UBS is a joint-stock company (Aktiengesellschaft), pursuant to Swiss laws. Its shares are listed at the SIX Swiss Exchange and the New York Stock Exchange (NYSE). As of December 2020, UBS is present in all major financial centres worldwide, such as New York, London, Zürich, Berlin, Sydney, and Singapore. UBS has offices in 50 countries, with about 30% of its approximately 73,000 employees working in the Americas, 30% in Switzerland, 19% in Europe (excluding Switzerland), the Middle East and Africa and 21% in the Asia Pacific region.[30] The bank has a major presence in the United States. Its American headquarters for investment banking are located in New York City, and for private wealth management advisory in Weehawken, New Jersey. They have sales & trading, along with private wealth management offices in Stamford, Connecticut.[citation needed]

The company's global business groups provide services that entail: global wealth management, investment banking, asset management and personal & corporate banking.[30] UBS is the leading provider of retail banking and commercial banking services in Switzerland, and they have been on top of the Swiss market since in 2009.[31] Looking holistically, UBS' overall invested assets is $3.101 billion, shareholders' equity is $52.928 billion and market capitalization is $45.907 billion at the end of the year 2018.[30] In November 2014, the shares in UBS Group AG were listed and started trading as a new holding company at the New York Stock Exchange (NYSE) and the SIX Swiss Exchange. Upon application and with effect as of 14 January 2015, the shares of UBS AG, a subsidiary of the UBS Group AG, were delisted from the NYSE.[32] As of September 2019, the largest institutional shareholders are:[33]

As of 31 December 2024, the geographical distribution of the shareholders presents itself as follows:[34]

As of June 2018, UBS's corporate structure includes four divisions in total, namely:[35][36]

Starting on 9 June 2003, all UBS business groups, including UBS Paine Webber and UBS Warburg, were rebranded under the UBS moniker following company's start of operations as a unified global entity.[37]

UBS's global wealth management advisory division offers high-net-worth individuals around the world a range of advisory and investment products and services.[38] As of the end of 2016, UBS Wealth Management's invested assets totalled CHF 977billion.[39] The whole companies assets under management (AUM) amounted to US$1,737.5 billion in 2015, representing a 1% decrease in AUM compared to the equivalent data of 2014.[40] As of 2018, UBS manages the largest amount of private wealth in the world, counting approximately half of The World's Billionaires among its clients.[nb 4] More than 60% of total invested assets in UBS Wealth Management belong to individuals with a net worth of CHF 10 million or more. Of the remaining 40% of total invested assets, 30% of the total belong to individuals with net worth between CHF 1 million and CHF 10 million and the last 10% of total assets belong to individuals with a net worth of less than CHF 1 million.[38]

UBS offers brokerage services and products as well as asset management and other investment advisory and portfolio management products and services.[43][44] Additionally, UBS provides a broad range of securities and savings products that are supported by the firm's underwriting and research activities as well as clients' orders management and execution and also clearing services for transactions originated by individual investors. The business is further divided geographically with separate businesses focused on the U.S. and other international markets.[31][45] Two-thirds of the total invested assets come from Europe and Switzerland, with the final third coming mainly from the Asia-Pacific region.

With its headquarters in Switzerland, UBS Wealth Management is present in more than 40 countries with approximately 190 offices (100 of which are in Switzerland).[38] As of the end of 2018, around 23,600 people worldwide were employed by Global Wealth Management.[30][39] In Switzerland, UBS Swiss Bank provides a complete set of retail banking services that includes chequing, savings, credit cards, and mortgage products for individuals.[46] They offer cash management and commercial banking services for small businesses and corporate clients as well.[46]

UBS global wealth management advisory operations in the Americas consists of U.S. and Canadian wealth management businesses, as well as international business booked in the U.S.[47] UBS Wealth Management in the U.S. is an outgrowth of the former Paine Webber brokerage business. The business was initially renamed UBS Paine Webber in March 2001, after it was acquired by UBS. The division offers wealth management advice for ultra-high net worth and high net worth clients.[48] UBS was named "Best Bank for Wealth Management in North America" at the Euromoney Awards for Excellence 2017.[49]

UBS's main competitors in this division are Bank of America, Morgan Stanley, JP Morgan Chase, Wells Fargo and Charles Schwab.[50]

UBS's Personal & Corporate Banking division delivers financial products and services to retail,[51] corporate and institutional clients[52] in Switzerland.[38] It also provides stable and substantial profits for the Group and revenue opportunities for businesses within the bank.[38] UBS maintains a leading position in the retail and corporate loan market in Switzerland; in fact, it serves one in three pension funds, more than 85% of the 1,000 largest Swiss corporations and 85% of banks that resides within the nation.[38] In 2015, 2017 and 2018, the international financial magazine Euromoney named UBS "Best Domestic Cash Manager Switzerland".[53] As of 31 December 2018, its lending portfolio reached US$131 billion (~$157 billion in 2023).[54][30]

The products that this UBS division offers range from cash accounts, payments, savings and retirement plans to investment fund products, residential mortgages and advisory services.[38] This business division constitutes a central building block of UBS's universal bank delivery model in Switzerland and it supports other divisions, such as Investment Bank, by referring clients to them and by assisting them to build their wealth to a level at which they can be transferred to UBS Wealth Management.[38] The retail and corporate distribution network comprises not only 279 branches in Switzerland, but 1,250 teller machines and self-service terminals, as well as digital banking services, serving 2.5 million personal banking clients.[38][30]

UBS Asset Management offers equity, fixed income, currency, hedge fund, global real estate, infrastructure and private equity investment capabilities that can also be combined in multi-asset strategies.[55] The 1998 UBS-SBC merger and subsequent restructuring resulted in the combination of three major asset management operations: UBS Asset Management, Phillips & Drew (owned by Union Bank of Switzerland), and Brinson Partners (owned by SBC). The investment teams were merged in 2000 and in 2002 the brands were consolidated to become UBS Global Asset Management.[56]

At the end of December 2018, UBS Asset Management was responsible for US$781 billion of invested assets and the assets under administration were US$413 billion.
[54][30] With around 2,300 employees in 23 countries, UBS Asset Management is the largest mutual fund manager in Switzerland, a leading fund house in Europe, and one of the largest hedge funds and real estate investment managers in the world.[57][58] It has main offices in Chicago, Hong Kong,[59] London,[60] New York, Singapore,[61] Sydney,[62] Tokyo, and Zürich.[63][30] With the aim to generate systematic products and services for clients, in 2017, UBS integrated Equities, Fixed Income and Solutions capabilities and hedge funds business within a new area named Investments. UBS also formed a new area of business named Real Estate and Private Markets by combining their Global Real Estate and Infrastructure and Private Equity businesses.[38]

In February 2017, UBS Group AG and the Northern Trust Corporation, an American international financial services company, announced an agreement for the acquisition of UBS Asset Management's fund administration servicing units in Luxembourg and Switzerland. This acquisition will facilitate the expansion of the Northern Trust Corporation into these two countries, turning the American company into the major fund administrator in the local markets and into one of the ten global leaders in the sector. At the end of the transaction, completed in October 2017,[64] the American company will administrate a total of CHF 420 billion in assets. UBS Asset Management will continue anyway to offer Management Company, White Labelling and Representative Services to its clients. Ulrich Körner, president of the UBS Asset Management, affirms that the continuous transformation of their platform is due to a major efficiency, effectiveness and geographical dislocation of the services offered by the bank.[65]

UBS's main competitors in this division are BlackRock, Vanguard Group, State Street Global Advisers (SSGA), Fidelity Investments and Allianz Asset Management (AAM).[66]

In April, 2025, UBS Asset Management, which oversees $1.8 trillion in assets, has removed restrictions on some of its sustainability funds investing in manufacturers of conventional weapons. This marks a shift in European asset management, as investors respond to rising geopolitical tensions and a regional push for rearmament. UBS still maintains bans on controversial weapons like cluster munitions and biological arms but gave no explanation for lifting the conventional weapons ban. The trend reflects growing investor enthusiasm to support Europe's defense buildup, particularly amidst pressure from U.S. government and rising military spending across the continent. As a result, defense stocks have surged in value.[67]

UBS Investment Bank provides services covering securities, other financial products, and research in equities, rates, credit, foreign exchange, precious metals and derivatives.[69] As of the end of December 2018, the personnel employed at UBS Investment Bank totalled 5,205, present in 33 countries (with principal offices in Chicago, Frankfurt, Hong Kong, London, New York, Shanghai, Singapore, Tokyo and Zurich).[54][30] This business division also advises and provides access to capital markets for corporate and institutional clients, governments, financial intermediaries, alternative asset managers, and private investors.[46] UBS Investment Bank was formerly known as UBS Warburg and as Warburg Dillon Read, before the merger of the Union Bank of Switzerland and the Swiss Bank Corporation (SBC). Within the UBS Investment Bank division, the Investment Banking Department (IBD) provides a range of advisory and underwriting services including mergers and acquisitions, restructuring, equity offerings, investment grade and high yield debt offerings, leveraged finance and leveraged loan structuring, and the private placement of equity, debt, and derivatives.

The Sales & Trading division comprises equities (brokering, dealing, market making and engaging in proprietary trading in equities, equity-related products, equity derivatives, and structured products) and FX, Rates and Credit (FRC) (brokering, dealing, market making and engaging in proprietary trading in interest rate products, credit products, mortgage-backed securities, leveraged loans, investment grade and high-yield debt, currencies, structured products, and derivative products). Following an expansion in 2002, the trading floor covers 9,600-square-metre (103,000 sq ft) with 12-metre (40 ft) arched ceilings. Over US$1 trillion in assets are traded here every trading day. In June 2011, it was announced that UBS was considering moving its North American headquarters back to New York City, and that the bank was looking for office spaces in Midtown and in the rebuilt World Trade Center.[70][71]

UBS's main competitors in this division are fellow members of the Bulge Bracket,[72] particularly Goldman Sachs, JPMorgan Chase, and Bank of America.[73][74]
The American division also offers its own credit card not sponsored by another bank unlike many of its competitors.[75]

On a global scale, UBS competes with the largest global investment banks,[76][77] particularly within the Bulge Bracket.[76][74] and until acquiring Credit Suisse in 2023 was regularly compared against it.[78][79] According to a 2018 study published by Coalition Research Institute, UBS was among the top 10 of the world's investment banks.[80]

UBS traces its history to 1862, the year when Bank in Winterthur, forerunner of Union Bank of Switzerland, was founded.[81] When Union Bank and SBC merged, officials originally wanted to name the merged company the "United Bank of Switzerland," but opted to call it simply UBS because of a name clash with the separate Swiss company United Bank Switzerland – a part of the United Bank Limited's Swiss subsidiary. Therefore, UBS is no longer an acronym but is the company's brand. Its logo of three keys, carried over from SBC, stands for the company's values of confidence, security, and discretion.[46]


UBS' earliest corporate ancestor was formed in 1854, when six private banking firms in Basel, Switzerland pooled their resources to form the Bankverein, a consortium that acted as an underwriting syndicate for its member banks.[82] In 1871, the Bankverein coordinated with the German Frankfurter Bankverein to form the Basler Bankverein, a joint-stock company replacing the original Bankverein consortium.[82] After the new bank started with an initial commitment of CHF 30 million and CHF 6 million of share capital, it soon experienced growing pains when heavy losses in Germany caused it to suspend its dividend until 1879.[82] Following the years 1885 and 1886, when the bank merged with the Zürcher Bankverein and acquired the Basler Depositenbank and the Schweizerische Unionbank, it changed its name to Schweizerischer Bankverein.[82] The English name of the bank was originally Swiss Bankverein, but was changed to Swiss Bank Corporation (SBC) in 1917.[83]
SBC subsequently experienced a period of growth, which was only interrupted by the onset of World War I, in which the bank lost investments in a number of large industrial companies. By the end of 1918, the bank had recovered and surpassed CHF 1 billion in total assets and grew to 2,000 employees by 1920. The impact of the stock market crash of 1929 and the Great Depression was severe, particularly as the Swiss franc suffered major devaluation in 1936. The bank saw its assets fall from a 1929 peak of CHF 1.6 billion to its 1918 levels of CHF 1 billion by 1936.[83][82]

In 1937, SBC adopted its three-keys logo, designed by Warja Honegger-Lavater, symbolizing confidence, security, and discretion, which remains an integral part in the current-day logo of UBS.[82] On the eve of World War II in 1939, SBC, like other Swiss banks, was the recipient of large influxes of foreign funds for safekeeping. Just prior to the outbreak of the war, SBC made the timely decision to open an office in New York City.[85] The office, located in the Equitable Building, was able to begin operations a few weeks after the outbreak of the war and was intended as a safe place to store assets in the case of an invasion.[86] During the war, the banks' traditional business fell off and the Swiss government became their largest client.[83]

In 1945, SBC acquired the Basler Handelsbank (Commercial Bank of Basel), which was one of the largest banks in Switzerland, but became insolvent by the end of the war. SBC remained among the Swiss government's leading underwriters of debt in the post-war years. SBC, which had entered the 1950s with 31 branch offices in Switzerland and three abroad, more than doubled its assets from the end of the war to CHF 4 billion by the end of the 1950s and doubled assets again in the mid-1960s, exceeding CHF 10 billion by 1965.[82] In 1961, SBC acquired Banque Populaire Valaisanne, based in Sion, Switzerland, and the Banque Populaire de Sierre.[56] The bank opened a full branch office in Tokyo in 1970.[82]

In 1992, SBC acquired O'Connor & Associates, a Chicago-based options trading firm and the largest market maker in the financial options exchanges in the U.S.[87] O'Connor was combined with SBC's money market, capital market, and currency market activities to form a globally integrated capital markets and treasury operation.[87] In 1994, SBC acquired Brinson Partners, an asset management firm focused on providing access for U.S. institutions to global markets, for US$750 million.[88] Following the acquisition, founder Gary P. Brinson ran SBC's asset management business and later when SBC merged with UBS was named chief investment officer of UBS Asset Management.[89] The acquisition of S.G. Warburg & Co., a leading British investment banking firm, in 1995 for the price of US$1.4 billion (~$2.56 billion in 2023) signified a major push into investment banking. S.G. Warburg & Co. had established a reputation as a daring merchant bank that grew to be one of the most respected investment banks in London.[90] However, a Warburg expansion into the U.S. had turned out flawed and costly, and talks in 1994 with Morgan Stanley about a merger had collapsed.[91] SBC merged the firm with its own existing investment banking unit to create SBC Warburg.[88][92]

Two years later, in 1997, SBC paid US$600 million (~$1.06 billion in 2023) to acquire Dillon, Read & Co., a U.S. bulge bracket investment bank.[93][94] Dillon, Read & Co., which traced its roots to the 1830s, was among the powerhouse firms on Wall Street in the 1920s and 1930s, and by the 1990s had a particularly strong mergers and acquisitions advisory group. Dillon Read had been in negotiations to sell itself to ING, which owned 25% of the firm already, but Dillon Read partners balked at ING's integration plans.[93] After its acquisition by SBC, Dillon Read was merged with SBC-Warburg to create SBC-Warburg Dillon Read. Following SBC's later merger with Union Bank of Switzerland, the SBC part was dropped from the name; in 2000 when the new UBS got restructured the Dillon Read name was dropped, although it was brought back in 2005 as Dillon Read Capital Management, UBS's ill-fated hedge fund operations.[82]

The Union Bank of Switzerland emerged in 1912 when the Bank in Winterthur fused with the Toggenburger Bank. The Bank in Winterthur, founded in 1862 with an initial share capital of CHF 5 million, focused on providing financing for industry and other companies,[83][82] and had profited considerably from its close railroad connections and large warehousing facilities during the American Civil War when cotton prices rose dramatically.[95] The Toggenburger Bank was founded in 1863 with an initial share capital of CHF 1.5 million,[82] and specialized as a savings and mortgage bank for individual customers, maintaining a branch office network in eastern Switzerland.[96][82] The new company was initially traded under the English name Swiss Banking Association, but in 1921 it was changed to Union Bank of Switzerland (UBS) to mirror its French name, Union de Banques Suisses. In German, the bank was known as the Schweizerische Bankgesellschaft (SBG).[97]

The combined bank had total assets of CHF 202 million and a total shareholders' equity of CHF 46 million.[82] In 1917, UBS completed the construction of a new headquarters in Zurich on Bahnhofstrasse, considered to be the Wall Street of Switzerland.[83] By 1923, offices were established throughout Switzerland.[96] Although the bank suffered in the aftermath of World War I and the Great Depression, it was able to make several smaller acquisitions; in 1937 it established Intrag AG, an asset management business responsible for investment trusts, (i.e. mutual funds).[82][96]

The activities of the Union Bank of Switzerland during World War II were not publicly known until decades after the war, when it was demonstrated that UBS likely took active roles in trading stolen gold, securities, and other assets during World War II.[98][99] The issue of "unclaimed property" of Holocaust victims became a major issue for UBS in the mid-1990s, and a series of revelations in 1997 brought the issue to the forefront of national attention in 1996 and 1997.[100] UBS confirmed that a large number of accounts had gone unclaimed as a result of the bank's policy of requiring death certificates from family members to claim the contents of the account.[101][102] UBS's handling of these revelations were largely criticized and the bank received significant negative attention in the U.S.[103][104] UBS came under significant pressure, particularly from American politicians, to compensate Holocaust survivors who were making claims against the bank.[105]

Shortly after the end of World War II, Union Bank of Switzerland completed the acquisition of the Eidgenössische Bank, a large Zürich-based bank that became insolvent. As a result of the merger, Union Bank of Switzerland exceeded CHF 1 billion in assets and moved its operations to Zürich. UBS opened branches and acquired a series of banks in Switzerland in the following years, growing from 31 offices in 1950 to 81 offices by the early 1960s.[82] In 1960, Union Bank of Switzerland acquired an 80% stake in Argor SA, a Swiss precious metals refinery founded in 1951 in the canton of Ticino.[56] UBS continues to issue gold bars via Argor-Heraeus which is famous for the unique kinebar holographic technology it uses to provide enhanced protection against bank gold bar counterfeiting.[106] By 1962, Union Bank of Switzerland reached CHF 6.96 billion of assets, narrowly edging ahead of Swiss Bank Corporation to become the largest bank in Switzerland.[107] The rapid growth was punctuated by the 1967 acquisition of Interhandel, which made UBS one of the strongest banks in Europe.[96]

By the 1980s, Union Bank of Switzerland established a position as a leading European underwriter of Eurobonds.[83] Following two major acquisitions in 1986 (Phillips & Drew and Deutsche Länderbank), UBS made its first purchase in the United States in 1991 with Chase Investors Management Corporation, the asset management business of Chase Manhattan Bank.[56] At the time of the acquisition, the business managed in excess of US$30 billion in assets.[108] The bank's investments had been in the conservative asset management and life insurance businesses; further, 60% of the bank's profits came from its even more conservative Swiss banking operations.[109][110] In 1993, Credit Suisse outbid Union Bank of Switzerland for Switzerland's Swiss Volksbank, the fifth largest bank in Switzerland which had run into financial difficulties in the early 1990s.[107] The acquisition propelled Credit Suisse ahead of Union Bank of Switzerland as the largest bank in Switzerland for the first time. Prior to the merger with Swiss Bank Corporation, UBS purchased a group of smaller Swiss banks in 1994 including the Cantonal Bank of Appenzell-Ausserrhoden in 1996,[82] and in 1997 Schröder, Münchmeyer, Hengst & Co. from Lloyds Bank was acquired to improve access to the German investment banking and private wealth management markets.[111]

During the mid-1990s, Union Bank of Switzerland came under fire from dissident shareholders critical of its conservative management and lower return on equity.[112] Martin Ebner, through his investment trust, BK Vision, became the largest shareholder in Union Bank of Switzerland and attempted to force a major restructuring of the bank's operations.[113] Looking to take advantage of the situation, Credit Suisse approached Union Bank of Switzerland about a merger that would have created the second largest bank in the world in 1996.[114] Union Bank of Switzerland's management and board unanimously rebuffed the proposed merger.[115] Ebner, who supported the idea of a merger, led a shareholder revolt that resulted in the replacement of Union Bank of Switzerland's chairman, Robert Studer with Mathis Cabiallavetta, one of the key architects of the merger with Swiss Bank Corporation.[83][116]

On 8 December 1997, Union Bank of Switzerland and Swiss Bank Corporation announced an all-stock merger. At the time of the merger, Union Bank of Switzerland and Swiss Bank Corporation were the second and third largest banks in Switzerland, respectively.[117] Discussions between the two banks had begun several months earlier, less than a year after rebuffing Credit Suisse's merger overtures.[118] The merger resulted in the creation of UBS AG, a new bank with total assets of more than US$590 billion (~$1.04 trillion in 2023), the largest of its kind.[119] During the merger, UBS chairman Marcel Ospel originally wanted to call the company "United Bank of Switzerland", but settled on simply using "UBS" following the acquisition of American brokerage firm, Paine Webber.[120]

Colloquially referred to as the "New UBS" to distinguish itself from the former Union Bank of Switzerland, the combined bank became the second largest in the world at the time, behind only the Bank of Tokyo-Mitsubishi.[119] Additionally, the merger pulled together the banks' various asset management businesses to create the world's largest money manager, with approximately US$910 billion in assets under management.[119] Union Bank of Switzerland's Mathis Cabiallavetta became chairman of the new bank while Swiss Bank's Marcel Ospel was named chief executive officer.[119] Nearly 80% of the top management positions were filled by legacy Swiss Bank professionals.[83] Prior to the merger, Swiss Bank Corporation was considered to be further along than Union Bank of Switzerland in developing its international investment banking business, particularly in the higher margin advisory businesses where Warburg Dillon Read was considered to be the more established platform.[121][122] Union Bank of Switzerland had a stronger retail and commercial banking business in Switzerland, while both banks had strong asset management capabilities.[119] After the merger was completed, it was speculated that a series of losses suffered by UBS on its equity derivative positions in late 1997 was a contributing factor in pushing UBS management to consummate the merger.[123][124]

On 3 November 2000, UBS merged with Paine Webber, an American stock brokerage and asset management firm led by chairman and CEO Donald Marron.[125][126] At the time of its merger with UBS, Paine Webber had emerged as the fourth largest private client firm in the United States with 385 offices employing 8,554 brokers. The acquisition pushed UBS to the top wealth and asset management firm in the world. Initially, the business was given the divisional name UBS PaineWebber but in 2003 the 123-year-old name Paine Webber disappeared when it was renamed UBS Wealth Management USA.[127] UBS took a CHF 1 billion write-down for the loss of goodwill associated with the retirement of the Paine Webber brand when it integrated its brands under the unified UBS name in 2003.[56]

John P. Costas, a former bond trader and co-head of Fixed income at Credit Suisse First Boston and head of Fixed Income Trading at Union Bank of Switzerland in 1998, was appointed CEO of UBS's investment banking division, which originated in SBC's Warburg Dillon Read division and was renamed UBS Warburg in December 2001.[128][129] In an attempt to break into the elite bulge bracket of investment banks, in which UBS then had little success while rival Credit Suisse was establishing itself as a major player on Wall Street with the acquisition of Donaldson, Lufkin & Jenrette in 2000, Costas shifted the growth strategy from acquiring entire firms to hiring individual investment bankers or teams of bankers from rival firms.[130][131] Costas had followed a similar approach in building out the UBS fixed income business, hiring over 500 sales and trading personnel and increasing revenues from US$300 million in 1998 to over US$3 billion by 2001.

The arrival of former Drexel Burnham Lambert investment banker Ken Moelis marked a major coup for Costas. Moelis joined UBS from Donaldson Lufkin & Jenrette in 2001 shortly after its acquisition by Credit Suisse First Boston (although Huw Jenkins claimed he had hired Moelis to the UK Parliamentary Banking commission while under oath, which is patently false). In his six years at UBS, Moelis ultimately assumed the role of president of UBS Investment Bank and was credited, along with Costas, with the build-out of UBS's investment banking operation in the United States.[132] Within weeks of joining, Moelis brought over a team of 70 bankers from Donaldson, Lufkin & Jenrette.[132] Costas and Moelis hired more than 30 senior U.S. bankers from 2001 through 2004.[128] It was estimated that UBS spent as much as US$600 million to US$700 million hiring top bankers in the U.S. during this three-year period.[133] Among the bank's other major recruits during this period were Olivier Sarkozy, Ben Lorello, Blair Effron, and Jeff McDermott.[134][135] By 2003, UBS had risen to fourth place from seventh in global investment banking fees, earning US$2.1 billion of the US$39 billion paid to investment banks that year, increasing 33%.[128] Over the next four years, UBS consistently ranked in the top 4 in the global fee pool and established a track record of 20 consecutive quarters of rising profits.[136]

In 2006, UBS set up a joint venture in China (see UBS Securities, China branch). However, by the end of 2006, UBS began to experience changing fortunes. In late 2005, Costas headed a new hedge fund unit within UBS known as Dillon Read Capital Management. His former position was taken over by Huw Jenkins, a long-time legacy UBS investment banker.[137] In 2006, UBS bankers Blair Effron and Michael Martin announced their departures.[138][139] In March 2007, Moelis announced that he was leaving the company, and shortly thereafter founded a new business, Moelis & Company.[140] As he had when joining UBS, Moelis took a large team of senior UBS investment bankers.[141][142] Moelis's departure was caused primarily by repeated conflict over the availability of capital from the bank's balance sheet to pursue large transactions, particularly leveraged buyouts.[143] The bank's apparent conservatism would be turned on its head when large losses were reported in various mortgage securities rather than corporate loans that generated investment banking fees. After Moelis, other notable departures included investment banking co-head Jeff McDermott in early 2007 and, as the 2008 financial crisis set in, other high-profile bankers such as Oliver Sarkozy in early 2008 and Ben Lorello in 2009.[134][139]

UBS was fined $100 million by the FED in 2004 for trading in dollars with Iran and other sanctioned countries.[144]

At the beginning of 2007, UBS became the first Wall Street firm to announce a heavy loss in the subprime mortgage sector as the subprime mortgage crisis began to develop.[145] In May 2007, UBS announced the closure of its Dillon Read Capital Management (DRCM) division.[146] Although in 2006, DCRM had generated a profit for the bank of US$720 million, after UBS took over DRCM's positions in May 2007, losses grew from the US$124 million recorded by DRCM, ultimately to "16% of the US$19 billion in losses UBS recorded." The UBS investment bank continued to expand subprime risk in the second quarter of 2007 while most market participants were reducing risk,[147] resulting in not only expanding DRCM losses but creating 84% of the other losses experienced by the bank.[148]

In response to the growing series of problems at UBS, and possibly his role in spearheading Costas' departure from the bank, Peter Wuffli unexpectedly stepped down as CEO of the firm during the second quarter of 2007.[149][150] Wuffli would be joined by many of his fellow managers in the next year, most notably the bank's chairman Marcel Ospel. However, the bank's problems continued through the end of 2007,[151] when the bank reported its first quarterly loss in over five years.[152] As its losses jeopardized the bank's capital position, UBS quickly raised US$11.5 billion of capital in December 2007, US$9.7 billion of which came from the Government of Singapore Investment Corporation (GIC)[153] and US$1.8 billion from an unnamed Middle Eastern investor.[154]

After a significant expansion of fixed income risk during 2006 and 2007 under the leadership of Huw Jenkins, the UBS Investment Bank CEO,[155] the bank's losses continued to mount in 2008 when UBS announced in April 2008 that it was writing down a further US$19 billion of investments in subprime and other mortgage assets.[156]

By this point, UBS's total losses in the mortgage market were in excess of US$37 billion, the largest such losses of any of its peers.[157] In response to its losses, UBS announced a CHF 15 billion rights offering to raise the additional funds need to shore up its depleted reserves of capital. UBS cut its dividend to protect its traditionally high Tier 1 capital ratio, seen by investors as a key to its credibility as the world's largest wealth management company.[158][159] In October 2008, UBS announced that it had placed CHF 6 billion of new capital, through mandatory convertible notes, with Swiss Confederation.[160] The Swiss National Bank and UBS made an agreement to transfer approximately US$60 billion of currently illiquid securities and various assets from UBS to a separate fund entity.[161][162] In November 2008, UBS put US$6 billion (~$8.34 billion in 2023) of equity into the new "bad bank" entity, keeping only an option to benefit if the value of its assets were to recover. Heralded as a "neat" package by The New York Times, the UBS structure guaranteed clarity for UBS investors by making an outright sale.[163] UBS announced in February 2009 that it had lost nearly CHF 20 billion (US$17.2 billion) in 2008, the biggest single-year loss of any company in Swiss history.[164] During the 2008 financial crisis, UBS wrote down more than US$50 billion (~$70.8 billion in 2023) from subprime mortgage investments and cut more than 11,000 jobs.[165][166]
By the spring of 2009, UBS announced another management restructuring and initiated a plan to return to profitability. Jerker Johansson, the head of the investment bank division, resigned in April 2009 and was replaced by Alex Wilmot-Sitwell and Carsten Kengeter.[167] At the same time, UBS announced the planned cut of 8,700 jobs[168] and had implemented a new compensation plan. Under the plan, no more than one-third of any cash bonus would be paid out in the year it is earned with the rest to be held in reserve and stock-based incentives that would vest after three years. In April 2009, UBS announced that it agreed to sell its Brazilian financial services business, UBS Pactual, for approximately US$2.5 billion (~$3.45 billion in 2023) to BTG Investments.[169]

The Swiss government sold its CHF 6 billion stake in UBS in late 2008 at a large profit; Switzerland had purchased convertible notes in 2008 to help UBS clear its balance sheets of toxic assets.[170] Taking advantage of improved conditions in the stock market in mid-2009, UBS placed US$3.5 billion of shares with a small number of large institutional investors.[171] Oswald Grübel announced, "We are building a new UBS, one that performs to the highest standards and behaves with integrity and honesty; one that distinguishes itself not only through the clarity and reliability of the advice and services it provides but in how it manages and executes."[172] Grübel reiterated plans to maintain an integrated business model of providing wealth management advisory, investment banking, and asset management services.[173]

In August 2010, UBS launched a new advertising campaign featuring the slogan: "We will not rest" and signed a global sponsorship agreement with Formula 1.[174][175] On 26 October 2010, UBS announced that its private bank recorded net new funds of CHF 900 million during the third quarter, compared to an outflow of CHF 5.5 billion in second quarter.[176] UBS's third quarter net profit of US$1.65 billion (~$2.25 billion in 2023) beat analyst estimates, continuing a string of profitability. After the elimination of almost 5,000 jobs, UBS announced on 23 August 2011 that it was further cutting another 3,500 positions to "improve operating efficiency" and save CHF 1.5 to CHF 2 billion a year. 45 percent of the job cuts would come from the investment banking unit, which continued to post dismal figures since the 2008 financial crisis, while the rest would come from the wealth management and asset management divisions. The firm has seen profits fall due to the rise of the Swiss franc.[177][178]

On 15 September 2011, UBS became aware of a massive loss, originally estimated at US$2 billion (~$2.67 billion in 2023), allegedly due to unauthorized trading by Kweku Adoboli, a then 31-year-old Ghanaian trader on the Delta One desk of the firm's investment bank.[179] Adoboli was arrested and later charged with fraud by abuse of position and false accounting dating as far back as 2008. UBS's actual losses were subsequently confirmed as US$2.3 billion, and according to the prosecutor in Adoboli's trial he "was a gamble or two from destroying Switzerland's largest bank for his own benefit."[180][181] On 24 September 2011 UBS announced chief executive Oswald Grübel's resignation, and the appointment of Sergio Ermotti as his replacement on an interim basis.[182][183]

On 30 October 2012, UBS announced that it was cutting 10,000 jobs worldwide in an effort to slim down its investment banking operations, of which 2,500 would be in Switzerland, followed by the United States and Great Britain. This 15-percent staff cut would make overall staff count come down from 63,745 to 54,000. (For comparison, the peak employment level in 2007 before the 2008 financial crisis was 83,500).[184][185] UBS also announced that the investment bank would focus on its traditional strengths and exit much of its fixed income trading business that was not economically profitable. On 19 December 2012, UBS was fined $1.5 billion (~$1.97 billion in 2023) for its role in the Libor scandal[186] over accusations that it tried to rig benchmark interest rates.[187] In November 2014, regulators including the FCA and CFTC hit UBS with fines, along with other banks, for currency manipulation.[188] On 6 January 2014, it was reported that UBS had become the largest private banker in the world, with $1.7 (~$2.16 trillion in 2023) trillion in assets.[189] In May 2015, media reports revealed UBS is planning to sell its Australian private banking division to some of its management after a review of underperforming businesses was conducted at the company.[190]

In late 2016, the bank created the digital currency "Utility Settlement Coin" (USC) to accelerate inter-bank settlements and established a blockchain technology research laboratory in London.[191][192] From 2012 to 2018, the investment bank, led by Andrea Orcel, initiated a major restructuring, firing over 10,000 employees and focusing on European underwriting business instead of traditional dealmaking.[193][194] UBS announced in January 2018 that it does not trade or expose clients to cryptocurrencies as it believes they have little to no elasticity, and are speculatively valued.[195] It partnered with technology company IBM to launch a blockchain trade finance platform called "Batavia" in early 2018.[196]

In April 2021, UBS reported a $774 million (~$859 million in 2023) loss from the collapse of US investment fund Archegos Capital Management.[197]

In July 2021, during the COVID-19 pandemic, UBS announced it would continue to allow for flextime and remote work by many employees, noting that they did not impede productivity.[198] The announcement distinguished the bank from its competitors, such as Morgan Stanley and Goldman Sachs, which pressured on employees to return to the office as COVID-19 lockdowns and measures eased.[199][200]

In January 2022, UBS agreed to acquire Wealthfront for $1.4 billion.[201] UBS expects to accelerate its growth in the US with the purchase, and will operate Wealthfront as a business within UBS Global Wealth Management.[201] The acquisition was mutually terminated in September 2022 with both companies not providing a reason.[202][203] UBS announced that it would instead invest in a $69.7 million note convertible into Wealthfront shares, valuing the latter at its acquisition price.[203]

In November 2022, Fang Xinghai, vice chairman of the China Securities Regulatory Commission, made remarks by prerecorded video to the Global Financial Leaders' Investment Summit.[204] In it, he warned investors to not read too much foreign news (international media) about China, and said that some international investors read "too much" of it.[204] Later at the Summit, Colm Kelleher said, regarding global bankers, that "we're all very pro-China," and in reference to Fang's comments, said "We're not reading the American press, we actually buy the [China] story."[205][206]

In March 2023, UBS agreed to buy Credit Suisse, one of its main competitors, for $3.25 billion (CHF 3 billion), in an emergency rescue deal.[207][208][209] On 29 March 2023 it was announced that Sergio Ermotti is returning as chief executive officer from 5 April 2023, replacing Ralph Hamers after just over two years in charge, after approval by 5 April 2023 annual general meeting.[210] Hamers is expected to stay with the bank for a transition period.[211] UBS completed the acquisition on 12 June 2023.[212][213]

On 28 June, it became known that UBS plans to lay off more than half of Credit Suisse's employees. First of all, the reduction will affect traders and support staff in London, New York and some Asian divisions. Prior to the takeover, Credit Suisse employed about 45,000 people.[214]

In July 2023, UBS was fined $269 million by the Federal Reserve and $119 million by the Bank of England for Credit Suisse's failure in risk management related to Archegos's collapse.[215] In August 2023, UBS settled with the US Justice Department by agreeing to pay $1.43 billion in civil penalties regarding allegations of fraud and misconduct in its residential mortgage-backed securities offerings it offered in 2006 and 2007.[216]

According to Citigroup, the new bank will account for 35% of domestic deposits, 31% of corporate loans and 26% of mortgages in Switzerland.[217] UBS will keep the Swiss business of Credit Suisse but will retire its brand. UBS says it plans to cut costs by $10 billion. UBS also announced at the end of August 2023 that money outflows have stopped at Credit Suisse.[217]

In early September 2023, UBS had clearly profited from the takeover as its stocks were increasingly valuable. From April to July, UBS made a record profit of 29.2 billion CHF and its stocks, which were initially depressed after the fusion, were traded at much higher prices. The record profit was based on the difference between the purchase price of Credit Suisse stocks and the apparently higher value of its assets.[218]
[219]

In September 2023, the US Department of Justice (DOJ) started investigating UBS for Credit Suisse's alleged compliance failures which enabled its Russian clients to dodge sanctions.[220] Later UBS stated that the bank is not aware of such a probe by the DOJ and that previous reports on such allegations were incorrect.[221]

As it exists today, UBS represents a conglomeration of dozens of individual firms, many of which date back to the 19th century. Over the years, these firms merged to form the bank's three major predecessors, Union Bank of Switzerland, Swiss Bank Corporation, ICO Markets Exchange Clearing Limited and Paine Webber. The following is a visual illustration of the company's major mergers and acquisitions and historical predecessors, although this is not necessarily a comprehensive list:[222]

Bank in Winterthur(est. 1862)

Toggenburger Bank(est. 1863)

Aargauische Kreditanstalt(est. 1872)

Bank in Baden(est. 1863)

Eidgenössische Bank(est. 1863, acq. 1945)

Interhandel(est. 1928, acq. 1967)

Phillips & Drew(est. 1895 as G.A. Phillips & Co., acq. 1986)

Chase Investors Management Corporation(est. 1972 as subsidiary)

Schröder Brothers & Co.(est. 1846)

Münchmeyer & Co.(est. 1855)

Frederick Hengst & Co.

Basler Bankverein(est. 1856 as Bankverein, renamed in 1872)

Zürcher Bankverein(est. 1889)

Basler Depositenbank(est. 1882)

Schweiz Unionbank(est. 1889)

Basler Handelsbank(est. 1862, acq. 1945)

O'Connor & Associates(est. 1977, acq. 1992)

Brinson Partners(est. 1989, acq. 1994)

S. G. Warburg & Co.(est. 1946, acq. 1995)

Dillon, Read & Co.(est. 1832, acq. 1997)

Paine & Webber(est. 1880)

Jackson & Curtis(est. 1879)

Mitchell Hutchins(est. 1938, acq. 1975)

Blyth & Co.est. 1914 as Blyth, Witter & Co.

Union Securitiesest. 1939 as spin-offfrom J. & W. Seligman & Co.

Eastman Dillon & Co.(est. 1912)

Kidder, Peabody & Co.(est. 1864, acq. 1995)

J.C. Bradford & Co.(est. 1928, acq. 2000)

ICO Markets Exchange Clearing Limited(est. 2021, acq. as a shareholder 2021)

First Boston(est. 1932, acq. 1988)

As disclosed under the Swiss Stock exchange Act, the most significant shareholders of UBS are GIC Private Limited with 7.07%, BlackRock Inc with 4.98%,[225] Norges Bank with 3.30%, MFS Investment Management with 3.05% and Capital Group Companies with 3.01% of total share capital.[226] In 2008 during the subprime mortgage crisis, GIC Private Limited invested CHF 11 billion into UBS to help bail it out, thus becoming the largest single shareholder.[38]

Additionally, the UBS Group AG disclosed shareholders registered in their share register with 3% or more of shares issued. As of 30 September 2017, these are Chase Nominees Ltd, DTC (Cede & Co.) and Nortrust Nominees Ltd with 10.32%, 6.63% and 4.04% of total share capital respectively.[227]

As of 30 June 2019, shareholdings of the Group were distributed as follows:[228]

UBS frequently cites Swiss culture—specifically its penchant for privacy, security and neutrality—as foundational to its company culture.[229] Although banking secrecy started in the 1700s, Switzerland drafted a series of banking regulations and statutes in the late 1800s and 1930s to protect and secure banks within its borders.[230] The most prominent was the Federal Act on Banks and Savings Banks, known simply as the "Banking Law of 1934".[231] The federal law prohibits and criminalizes the distribution and release of client information to third parties.[231] The bill was passed by the Swiss Federal Assembly to combat the seizure of client assets and information for reasons debated by historians.[230] UBS, then known as the Swiss Bank Corporation, received large influxes of capital from Europe for safe keeping during the war.[232] More than two dozen Swiss banking statutes were drafted from 1934 to 2008 to strengthen banking secrecy at UBS Switzerland AG.[232] In 2018, Switzerland, alongside major Swiss banks including UBS, was ranked first on the Financial Secrecy Index.[233] UBS was the largest wealth manager in 23 of the top 25 countries on the 2018 Financial Secrecy Index.[233]

While UBS maintains the strictest banking secrecy policies in Switzerland, its policies across Europe and especially the United States are comparable.[232][better source needed] Within the U.S., the bank is prohibited from disclosing client activities and information both internally and through regulation imposed by the Financial Industry Regulatory Authority (FINRA), Federal Reserve, Federal Deposit Insurance Corporation (FDIC), U.S. Treasury, Securities and Exchange Commission (SEC), and various U.S. state regulators.[234] UBS employees are prohibited from discussing client activity or information publicly, sharing information across borders, retaining client information insecurely and required to maintain robust bank-client confidentiality agreements.[234] In 2018, UBS operations within the U.S. were ranked second on the Financial Secrecy Index, following UBS Switzerland AG.[233] Within the European Union (E.U.), UBS operations maintain similar banking secrecy policies to Switzerland in the following countries and crown dependencies: Jersey, Austria, Luxembourg, Liechtenstein, Monaco, and the Isle of Man.[235] Substantial, albeit greatly reduced, banking secrecy provisions are afforded to UBS operations in France, Germany, Ireland, and the Netherlands.[235]

UBS, along with other Swiss banks, maintains a variety of hidden assets and numbered bank accounts in an effort to preserve anonymity and confidentiality.[236] Despite its name, hidden accounts are not truly hidden.[237] The usage of these types of accounts (and assets) limits the knowledge of the account between the client and a restricted number of private bankers who retain record of who the account belongs to.[237]

In January 1997, Christoph Meili, a night guard at the Union Bank of Switzerland (precursor of UBS) in Zürich, publicly announced that bank officials were destroying documents about orphaned assets, believed to be the credit balances of both Nazi German and Jewish clients attained during World War II.[229] Soon after, Zurich authorities opened a judicial investigation against Meili for suspected violations of the Swiss laws on banking secrecy.[238] After a US$2.56 billion lawsuit was filed against UBS and other Swiss banks on behalf of the Jewish victims of the Holocaust, a settlement was reached that totalled US$1.25 billion in August 1998.[239][240]

The Swiss government has taken steps to curb the usage of hidden services by foreign account holders as they have been frequently used to facilitate the transfer of "black money".[241] In May 2013, Switzerland announced that it would amend certain banking secrecy laws applicable to UBS Switzerland AG to allow the disclosure of hidden client accounts to various investigative authorities.[241] However, the disclosure of such information is heavily regulated and only "occur exclusively within the scope of administrative assistance procedures based on a valid double taxation agreement."[236]

UBS, along with other Swiss banks, owns and operates undisclosed or otherwise secretive bank vaults, storage facilities or underground bunkers for gold bars, diamonds, cash, or other valuable physical assets.[229][242] The geographical location of these facilities are undisclosed to the public but are known to be present in the mountainous regions of the Swiss Alps.[243] These facilities are not subject to the same banking regulations as banks in Switzerland and do not have to report holdings to regulatory agencies.[243][244] According to the Swiss Armed Forces, UBS purchased four former military bunkers to convert into storage facilities throughout the 1980s and 1990s.[229][245] Three of these bunkers are not accessible by road or foot and require aircraft transportation.[229] The transfer of assets to these bunkers is selective as a multi-stage security clearance is required and is not available to all UBS clients.[243] In special circumstances, UBS contracts smaller banks in Southern Switzerland to maintain company assets.[229]

The largest disclosed Swiss bank vault is five floors (19 metres or 62 feet) under the bank's Geneva headquarters.[246] In July 2013, UBS established a gold storage facility and depository in Singapore for high net worth and ultra high net worth clients in their Hong Kong, China, and Malaysia markets who are willing to pay high fees and commissions for the highest level of secrecy and safety for their assets.[245]

The strict banking secrecy policies and bank-client confidentiality agreements at UBS have frequently been used to avoid, evade or otherwise escape foreign direct taxation. UBS reached multilateral agreements with the U.S. Internal Revenue Service (IRS) and U.K. HM Revenue and Customs in 2009 and 2010, respectively.[247] These agreements ensured a line of communication between the tax agencies and all registered Swiss banks.[248] The most commonly used stipulation triggered by select UBS Switzerland AG clients regard the following statute: Swiss banks are only allowed to disclose client information if a client is legally charged with proof of deliberate financial fraud, not merely the non-reporting of assets to avoid taxation.[236]

The banking privacy policies of UBS have led to numerous controversies and disagreements with foreign governments:

In December 2021, UBS was criminally convicted by an appeals court in France for "illegal banking activities", money laundering and "aggravated tax fraud" and fined €1.8 billion.[257][258] UBS has said it is appealing that decision.[259]

In January 2010, UBS issued a new code of conduct and business ethics which all employees were encouraged to sign. The code addressed issues such as financial crime, competition, confidentiality, as well as human rights and environmental issues. The eight-page code also lays out potential sanctions against employees who violate it, including warnings, demotions, or dismissal.[260] According to Kaspar Villiger, former chairman of the board, and Oswald J. Grübel, former Group CEO, the code is "an integral part of changing the way UBS conducts business".[261]

In 2011, UBS expanded its global compliance database to include information on environmental and social issues provided by RepRisk,[262] a global research firm specialized in environmental, social and corporate governance (e.g., ESG) risk analytics and metrics.[263] This was done in an effort to mitigate environmental and social risks that could impact the bank's reputation or financial performance and to simultaneously help globally standardize and systematically implement the firm's due diligence processes.[264] RepRisk data is used in the on-boarding process to screen potential new clients and sourcing partners,[265] alongside periodic client reviews and, also, to evaluate the risks related to transactions in investment banking and institutional lending.[266]

In 2018, UBS held 0.72% of shares in HikVision[267] (surveillance cameras), a subsidiary of the Chinese military conglomerate CETC.

In October 2019, UBS joined UN's Global Investors for Sustainable Development Alliance (GISD).[268] UBS has committed to raise US$5 billion (~$5.55 billion in 2023) of SDG-related impact investments by the end of 2021, which aim to create a measurable positive social or environmental impact.[268]

UBS has been an early adopter regarding the use of blockchain technology in financial services. In April 2015, UBS opened an innovation lab at the Level39 technology accelerator space in London.[269]

In August 2016, UBS announced that it will team up with BNY Mellon, Deutsche Bank, Banco Santander, brokerage company ICAP and the fintech company Clearmatics, to promote UBS's "Utility Settlement Coin" (USC). The USC is a blockchain-based digital currency that financial institutions could use to transact securities with each other, bypassing the traditional settlement processes which is ongoing.[270]

In 2021, UBS buys 31% ICO Markets Exchange Clearing Limited, Digital Asset Exchange, Regulator Company in the European Digital Asset Market which financial institutions and private clients they will use to trade securities among themselves, bypassing the traditional ongoing settlement processes.[271]

In 2018, UBS digitally cloned Daniel Kalt, one of its chief economists. Artificial intelligence expert FaceMe was hired to create an interactive avatar of Kalt that can meet with clients via television screen. The clients will be able to ask questions and receive answers, made possible by IBM's Watson AI technology.[272][273]

In 2006, for the fourth consecutive year, UBS was named one of the 100 Best Companies for Working Mothers living in the U.S. by Working Mother magazine.[274] It is a member of the Stonewall Diversity Champions scheme[275] and has active gay and lesbian, ethnic minority, and women's networking groups. UBS was included on Business Week's The Best Places to Launch a Career 2008, and ranked No. 96 out of the 119 total companies listed.[275] On 2 February 2010, UBS topped the charts for the ninth year in a row in Institutional Investor's annual ranking of Europe's most highly regarded equity analysts. In a year of extremes for equity markets, money managers say that no firm did a better job than UBS to keep them informed about which European sectors, countries, and industries offered the greatest potential.[276] On 4 May 2010, UBS Investment Bank was voted the leading pan-European brokerage firm for equity and equity linked research for a record tenth successive year. A Thomson Reuters Extel survey ranked UBS number one in all three of the key disciplines of research: Research (tenth year); Sales (ninth year running); and Equity Trading and Execution (up from second place in 2009). UBS was also named as the number one leading pan-European brokerage firm for economics and strategy research.[277]

On 31 October 2013, UBS Wealth Management was voted the Best Global Private Bank by Professional Wealth Management,[278] retaining the title in 2014 while also being recognized as the Best Private Bank for Philanthropy Services, and Best Global Brand in Private Banking.[279] On 27 October 2016, for the 4th consecutive year, UBS Wealth Management won the Best Global Private Bank title, as well as the Best Private Bank in Asia award for the 5th consecutive year.[280] UBS won the top prize again in 2018.[281]

In 2014, the Group received Euromoney's Awards for Excellence 2014 as the Best global bank,[282][283] and as the Best Bank in Switzerland.[284] In 2017, UBS not only retained its leading position taken in 2016 in the main category best private banking services overall at the Euromoney's Private Banking Awards,[285] but also received recognition as Western Europe's best bank for advisory 2017.[286]

In 2018, for the third consecutive year, RobecoSAM, an organization specialized exclusively on Sustainability Investing and conducting extensive research,[287] named UBS in its Industry group leader report 2018 for each of the industry groups represented in the Dow Jones Sustainability Index the group leader[288] in Diversified Financials.[289] The report highlighted Group's sustainability efforts directed through its UBS and Society program: a cross-divisional platform involving activities and capabilities in sustainable investing and philanthropy, environmental and human rights policies, UBS's own environmental footprint, as well as the community investment.[290] The Group also received recognition from Global Finance which rates financial services providers that best meet the specialized needs of corporations on a global level. The selection criteria are focused less on the size, but rather on qualities that companies look for when choosing a provider.[291][292] UBS was named in the category Global Winners as Best Private Bank in the World 2017,[293] and in the list of global best banks 2017, the Group received the award as Global Winner in the category World's Best Investment Banks 2017.[292][294]

In 2019, UBS was listed as one of the Top 50 World's Most Attractive Employers Global Business Ranking 2019 by Universum Global Survey.[295]

UBS is particularly active in sponsoring various golf tournaments, cross-country skiing in Switzerland, ice hockey, and a range of other events around the world. UBS was the sponsor of the Alinghi sailing ship, winner of the Americas Cup in 2003. UBS has been or currently is a sponsor of the following sporting events and organizations:




UBS's cultural sponsorships are typically related to classical music and contemporary art, although the company also sponsors a range of film festivals, music festivals, and other cultural events and organizations. UBS supported the Guggenheim UBS MAP Global Art Initiative in which the Solomon R. Guggenheim Foundation identified and worked with artists, curators and educators from South and Southeast Asia, Latin America, and the Middle East and North Africa to expand their reach in the international art world and challenge the Western-centric view of art history.[296] UBS has previously been or currently is a sponsor of the following cultural events and organizations:


UBS currently holds the naming rights to UBS Arena which is the home of the New York Islanders.Operation Gothic Serpent

Operation Uphold Democracy
War on Terror

The United States Special Operations Command (USSOCOM or SOCOM) is the unified combatant command charged with overseeing the various special operations component commands of the Army, Marine Corps, Navy, and Air Force of the United States Armed Forces. The command is part of the Department of Defense and is the only unified combatant command created by an Act of Congress. USSOCOM is headquartered at MacDill Air Force Base in Tampa, Florida.

The idea of an American unified special operations command had its origins in the aftermath of Operation Eagle Claw, the disastrous attempted rescue of hostages at the American embassy in Iran in 1980. The ensuing investigation, chaired by Admiral James L. Holloway III, the retired Chief of Naval Operations, cited lack of command and control and inter-service coordination as significant factors in the failure of the mission.[8] Since its activation on 16 April 1987, U.S. Special Operations Command has participated in many operations, from the 1989 invasion of Panama to the War on Terror.[9][10]

USSOCOM is involved with clandestine activity, such as direct action, special reconnaissance, counter-terrorism, foreign internal defense, unconventional warfare, psychological warfare, civil affairs, and counter-narcotics operations. Each branch has a distinct Special Operations Command that is capable of running its own operations, but when the different special operations forces need to work together for an operation, USSOCOM becomes the joint component command of the operation, instead of a SOC of a specific branch.[11]

The unwieldy command and control structure of separate U.S. military special operations forces (SOF), which led to the failure of Operation Eagle Claw in 1980, highlighted the need within the US Department of Defense for reform and reorganization. The US Army Chief of Staff, General Edward C. "Shy" Meyer, had already helped create the U.S. Delta Force in 1977.[12] Following Eagle Claw, he called for a further restructuring of special operations capabilities. Although unsuccessful at the joint level, Meyer nevertheless went on to consolidate Army SOF units under the new 1st Special Operations Command in 1982.[13]

By 1983, there was a small but growing sense in the US Congress of the need for military reforms. In June, the Senate Armed Services Committee (SASC) began a two-year-long study of the Defense Department,  which included an examination of SOF spearheaded by Senator Barry Goldwater (R-AZ). With concern mounting on Capitol Hill, the Department of Defense created the Joint Special Operations Agency on 1 January 1984; this agency, however, had neither operational nor command authority over any SOF.[14][15] The Joint Special Operations Agency thus did little to improve SOF readiness, capabilities, or policies, and therefore was deemed insufficient. Within the Defense Department, there were a few staunch SOF supporters. Noel Koch, Principal Deputy Assistant Secretary of Defense for International Security Affairs, and his deputy, Lynn Rylander, both advocated SOF reforms.[16]

At the same time, a few on Capitol Hill were determined to overhaul United States Special Operations Forces. They included Senators Sam Nunn (D-GA) and William Cohen (R-ME), both members of the Armed Services Committee, and Representative Dan Daniel (D-VA), the chairman of the United States House Armed Services Subcommittee on Readiness. Congressman Daniel had become convinced that the U.S. military establishment was not interested in special operations, that the country's capability in this area was second rate, and that SOF operational command and control was an endemic problem.[16] Senators Nunn and Cohen also felt strongly that the Department of Defense was not preparing adequately for future threats. Senator Cohen agreed that the U.S. needed a clearer organizational focus and chain of command for special operations to deal with low-intensity conflicts.[14]

In October 1985, the Senate Armed Services Committee published the results of its two-year review of the U.S. military structure, entitled "Defense Organization: The Need For Change."[17] James R. Locher III, the principal author of this study, also examined past special operations and speculated on the most likely future threats. This influential document led to the 1986 Goldwater-Nichols Act.[18][19] By spring 1986, SOF advocates had introduced reform bills in both houses of Congress. On 15 May, Senator Cohen introduced the Senate bill, co-sponsored by Senator Nunn and others, which called for a joint military organization for SOF and the establishment of an office in the Defense Department to ensure adequate funding and policy emphasis for low-intensity conflict and special operations.[20] Representative Daniel's proposal went even further—he wanted a national special operations agency headed by a civilian who would bypass the Joint Chiefs and report directly to the US Secretary of Defense; this would keep Joint Chiefs and the Services out of the SOF budget process.[15]

Congress held hearings on the two bills in the summer of 1986. Admiral William J. Crowe Jr., Chairman of the Joint Chiefs of Staff, led the Pentagon's opposition to the bills. As an alternative, he proposed a new Special Operations Forces command led by a three-star general. This proposal was not well received on Capitol Hill—Congress wanted a four-star general in charge to give SOF more influence. A number of retired military officers and others testified in favor of the need for reform.[16] By most accounts, retired Army Major General Richard Scholtes gave the most compelling reasons for the change. Scholtes, who commanded the joint special operations task force during Operation Urgent Fury, explained how conventional force leaders misused SOF during the operation, not allowing them to use their unique capabilities, which resulted in high SOF casualties.[21] After his formal testimony, Scholtes met privately with a small number of Senators to elaborate on the problems that he had encountered in Grenada.[22]

Both the House and Senate passed SOF reform bills, and these went to a conference committee for reconciliation. Senate and House conferees forged a compromise. The bill called for a unified combatant command headed by a four-star general for all SOF, an Assistant Secretary of Defense for Special Operations and Low-Intensity Conflict, a coordinating board for low-intensity conflict within the National Security Council, and a new Major Force Program (MFP-11) for SOF (the so-called "SOF checkbook").[23][24] The final bill, attached as a rider to the 1987 Defense Authorization Act, amended the Goldwater-Nichols Act and was signed into law in October 1986. This was interpreted as Congress forcing the hand of the DOD and the Reagan administration regarding what it saw as the past failures and emerging threats. The DOD and the administration were responsible for implementing the law, and Congress subsequently passed two additional bills to ensure implementation.[16] The legislation promised to improve SOF in several respects. Once implemented, MFP-11 provided SOF with control over its own resources, better enabling it to modernize the force. Additionally, the law fostered interservice cooperation: a single commander for all SOF promoted interoperability among the same command forces. The establishment of a four-star commander-in-chief and an Assistant Secretary of Defense for Special Operations and Low-Intensity Conflict eventually gave SOF a voice in the highest councils of the Defense Department.[23]

However, implementing the provisions and mandates of the Nunn-Cohen Amendment to the National Defense Authorization Act for Fiscal Year 1987 was neither rapid nor smooth. One of the first issues to arise was the appointment of an Assistant Secretary of Defense for Special Operations and Low Intensity Conflict, whose principal duties included monitorship of special operations activities and the low-intensity conflict activities of the Department of Defense. Congress increased the number of assistant secretaries of defense from 11 to 12, but the Department of Defense still did not fill this new billet. In December 1987, Congress directed Secretary of the Army John O. Marsh to carry out the ASD (SO/LIC) duties until the Senate approved a suitable replacement. Not until 18 months after the legislation passed did Ambassador Charles Whitehouse assume the duties of ASD (SO/LIC).[25]

Meanwhile, the establishment of USSOCOM provided its own measure of excitement. A quick solution to manning and basing a brand new unified command was to abolish an existing command. United States Readiness Command (USREDCOM), with an often misunderstood mission, did not appear to have a viable mission in the post-Goldwater-Nichols era, and its commander-in-chief, General James Lindsay, had had some special operations experience. On 23 January 1987, the Joint Chiefs of Staff recommended to the Secretary of Defense that USREDCOM be disestablished to provide billets and facilities for USSOCOM. President Ronald Reagan approved the establishment of the new command on 13 April 1987. The Department of Defense activated USSOCOM on 16 April 1987 and nominated General Lindsay to be the first Commander in Chief Special Operations Command (USCINCSOC). The Senate accepted him without debate.[16]

USSOCOM's first tactical operation involved 160th Special Operations Aviation Regiment (Airborne) ("Night Stalkers") aviators,  SEALs, and Special Boat Teams (SBT) working together during Operation Earnest Will in September 1987. During Operation Earnest Will, the United States ensured that neutral oil tankers and other merchant ships could safely transit the Persian Gulf during the Iran–Iraq War. Iranian attacks on tankers prompted Kuwait to ask the United States in December 1986 to register 11 Kuwaiti tankers as American ships so that they could be escorted by the U.S. Navy. President Reagan agreed to the Kuwaiti request on 10 March 1987, hoping it would deter Iranian attacks.[16] The protection offered by U.S. naval vessels, however, did not stop Iran, which used mines and small boats to harass the convoys steaming to and from Kuwait. In late July 1987, Rear Admiral Harold J. Bernsen, commander of the Middle East Force, requested NSW assets. Special Boat Teams deployed with six Mark III Patrol Boats and two SEAL platoons in August.[16] The Middle East Force decided to convert two oil servicing barges, Hercules and Wimbrown VII, into mobile sea bases. The mobile sea bases allowed SOF in the northern Persian Gulf to thwart clandestine Iranian mining and small boat attacks.[26]

On 21 September, Nightstalkers flying MH-60 and Little Birds took off from the frigate USS Jarrett to track an Iranian ship, Iran Ajr. The Nightstalkers observed Iran Ajr turn off her lights and begin laying mines. After receiving permission to attack, the helicopters fired guns and rockets, stopping the ship. As Iran Ajr's crew began to push mines over the side, the helicopters resumed firing until the crew abandoned the ship. Special Boat Teams provided security while a SEAL team boarded the vessel at first light and discovered nine mines on the vessel's deck, as well as a logbook revealing areas where previous mines had been laid. The logbook implicated Iran in mining international waters.[16]

Within a few days, the Special Operations forces had determined the Iranian pattern of activity; the Iranians hid during the day near oil and gas platforms in Iranian waters and at night they headed toward the Middle Shoals Buoy, a navigation aid for tankers. With this knowledge, SOF launched three Little Bird helicopters and two patrol craft to the buoy. The Little Bird helicopters arrived first and were fired upon by three Iranian boats anchored near the buoy. After a short but intense firefight, the helicopters sank all three boats. Three days later, in mid-October, an Iranian Silkworm missile hit the tanker Sea Isle City near the oil terminal outside Kuwait City. Seventeen crewmen and the American captain were injured in the missile attack.[16][27] During Operation Nimble Archer, four destroyers shelled two oil platforms in the Rostam oil field. After the shelling, a SEAL platoon and a demolition unit planted explosives on one of the platforms to destroy it. The SEALs next boarded and searched a third-platform 2 miles (3 km) away. Documents and radios were taken for intelligence purposes.[citation needed]

On 14 April 1988, 65 miles (100 km) east of Bahrain, the frigate USS Samuel B. Roberts hit a mine, blowing an immense hole in its hull.[28] Ten sailors were injured. During Operation Praying Mantis the U.S. retaliated fiercely, attacking the Iranian frigate Sahand and oil platforms in the Sirri and Sassan oil fields.[27] After U.S. warships bombarded the Sirri platform and set it ablaze, a UH-60 with a SEAL platoon flew toward the platform but was unable to get close enough because of the roaring fire. Secondary explosions soon wrecked the platform.[16] Thereafter, Iranian attacks on neutral ships dropped drastically.  On 18 July, Iran accepted the United Nations cease-fire; on 20 August 1988, the Iran–Iraq War ended. The remaining SEALs, patrol boats, and helicopters then returned to the United States.[16] Special operations forces provided critical skills necessary to help CENTCOM gain control of the northern Persian Gulf and balk Iran's small boats and minelayers. The ability to work at night proved vital because Iranian units used darkness to conceal their actions. Additionally, because of Earnest Will operational requirements, USSOCOM would acquire new weapons systems—the patrol coastal ships and the Mark V Special Operations Craft.[16]

Special Operations Command first became involved in Somalia in 1992 as part of Operation Provide Relief. C-130s circled over Somali airstrips during the delivery of relief supplies. Special Forces medics accompanied many relief flights into the airstrips throughout southern Somalia to assess the area. They were the first U.S. soldiers in Somalia, arriving before U.S. forces who supported the expanded relief operations of Restore Hope.[16][29][30] The first teams into Somalia were CIA Special Activities Division paramilitary officers with elements of JSOC. They conducted very high-risk advanced force operations prior to the entry of the follow-on forces. The first casualty of the conflict came from this team and was a Paramilitary officer and former Delta Force operator named Larry Freedman. Freedman was awarded the Intelligence Star for "extraordinary heroism" for his actions.[31]

The earliest missions during Operation Restore Hope were conducted by Navy SEALs. The SEALs performed several hydrographic reconnaissance missions to find suitable landing sites for Marines. On 7 December, the SEALs swam into Mogadishu Harbor, where they found suitable landing sites, assessed the area for threats, and concluded that the port could support offloading ships. This was a tough mission because the SEALs swam against a strong current which left many of them overheated and exhausted. Furthermore, they swam through raw sewage in the harbor, which made them sick.[16] When the first SEALs hit the shore the following night, they were surprised to meet members of the news media. The first Marines came ashore soon thereafter, and the press redirected their attention to them. Later, the SEALs provided personal security for President George Bush during a visit to Somalia.[16][30] In December 1992, Special Forces assets in Kenya moved to Somalia and joined Operation Restore Hope. January 1993, a Special Forces command element deployed to Mogadishu as the Joint Special Operations Forces-Somalia (JSOFOR) that would command and control all special operations for Restore Hope. JSOFOR's mission was to make initial contact with indigenous factions and leaders; provide information for force protection; and provide reports on the area for future relief and security operations. Before redeploying in April, JSOFOR elements drove over 26,000 miles (42,000 km), captured 277 weapons, and destroyed over 45,320 pounds (20,560 kg) of explosives.[16]

In August 1993, Secretary of Defense Les Aspin directed the deployment of a Joint Special Operations Task Force (JSOTF) to Somalia in response to attacks made by General Mohamed Farrah Aidid's supporters upon U.S. and UN forces. The JSOTF, named Task Force (TF)
Ranger was charged with a mission named Operation Gothic Serpent to capture Aidid. This was an especially arduous mission, for Aidid had gone underground, after several Lockheed AC-130 air raids and UN assaults on his strongholds.[16][32][33]

While Marines from the 24th MEU provided an interim QRF (Force Recon Det and helicopters from HMM-263), the task force arrived in the country and began training exercises. The Marines were asked to take on the Aidid snatch mission, but having the advantage of being in the area for more than two months, decided after mission analysis that the mission was a "no-go" due to several factors, centered around the inability to rescue the crew of a downed helicopter (re: the indigenous forces technique of using RPGs against helicopters and blocking the narrow streets in order to restrict the movement of a ground rescue force). This knowledge was not passed on to the Rangers, due to the Marines operating from the USS Wasp and the Rangers remaining on land. TF Ranger was made up of operators from Delta Force, 75th Ranger Regiment, 160th SOAR, SEALs from the Naval Special Warfare Development Group, and Air Force special tactics units.[16][32] During August and September 1993, the task force conducted six missions into Mogadishu, all of which were successes. Although Aidid remained free, the effect of these missions seriously limited his movements.[33]

On 3 October, TF Ranger launched its seventh mission, this time into Aidid's stronghold the Bakara Market to capture two of his key lieutenants. The mission was expected to take only one or two hours.[32] Helicopters carried an assault and a ground convoy of security teams launched in the late afternoon from the TF Ranger compound at Mogadishu airport. The TF came under increasingly heavy fire, more intense than during previous missions. The assault team captured 24 Somalis including Aidid's lieutenants and were loading them onto the convoy trucks when a MH-60 Blackhawk was hit by a rocket-propelled grenade (RPG).[16][33] A small element from the security forces, as well as an MH-6 assault helicopter and an MH-60 carrying a fifteen-man combat search and rescue (CSAR) team, rushed to the crash site.[16][32][33] The battle became increasingly worse. An RPG struck another MH-60, crashing less than 1 mile (1.6 km) to the south of the first downed helicopter. The task force faced overwhelming Somali mobs that overran the crash sites, causing a dire situation.[32] A Somali mob overran the second site and, despite a heroic defense, killed everyone except the pilot, whom they took prisoner. Two defenders of this crash site, Master Sergeant Gary Gordon and Sergeant First Class Randall Shughart, were posthumously awarded the Medal of Honor.[16][32][33] About this time, the mission's quick reaction force (QRF) also tried to reach the second crash site. This force too was pinned by the Somali fire and required the fire support of two AH-6 helicopters before it could break contact and make its way back to the base.[16]

The assault and security elements moved on foot towards the first crash area, passing through heavy fire, and occupied buildings south and southwest of the downed helicopter. They fought to establish defensive positions so as not to be pinned down by the very heavy enemy fire while treating their wounded and worked to free the pilot's body from the downed helicopter. With the detainees loaded on trucks, the ground convoy force attempted to reach the first crash site. Unable to find it amongst the narrow, winding alleyways, the convoy came under devastating small arms and RPG fire. The convoy had to return to base after suffering numerous casualties and sustaining substantial damage to their vehicles.[34]

Reinforcements, consisting of elements from the QRF, 10th Mountain Division soldiers, Rangers, SEALs, Pakistan Army tanks and Malaysian armored personnel carriers, finally arrived at 1:55 am on 4 October. The combined force worked until dawn to free the pilot's body, receiving RPG and small arms fire throughout the night.[16] All the casualties were loaded onto the armored personnel carriers, and the remainder of the force was left behind and had no choice but to move out on foot.[32] AH-6 gunships raked the streets with fire to support the movement. The main force of the convoy arrived at the Pakistani Stadium-compound for the QRF-at 6:30 am,[32] thus concluding one of the bloodiest and fiercest urban firefights since the Vietnam War. Task Force Ranger experienced a total of 17 killed in action and 106 wounded. Various estimates placed Somali casualties above 1,000.[32] Although Task Force Ranger's few missions were successes, the overall outcome of Operation Gothic Serpent was deemed a failure because of the Task Force's failure to complete their stated mission, capturing Mohamed Farrah Aidid.[32] Most U.S. forces pulled out of Somalia by March 1994. The withdrawal from Somalia was completed in March 1995.[16] Even though Operation Gothic Serpent failed, USSOCOM still made significant contributions to operations in Somalia. SOF performed reconnaissance and surveillance missions, assisted with humanitarian relief, protected American forces, and conducted riverine patrols. Additionally, they ensured the safe landing of the Marines and safeguarded the arrival of merchant ships carrying food.[16][27]

USSOCOM's 10th Special Forces Group, elements of JSOC, and CIA/SAD Paramilitary Officers linked up again and were the first to enter Iraq prior to the invasion. Their efforts organized the Kurdish Peshmerga to defeat Ansar Al Islam in Northern Iraq before the invasion. This battle was for control of a territory in Northeastern Iraq that was completely occupied by Ansar Al Islam, an ally of Al Qaeda. This was a very significant battle and led to the death of a substantial number of terrorists and the uncovering of a chemical weapons facility at Sargat. These terrorists would have been in the subsequent insurgency had they not been eliminated during this battle. Sargat was the only facility of its type discovered in the Iraq war. This battle may have been the Tora Bora of Iraq, but it was a sound defeat for Al Qaeda and their ally Ansar Al Islam.[35] This combined team then led the Peshmerga against Saddam's Northern Army. This effort kept Saddam's forces in the north and denied the ability to redeploy to contest the invasion force coming from the south. This effort may have saved the lives of hundreds if not thousands of coalition servicemen and women.[36]

At the launch of the Iraq War, dozens of 12-member Special Forces teams infiltrated southern and western Iraq to hunt for Scud missiles and pinpoint bombing targets. Scores of Navy SEALs seized oil terminals and pumping stations on the southern coast.[37] Air Force combat controllers flew combat missions in MC-130H Combat Talon IIs and established austere desert airstrips to begin the flow of soldiers and supplies deep into Iraq. It was notably different from the Persian Gulf war of 1991, where Special Operations forces were mostly kept participating. But it would not be a replay of Afghanistan, where Army Special Forces and Navy SEALs led the fighting. After their star turn in Afghanistan, many special operators were disappointed to play a supporting role in Iraq. Many special operators felt restricted by cautious commanders.[38] From that point, USSOCOM has since killed or captured hundreds of insurgents and Al-Qaeda terrorists. It has conducted several foreign internal defense missions successfully training the Iraqi security forces.[39][40]

United States Special Operations Command played a pivotal role in fighting the former Taliban government in Afghanistan in 2001[41] and toppling it thereafter, as well as combating the insurgency and capturing Saddam Hussein in Iraq. USSOCOM in 2004 was developing plans to have an expanded and more complex role in the global campaign against terrorism,[42] and that role continued to emerge before and after the killing of Osama bin Laden in Pakistan in 2011.[43] In 2010, "of about 13,000 Special Operations forces deployed overseas, about 9,000 [were] evenly divided between Iraq and Afghanistan."[43]

In the initial stages of the War in Afghanistan, USSOCOM forces linked up with CIA Paramilitary Officers from Special Activities Division to defeat the Taliban without the need for large-scale conventional forces.[44] This was one of the biggest successes of the global War on Terrorism.[45]
These units linked up several times during this war and engaged in several furious battles with the enemy. One such battle happened during Operation Anaconda, the mission to squeeze the life out of a Taliban and Al-Qaeda stronghold dug deep into the Shah-i-Kot Valley and Arma Mountains of eastern Afghanistan. The operation was seen as one of the heaviest and bloodiest fights in the War in Afghanistan.[46] The battle on an Afghan mountaintop called Takur Ghar featured special operations forces from all 4 services and the CIA. Navy SEALs, Army Rangers, Air Force Combat Controllers, and Pararescuemen fought against entrenched Al-Qaeda fighters atop a 10,000-foot (3,000 m) mountain. Subsequently, the entrenched Taliban became targets of every asset in the sky. According to an executive summary, the Battle of Takur Ghar was the most intense firefight American special operators have been involved in since 18 U.S. Army Rangers were killed in Mogadishu, Somalia, in 1993.[47][48][49] During Operation Red Wings on 28 June 2005, four Navy SEALs, pinned down in a firefight, radioed for help. A Chinook helicopter, carrying 16 service members, responded but was shot down. All members of the rescue team and three of four SEALs on the ground died. It was the worst loss of life in Afghanistan since the invasion in 2001. The Navy SEAL Marcus Luttrell alone survived.[50][51] Team leader Michael P. Murphy was awarded the Medal of Honor for his actions in the battle.[52]

In 2010, special operations forces were deployed in 75 countries, compared with about 60 at the beginning of 2009.[43] In 2011, SOC spokesman Colonel Tim Nye (Army[53]) was reported to have said that the number of countries with SOC presence will likely reach 120 and that joint training exercises will have been carried out in most or all of those countries during the year. One study identified joint-training exercises in Belize, Brazil, Bulgaria, Burkina Faso, Germany, Indonesia, Mali, Norway, Panama, and Poland in 2010 and also, through mid-year 2011, in the Dominican Republic, Jordan, Romania, Senegal, South Korea, and Thailand, among other nations. In addition, SOC forces executed the high-profile killing of Osama bin Laden in Pakistan in 2011.[citation needed]

In November 2009 The Nation reported on a covert JSOC/Blackwater anti-terrorist operation in Pakistan.[54]

In 2010, White House counterterrorism director John O. Brennan said that the United States "will not merely respond after the fact" of a terrorist attack but will "take the fight to al-Qaeda and its extremist affiliates whether they plot and train in Afghanistan, Pakistan, Yemen, Somalia and beyond." Olson said, "In some places, in deference to host-country sensitivities, we are lower in profile. In every place, Special Operations forces activities are coordinated with the U.S. ambassador and are under the operational control of the four-star regional commander."[43]

The conduct of actions by SOC forces outside of Iraq and Afghan war zones has been the subject of internal U.S. debate, including between representatives of the Bush administration such as John B. Bellinger III, on one hand, and the Obama administration on another. The United Nations in 2010 also "questioned the administration's authority under international law to conduct such raids, particularly when they kill innocent civilians. One possible legal justification – the permission of the country in question – is complicated in places such as Pakistan and Yemen, where the governments privately agree but do not publicly acknowledge approving the attacks," as one report put it.[43]

In two decades fighting terrorism, 660 members of the special operation community have been killed and a further 2,738 were wounded.[55]

Joint Special Operations Command (JSOC)[56] is a component command of the USSOCOM and is charged to study special operations requirements and techniques to ensure interoperability and equipment standardization, plan and conduct special operations exercises and training, and develop Joint Special Operations Tactics.[1] It was established in 1980 on the recommendation of Col. Charlie Beckwith, in the aftermath of the failure of Operation Eagle Claw.[57]

Units

Portions of JSOC units have made up the constantly changing special operations task force, operating in the U.S. Central Command area of operations. The Task Force 11, Task Force 121, Task Force 6-26 and Task Force 145 are creations of the Pentagon's post-11 September campaign against terrorism, and it quickly became the model for how the military would gain intelligence and battle insurgents in the future. Originally known as Task Force 121, it was formed in the summer of 2003 when the military merged two existing Special Operations units, one hunting Osama bin Laden in and around Afghanistan, and the other tracking Sadaam Hussein in Iraq.[65][66][67]

Special Operations Command – Joint Capabilities (SOC-JC) was transferred to USSOCOM from the soon-to-be disestablished United States Joint Forces Command in 2011.[68] Its primary mission was to train conventional and SOF commanders and their staffs to support USSOCOM international engagement training requirements, and support the implementation of capability solutions in order to improve strategic and operational Warfighting readiness and joint interoperability. SOC-JC must also be prepared to support the deployed Special Operations Joint Task Force (SOJTF) Headquarters (HQ).[citation needed]

The Government Accountability Office wrote that SOC-JC was disestablished in 2013, and positions were to be zeroed out in 2014.[69]

On 1 December 1989, the United States Army Special Operations Command (USASOC) activated as the 16th major Army command. These special operations forces have been America's spearhead for unconventional warfare for more than 40 years. USASOC commands such units as the well known Special Forces (SF, or the "Green Berets"), the Rangers, and such relatively unknown units as two psychological operations groups, a special aviation regiment, a civil affairs brigade, and a special sustainment brigade. These are one of the USSOCOM's main weapons for waging unconventional warfare and counter-insurgency. The significance of these units is emphasized as conventional conflicts are becoming less prevalent as insurgent and guerrilla warfare increases.[70][71][72][73]

Units:

In October 2005, the Secretary of Defense directed the formation of United States Marine Forces Special Operations Command (MARSOC), the Marine component of United States Special Operations Command. It was determined that the Marine Corps would initially form a unit of approximately 2,500 to serve with USSOCOM. On February 24, 2006 MARSOC activated at Camp Lejeune, North Carolina. MARSOC initially consisted of a small staff and the Foreign Military Training Unit (FMTU), which had been formed to conduct foreign internal defense. FMTU is now designated as the Marine Special Operations Advisor Group (MSOAG).[89]

As a service component of USSOCOM, MARSOC is tasked by the Commander USSOCOM to train, organize, equip, and deploy responsive U.S. Marine Corps special operations forces worldwide, in support of combatant commanders and other agencies. MARSOC has been directed to conduct foreign internal defense, direct action, and special reconnaissance. MARSOC has also been directed to develop a capability in unconventional warfare, counter-terrorism, and information operations.
MARSOC deployed its first units in August 2006, six months after the group's initial activation. MARSOC reached full operational capability in October 2008.[90]

Units

The United States Naval Special Warfare Command (NAVSPECWARCOM, NAVSOC, or NSWC) was commissioned April 16, 1987, at Naval Amphibious Base Coronado in San Diego as the Naval component to the United States Special Operations Command. Naval Special Warfare Command provides vision, leadership, doctrinal guidance, resources and oversight to ensure component special operations forces are ready to meet the operational requirements of combatant commanders.[94] Today, SEAL Teams and Special Boat Teams comprise the elite combat units of Naval Special Warfare. These teams are organized, trained, and equipped to conduct a variety of missions to include direct action, special reconnaissance, counter-terrorism, foreign internal defense, unconventional warfare and support psychological and civil affairs operations. Their operators are deployed worldwide in support of National Command Authority objectives, conducting operations with other conventional and special operations forces.[citation needed]

Units

Air Force Special Operations Command was established on May 22, 1990, with headquarters at Hurlburt Field, Florida. AFSOC is one of the 10 Air Force Major Commands or MAJCOMs, and the Air Force component of United States Special Operations Command. It holds operational and administrative oversight of subordinate special operations wings and groups in the regular Air Force, Air Force Reserve Command and the Air National Guard.[citation needed]

AFSOC provides Air Force special operations forces for worldwide deployment and assignment to regional unified commands. The command's SOF are composed of highly trained, rapidly deployable airmen, conducting global special operations missions ranging from the precision application of firepower via airstrikes or close air support, to infiltration, exfiltration, resupply and refueling of SOF operational elements.[100] AFSOC's unique capabilities include airborne radio and television broadcast for psychological operations, as well as aviation foreign internal defense instructors to provide other governments military expertise for their internal development.[citation needed]

The command's core missions include battlefield air operations; agile combat support; aviation foreign internal defense; information operations; precision aerospace fires; psychological operations; specialized air mobility; specialized refueling; and intelligence, surveillance and reconnaissance.[38][101][102]

Components

Organization

As of October 2023, the United States Space Force has not announced the formation of a Special Operations Commander. However, in July 2023, the United States Space Force assigned Col. Stephan Cummings as an "Element Commander" to U.S. Special Operations Command. As of October 2023, the United States Space Force has not announced any heraldry for Space Force Special Operations Command.

The Special Operations Forces Liaison Element (SOFLE) is small group of special forces personnel, sometimes just one or two at a time, attached to embassies in Africa, Southeast Asia, South America, or elsewhere that terrorists are thought to be operating, planning attacks, raising money or seeking safe haven, especially those teams in the United States. MLEs report to the local US combat commanders and the Special Operations Command (SOCOM), instead of reporting to the local ambassador or CIA station chief.[citation needed]

The commander of U.S. Special Operations Command is a statutory office (10 U.S.C. § 167), and is held by a four-star general or admiral.

The United States Special Operations Command Medal was introduced in 1994 to recognize individuals for outstanding contributions to, and in support of, special operations. Some notable recipients include:[citation needed]

Since it was created, there have been more than 50 recipients, only six of whom were not American, including;

(† posthumously)Water supply and sanitation in the United States involves a number of issues including water scarcity, pollution, a backlog of investment, concerns about the affordability of water for the poorest, and a rapidly retiring workforce. Increased variability and intensity of rainfall as a result of climate change is expected to produce both more severe droughts and flooding, with potentially serious consequences for water supply and for pollution from combined sewer overflows.[8][9] Droughts are likely to particularly affect the 66 percent of Americans whose communities depend on surface water.[10] As for drinking water quality, there are concerns about disinfection by-products, lead, perchlorates,  PFAS and pharmaceutical substances, but generally drinking water quality in the U.S. is good.

Cities, utilities, state governments and the federal government have addressed the above issues in various ways. To keep pace with demand from an increasing population, utilities traditionally have augmented supplies. However, faced with increasing costs and droughts, water conservation is beginning to receive more attention and is being supported through the federal WaterSense program. The reuse of treated wastewater for non-potable uses is also becoming increasingly common. Pollution through wastewater discharges, a major issue in the 1960s, has been brought largely under control.

Most Americans are served by publicly owned water and sewer utilities. Public water systems, which serve more than 25 customers or 15 service connections, are regulated by the U.S. Environmental Protection Agency (EPA) and state agencies under the Safe Drinking Water Act (SDWA).[11] Eleven percent of Americans receive water from private (so-called "investor-owned") utilities. In rural areas, cooperatives often provide drinking water. Finally, over 13 million households are served by their own wells.[12][13] The accessibility of water not only depends on geographical location, but on the communities that belong to those regions.[14] Of the millions who lack access to clean water, the majority are low-income minority individuals. Wastewater systems are also regulated by EPA and state governments under the Clean Water Act (CWA). Public utilities commissions or public service commissions regulate tariffs charged by private utilities. In some states they also regulate tariffs by public utilities. EPA also provides funding to utilities through state revolving funds.[15][16]

Water consumption in the United States is more than double that in Central Europe, with large variations among the states. In 2002 the average American family spent $474 on water and sewerage charges,[10] which is about the same level as in Europe. The median household spent about 1.1 percent of its income on water and sewage.[17] By 2018, 87% of the American population receives water from publicly owned water companies.[18]

In the 19th century numerous American cities were afflicted with major outbreaks of disease, including cholera in 1832, 1849 and 1866 and typhoid in 1848.[19] The fast-growing cities did not have sewers and relied on contaminated wells within the city confines for drinking water supply. In the mid-19th century many cities built centralized water supply systems. However, initially these systems provided raw river water without any treatment. Only after John Snow established the link between contaminated water and disease in 1854 and after authorities became gradually convinced of that link, water treatment plants were added and public health improved. Sewers were built since the 1850s, initially based on the erroneous belief that bad air (miasma theory) caused cholera and typhoid. It took until the 1890s for the now universally accepted germ theory of disease to prevail.

However, most wastewater was still discharged without any treatment, because wastewater was not believed to be harmful to receiving waters due to the natural dilution and self-purifying capacity of rivers, lakes and the sea. Wastewater treatment only became widespread after the introduction of federal funding in 1948 and especially after an increase in environmental consciousness and the upscaling of financing in the 1970s. From 1948 to 1987 federal funding for sanitation was provided through grants to local governments. Congress amended the CWA in 1987 and changed the funding system for sewage treatment to loans through revolving funds. Congress added a state revolving fund for drinking water utilities to the SDWA in 1996.

In the 1840s and 1850s the largest cities in the U.S. built pipelines to supply drinking water from rivers or lakes. However, the drinking water was initially not treated, since the link between waterborne pathogens and diseases was not yet well known. In 1842 New York City was one of the first cities in the U.S. to tap water resources outside the city limits. It dammed the Croton River in Westchester County, New York, and built an aqueduct from the reservoir to the city.[20] Also in 1842, construction was completed on Chicago's first water works, with water mains made of cedar and a water intake located about 150 feet (46 m) into Lake Michigan.[21] In 1848, Boston began construction of a water transmission system. A tributary of the Sudbury River was impounded creating Lake Cochituate, from where the Cochituate Aqueduct transported water to the Brookline Reservoir that fed the city's distribution system.[22] In 1853, Washington, D.C., followed suit by beginning the construction of the Washington Aqueduct to provide water from the Great Falls on the Potomac River.[23]

In 1854, the British physician John Snow found that cholera was spread through contaminated water. As a result of his findings, several cities began to treat all water with sand filters and chlorine before distributing it to the public. Before efforts to clean drinking water were implemented at the turn of the 20th century, mortality among 1- to 5-year-olds in the United States in some major river cities was nearly one in five. Clean water is estimated to have reduced about three-fourths of infant mortality, and two-thirds of child mortality.[11] By 1900, sand filtration was widely used. In 1908, the first continuous application of chlorination to drinking U.S. water was in Jersey City, New Jersey (and not without controversy).[24] Cities also began to construct sewers in the late 19th century.[19] As a result of water treatment and sanitation, the incidence of cholera and typhoid rapidly decreased. Slow sand filtration was initially the technology of choice for water treatment,[24]: 2  later being gradually displaced by rapid sand filtration.[25] As a result of the water purification efforts, mortality among black infants declined in particular,[26] leading to a 13 percent reduction in the black-white infant mortality gap.[27]

In the arid American Southwest, the water demand of rapidly growing cities such as Los Angeles exceeded local water availability, requiring the construction of large pipelines to bring in water from far-away sources. The most spectacular example is the first Los Angeles Aqueduct built between 1905 and 1913 to supply water from the Owens Valley over a distance of 233 mi (375 km).

Drinking water quality standards were first issued in 1914 by the United States Public Health Service. However they were only enforceable for interstate transportation carriers (such as railroads) at specific points where water was transferred.[11]

Most of the first sewer systems in the United States were built as combined sewers (carrying both storm water and sewerage). They discharged into rivers, lakes and the sea without any treatment. The main reason for choosing combined sewers over separate systems (separating sanitary sewers from storm water drains) was a belief that combined sewer systems were cheaper to build than separate systems. Also, there was no European precedent for successful separate sewer systems at the time.[19] The first large-scale sewer systems in the United States were constructed in Chicago and Brooklyn in the late 1850s, followed by other major U.S. cities.[19]

Few sewage treatment facilities were constructed in the late 19th century to treat combined wastewater because of the associated difficulties. There were only 27 U.S. cities with wastewater treatment works by 1892, most of them "treating" wastewater through land application. Of these 27 cities, 26 had separate sanitary and storm water sewer systems, thus facilitating wastewater treatment, because there was no need for large capacities to accommodate wet weather flows. Furthermore, there was a belief that the diluted combined wastewater was not harmful to receiving waters, due to the natural dilution and self-purifying capacity of rivers, lakes and the sea.[19] In the early 20th century a debate evolved between those who thought it was in the best interest of public health to construct wastewater treatment facilities and those who believed building them was unnecessary. Nevertheless, many cities began to opt for separate sewer systems, creating favorable conditions for adding wastewater treatment plants in the future.[19]

Where wastewater was being treated it was typically discharged into rivers or lakes. However, in 1932, the first reclaimed water facility in the U.S. was built in Golden Gate Park, San Francisco, for the reuse of treated wastewater in landscape irrigation.[28]

Sanitary sewers were not the only sanitation solution applied. They were particularly useful in high-density urban areas. However, in some newly built lower-density areas, decentralized septic systems were built. They were attractive because they reduced capital expenditures and had fewer operation and maintenance costs compared to wastewater treatment plants.[19]

In the first half of the 20th century water supply and sanitation were a local government responsibility with regulation at the state level; the federal government played almost no role in the sector at that time. This changed with the enactment of the Federal Water Pollution Control Act of 1948, which provided for comprehensive planning, technical services, research, and financial assistance by the federal government to state and local governments for sanitary infrastructure. The Act was amended in 1965, establishing a uniform set of water quality standards and creating a Federal Water Pollution Control Administration authorized to set standards where states failed to do so.[19]

Comprehensive federal regulations for water supply and sanitation were introduced in the 1970s, in reaction to increased public awareness of environmental degradation nationwide. In 1970 EPA was established by the Richard Nixon administration, and authority for managing various environmental programs was transferred to the new agency.[29] In 1972, Congress passed the Clean Water Act (CWA), requiring industrial plants and municipal sewage plants to improve their waste treatment practices in order to limit the effect of contaminants on freshwater sources.[30] In 1974, the Safe Drinking Water Act was adopted for the regulation of public water systems. It was motivated by a resurgence in concern about the safety of drinking water due to breakthroughs in chemistry that revealed organic chemicals in water that were tentatively linked with cancer.[11] This law specified a number of contaminants that must be closely monitored and reported to residents should they exceed the maximum contaminant levels (MCLs) allowed. EPA was charged with creating standards for drinking water for all public systems, defined as those that served more than 25 customers or 15 service connections.[11] The new law required federal and state governments to closely monitor local drinking water utilities for safety and compliance with federal standards.[31] The CWA set the unprecedented goal of eliminating all water pollution by 1985 and authorized massive expenditures of $24.6 billion in research and construction grants for municipal sewage treatment. The funds initially provided an incentive to build centralized wastewater collection and treatment infrastructure for municipalities, instead of decentralized systems.[19] However, the 1977 amendments to the CWA required communities to consider alternatives to the conventional centralized sewer systems, and financial assistance was made available for such alternatives.[19] In the mid-1990s decentralized systems served approximately 25 percent of the U.S. population, and approximately 37 percent of new housing developments.[32]

The vast majority of municipal wastewater in the U.S. is treated to the national secondary treatment standard or better.[33]: 25  There have been a few disagreements between EPA and some local governments about the appropriate level of treatment, with the former arguing for more stringent standards. For example, in the late 1980s, the city of San Diego and EPA were involved in a legal dispute over the requirement to treat sewage at the Point Loma Wastewater Treatment Plant to secondary treatment standards. The city prevailed, saying that it saved ratepayers an estimated $3 billion and that process had proved successful in maintaining a healthy ocean environment. The Point Loma plant uses an advanced primary process.[34] The requirement to perform secondary treatment on wastewater before ocean discharge was waived by the EPA in 1995, "taking into account the city's unique circumstances".[35]

In 1987 Congress passed the Water Quality Act, which replaced the construction grant program with a system of subsidized loans using the Clean Water State Revolving Fund (CWSRF).[36] The intention at the time was to completely phase out federal funding after a few years. Funding peaked in 1991 and continued at high levels thereafter, despite the original intentions. New challenges arose, such as the need to address combined sewer overflows for which EPA issued a policy in 1994.[37] In 1996 Congress established the Drinking Water State Revolving Fund, building on the success of the CWSRF, in order to finance investments to improve compliance with more stringent drinking water quality standards.[38]

This section provides a brief overview of the water supply and sanitation infrastructure in the U.S., water sources of some of the main cities, and the main types of residential water use.

The centralized drinking water supply infrastructure in the United States consists of dams and reservoirs, well fields, pumping stations, aqueducts for the transport of large quantities of water over long distances, water treatment plants, reservoirs in the water distribution system (including water towers), and 1.8 million miles of distribution lines.[5]: 14  Depending on the location and quality of the water source, all or some of these elements may be present in a particular water supply system. In addition to this infrastructure for centralized network distribution, over 13 million households rely on their own water sources, usually wells.[12][13]

The centralized sanitation infrastructure in the U.S. consists of 1.2 million miles of sewers—including both sanitary sewers and combined sewers, sewage pumping stations and publicly owned treatment works (POTW). EPA estimated that there were at least 16,583 POTWs operating in 2004, serving a population of 222.8 million.[33]: 26  About 860 communities in the U.S. have combined sewer systems, serving about 40 million people.[39] In addition, at least 17% of Americans are served by on-site sanitation systems such as septic tanks.[40]

In the United States over 75 percent of the population is served by over 16,000 municipal sewage treatment plants.[41] Most plants are required to meet national secondary treatment standards.[42][43]

About 66% of the U.S. population (195 million people) are served by surface water systems, and 34% (101 million) are served by groundwater-supplied systems (as of 2009). Most groundwater systems are in small communities, and comprise 90% of the overall population of public water systems.[10]

For a surface water system to operate without filtration it has to fulfill certain criteria set by the EPA under its 2006 Surface Water Treatment Rule, including the implementation of a watershed control program. The water system of New York City has repeatedly fulfilled these criteria for most of the water processed through its facilities.[44]

Boston, New York City, San Francisco, Denver, and Portland, Oregon are among the large cities in the U.S. that do not need to treat most of their surface water sources beyond disinfection, because their water sources are located in the upper reaches of protected watersheds and thus are naturally very pure.[45]

Boston receives most of its water from the Quabbin and Wachusett Reservoirs and the Ware River in central and western Massachusetts.

New York City's water supply is fed by three watershed systems. The two larger systems, Catskill and Delaware, do not have filtration. The Catskill watershed is in one of the largest protected wilderness areas in the United States.[46] Water from the two systems has been treated with ultraviolet germicidal irradiation since 2013.[47][48] The Croton system, which supplies 10% of the city's water, has been filtered since 2015.[49]

San Francisco obtains 85% of its drinking water from high Sierra snowmelt through the Hetch Hetchy Reservoir in Yosemite National Park.[50] However, to supplement the imported water supply, and to help maintain delivery of drinking water in the event of a major earthquake, drought or decline in the snowpack, San Francisco considers the use of alternative locally produced, sustainable water sources such as reclaimed water for irrigation, local groundwater and desalination during drought periods, all as part of its Water Supply Diversification Program.[51]

The largest source of water supply for Portland, Oregon, is the Bull Run Watershed.[52]

Denver receives its water almost entirely from mountain snowmelt in a number of highly protected watersheds in more than 9 counties. Its water is stored in 14 reservoirs, the largest of which is the Dillon Reservoir on the Blue River in the Colorado River. Water is diverted from there through the Harold D. Roberts Tunnel under the Continental Divide into the South Platte River Basin.[53]

 Cities that rely on more or less polluted surface water from the lower reaches of rivers have to rely on extensive and costly water purification plants. The Las Vegas Valley obtains 90% of its water from Lake Mead on the Colorado River, which has been affected by drought.[54] To supply a portion of the future water supply, Las Vegas plans to buy water rights in the Snake Valley in White Pine County, 250 mi (400 km) north of the city straddling the Utah border and other areas, pumping it to Las Vegas through a US$2 billion pipeline.[55] Phoenix draws about half of its drinking water from the Salt River–Verde River watershed, and about 40% from the Colorado River further downstream at Lake Havasu through the Central Arizona Project. Los Angeles obtains about half of its drinking water from the Owens River and Mono Lake through the Los Angeles Aqueduct,[56] with additional supplies from Lake Havasu through the Colorado River Aqueduct.[57] San Diego imports nearly 90 percent of its water from other areas, specifically northern California and the Colorado River.[58]

The cities on the Mississippi River are supplied by water from that river except for Memphis. The metropolitan area of Atlanta receives 70% of its water from the Chattahoochee River and another 28% from the Etowah, Flint, Ocmulgee and Oconee rivers.[59] Chicago is supplied by water from Lake Michigan and Detroit receives its water from the Detroit River.[60] Philadelphia receives 60% of its water from the Delaware River and 40% from the Schuylkill River.[61] Washington, D.C. receives its water from the Potomac River through the Washington Aqueduct.[62]

Miami and its metropolitan area obtain drinking water primarily from the Biscayne Aquifer. Given increasing water demand, Miami-Dade County is considering the use of reclaimed water to help preserve the Biscayne Aquifer.[63] Memphis receives its water from artesian aquifers.[64] San Antonio draws the bulk of its water from the Edwards Aquifer;[65][66] it did not use any surface water until 2006.[67]

Seventy-one percent of Houston's supply flows from the Trinity River into Lake Livingston, and from the San Jacinto River into Lake Conroe and Lake Houston. Deep underground wells drilled into the Evangeline and Chicot aquifers provide the other 29 percent of the city's water supply.[68]

In the United States, until 2009 in Colorado, water rights laws almost completely restricted rainwater harvesting; a property owner who captured rainwater was deemed to be stealing it from those who have the rights to take water from the watershed. Now, residential good owners who meet certain criteria may obtain a permit to install a rooftop precipitation collection system (SB 09-080).[69] Up to 10 large scale pilot studies may also be permitted (HB 09–1129).[70] The main factor in persuading the Colorado Legislature to change the law was a 2007 study that found that in an average year, 97% of the precipitation that fell in Douglas County, in the southern suburbs of Denver, never reached a stream—it was used by plants or evaporated on the ground. Rainwater catchment is mandatory for new dwellings in Santa Fe, New Mexico.[71] Texas offers a sales tax exemption on the purchase of rainwater harvesting equipment. Both Texas[72] and Ohio allow the practice even for potable purposes. Oklahoma passed the Water for 2060 Act in 2012, to promote pilot projects for rainwater and graywater use among other water-saving techniques.[73]

Domestic water use (also called home or residential water use) in the United States was estimated by the United States Geological Survey at 29.4 billion US gallons (111,000,000 m3) per day in 2005,[74] and 27.4 billion US gallons (104,000,000 m3) per day in 2010 (7 percent lower).[2] The bulk of domestic water is provided through public networks. 13% or 3.6 billion US gallons (14,000,000 m3) of water is self-supplied.[2] The average domestic water use per person in the U.S. was 98-US-gallon (370 L) per day in 2005,[74] and 88-US-gallon (330 L) per day in 2010.[2] This is about 2.2 times as high as in England (150 Liter)[75] and 2.6 times as high as in Germany (126 Liter).[76][77]

One of the reasons for the high domestic water use in the U.S. is the high share of outdoor water use. For example, the arid West has some of the highest per capita domestic water use, largely because of landscape irrigation. Per capita domestic water use varied from 51-US-gallon (190 L) per day in Maine to 148-US-gallon (560 L) per day in Arizona and 167-US-gallon (630 L) per day in Utah.[2] According to a 1999 study, on average all over the U.S. 58% of domestic water use is outdoors for gardening, swimming pools etc. and 42% is used indoors.[78] A 2016 update of the 1999 study measured the average quantities and percent shares of seven indoor end uses of water:[79]

Only a very small share of public water supply is used for drinking. According to one 2002 survey of 1,000 households, an estimated 56% of Americans drank water straight from the tap and an additional 37% drank tap water after filtering it.[80] 74% of Americans said they bought bottled water.[80] According to a non-representative survey conducted among 216 parents (173 Latinos and 43 non-Latinos), 63 (29%) never drank tap water. The share is much higher among Latinos (34%) than among non-Latinos (12%). The study concluded that many Latino families avoid drinking tap water because they fear it causes illness, resulting in greater cost for the purchase of bottled and filtered water.[81] This notion is also repeated among Asians.[81]

EPA defines a public water system (PWS) as one that provides water for human consumption through pipes or other constructed conveyances to at least 15 service connections or serves an average of at least 25 people for at least 60 days a year. The agency has defined three types of PWS:

In 2007, there were about 155,000 PWSs in the United States, of which 52,000 CWSs. PWSs are either publicly owned, cooperatives or privately owned,[6] serving a total of about 242 million people in 2000. EPA estimates the number of beneficiaries of community water systems at 288 million in 2007[6] The United States Geological Survey estimates that "About 242 million people depended on water from public suppliers" in 2000.[83] Four thousand systems provide water in localities with more than 10,000 inhabitants, and the remaining 50,000 systems provide water in localities with less than 10,000 inhabitants.[6] In 2000, 15% of Americans (43.5 million people) relied on their own water source, usually a well, for drinking water.[74][12]

Utilities in charge of public water supply and sanitation systems can be owned, financed, operated and maintained by a public entity, a private company or both can share responsibilities through a public-private partnership. Utilities can either be in charge of only water supply and/or sanitation, or they can also be in charge of providing other services, in particular electricity and gas. In the latter case they are called multi-utilities. Bulk water suppliers are entities that manage large aqueducts and sell either treated or untreated water to various users, including utilities.

Public service providers. Eighty-nine percent of Americans served by a public water system are served by a public or cooperative entity.[84][85] Usually public systems are managed by utilities that are owned by a city or county, but have a separate legal personality, management and finances. Examples are the District of Columbia Water and Sewer Authority, the Los Angeles Department of Water and Power and Denver Water. In some cases public utilities span several jurisdictions. An example is the Washington Suburban Sanitary Commission that spans two counties in Maryland. Utility cooperatives are a major provider of water services, especially in small towns and rural areas[86][87]

Private utilities. About half of American drinking water utilities, or about 26,700, are privately owned, providing water to 11% of Americans served by public water systems.[84] Most of the private utilities are small, but a few are large and are traded on the stock exchange. The largest private water company in the U.S. is American Water, which serves 15 million customers in 1,600 communities in the U.S. and Canada.[88] It is followed by United Water, which serves 7 million customers and is owned by the French firm Suez Environnement.[89] Overall, about 33.5 million Americans (11% of the population) get water from a privately owned drinking water utility.[84] In addition, 20% of all wastewater utilities in the U.S. are privately owned, many of them relatively small. About 3% of Americans get wastewater service from private wastewater utilities. In addition, more than 1,300 government entities (typically municipalities) contract with private companies to provide water and/or wastewater services.[84]

Multi-utilities. Some utilities in the U.S. provide only water and/or sewer services, while others are multi-utilities that also provide power and gas services. Examples of utilities that provide only water and sewer services are the Boston Water and Sewer Commission, Dallas Water Utilities, the New York City Department of Environmental Protection, Seattle Public Utilities and the Washington Suburban Sanitary Commission. Other utilities, such as the San Francisco Public Utilities Commission, provide power in addition to water and sewer services. Other multi-utilities provide power and water services, but no sewer services, such as the Los Angeles Department of Water and Power and the Orlando Utilities Commission. There are also some utilities that provide only sewer services, such as the Metropolitan Water Reclamation District of Greater Chicago or the sewer utility in the city of Santa Clara.[90]

Bulk water suppliers. There are also a few large bulk water suppliers in the arid Southwest of the United States, which sell water to utilities. The Metropolitan Water District of Southern California (MWD) sells treated water from the Colorado River and Northern California to its member utilities in Southern California through the California Aqueduct. Twenty-six cities and water districts serving 18 million people are members of MWD.[91] The Central Arizona Water Conservation district supplies water from the Colorado River to 80 municipal, industrial, agricultural and Indian customers in Central and Southern Arizona through the Central Arizona Project Aqueduct (CAP).[92]

The economic regulation of water and sanitation service providers in the U.S. (in particular in relation to the setting of user water rates) is usually the responsibility of regulators such as Public Utility Commissions at the state level, which are organized in the National Association of Regulatory Utility Commissioners.[93] (see economic regulator). However, while all investor-owned utilities are subject to tariff regulation, only few public utilities are subjected to the same regulation. In fact, only 12 states have laws restricting pricing practices by public water and sanitation utilities.[94]

The environmental and drinking water quality regulation is the responsibility of state departments of health or environment and the EPA.[95]

The Resource Conservation and Recovery Act (RCRA), protects groundwater by regulating the disposal of solid waste and hazardous waste.[96] The Comprehensive Environmental Response, Compensation, and Liability Act (CERCLA), also known as "Superfund," requires remediation of abandoned hazardous waste sites.[97]

The United States Environmental Protection Agency (EPA) and state environmental agencies set wastewater standards under the Clean Water Act.[98] Point sources must obtain surface water discharge permits through the National Pollutant Discharge Elimination System (NPDES). Point sources include industrial facilities, municipal governments (sewage treatment plants and storm sewer systems), other government facilities such as military bases, and some agricultural facilities, such as animal feedlots.[99] EPA sets basic national wastewater standards: The "Secondary Treatment Regulation" applies to municipal sewage treatment plants,[100] and the "Effluent guidelines" which are regulations for categories of industrial facilities.[101]

These standards are incorporated into the permits, which may include additional treatment requirements for individual plants developed on a case-by-case basis. NPDES permits must be renewed every five years.[102] EPA has authorized 47 state agencies to issue and enforce NPDES permits. EPA regional offices issue permits for the rest of the country.[103]

Wastewater discharges to groundwater are regulated by the Underground Injection Control Program (UIC) under the Safe Drinking Water Act.[104] UIC permits are issued by 34 state agencies and EPA regional offices.[105]

Financial assistance for improvements to sewage treatment facilities is available to state and local governments through the Clean Water State Revolving Fund, a low interest loan program.[106]

There are a number of professional associations, trade associations and other non-governmental organizations (NGOs) that are actively engaged in water supply and sanitation.

Professional associations include the American Society of Civil Engineers focused on advocacy for state revolving fund and water resource development legislation, American Water Works Association (AWWA) oriented mainly towards drinking water professionals and the Water Environment Federation (WEF) geared mainly at wastewater professionals. The geographical scope of both is greater than the U.S.: AWWA has members in 100 countries,[107] with a focus on the U.S. and Canada, and WEF has member associations in 30 countries.[108]

There are a number of trade associations in the sector, including:

In addition to lobbying, some of these trade associations also provide public education, as well as training and technical assistance to their members.[114][115]

An example of an NGO active in water supply and sanitation is Food & Water Watch, a consumer rights group created in 2005 which focuses on corporate and government accountability relating to food, water, and fishing. Another example is the Alliance for Water Efficiency (AWE), which was created in 2007 with seed funding from the EPA to "advocate for water efficiency research, evaluation, and education" at the national level. Its Board members "represent water utilities, environmental organizations, plumbing and appliance associations, irrigation manufacturers, the academic community, government, and others."[116]

Among the main issues facing water users and the water industry in the U.S. in 2009 are water scarcity and adaptation to climate change; concerns about combined sewer overflows and drinking water quality; as well as concerns about a gap between investment needs and actual investments. Other issues are concerns about a swiftly retiring workforce, the affordability of water bills for the poor during a recession, and water fluoridation, which is opposed by some mainly on ethical and safety grounds.

With water use in the United States increasing every year, many regions are starting to feel the pressure. At least 36 states are anticipating local, regional, or statewide water shortages by 2013, even under non-drought conditions.[118]

According to the National Academies, climate change affects water supply in the U.S. in the following ways:

In some parts of the country water supplies are dangerously low due to drought and depletion of the aquifers, particularly in the West and the South East region of the U.S.[119][better source needed] Many of the dry, desert areas in the U.S. have this problem. According to AZCentral, "Arizona's groundwater levels are plummeting in many areas... The water levels in more than 2,000 wells have dropped more than 100 feet since they were first drilled."[120] That sample size is approximately a fourth of Arizona's drinking-water wells.

Water security is projected to be a problem in the future since future population growth will most likely occur in areas that are currently water stressed.[121] Ensuring that the United States remains water secure will require policies that will ensure fair distribution of existing water sources, protecting water sources from becoming depleted, maintaining good wastewater disposal, and maintaining existing water infrastructure.[122][123] Currently there are no national limits for US groundwater or surface water withdrawal. If limits are imposed, the people most impacted will be the largest water withdrawers from a water source.

In 2005, 31% of US water use was for irrigation, 49% was thermoelectric power, public supply 11%, industrial was 4%, aquaculture 2%, mining 1%, domestic 1%, and livestock less than 1%.[124]

An important turning point in managing drinking water contamination occurred after passage of the 1974 SDWA, which required the National Academy of Sciences (NAS) to study the issue. The NAS found that there really was not a lot of information available on drinking water quality. Perhaps the most important part of the study, according to senior EPA officials responsible for implementing the 1974 law, was that it described some methodologies for doing risk assessments for chemicals that were suspected carcinogens.[125]

According to the 2015 US census, one-fifth of all households are not connected to a community sewer system. Furthermore, many households have wastewater disposal systems that have inadequate treatment, such as sewage systems directly piped to nearby bodies of water and septic systems leading to fecal contamination.[126]

For example, in Barry-Easton District, Michigan 10% of the state's 1.3 million on-site wastewater treatment systems are malfunctioning, causing sewage to be running into the lakes and streams.[127]

This pollution of water is contributed to several health concerns in the US, especially for minorities and low-income individuals. In Lowes County, Alabama, hookworm is affecting people today due to unsanitary waste disposal. 73% of residents reported to have sewage running into their homes and 34% of residents surveyed tested positive for hookworm.[128] These contaminated bodies of water also directly affect drinking water supplies, habitats and recreational sites, creating more issues for the environment. Overall, the cost to replace failed sewer systems and remove fecal waste from the water is typically higher than placing alternative infrastructure and maintaining adequate functioning systems.[128]

Combined sewer overflows (CSO) and sanitary sewer overflows affect the quality of water resources in many parts of the U.S. About 860 communities have combined sewer systems, serving about 40 million people, mostly in the Northeast and the Great Lakes Region.[39] CSO discharges during heavy storms can cause serious water pollution. A 2004 EPA report to Congress estimated that there are 9,348 CSO outflows in the U.S., discharging about 850 billion US gallons (3.2×109 m3) of untreated wastewater and storm water to the environment.[129] EPA estimates that between 23,000 and 75,000 sanitary sewer overflows occur each year, resulting in releases of between 3 and 10 billion US gallons (38,000,000 m3) of untreated wastewater.[129]

The increased frequency and intensity of rainfall as a result of climate change[8][130] will result in additional water pollution from wastewater treatment, storage, and conveyance systems."[130] For the most part, wastewater treatment plants and CSO control programs have been designed on the basis of the historic hydrologic record, taking no account of prospective changes in flow conditions due to climate change.[130]

In 2015, 9% of 500-person-or-larger community water systems monitored by the EPA, covering approximately 21 million people, violated at least one health standard. Between 1984 and 2018, between 4 and 28% of the American population received contaminated water in any given year.[131] There are several aspects of drinking water quality that are of some concern in the United States, including Cryptosporidium,[132] disinfection by-products, lead, perchlorates, per- and polyfluoroalkyl substances (PFAS) and pharmaceutical substances.

While lead in drinking water continues to persist as a public health problem in some communities, the source of the lead is generally from the lead service lines, rather than the water delivered by the utility.[133] EPA's Lead and Copper Rule (LCR) does not set an MCL, but requires a utility to take action when the lead level at a customer location reaches 0.015 mg/L.[134] A typical utility action is to adjust the chemistry of the drinking water with anti-corrosive additives, but replacement of lead service lines (pipes that connect the water main to the home) is also an option.[135] Most communities have avoided lead service line replacement due to the high cost.[136]

Congress passed the Reduction of Lead in Drinking Water Act tightening the definition of "lead free" plumbing, in 2011.[137] EPA published a final rule implementing the law on September 1, 2020.[138]

In response to the Flint, Michigan water crisis, EPA published a revised LCR on January 15, 2021, addressing testing, pipe replacement and related issues. The rule mandates additional requirements for sampling tap water, corrosion control, public outreach and testing water in schools. The rule continues the requirement for replacement of lead service lines when the "action level" for lead is exceeded, but requires that a utility replace at least 3 percent of its lines annually, compared to 7 percent under the prior regulation.[139][140] EPA published an updated LCR on October 8, 2024, which requires the removal of all lead pipes within ten years. The regulation also lowers the action level of lead contamination to 10ppb from the current limit of 15ppb (effective in 2027).[141][142] EPA's Consumer Confidence Rule of 1998 requires most public water suppliers to provide consumer confidence reports, also known as annual water quality reports, to their customers.[143] Each year by July 1 anyone connected to a public water system should receive in the mail an annual water quality report that tells where water in a specific locality comes from and what's in it. Consumers can find out about these local reports on a map provided by EPA.[144] 29% of Americans are reading their water quality reports. A 2003 survey found that customers were generally satisfied with the information they are receiving from their water companies and their local or state environmental offices.[80]

EPA published a proposed rule for perchlorate on June 26, 2019, with a proposed MCL of 0.056 mg/L for public water systems.[145] On July 21, 2020, EPA announced that it was withdrawing its 2019 proposal, stating that it had taken "proactive steps" with state and local governments to address perchlorate contamination.[146] In September 2020 the Natural Resources Defense Council filed suit against EPA for its failure to regulate perchlorate, and stated that 26 million people may be affected by perchlorate in their drinking water.[147] Under a court order, EPA agreed to publish a proposed perchlorate rule in 2025 and issue a final rule in 2027.[146]

EPA issued drinking water standards in April 2024 for six PFAS chemicals: perfluorooctanoic acid (PFOA), perfluorooctanesulfonic acid (PFOS), GenX, perfluorobutanesulfonic acid (PFBS), perfluorohexanesulfonic acid (PFHxS) and perfluorononanoic acid (PFNA).[148][149]

In November 2006, EPA published its Ground Water Rule, due to concerns that public water systems supplied by ground water would be vulnerable to contamination from harmful microorganisms, including fecal matter.[150] The objective of the regulation, promulgated under the authority of the Safe Drinking Water Act, is to keep microbial pathogens out of public water sources.[151]

Congress recognized that injection wells were a potential threat to groundwater quality when they passed the Safe Drinking Water Act (SDWA) of 1974.[152] This instructed the U.S. Environmental Protection Agency (EPA) to create a national program that would prevent underground injection activities that would endanger underground drinking sources. The EPA must regulate underground injection of fluids and wastes through wells that discharge or may release such material into or above an underground reservoir of drinkable water. The EPA has promoted several Underground Injection Control (UCI) regulations in order to protect underground reservoirs of drinkable water from being contaminated.[152]

In its Infrastructure Report Card the American Society of Civil Engineers (ASCE) gave both the U.S. drinking water and wastewater infrastructure a grade of D− in 2005, down from D in 2001. According to the report, "the nation's drinking water system faces a staggering public investment need to replace aging facilities, comply with safe drinking water regulations and meet future needs."[153] Investment needs are about $19 billion/year for sanitation and $14 billion/year for drinking water, totaling $33 billion/year.[154] State and local governments invested $35.1 billion in water supply and sanitation in 2008, including 16.3 billion for drinking water supply and 18.8 billion for sanitation.[155]

In 2013 the ASCE rating remained at the "D" level, and a 2013 paper from Stanford University's Center for Reinventing the Nation's Urban Water Infrastructure (ReNUWIt) describes why "water infrastructure is systemically resistant to innovation":

Despite a growing sense that water will be as important a global issue as energy in the coming century, capital deployed for water resources "pales in comparison to that for renewable energy."...  Only 5 percent of the $4.3 billion in VC money invested in the clean tech industry goes to water technologies. Federal support is also on the decline. The membranes that today enable desalinization and water reuse, for example, were the fruits of R&D undertaken during the Kennedy administration. We now spend ten times less on that research.[156]
The Stanford paper notes that innovations occur when utilities see opportunities for "short-term benefits and immediate savings," when there are water shortages, and in quality of life situations, like Philadelphia's "green infrastructure initiative designed to reduce combined sewer overflow."[156]

Concerning drinking water supply, EPA estimated in 2003 that $276.8 billion would have to be invested between 2003 and 2023.[157][158] Concerning sanitation, EPA estimated in 2007 that investment of $202.5 billion is needed over the next 20 years to control wastewater pollution. This includes $134 billion for wastewater treatment and collection, $54.8 billion for resolving unsatisfactory combined sewer overflows and $9 billion for stormwater management.[159] The EPA needs surveys do not capture all investment needs, in particular concerning capital replacement.[154]

In the U.S., in 2020, about 8.9 million people (about 2.68% of the population at that time) still lacked access to a household "safely managed" water sources. Regarding sanitation, in 2020, around 5.7 million (about 1.74% of the population at that time) people did not have access to a household "safely managed sanitation" system.[160][161] More than 99% of the U.S. population has access to "complete plumbing facilities" which is defined as having hot and cold piped water, a bathtub or shower, and a flush toilet.

However, more than 1.6 million people in the United States, in 630,000 households, still lacked basic plumbing facilities as of 2014.[162] This includes access to a toilet, shower, or running water. Of the millions who lack access to clean water, the majority are low-income individuals who are people of color, belong to tribal communities, and/or are immigrants.[14] These groups often live in rural areas which are more susceptible to water quality violations than urban areas.[14] Many of these violations are due to poor-quality sources of water and the lack of resources to maintain the current water infrastructure regulations.[14] These violations are heightened in low-income minority communities because these communities were often disregarded when it came to drafting regulations and creating new infrastructure.[163] A 2017 study utilized data from the Safe Drinking Water Information System (SDWIS) and discovered that a significant amount of water quality violations were associated with higher Hispanic and Black populations. In addition, low income people of color experienced more violations than low income White, non-Hispanic communities.[164]

In results from the American Community Survey (ACS), race was established as the biggest factor in determining people's access to clean water. In the United States, 0.3 percent of White households lack complete plumbing. When looking at this same variable under different ethnic groups, 0.5 percent of African-American and Latino households and 5.8 percent of Native-American households lack complete plumbing. The ACS highlights the disparities in access to clean water that come alongside different racial and ethnic groups.[165]

These disparities in access to clean water are most pronounced when viewing certain minority groups that are also poor. The previous 2017 SDWIS study examined the number of violations reported when looking at different ethnic groups and their level of poverty. When looking at communities where 10% were below the poverty line, an increase from 0% to 80% of the population being Hispanic resulted in a change in the number of violations reported from 0.10 to 0.11. However, when looking at communities where 40% were below the poverty line, the same increase in the Hispanic population resulted in a change from 0.09 to 0.17. This study concluded that these ethnic disparities in access to clean water are prominent when incorporating low-income communities as well.[164]

As a direct result of structural racism as well as environmental racism, there has been this systemic failure to provide equal resources and services to all Americans regardless of their race or income. Individuals do not choose to have incomplete plumbing, but rather, it is a result of government policy that intentionally excluded those communities. In the 1930s, the U.S. government relocated many American Indians to rural and isolated areas. These areas lacked the resources to not only create proper water infrastructure but also employ proper sanitation techniques. In addition to this mistreatment, the passing of new laws and regulations made it difficult for Native Americans to voice their concerns which only furthered their decay regarding their access to clean water.[163]

In 1954, the city of Zanesville, Ohio discriminated against African American residents by limiting their access to water lines.[166] When constructing these water lines, the city specifically ignored entire neighborhoods that were home to African-Americans.[167] This incident highlights the discrimination that many disadvantaged communities faced that prevented their access to clean water.

Between 1950 and 2000, several water infrastructure initiatives were debuted in California, but they were exclusive in the communities that they protected. During this time period, California took measures to prevent the integration of the Hispanic/Latino community which resulted in the lack of consideration when crafting certain infrastructure. These communities were not accounted for and had to resort to decentralized water sources and poor sanitation techniques. The lack of inclusion of certain groups in California in water policy contributed to the overall deterioration of these regions.[168]

When examining specific aspects of water policy, the majority tends to focus on protecting drinking water from over extraction by businesses for commercial purposes rather than outlining local requirements for drinking water. While the rest of the United States began to benefit from closer accessibility to water, these specific communities were ignored, evident through the creation of new laws and regulations.[169]

Today, there are still communities around the United States that do not have access to clean water. While some regions have developed alternatives to deal with this inaccessibility, others are still struggling with this issue.[165]

In Central Valley, California, residents claim that water is their biggest issue threatening their security and survival. Although they have water that is accessible, it is extremely contaminated therefore residents are forced to travel long distances to acquire suitable drinking water. There is a high cost that comes with attaining potable water which is why many in Central Valley have had no choice other than to relocate to different regions. Regarding the demographics of Central Valley, the population is a combination of migrants that settled in the 1900s. These migrants were Black, Latino, and Asian and the majority were low-income. This initial establishment set the tone for the current state of Central Valley today.[170] More than 50% of all drinking water violations that are reported in California come from the Central Valley.[171] In 2006, it was discovered that 20% of the public water in Central Valley violated the state's maximum contamination level. As a result, many are exposed to higher rates of nitrate which can damage one's reproductive ability or even increase the likelihood of birth defects and miscarriages. In addition, coliform bacteria and other bacterial contaminants can be found in the public water and are responsible for a number of waterborne diseases. This situation is a direct result of these communities being continuously disregarded when it comes to the implementation of policy. The combination of racial discrimination and poverty can be credited to the ongoing water crisis in Central Valley.[170]

In the American South, many experience issues with wastewater, specifically due to the development of contamination sites and hazardous waste sites.[172] Spanning all the way from Delaware to Texas, the southern "blackbelt" is  54% Black.[173] These residents lack proper plumbing systems and access to proper sanitation. These contamination sites are often placed around communities that are composed of minorities, specifically Black and Hispanic individuals. In 1983 the Government Accountability Office (formerly the General Accounting Office) published a study examining the relationship between the location of hazardous waste sites and the racial and economic status of the surrounding residents. It was discovered that race was the greatest predictor of the location of a contamination site. Close proximity to toxic landfills and long term exposure to wastewater results in a myriad of health implications.[174] A 2017 study performed by Baylor College of Medicine examined the development of parasites in Lowndes County, Alabama — a region where 73% of its residents are Black and 31.4% of residents live below the poverty line.[175][165] It was discovered that 34.5% of individuals living in this high-risk environment tested positive for hookworm, a parasite linked to wastewater. In addition, 73% of the participants in the study reported exposure to raw sewage inside of their homes. The establishment of contamination sites is directly linked to the location of certain racial groups, emphasizing how the accessibility of clean water disproportionately affects people of color.[175]

In Flint, Michigan, residents consumed drinking water that contained high amounts of lead which resulted in a variety of negative health effects.[176] The city of Flint is composed of low income African Americans who experienced a shift in their water supply due to financial limitations.[177] Their water supply was switched from the City of Detroit's Huron Water Supply to water from the Flint River in 2014. A group of researchers at Virginia Tech discovered that the water in the Flint River was highly corrosive due to the inflated levels of lead found in the water. The decision to switch water sources was made on the state level and completely disregarded the potential impacts it could have had on the communities living in Flint, Michigan.[176] Residents argue that this issue was a result of Michigan's economic history and political decision-making that often left Flint in the dark. The water crisis in Flint highlights the racial disparities that are evident when examining certain regions' access to clean water.[178]

Since the late twentieth century, there has been a shift in money and resources allocated towards funding new water infrastructure. As of 2000, federal funding for water accessibility had dropped nearly 70% since the 1980s.[179] In addition, federal spending declined from $76 per person in 1977 to just $11 per person in 2014. The responsibility of funding water and sanitation projects has moved from the federal government to local governments. While this method may be sustainable for a large majority of communities, groups that are located in impoverished areas do not have the financial resources to embark on large water projects. In the last several years, local and state governments have been forced to increase their spending to compensate for the withdrawal of federal funds.[180]

The main reason for this redistribution of money is due to the federal government's shift in focus onto other projects such as transportation, research, and education.[179] In 2008 the New York State Department of Environmental Conservation performed a study and determined that in order for New York to maintain and update current water infrastructure, it would require $36.2 billion, a number much greater than the EPA's estimate of $21.8 billion.[181] By reallocating funds to meet water infrastructure needs, the estimated gain in annual economic activity would be above $220 billion and improve the accessibility of water for many.[180]

With low income minority communities already being disproportionately affected by water access issues, the cut in federal spending only furthers the disparities that these groups face. These communities lack the resources to solve this issue on their own and look towards the federal government for assistance. However, their lack of involvement and general disregard for these communities leads to the continuation of their troubles.[180]

The median household in the U.S. spent about 1.1% of its income on water and sewerage in 2002. However, poor households face a different situation: In 1997 18% of U.S. households, many of them poor, paid more than 4% of their income on their water and sewer bill.[182]

The mean U.S. water tariff – excluding sewer tariffs – was $2.72 per 1,000 gallons ($0.72 per cubic meter) in 2000,[183]: 29  with significant variations between localities. Average residential water tariffs for a monthly consumption of 15 cubic meters varied between $0.35 per cubic meter in Chicago and $3.01 in Atlanta in 2007. The combined water and sewer tariff was $0.64 in Chicago and $3.01 in Atlanta, with Atlanta not charging separately for sewer services.[184] Annual combined water and sewer bills vary between $228 in Chicago and $1,476 in Atlanta in 2008.[185] For purposes of comparison, the average water and sewer bill in England and Wales in 2008 was equivalent to $466.[186]

The average annual increase in typical residential water bills was approximately 5.3 percent from 2001 through 2009, while the increase in typical residential sewer bills was approximately 5.5 percent according to data from the 50 Largest Cities Water and Wastewater Rate Survey by Black & Veatch.[187]

The water community in the US is faced with a swiftly retiring workforce and a tightening market place for new workers. In 2008, approximately one third of executives and managers were expected to retire in the following five years.[188] Water and sanitation utilities in the United States had 41,922 employees in 2002.[189]

Water fluoridation, the controlled addition of moderate concentrations of fluoride to a public water supply to reduce tooth decay, is used for about two-thirds of the U.S. population on public water systems.[190] Almost all major public health and dental organizations support water fluoridation, or consider it safe.[191] Nevertheless, it is contentious for ethical, safety, and efficacy reasons.[192]

Historically the predominant response to increasing water demand in the U.S. has been to tap into ever more distant sources of conventional water supply, in particular rivers. Because of environmental concerns and limitations in the availability of water resources, including droughts that may be due to climate change, this approach now is in many cases not feasible any more. Still, supply-side management is often being pursued tapping into non-conventional water resources, in particular seawater desalination in coastal areas with high population growth. California alone had plans to build 21 desalination plants in 2006 with a total capacity of 450 million US gallons (1,700,000 m3) per day, which would represent a massive 70-fold increase over current seawater desalination capacity in the state.[193] In 2007 the largest desalination plant in the United States is the one at Tampa Bay, Florida, which began desalinating 25 million US gallons (95,000 m3) of water per day in December 2007.[194]

In 2005 over 2,000 desalination plants with a capacity of more than 100m3/day had been installed or contracted in every state in the U.S. with a total capacity of more than 6 million m3/day. Only 7% of that capacity was for seawater desalination, while 51% used brackish water and 26%  used river water as water source.[195] The contracted capacity corresponds to 2.4% of total municipal and industrial water use in the country in 2000.[196] The actual share of desalinated water is lower, because some of the contracted capacity was never built or never operated, was closed down or is not operated at full capacity.[195]

In 2017, the U.S. Global Water Strategy was passed, where the U.S. Government will work with countries in order to achieve four objectives: increase access to safe drinking water, while promoting hygienes and sanitation services, protect freshwater resources, promote cooperation on shared waters, and strengthen water financing.[197]

Demand-side management, including the reduction of leakage in the distribution network and water conservation, are other options that are being considered and, in some cases, also applied to address water scarcity. For example, Seattle has reduced per capita water use from 152 US gallons (580 L) per day in 1990 to 97 US gallons (370 L) per day in 2007 through a comprehensive water conservation program including pricing policies, education, regulations and rebates for water-saving appliances. Other cities such as Atlanta and Las Vegas have also launched water conservation programs that are somewhat less comprehensive than the one in Seattle concerning indoor water use. However, Las Vegas has intentionally focused on curbing outdoor water demand, which accounts for 70% of residential water use in the city, through reductions in turf area and incentives for the use of rains sensors, irrigation controllers and pool covers.[198] At the federal level, the Energy Policy Act of 1992 set standards for water-efficient appliances, replacing the 3.5 US gallons (13 L) per flush (gpf) toilet with a new 1.6 gpf/6 litres per flush maximum standard for all new toilets.
By 1994, federal law mandated that showerheads and faucets sold in the U.S. release no more than 2.5 and 2.2 US gallons (8.3 L) of water per minute respectively.[199] Also in 1994 the AWWA established a clearinghouse for water conservation, efficiency, and demand management, called WaterWiser, to assist water conservation professionals and the general public in using water more efficiently.[200] In 2006 the EPA launched its WaterSense program to encourage water efficiency beyond the standards set by the Energy Policy Act through the use of a special label on consumer products.[201][202] The Obama administration further tightened the 2.5-gallons-per-minute rule so that fixtures with multiple showerheads could only use that amount collectively. The Trump administration loosened that part of the rule to consider each showerhead individually. In 2021, the Biden administration proposed restoring the Obama version.[203]

Distributional losses in the U.S. are typically 10–15% of total withdrawals, although they can exceed 25% of total water use in older systems.[198]
According to another source unaccounted-for water (UFW) – which includes system losses, water used for firefighting and water used in the treatment process – was estimated to be only 8% in systems with more than 500,000 connections in 2000.[204]: 17  In comparison, the level of water losses is 7% in Germany, 19% in England and Wales, and 26% in France. Together with Germany water losses in the U.S. are thus among the lowest in 16 industrial countries.[205]

Low water tariffs and inappropriate tariff structures do not encourage water conservation. For example, decreasing-block rates, under which the unit rate decreases with consumption, offer hardly any incentive for water conservation. In 2000 about 51% of water tariffs in the U.S.were uniform (i.e. the unit tariff is independent of the level of consumption), 12% were increasing-block tariffs (the unit rate increases with consumption) and 19% were decreasing-block tariffs.[183] The use of decreasing-block tariffs declined sharply from 45% of all tariff structures in 1992.[94] Sewer rates are often flat rates that are not linked to consumption, thus offering no incentive to conserve water.[198]

Reuse of reclaimed water is an increasingly common response to water scarcity in many parts of the United States. Reclaimed water is being reused directly for various non-potable uses in the United States, including urban landscape irrigation of parks, school yards, highway medians and golf courses; fire protection; commercial uses such as vehicle washing; industrial reuse such as cooling water, boiler water and process water; environmental and recreational uses such as the creation or restoration of wetlands; as well as agricultural irrigation.[206] In some cases, such as in Irvine Ranch Water District in Orange County, it is also used for flushing toilets.[207]

It was estimated that in 2002 a total of 1.7 billion US gallons (6,400,000 m3) per day, or almost 3% of public water supply, were being directly reused. California reused 0.6 and Florida 0.5 billion US gallons (1,900,000 m3) per day respectively. Twenty-five states had regulations regarding the use of reclaimed water in 2002.[206] Planned direct reuse of reclaimed water was initiated in 1932 with the construction of a reclaimed water facility at San Francisco's Golden Gate Park. Reclaimed water is typically distributed with a color-coded dual piping network that keeps reclaimed water pipes completely separate from potable water pipes.[208]

The leaders in use of reclaimed water in the U.S. are Florida and California,[209] with Irvine Ranch Water District as one of the leading developers. They were the first district to approve the use of reclaimed water for in-building piping and use in flushing toilets. In places like Florida, where it is necessary to avoid nutrient overload of sensitive receiving water, reuse of treated or reclaimed water can be more economically feasible than meeting the higher standards for surface water disposal mandated by the Clean Water Act.[210]

In a January 2012 U.S. National Research Council report,[211] a committee of independent experts found that expanding the reuse of municipal wastewater for irrigation, industrial uses, and drinking water augmentation could significantly increase the United States' total available water resources.[212] The committee noted that a portfolio of treatment options is available to mitigate water quality issues in reclaimed water. The report also includes a risk analysis that suggests the risk of exposure to certain microbial and chemical contaminants from drinking reclaimed water is not any higher than the risk from drinking water from current water treatment systems—and in some cases, may be orders of magnitude lower. The report concludes that adjustments to the federal regulatory framework could enhance public health protection and increase public confidence in water reuse.

There are examples of communities that have safely used recycled water for many years. Los Angeles County's sanitation districts have provided treated wastewater for landscape irrigation in parks and golf courses since 1929.  The first reclaimed water facility in California was built at San Francisco's Golden Gate Park in 1932.  The Water Replenishment District of Southern California was the first groundwater agency to obtain permitted use of recycled water for groundwater recharge in 1962.

Orange County is located in Southern California, USA, and houses a classic example in indirect potable reuse.[213] A large-scale artificial groundwater recharge scheme exists in the area, providing a much-needed freshwater barrier to intruding seawater.[214] Part of the injected water consists of recycled water, which started in 1976 with Water Factory 21, which used RO and high lime to clean the water (production capacity of 19,000 m3 per day).[215] This plant was decommissioned in 2004 and has since made place for a new project with a higher capacity (265,000 m3 per day with an ultimate capacity of 492,000 m3 per day), under the name of Groundwater Replenishment System.[213]

The Irvine Ranch Water District (IRWD) was the first water district in California to receive an unrestricted use permit from the state for its recycled water; such a permit means that water can be used for any purpose except drinking. IRWD maintains one of the largest recycled water systems in the nation with more than 400 miles serving more than 4,500 metered connections. The Irvine Ranch Water District and Orange County Water District in Southern California are established leaders in recycled water. Further, the Orange County Water District, located in Orange County, water is given more advanced treatments and is used indirectly for drinking.[216]

The Trinity River in Texas is a representative example of an effluent-dominated surface water system where de facto potable water reuse occurs. The section of the river south of Dallas/Fort Worth consists almost entirely of wastewater effluent under base flow conditions. In response to concerns about nutrients, the wastewater treatment plants in Dallas/Fort Worth that collectively discharge about 2 million m3 per day of effluent employ nutrient removal processes. Little dilution of the effluent-dominated waters occurs as the water travels from Dallas/Fort Worth to Lake Livingston, which is one of the main drinking water reservoirs for Houston. Once the water reaches Lake Livingston, it is subjected to conventional drinking water treatment prior to delivery to consumers in Houston.[217]

Orange County is located in Southern California, USA, and houses a classic example in IPR.[226] A large-scale artificial groundwater recharge scheme exists in the area, providing a much-needed freshwater barrier to intruding seawater.[227] Part of the injected water consists of recycled water, starting as of 1976 with Water Factory 21, which used RO and high lime to clean the water (production capacity of 19,000 m3 per day).[228][229] This plant was de-commissioned in 2004 and has since made place for a new project with a higher capacity (265,000 m3 per day with an ultimate capacity of 492,000 m3 per day), under the name of Groundwater Replenishment System.[229][226] This newer scheme uses the newer technological combination of RO, MF, and ultraviolet light with hydrogen peroxide.[229][228] Plans are also underway to further increase the capacity of the system,[226] which already provides up to 20% of the water used by the country.[230]

In the US, San Diego, California is the leading state implementing IPR. MF, RO and UV/H2O2 are employed prior to groundwater replenishment with the treated effluents (CDPH, 2013). In San Diego, the effort to increase the share of recycled water was rekindled with an extensive study in 2006.[229] MF provides substantial removal of the dissolved effluent organic matter (dEfOM), while dEfOM reduction down to 0.5 mg/L (in terms of TOC) is achieved through RO application. The chemical oxidation treatment (UV/H2O2) following the membrane steps, results in the mitigation of N-nitrosodimethylamine (NDMA), as well as in the improvement of the effluent quality with respect to its organic content.[231]

The City of El Paso's (Texas, USA) water sources include groundwater aquifers and surface water from the Rio Grande. In order to increase groundwater levels, the El Paso Water Utilities injects advanced treated reclaimed water into the aquifer. The advanced treatment facilities use two-stage powdered activated carbon (PAC), addition of lime, two-stage recarbonation, sand filtration, ozonation, granular activated carbon (GAC), and chlorination for purifying the water.[232] The Hueco Bolson Recharge Project, which initially began in 1985, currently recharges 1,700 acre-feet per year of reclaimed water at 10 injection wells and 800 acre-feet per year at an infiltration basin for groundwater recharge.[233]

In Colorado, USA, the Colorado River Municipal Water District implemented a project to capture treated municipal effluent from the City of Big Spring, and provide additional advanced treatment prior to blending into their raw surface water delivery system (2012). Advanced treatment of the municipal effluent consisted of MF, RO, and ultraviolet oxidation, producing very high quality water, which is blended with surface water from Lake E.V. Spence for distribution to their member and customer cities (production of 6,700 m3 per day).[233]

Further examples:

In July 2014, the city of Wichita Falls, Texas (USA), became one of the first in the United States to use treated wastewater directly in its drinking water supply (production of 45,000–60,000 m3 per day). Treated wastewater is disinfected and pumped to the Cypress Water Treatment Plant where it goes through clarification, microfiltration (MF), reverse osmosis (RO), and ultraviolet light disinfection before being released into a holding lagoon where it is blended with lake water (50:50). The blended water goes through a seven-step conventional surface water treatment.[243]

Proposed projects:

Numerous efforts have been undertaken in the United States to control the pollution of water resources and to make drinking water safe. The most comprehensive federal regulations and standards for the water treatment industry were implemented in the 1970s, in reaction to a huge increase in environmental concerns in the country. In 1972, Congress passed the Clean Water Act (CWA), with the unprecedented goal of eliminating all water pollution by 1985 and authorized expenditures of $24.6 billion in research and construction grants.[19] In 1974, Congress passed the Safe Drinking Water Act, specifying a number of contaminants that had to be closely monitored and reported to residents should they exceed the maximum contaminant levels.[246] The CWA included substantial federal grant funding to improve sewage treatment infrastructure in the form of construction grants to local governments.

The 1987 Water Quality Act amended the CWA, replacing the sewage treatment construction grant program with a system of subsidized loans, using the Clean Water State Revolving Fund (CWSRF). The loans use a combination of 80% federal funds and 20% matching funds from states.[15] New challenges arose, such as the need to address combined sewer overflows for which EPA issued a policy in 1994, and which was codified into law by Congress in 2000.[247] In 1996 Congress established the Drinking Water State Revolving Fund, in order to finance investments to improve compliance with more stringent drinking water quality standards.[38]

Today cities make significant investments in the control of combined sewer overflows, including through the construction of storage facilities in the sewerage system in order to allow for the subsequent controlled release of sewage into treatment plants.

One way to address the funding needs of utilities to respond to the various challenges they face without increasing the burden of water bills on users is federal financial assistance.

Centralized water and sanitation infrastructure is typically financed through utilities' own revenue or debt. Debt can be in the form of soft loans from state revolving funds (SRF), credits from commercial Banks or – in the case of large utilities – from bonds issued directly in the capital market. In the case of water supply (i.e. excluding sanitation), 42% of investments were financed by private sector borrowing, 39% by current revenues, 13% by government loans including the Drinking Water SRF, 5% by government grants and 1% from other sources.[5] There are two SRFs: The larger Clean Water State Revolving Fund, created in 1987,[248][249][250] and the smaller Drinking Water State Revolving Fund, created in 1997.[251] They receive federal and state contributions and issue bonds. In turn, they provide soft loans to utilities in their respective states, with average interest rates at 2% for up to 20 years in the case of the Clean Water State Revolving Fund. In addition to the SRFs, the United States Department of Agriculture provides grants, loans and loan guarantees for water supply and sanitation in small communities (those with less than 10,000 inhabitants), together with technical assistance and training.[252]

The American Recovery and Reinvestment Act of February 17, 2009, provided $4 billion for the Clean Water SRF, $2 billion for the Drinking Water SRF and, among others, $126 million for water recycling projects through the United States Bureau of Reclamation.[253] This program exceeded previous levels of financing, since Congress approved only US$1.5 billion of federal funding for State Revolving Funds in 2008. This was much below the historical average of US$3 billion/year for the Clean Water State Revolving Fund (1987–2006)[249][254] and US$1.2 billion/year for the Drinking Water State Revolving Fund (1997–2005).[251] The share of federal funding for sanitation has declined from almost 50% in the early 1980s to about 20% in the early 1990s.[255] A May 2016 article asserted that there has been "a huge federal retreat from helping cities fund water projects," stating that overall federal spending on water utilities "has dropped 75 percent since 1977," and that experts expect more situations like the Flint water crisis to emerge.[256]

Congress passed the Water Infrastructure Finance and Innovation Act of 2014 (WIFIA) to provide an expanded credit program for water and wastewater infrastructure projects, with broader eligibility criteria than the previously authorized revolving funds.[257] Pursuant to the act, EPA established its Water Infrastructure and Resiliency Finance Center in 2015 to help local governments and municipal utilities design innovative financing mechanisms, including public-private partnerships. It is part of the federal government's Build American Investment Initiative.[258] Congress amended the WIFIA program in 2015 and 2016.[259] One of the envisaged instruments to boost financing in water infrastructure are Qualified Public Infrastructure Bonds (QPIBs), tax-exempt municipal bonds that can be used by private companies.[260]

In 2007 EPA stated, "Improving the security of our nation's drinking water and wastewater infrastructures has become a top priority since the events of 9/11. Significant actions are underway to assess and reduce vulnerabilities to potential terrorist attacks; to plan for and practice response to emergencies and incidents; and to develop new security technologies to detect and monitor contaminants and prevent security breaches."[261][262]

One of the most important elements of water security is early and accurate contamination detection. The EPA has issued advisory material and guidelines for contamination warning systems to be implemented in water utilities and supplies. The security challenges that utilities frequently revolve around fast detection, accuracy, and the ability to take fast action when there is a water problem. If contamination is detected early enough, it can be prevented from reaching consumers, and emergency water supplies can be put into effect.[263]

In cases where contamination might still reach consumers, fast and efficient communication systems are necessary. All these factors also point to the need for organized and practiced emergency procedures and preparedness.

Since 2002, under the Bioterrorism Act, a water utility supplying more than 3,300 people must take at least the following measures to ensure security of the water supply:[264][265][266]

More recently, under the Drinking Water Security Act of 2009, the EPA is now required to establish risk-based performance standards for community water systems serving more than 3,300 people.[265]The West Virginia Mountaineers football team represents West Virginia University (WVU) in the NCAA Division I Football Bowl Subdivision (FBS) of college football. West Virginia plays its home games at Milan Puskar Stadium on the campus of West Virginia University in Morgantown, West Virginia. The Mountaineers have won or shared a total of 15 conference championships, including eight Southern Conference titles and seven Big East Conference titles. The Mountaineers compete in the Big 12 Conference.

The West Virginia University football program traces its origin back to November 28, 1891, when its first team fell to Washington & Jefferson 72–0 on a converted cow pasture.[3]  Despite its humble beginning, West Virginia enjoyed a 25–23–3 overall record prior to 1900, which proved to be a fruitful century of Mountaineer football.  The early 1900s brought about early successes for the program, namely during the 1903 and 1905 seasons when WVU posted records of 7–1 and 8–1 respectively.[4]  WVU produced a 6–3 record in the 1904 season, despite losing to Penn State, Pitt, and Michigan by a combined score of 217–0.[5]  The 1908–20 period produced the four-year head coaching tenures of C.A. Lueder (1908–11) and Mont McIntire (1916–17, 1919–20), representing the longest coaching tenures during this early period of Mountaineer football.  Lueder's Mountaineers produced a 17–13–3 record, while McIntire's teams produced the most success of any Mountaineer team prior to 1921, compiling a 24–11–4 record including an 8–2 finish in 1919.[4]  That same Mountaineer team also produced West Virginia's first ever Consensus All-American and potential College Football Hall of Fame inductee, Ira Errett Rodgers.[3]  Rodgers scored 19 touchdowns and kicked 33 extra points for WVU in 1919 season, leading the nation with 147 total points.  Rodgers also threw 11 touchdown passes that season, an unheard of feat at the time and a Mountaineer record until 1949.[6] The Mountaineers enjoyed their first period of success during the 1920s, coinciding with the successful coaching tenures of Clarence Spears (1921–24) and Ira Errett Rodgers (1925–30, 1943–45).  Under the tutelage of Spears, West Virginia compiled a 30–6–3 record with its best performance coming in the 1922 season.  The 1922 edition of the Mountaineers remains the only team in West Virginia history to produce an unbeaten season, finishing with a 10–0–1 mark.[4]  Spears's Mountaineers surrendered only 34 total points in 1922, posting six consecutive shutouts to finish the regular season.[7] The 1922 season also produced notable victories against rival Pitt and against Gonzaga in the East-West Bowl, the program's first bowl game appearance. Offensive tackle Russell Meredith garnered First-Team All-American honors.  In homage to the successes of the 1922 season, West Virginia University undertook construction of what became the first incarnation of Mountaineer Field.[3]

The Mountaineers continued their success under Spears in posting subsequent one-loss seasons in 1923 (7–1–1) and in 1924 (8–1), with Spears departing the program for Minnesota thereafter.  Ira Errett Rodgers replaced Spears and the Mountaineers posted an 8–1 record in 1925.  After a 6–4 finish in 1926 and a 2–4–3 record in 1927, the program produced an 8–2 finish in 1928.[4] Rodgers's first tenure as West Virginia coach ended with records of 4–3–3 in 1929 and 5–5 in 1930.[4] Taking over for Rodgers in 1931 was Earle "Greasy" Neale, but his tenure was short-lived as the Mountaineers failed to produce a single winning season under his guise, going a combined 12–16–3 over Neale's three years as coach.[4] Charles Tallman, an End who achieved All-American status with the Mountaineers in 1923 with the Mountaineers, replaced Neale in 1934 and produced immediate results as the program posted 6–4 records in 1934 and 1936.[4] Although West Virginia posted a 3–4–2 record in 1935, the program produced an All-American in Joe Stydahar, an offensive tackle. "Jumbo Joe" later became both a College Football Hall of Fame and Pro Football Hall of Fame inductee. Despite his winning record, Tallman resigned after the 1936 season to pursue his career in law enforcement as Superintendent of the West Virginia State Police.[8] Marshall Glenn picked up right where Tallman left off, leading West Virginia to an 8–1–1 record in 1937.  The season concluded with an upset of Texas Tech in the 1938 Sun Bowl.[9]  Running back Harry Clarke led the way for the Mountaineers that season, rushing for a then school record 921 yards and 10 touchdowns.[10] Glenn's success was short-lived, however, as subsequent WVU teams posted losing records of 4–5–1 in 1938 and 2–6–1 in 1939, leading to his ouster.[4] West Virginia experienced a lag in success during much of the 1940s, producing only three winning seasons while witnessing the split coaching tenures of Bill Kern (1940–42, 1946–47) and the second appearance of Ira Errett Rodgers (1943–45).[4] Under the direction of head coach Dudley DeGroot in the 1948 season, the Mountaineers returned to prominence with a 9–3 finish, adding another Sun Bowl victory to its resume with a 21–12 defeat of Texas Western (now known as UTEP).[11]  Despite that successful first season at the program's helm, DeGroot resigned after a disappointing 4–6–1 finish in 1949.[12]

When Art "Pappy" Lewis became West Virginia's head coach in 1950, he remarked that it was the job that he had always wanted.[12]  Known by his peers as an exceptional recruiter and by his players as a father figure, Lewis established a family-like atmosphere within the Mountaineer football program.[12]  Lewis's Mountaineer teams held true to form, experiencing their most consistent success during the 1950s as it ever had previously.  After forgettable campaigns in 1950 and '1951, the 1952 season brought WVU its first winning season since 1948.  The Mountaineers finished with a 7–2 record, highlighted by a 16–0 upset victory of No. 18 Pitt in Pittsburgh.[13]

Beginning with the 1953 season, the Mountaineers would reel off three consecutive eight-win seasons and five Southern Conference (SoCon) championships in six seasons.[4] In 1953, the Mountaineers finished with an 8–2 record, their first SoCon championship, a No. 10 ranking in the Associated Press (AP) Poll, and a berth in the Sugar Bowl with Georgia Tech.[14]  The 1954 edition of the Mountaineers also finished the regular season with an 8–1 mark, losing their only game to Pitt by a 13–10 score.  The Mountaineers did not earn a bowl bid, however, despite winning their second consecutive SoCon title and earning a No. 12 ranking in the AP Poll.[15]  In 1955, the Mountaineers engineered yet another eight-win season and SoCon championship, but upset losses to Pitt and Syracuse doomed West Virginia's shot at a bowl bid.[16]  Despite its disappointing finish, WVU produced two All-American offensive linemen and future College Football Hall of Fame inductees in Bruce Bosley and Sam Huff. Bosley earned Consensus All-American status that season, becoming the first Mountaineer to do so since Ira Errett Rodgers in 1919.

Despite finishing with a modest 6–4 record in 1956, West Virginia won its fourth consecutive SoCon title with a 5–0 record in conference play.[17]  The 1957 season resulted in a 7–2–1 record and a 3–0 mark in Southern Conference play. Although the Mountaineers once again compiled an undefeated SoCon record, they were not awarded the conference championship, as VMI earned the title with a 9–0–1 overall record and 6–0 record in SoCon play.[18] In 1958, the Mountaineers had their first losing season in eight years, but their 4–0 record in SoCon play earned them a 5th conference title in six seasons.[19] The 1958 season was the final season that West Virginia would win a conference championship under Lewis's tenure. The Mountaineers finished 3–7 in 1959, losing the final five games of the season by a combined score of 24–140.[20]  Lewis resigned as head coach afterward. Despite the program's drop off in success in his final two seasons as coach, Lewis produced 58 victories overall during his tenure at West Virginia, placing him fourth overall in the program's history.[21]

After Lewis's departure, the program hit an all-time low in 1960 under first-year head coach Gene Corum, posting its worst season to date: 0–8–2. The Mountaineers were simply outclassed by their opponents, being outscored 40–259 on the season.[22]  The Mountaineers rebounded, however, and by 1962 were back to their winning ways, posting an 8–2 record and 4–0 conference record. Despite their undefeated conference record, once again the SoCon crown eluded the Mountaineers in favor of the VMI Keydets and their 6–0 record in conference play.[23]  West Virginia did not have to wait long for its next SoCon title, however, as the program won the title in the 1964 and 1965 seasons consecutively.  The Mountaineers finished 7–4 in 1964 and participated in the Liberty Bowl against Utah, West Virginia's first bowl game in 11 years as well as the first major college football bowl game ever played indoors and to be broadcast nationwide in the United States.[24] Corum's tenure ended thereafter, posting a 29–30–2 record over his six seasons as head coach. Corum's legacy went well beyond wins and losses, however, as he integrated WVU football in 1963 with the program's first African-American recruits in Roger Alford and Dick Leftridge.[25]

Following the 1965 season, Jim Carlen took over for Corum as head coach.  After a 3–5–2 finish in 1966, Carlen guided the Mountaineers to their 8th and final SoCon championship in 1967.[26] West Virginia left the Southern Conference thereafter, participating as an independent until 1991. Carlen's Mountaineers would produce subsequent winning seasons in 1968 and 1969, posting records of 7–3 and 10–1, respectively.  The 1969 edition of the Mountaineers was the most successful West Virginia team since the 1922 season.[4]  Not only did the Mountaineers win 10 games, but they earned their first bowl game victory since 1948 with a Peach Bowl victory over No. 19 South Carolina, as well as a No. 18 final ranking in the Coaches Poll. The dynamic rushing tandem of running back Bob Gresham (1,155 yards and 9 touchdowns) and fullback Jim Braxton (843 yards, 12 touchdowns) led the Mountaineers.[27] Gresham became the second Mountaineer to ever rush for more than 1,000 yards (Garrett Ford, Sr. was the first with 1,068 yards in 1966).[28] At the conclusion of the 1969 season, Carlen departed West Virginia for Texas Tech. The Mountaineers responded by hiring Bobby Bowden.

It appeared that the Bobby Bowden era of Mountaineer football could not have begun more smoothly early in the 1970 season, or so it seemed. The Mountaineers were 4–1 to start the season and led arch rival Pitt 35–8 at halftime in week six.[29] What transpired was one of the most infamous collapses in Backyard Brawl and West Virginia football history.  The Mountaineers surrendered 28 unanswered points, losing to the Panthers 36–35 and leading Bowden to remark that he had "embarrassed the whole state of West Virginia" in the process.[30]  Despite the disappointment of the Pitt defeat, West Virginia went on to finish the 1970 season with an 8–3 record. Fullback Jim Braxton and linebacker Dale Farley earned All-American honors.

The Mountaineers continued their winning ways under Bowden in 1971 and 1972, posting records of 7–4 and 8–4 respectively.  The 1972 West Virginia team earned the program's first trip back to a bowl game in three years, participating once again in the Peach Bowl against North Carolina State. The season also witnessed the offensive prowess of running back Kerry Marbury and wide receiver Danny Buggs. Marbury ran for 16 touchdowns in 1972, a record that remained unbroken until 2002.[31] Buggs recorded 35 receptions for 791 yards and eight touchdowns, ran for four touchdowns, and returned two punts for touchdowns to amass 14 total touchdowns.[32]

The 1973 and 1974 seasons, however, were not successful campaigns for the Mountaineers, as they finished with records of 6–5 and 4–7.  Despite the disappointment of those seasons, Danny Buggs earned All-American status for his contributions in both campaigns.  The 1975 season was successful as the Mountaineers compiled a 9–3 record, a 13–10 Peach Bowl victory over North Carolina State, and a final ranking of No. 17 in the Coaches Poll and No. 20 in the AP Poll. Additionally, the Mountaineers upset the No. 20 Pitt Panthers 17–14 on a game-winning field goal in the closing seconds in yet another memorable chapter of the Backyard Brawl. Bowden later described the victory as one of the most exciting ones of his coaching career.[30]  Following the 1975 season, Bowden left WVU to become the head coach at Florida State, where he would become the second winningest coach in NCAA Division I-A/FBS history.  In just six seasons with the Mountaineers, Bowden produced a 42–26 record, good for fifth all-time in the program.[21]  Bowden's departure not only signaled the end of his tenure at West Virginia, but to the end of WVU's winning ways in the 1970s.

Under the direction of Frank Cignetti, the Mountaineers endured four consecutive losing seasons. West Virginia completed the 1976 season with a 5–6 record, losing four of its final six games. The disappointment of 1976 was realized again the following season, as the Mountaineers posted another 5–6 finish in 1977.  After a promising 4–1 start to the season, including an upset road victory over No. 11 Maryland, WVU lost five of its final six games. The Mountaineers finished 2–9 in 1978, being outscored 364–167. It was later revealed that Cignetti had suffered from a rare form of cancer during the season, nearly losing his life on the operating table during a procedure to remove his spleen in the winter of 1978.[33] The 1979 season was Cignetti's final with the program. The Mountaineers produced another 5–6 finish, losing their first three games and later dropping three out of their final four games.

Despite the program's losing seasons during Cignetti's tenure, Cignetti managed to land prized recruit and future Consensus All-American linebacker Darryl Talley, as well as standout quarterback and future Athletic Director Oliver Luck and running back Robert Alexander.  Cignetti's coaching staffs also consisted of some of the best coaches in college football, including Nick Saban, Joe Pendry and Rick Trickett (who, along with Rich Rodriguez, was later credited as an innovator in utilizing the zone blocking scheme in conjunction with the run-based spread offense at WVU).[33]  However, with a 17–27 record during his four years with the program, and in having to follow in the footsteps of the great Bobby Bowden (who later became the second all-time leader in victories amongst NCAA FBS coaches), Cignetti's legacy is one of the most conflicted in the program's history.[34]

In the wake of Frank Cignetti's firing, the West Virginia Athletic Department determined that a full rebuild was in order.[35] On December 10, 1979, West Virginia introduced Don Nehlen as its new head coach, the 30th coach in the program's history. Coinciding with Nehlen's hire was the construction of the second incarnation of Mountaineer Field, the program's current home stadium.[35] Nehlen brought several changes to the Mountaineer football program, including a new logo and color scheme that remains in use to this day.[35] The result was consistency and success for the program during his two-decade tenure at West Virginia.  After a 6–6 campaign in 1980, Nehlen's 1981 Mountaineer team produced the first of 15 winning seasons under his direction.  It also marked the first of three consecutive nine-win seasons and four consecutive bowl game appearances for the Mountaineers.  Led by senior quarterback Oliver Luck's 2,448 yards passing and 16 touchdowns, the 1981 team posted a 9–3 record and earned a trip back to the Peach Bowl, where they defeated the Florida Gators 26–6. WVU also finished ranked in the polls for the first time since 1975, coming in at No. 17 and No. 18 in the AP and Coaches Polls, respectively.

The 1982 Mountaineers experienced similar success.  Sparked by their come-from-behind upset victory over No. 9 Oklahoma to open the season, the Mountaineers finished with a 9–2 record, remaining ranked in the AP poll throughout the season en route to a Gator Bowl berth with Bobby Bowden and Florida State.  Despite its Gator Bowl loss, West Virginia once again finished the season 9–3 and ranked 19th in both final polls. The team also produced the program's first Consensus All-American since 1955 in senior linebacker Darryl Talley.  The Mountaineers won their first six games at the outset of the 1983 season, attaining a No. 4 ranking in the AP Poll. With a 41–23 upset loss to 1983 in week 8, however, West Virginia's hopes of an undefeated season collapsed. WVU lost three of its final five regular season games before defeating Kentucky in the Hall of Fame Classic to finish the season at 9–3. It was the third consecutive season for WVU to finish ranked, coming in at No. 16 in both the AP and Coaches Polls. Quarterback Jeff Hostetler led the offensive attack with 2,345 yards passing and 16 touchdowns, while Kicker Paul Woodside received All-American honors in converting 21 of 25 field goal attempts and all 37 of his extra point attempts en route to a team-leading 100 points.[36]

Although the 1984 season had all off the makings of a memorable one for West Virginia, the Mountaineers experienced another letdown.  WVU started the season with a 7–1 record, posting an upset victory over No. 4 Boston College and its first victory over Penn State in 25 meetings along the way.  The Mountaineers were upset in each of their final three regular season games, however, losing to Virginia, Rutgers and Temple. WVU rebounded to defeat Texas Christian in the Bluebonnet Bowl, finishing the season at 8–4 with a No. 21 ranking in the final Coaches poll.  The Mountaineers also produced three All-Americans in return specialist Willie Drewrey, kicker Paul Woodside, and tight end Rob Bennett.[37] After four consecutive seasons of bowl berths and finishes in at least one of the polls, West Virginia went on a two-year drought in 1985 and 1986, finishing those seasons with records of 7–3–1 and 4–7 (Nehlen's first of only four losing seasons) respectively.  The shortcomings of those seasons came to a head in 1987, where the Mountaineers endured a season of growing pains and near-misses.  Despite a 1–3 start, West Virginia rallied to finish the regular season at 6–5 with four of its five losses coming by deficits of 5 points or less. Freshman quarterback Major Harris led the way for the Mountaineers, compiling 16 total touchdowns and providing glimpses of what was to come in his illustrious collegiate career.[38]  The season culminated in a 35–33 loss to No. 11 Oklahoma State in the Sun Bowl, a game in which the Mountaineers led 24–14 at halftime and lost on a failed two-point conversion attempt with 1:13 remaining.[39]

The 1988 edition of the Mountaineers achieved an undefeated regular season, compiling 11 wins for the first time in its history. The success of the regular season culminated in West Virginia's first and only trip to a National Championship Game in its history, where the No. 3 Mountaineers met No. 1 Notre Dame in the Fiesta Bowl. Notre Dame went on to win 34–21 and claimed the national championship.  West Virginia finished the 1988 campaign ranked No. 5 in both the AP and Coaches Polls.  Major Harris compiled 610 yards rushing, 1,915 yards passing, and 20 total touchdowns on the season.[40] Coming off of its first ever 11-win season and with junior Major Harris returning to lead a potent offense, West Virginia entered the 1989 season with high expectations and a No. 17 ranking in the AP Poll.  The Mountaineers raced to a 4–0 record and to No. 9 in the AP Poll. In Week 5 against No. 10 Pitt, however, West Virginia fell victim to another memorable collapse in the Backyard Brawl. Trailing 31–9 in the 4th quarter, Pitt scored 22 unanswered points and kicked a game-tying field goal as time expired to force a 31–31 tie. The Mountaineers suffered another defeat the following week with a 12–10 home loss to Virginia Tech, followed by a 19–9 loss to No. 16 Penn State in State College. Despite those defeats, WVU finished the regular season at 8–2–1, a No. 17 ranking in the AP Poll, and a trip to the Gator Bowl to face No. 14 Clemson.  The Mountaineers lost 27–7 and finished the season at 8–3–1 with a No. 21 ranking in the final AP Poll. The 1990 season, West Virginia's final as an NCAA Division I-A Independent, coincided with a 4–7 finish.

West Virginia entered the 1991 season as new members of the Big East in what became a 20-year affiliation with the conference. After finishes of 6–5 in 1991 and 5–4–2 in 1992, the Mountaineers returned to ranks of the college football elite in the 1993 season. For the second time in six seasons, West Virginia produced an undefeated, 11-win regular season in 1993.  The Mountaineers engineered several close victories, beginning with a 36–34 upset of No. 17 Louisville at home in Morgantown. In its final two regular season games, WVU twice erased 4th quarter deficits to defeat No. 4 Miami and No. 11 Boston College. Despite finishing the regular season undefeated and ranked No. 2 in the Coaches Poll and No. 3 in the AP Poll, West Virginia was not selected to play in the Orange Bowl for a possible national championship. The Bowl Coalition system, designed to place the top two ranked teams in a bowl to determine the National Champion, slotted the Mountaineers at No. 3 behind 11–1 Florida State. The Seminoles were selected to play No. 1 Nebraska in the Orange Bowl for the national championship, while West Virginia settled for a Sugar Bowl berth against SEC Champion Florida.  The Gators routed the Mountaineers 41–7, denying West Virginia its perfect season.  WVU finished the season at 11–1, ranked No. 6 in the Coaches Poll and No. 7 in the AP Poll.  Robert Walker amassed a then-school record 1,250 rushing yards, along with 11 touchdowns on the season.[41]

The 1993 season was WVU's final season with double-digit victories during Nehlen's tenure as subsequent Mountaineer teams failed to recapture that level of success.  After posting a 7–6 record in 1994 and a 5–6 mark in 1995, the 1996 Mountaineers showed promise of returning the program to national prominence.  West Virginia began the 1996 season with a 7–0 record, only to lose three of its final four regular season games en route to an 8–3 record and a 20–13 defeat in the Gator Bowl at the hands of No. 12 North Carolina to finish 8–4. The Mountaineers put together another strong start in 1997, taking a 7–2 record into the final two weeks of the season.  Once again, the Mountaineers faltered as they lost 21–14 at Notre Dame and 41–38 in triple overtime to a 5–5 Pitt team to finish the regular season at 7–4.  The late season collapse culminated in another bowl game defeat, this time to Georgia Tech in the Carquest Bowl. Despite the disappointing conclusion to the season, quarterback Marc Bulger emerged as a capable leader throwing for 2,465 yards and 14 touchdowns. Running back Amos Zereoué shattered Robert Walker's rushing record with 1,589 yards, and his 18 rushing touchdowns are the second most in a single season at WVU.[42]

The 1998 season brought high expectations for the Mountaineers, as WVU entered the season ranked No. 11 in the AP Poll. Despite dropping its opening game to No. 1 Ohio State,[43]  West Virginia rebounded to win its next four games and went on to finish the season with an 8–3 record and 5–2 mark in Big East conference play.  The Mountaineers failed to attain nine wins, however, as they lost their 8th consecutive bowl game in the Insight.com Bowl to Missouri. Bulger set two WVU records with 3,607 yards passing and 31 touchdown passes, while Zereoué amassed 1,462 yards rushing and 13 touchdowns in his final season as a Mountaineer. Receivers Shawn Foreman and David Saunders finished with eight touchdown receptions each.[44]  After a 4–7 finish in 1999, Don Nehlen's final season with the Mountaineers in 2000 culminated in a 7–5 record with a victory in the Music City Bowl over Ole Miss, ending West Virginia's streak of futility in bowl games. Overall, Nehlen posted a 149–93–4 record during his tenure at West Virginia, making him both the longest-serving and most successful head coach in Mountaineer history.[21]  While his coaching tenure contained numerous successes, Nehlen's time at WVU also included its share of shortcomings as his Mountaineer teams often struggled against ranked opponents and in bowl games.[Note 1]  However, as the man responsible for shaping the Mountaineer football program and bringing it to national relevancy in his 21 seasons in Morgantown, Nehlen was inducted into the College Football Hall of Fame in 2005. Most importantly, his tenure laid the foundation for the program's most successful and prominent era.[45]

After Nehlen's retirement, WVU named then-Clemson offensive coordinator and West Virginia native Rich Rodriguez as its first new head coach in 20 years.[46] Rodriguez's tenure began ignominiously, as the 2001 edition of the Mountaineers finished 3–8, its worst record since 1978. The failures of 2001, however, set the stage for the emergence of the most successful era in Mountaineer football history.[4] The 2002 season represented the biggest single-season turnaround in program history.  Despite a 5–3 record through the season's first eight games, the Mountaineers reeled off four consecutive victories, including upset road wins over then-ranked rivals Virginia Tech (No. 13) and Pitt (No. 17). West Virginia finished the regular season at 9–3 overall, with a 6–1 conference record for second place in the Big East, and a berth in the Continental Tire Bowl with Virginia.  Despite losing its bowl game, West Virginia finished with a 9–4 record and was ranked in both the final Coaches (No. 20) and AP (No. 25) polls for the first time since 1993.  The momentum generated from the 2002 campaign was short-lived as the Mountaineers stumbled to a 1–4 record early in the 2003 season. In similar fashion to the previous season, West Virginia rebounded and recorded seven wins in a row, including upsets of No. 3 Virginia Tech and No. 16 Pitt.  The Mountaineers ended the regular season at 8–4 with a 6–1 conference mark, earning them a share of their first Big East title since 1993.  West Virginia earned a trip to the Gator Bowl for a rematch with rival Maryland. The result for the Mountaineers was a near duplicate of their 34–7 defeat to the Terrapins earlier in the season, as they fell 41–7 and finished the season 8–5. In contrast to 2002 and 2003, the 2004 season may best be remembered for what the Mountaineers failed to accomplish.  West Virginia, ranked No. 10 in the AP Poll to begin the season, carried an 8–1 record through its first nine games. The Mountaineers collapsed in the final two games of the regular season, however, losing to No. 21 Boston College and to Pitt. West Virginia squandered its opportunity to win the Big East outright, leading to a four-way tie for first place and the BCS Fiesta Bowl nomination going to Pitt by tiebreaker.  The disappointing season drew to a close with 30–18 loss to Florida State in the Gator Bowl, giving WVU an 8–4 record.[47]

The 2005 season was a noteworthy one for the Mountaineers.  After a 5–1 (albeit offensively sluggish) start to the season, the Mountaineers came alive in Week 7 against No. 19 Louisville.[48] Quarterback Pat White and running back Steve Slaton helped to erase a 24–7 4th quarter deficit en route to a thrilling 46–44 triple overtime victory.[49]  From that point forward, the Mountaineers outscored their opponents 156–39 en route to a 10–1 finish and a 7–0 record in conference play for their second outright Big East championship. The Mountaineers also earned their first ever BCS bowl game berth, facing No. 8 Georgia in the Sugar Bowl.  West Virginia scored 21 points in the 1st quarter, holding on for a 38–35 upset victory.[50] The Mountaineers finished the 2005 season with their third 11-win season and achieved rankings of No. 5 and No. 6 in the AP and Coaches Polls, respectively. The Mountaineers once again posted 11 wins in the 2006 season, narrowly missing out on another Big East championship after losses to Louisville and South Florida.[51] West Virginia remained ranked in the top 15 in both polls throughout the season, earning another New Year's Day bowl game as they met Georgia Tech in the Gator Bowl.  The Mountaineers came away with another 38–35 victory, winning consecutive bowl games for the first time since the 1983 and 1984 seasons.  Additionally, center Dan Mozes and running back Steve Slaton earned Consensus All-American honors.  Slaton's 1,744 yards rushing set the WVU single-season rushing record.[52]

The 2007 season may well be regarded as the most infamous season in West Virginia football history.[53]  The Mountaineers attained a preseason ranking of No. 3 and had national championship aspirations.  WVU raced to a 10–1 record, including a 66–21 victory over UConn to secure its fifth Big East title and its second BCS bowl appearance. The Mountaineers rose to as high as No. 2 in the AP Poll and No. 1 in the Coaches Poll, needing only a victory at home over a 4–7, 28-point underdog Pitt team in the 100th installment of the Backyard Brawl to secure its second ever National Championship Game appearance.  That victory did not come, as the Mountaineers suffered a devastating 13–9 defeat.[54] The fallout of the Pitt defeat reached beyond national championship implications for the program, as it culminated in the departure of Rich Rodriguez to Michigan.[55] Rodriguez left prior to West Virginia's meeting with No. 3 Oklahoma in the Fiesta Bowl.[56] The Mountaineers rebounded, posting a 48–28 victory over the heavily favored Sooners.[57]

"Leave no doubt tonight. Leave no...doubt...tonight. No doubt they shouldn’t have played the 'Old Gold and Blue.' Not. This. Night."

Long-time assistant coach Bill Stewart, named as interim head coach for the game, received a five-year contract to become West Virginia's 32nd head coach.[59] The Mountaineers concluded the 2007 season with an 11–2 record and were ranked at No. 6 in both of the final AP and Coaches Polls. The Mountaineers transitioned into the Bill Stewart era in the 2008 season.  WVU amassed a 9–4 record and a second-place finish in the Big East, closing the season Meineke Car Care Bowl victory over North Carolina and a No. 23 ranking in the AP Poll.[60] The bowl victory was West Virginia's fourth in a row, giving Pat White a postseason record of 4–0 as a starting quarterback.[61] During Week 13 of the 2008 season, White set the NCAA rushing yardage record for quarterbacks with a 200-yard performance in a 35–21 win over Louisville.[62]

The 2009 season culminated in another nine-win campaign and second-place finish in the Big East for the Mountaineers.  WVU ended its two-year losing streak in the Backyard Brawl with an upset victory over No. 8 Pitt.  West Virginia's season concluded with a loss in the Gator Bowl to Florida State in Bobby Bowden's final game as a head coach.[63]  The 2010 season brought the program its third consecutive nine-win season. Nonetheless, the season was ultimately a disappointment for the Mountaineers.  Despite assembling arguably the strongest defense in program history (surrendering only 176 total points, an average of 13.5 per game) and having a talented offense, West Virginia struggled with consistency all season.[64] The Mountaineers lost to No. 15 LSU, Syracuse and UConn by a combined 14 points, while the Mountaineer defense did not surrender more than 23 points scored against in a single game throughout the season. WVU's loss to Connecticut in Week 9 came back to haunt the Mountaineers as they lost out on a BCS Bowl bid by virtue of a tiebreaker to the Huskies.

Prior to West Virginia's Champs Sports Bowl match up with North Carolina State, Dana Holgorsen was hired as the "coach-in-waiting," serving as offensive coordinator during the 2011 season and replacing Stewart as head coach in 2012.[65] Luck didn't believe Stewart was capable of leading the Mountaineers to a national championship,[66] and was also concerned about declining season ticket sales.[65] The relationship between Stewart and Holgorsen imploded just months later.[67] Colin Dunlap of KDKA-FM in Pittsburgh claimed that Stewart had asked him to dig up dirt on Holgorsen while Dunlap was a reporter at the Pittsburgh Post-Gazette. While athletic director Luck was unable to substantiate the claims, he decided that the coach-in-waiting arrangement was untenable, leading to Stewart's resignation and Holgorsen becoming head coach.[65][68]

The Mountaineers finished the 2011 regular season with a 9–3 record (5–2 in Big East play) and a share of its 7th Big East title.[69] The Mountaineers were the only Big East team ranked in the final BCS standings (No. 23), earning the BCS bid by tiebreaker and an Orange Bowl berth against ACC champion No. 14 Clemson.[70] In its first Orange Bowl appearance, the Mountaineers soundly defeated Clemson 70–33 and set an NCAA record for points scored in a bowl game.[71]  The Mountaineers finished the season at 10–3 and ranked No. 17 in the AP Poll and No. 18 in the Coaches Poll. From 2002 to 2011, the Mountaineer football program yielded its most prolific era to date, producing a 95–33 record.[4]  During that span WVU participated in ten bowl games, finished ranked in at least one of the AP or Coaches Polls on seven occasions, won six Big East Conference titles, and produced three BCS bowl game victories.

In the midst of continued college football conference realignment, WVU joined the Big 12 Conference as of July 1, 2012.[72][73] Despite starting the 2012 season at 5–0 and climbing into the top 5 in the AP and Coaches Polls, WVU lost six of its final eight games en route to a 7–6 finish. The season culminated in a loss to rival Syracuse in the Pinstripe Bowl.[74] The 2013 season brought WVU's first losing campaign since 2001.  After a 4–5 start and an opportunity to secure bowl eligibility with two victories, WVU faltered in its final three games to finish 4–8. The Mountaineers rebounded in the 2014 season, posting a 7–6 record against a schedule featuring five opponents ranked in the top 15. WVU raced to a 6–2 start, only to lose four of its final five games. The Mountaineers returned to a bowl game in 2014, losing in the Liberty Bowl. WVU returned to the eight-win plateau in 2015 season, posting a 7–5 record during the regular season and winning the Cactus Bowl. In the 2016 season, the Mountaineers finished in second place in the Big 12 and finished the season with a 10–3 record, the ninth season in the program's history with at least ten victories. The season culminated in a loss to Miami in the Russell Athletic Bowl. Prior to the conclusion of the regular season, Holgorsen and WVU agreed to a five-year contract extension.[75] In 2017, WVU finished 7–6. The season concluded with a loss in the Heart of Dallas Bowl. The Mountaineers started the season 7–3, only to lose their final three games after a season-ending hand injury to quarterback Will Grier. The 2018 season saw the Mountaineers start with an 8–1 record, earning top 10 rankings in the polls and in the College Football Playoff. WVU faltered in its final two regular season games, however, and failed to earn a berth in the Big 12 Championship Game. WVU finished 8–4 with a loss in the Camping World Bowl. Soon thereafter, Dana Holgorsen left the program to take the head coaching position at Houston.[76][77]

On January 5, 2019, Troy head coach Neal Brown was named the 34th head coach of the program.[78][79] Brown brought with him an exciting, up-tempo- pass-oriented offense known as the Air raid.[80] WVU finished the 2019 season with a 5–7 record, failing to qualify for a bowl game for the first time since 2013. The Mountaineers rebounded in 2020, posting 6–4 record culminating in a Liberty Bowl victory against Army in their first meeting since the 1961 season. 

In 2021, the Mountaineers were looking for improvement, but were met with more of the same, finishing with a 6–6 record through the regular season. They began the season with a rivalry loss to Maryland, and then defeated LIU and won back the Black Diamond Trophy from Virginia Tech. The team finished the season in the Guaranteed Rate Bowl at Chase Field in Phoenix, resulting in an 18–6 loss to the Minnesota Golden Gophers. The Mountaineer's 2022 season was looked at with much excitement after the team was able to successfully sign former five-star quarterback JT Daniels in the offseason, and the anticipation of the renewal of the Backyard Brawl for the first time since 2011.[81] The season kicked off with the Mountaineers traveling to Pittsburgh to play the Panthers at Acrisure Stadium. However, the Mountaineers failed to start the season with a win, losing the game 38-31. A few weeks later, after losing to Kansas at home in overtime and defeating Towson, the team successfully defended the Black Diamond Trophy at Virginia Tech, winning 33–10. The Mountaineers went on to lose four out of the five conference games. The season ended with optimism after Sophomore Backup-QB Garrett Greene defeated the Oklahoma Sooners 23–20 for the first time since entering the  Big 12 Conference. After this game, Greene was named the Mountaineer's starter. The Mountaineers finished the season 5–7 (3–6 conference), failing to qualify for a bowl game for the 2nd time in the Neal Brown Era.

The 2023 season started with the renewal of another West Virginia rival. The team was set to play the Penn State Nittany Lions for the first time since 1992. After the Mountaineers lost the season opener 38-15,[82] the team rallied to win four games in a row, including a win against Pitt at home. After dropping the next two games to Houston and Oklahoma State, they were able to string together four wins in the last five games to give them a regular season record of 8-4, the best of Neal Brown's WVU tenure. They went on to defeat the North Carolina Tar Heels in the 2023 Duke's Mayo Bowl by a score of 30-10. However, the success of the 2023 season was unable to continue through the 2024 season. The Mountaineers dropped to a 1-2 record in non-conference play with losses to rivals Penn State and Pitt. Conference play wasn't a success either, and the team finished the season with a 6-6 record. On December 1, 2024, the day after losing to Texas Tech in the regular season finale 52-15, West Virginia University fired Neal Brown. He went 37-35 during his tenure in Morgantown.[83] Offensive coordinator Chad Scott took over as interim head coach until the conclusion of the Frisco Bowl against the Memphis Tigers, which the Mountaineers would go on to lose by a score of 37-42.

On December 11, 2024, it was reported that Rodriguez and West Virginia University had agreed in principle that he would become the next head football coach of the West Virginia Mountaineers with contract terms and language yet to be finalized.[84]  On December 12, 2024, he was officially named the 36th head football coach in the history of the West Virginia Mountaineers.

[citation needed]

West Virginia has won or shared a conference championship on 15 occasions, ten outright and five shared, including eight Southern Conference (SoCon) titles and seven Big East Conference titles.[85]

† Co-champion

The ECAC Lambert-Meadowlands Trophy is an annual award given to the best team in the Eastern Region of FBS-level college football. West Virginia has received the award as Eastern Champion on four occasions.[citation needed]

West Virginia has participated in 41 bowl games throughout its history, compiling a 17–24 record through the 2024 season.[86]  The Mountaineers endured a dubious string of post-season futility from 1987 to 2004, losing 11 of 12 bowl games including eight consecutive losses between 1987 and 1998. However, West Virginia won four straight bowl games from 2005 to 2008 with Pat White, who became the first ever quarterback to win all four bowl games as a starter in Division I college football.[87] These are West Virginia's last ten bowl games.

The West Virginia Mountaineers have had 34 head coaches throughout the program's history. With 149 victories, Don Nehlen is first overall in the program's history, followed by Dana Holgorsen (61 wins) and Rich Rodriguez (60).[21]

West Virginia has finished a season ranked in at least one of the Associated Press (AP) or Coaches polls on 22 occasions.  The Mountaineers have finished ranked amongst the top 10 in college football on five occasions. West Virginia attained its highest-ever ranking in the polls during week 14 of the 2007 season, when they were ranked No. 1 in the Coaches Poll and No. 2 in the AP Poll.[88]

The Mountaineers have concluded the regular season ranked in the final rankings of the College Football Playoff (CFP) on two occasions. West Virginia finished the regular season ranked seven times in the final rankings of the Bowl Championship Series (BCS), a predecessor to the CFP. The Bowl Coalition, a predecessor to the CFP and BCS systems, ranked WVU 3rd in its final standings at the conclusion of the 1993 regular season. As of the end of the 2020 season, the Mountaineers were 49–122–2 against opponents ranked in the AP Poll.

Since 1891, the Mountaineers have played their home games in Morgantown, West Virginia along with neutral-site games at numerous locations throughout West Virginia, most notably in Charleston, Clarksburg, Fairmont, Parkersburg and Wheeling.[4] The construction of Old Mountaineer Field in 1924 gave WVU its first permanent home facility.  Located next to Woodburn Hall in what is now considered the Downtown portion of the WVU campus, the first incarnation of Mountaineer Field consisted of a horseshoe-type seating arrangement. The stadium eventually grew in capacity to its peak of 38,000 by 1979. The physical location of the stadium made it impossible for further expansion to take place, however, and led to the relocation of the football program to the new Mountaineer Field in 1980. The old stadium was razed in 1987. At the southwest corner where the stadium once stood, there is a horseshoe-shaped monument commemorating the stadium.  From 1924 to 1979 the Mountaineers played 267 games at Old Mountaineer Field, compiling a 171–82–14 record.[89]

The Mountaineers have played their home games at the second incarnation of Mountaineer Field since 1980.  The bowl-shaped stadium is located on the Evansdale section of the WVU campus.  Originally constructed with an east–west configuration of the seating areas and a capacity of 50,000, subsequent seating additions at the north and south ends of the facility increased the capacity to over 63,000 by 1986 through the 2003 season. Suites were first introduced to Mountaineer Field in 1994, with 12 suites being constructed in the first row of the press box on the stadium's west end. General admission seating in the north end zone was replaced with 18 suites in 2004 to create the "Touchdown Terrace" section, while four additional suites were added in the south end zone in 2007. The construction of Touchdown Terrace in 2004 brought the stadium's capacity to 60,000.[90]

As of November 29, 2003, the stadium has been named "Milan Puskar Stadium" in honor of Milan Puskar, the founder of Morgantown-based Mylan Pharmaceuticals, in recognition of his $20 million donation to the university.[91]

Due to Mountaineer Field's capacity and the relatively smaller populations of West Virginia's largest cities, it has been suggested that Morgantown becomes the largest "city" in the state on game days due to the influx of spectators at the stadium.[92][93] Crowds at Mountaineer Field have earned the reputation of being loud and boisterous, creating a hostile atmosphere for opposing teams.[94][95]

The largest crowd to ever attend a game at the stadium was 70,222, set on November 20, 1993.[96]

Also constructed in 1980 was the "Facilities Building" (now the Milan Puskar Center) to house the program's football offices. Originally located south of Mountaineer Field, in 1985 the facility was connected to the stadium when an 11,000-seat expansion enclosed the South end zone bowl.[97] The 39,000-square-foot facility houses the team's locker room and training facilities, including a 23,000-square-foot weight training facility on the first floor of the complex.  The second floor of the Puskar Center houses the offensive and defensive wings for the coaching staffs, the team meeting room, player position rooms, the football staff conference room, and the Reynolds Family Academic Performance Center.[97] Also located on the second floor of the Puskar Center is the Donald J. Brohard Hall of Traditions. Made possible through a gift by WVU alumnus and Datatel, Inc. founder Ken Kendrick, the Hall of Traditions opened in 2006 to honor the history of the WVU football program. The Hall of Traditions houses interactive displays, videos, photos, records and information on the program. The Hall is open to the public on weekdays throughout the year.

The Puskar Center underwent significant renovations in 2012 and 2013, aimed specifically at improving the weight room, the aesthetics of the facility's interior, lounge space for the football players and upgrading the coaches' meeting areas.[98] Further renovation of the Puskar Center in 2015 culminated with the construction of a new team meeting room for the players and coaches. 
The structure has 162 theater-style chairs to seat an entire football team, as well as support staff and personnel. The room is also utilized for the head coach's weekly news conferences during the season, as well as media interviews with players and other coaches.[99]

Further renovations to the Puskar Center are planned for the future as part of WVU's master plan for athletic facilities.[100]

The Mountaineers utilize two facilities for indoor and outdoor football practices, the Caperton Indoor Practice Facility and the Steve Antoline Family Football Practice Field.

The Caperton facility opened in August 1998. The indoor space is equipped with a 90-yard FieldTurf playing surface with seven yards of safety zone surrounding the entire field. Total length from wall to wall is 105 yards, with more than 75,000 square feet of practice room. Located behind the Caperton facility is the Steve Antoline Family Practice Field, a FieldTurf facility previously consisting of two grass fields. The Antoline facility consists of a full-length 120-yard field along with an adjacent 60-yard field.

The theme song of West Virginia University, John Denver's "Take Me Home, Country Roads" has been performed at every home football game pregame show since 1972. In 1980, John Denver performed his hit song during pregame festivities to a sold-out crowd at Mountaineer Field. His performance marked the dedication of the second incarnation of Mountaineer Field and the first game for head coach Don Nehlen.[101]  After every home win, WVU players and fans link arm-in-arm and sing along to a recording of the song.[102]


Performed by the Pride of West Virginia marching band, the pregame show includes such traditions as the 220-beat per minute run-on introductory drum cadence, the formation of the "Flying WV" logo to the tune of "Fight Mountaineers," and the forming of the state of West Virginia while playing the university fight song "Hail, West Virginia."[103] The band also performs "Take Me Home, Country Roads" as well as Aaron Copland's "Simple Gifts."[103] The fans participate in several cheers during the pregame show, notably chanting "W-V-U" to the roll of the band's drum line prior to the playing of "Fight Mountaineers," as well as chanting "Let's Go Mountaineers" in between playings of "Hail, West Virginia."[104] 
WVU students encompassing the "Mountaineer Maniacs" section and fans alike participate in several chants during WVU home games. The "Let's Go...Mountaineers" chant, with the east end of the stadium shouting "Let's Go..." and the west end responding with "Mountaineers," is the most popular amongst those in attendance.[105]  Since the mid 2000s, West Virginia fans also participate in the "1st Down" and "3rd Down" cheers. The "1st Down" cheer can be heard at both home and away games prior to the announcement of a Mountaineer first down. Mountaineer fans raise their hands and hold a cheer of "OH!" in unison until the first down call is made by the public address announcer. Following the call, the fans pump their arms up and down three times to a chant of "W-V-U," clap and then signal to the end zone chanting "first down!"[106][107]  The "3rd Down" cheer is similar, with Mountaineer fans raising their arms and waving three fingers upon the announcement of "third down" by the public address. 

WVU incorporated the Mountaineers nickname in 1905 after the coining of West Virginia's state motto, "Mountaineers are Always Free." Prior to 1905, the team was referred to as the "Snakes."[108] The Mountaineer mascot first appeared at WVU sporting events during the 1934–35 school year and has been a fixture ever since. The Mountaineer is selected each year by the Mountain Honorary, composed of members of West Virginia University's senior class.[109] The Mountaineer's costume is tailored to fit each winner, and male Mountaineers customarily grow beards during their tenure to go along with the coonskin cap and rifle, although the beard is not a requirement for the mascot position.[109] The mascot is modeled after the Mountaineer bronze statue located in front of the Mountainlair student union building on the WVU campus. During football games, the Mountaineer mascot will fire his musket upon the team's entrance prior to kickoff, at the conclusion of each quarter and following every score.

Introduced by head coach Rich Rodriguez during the 2007 season, the "Gold Rush" is an ongoing tradition with WVU fans at Mountaineer Field. Partially inspired by the Penn State "White Out" tradition, as well as the "black-out" effect created by Louisville Cardinals fans dressed in black during their game against WVU in 2006, Rodriguez encouraged Mountaineer fans to dress entirely in gold for the rematch between WVU and Louisville in 2007.[110]  WVU's home schedule has featured a Gold Rush home game in each of its subsequent seasons. Since 2008, West Virginia University has worked in conjunction with the United Way to promote the event, selling gold T-shirts to fans with the proceeds benefiting the WVU United Way Campaign.[111]

In contrast, WVU designates one home game per season as a "True Blue" game where the fans in attendance are encouraged to wear blue throughout the stadium. The Mountaineer players wear their all-blue uniform sets for these particular games.[112]

The fan tradition of "Stripe the Stadium" has been in place since the Mountaineers joined the Big 12 Conference in 2012. Fans are encouraged to wear a designated color for their respective seating section. Fans in even sections of the stadium and students in the lower section of the student section are asked to wear blue. Fans in odd sections and students in the upper section of the student section are asked to wear gold. The result is the stadium appearing to be "striped" in blue and gold".[113]

Instituted during the 2011 season by head coach Dana Holgorsen, the Mountaineer Mantrip is a part of West Virginia's gameday traditions and a recognition of the significance of West Virginia's coal industry.[114]  The event is named for the shuttle that transports coalminers into and out of an underground mine at the start and end of their shift and takes place at every home game. 

The walk begins when the team is dropped off at the corner of the WVU Medical Center and Don Nehlen Drive prior to game time. They are accompanied by the Mountaineer mascot, the Pride of West Virginia Marching Band, and the Mountaineer cheerleaders. WVU students and fans line the path to create a tunnel-like effect for the passing team members.[114] When the team reaches the east end of Mountaineer Field, they stop to rub a 350-pound mounted chunk of coal donated by Alpha Natural Resources from the Upper Big Branch coal mine.[115]

A new feature at the completion of the Mantrip was introduced during the 2018 season. After players and coaches have reached Mountaineer Field and touched the mounted piece of coal, they turn and wave toward the parents and patients inside WVU Medicine Children's Hospital adjacent to Mountaineer Field. Dana Holgorsen added this portion of the festivity having drawn inspiration from the Kinnick Stadium "Wave".[116]

Beginning in 1970, the Mountaineers donned the program's first official logo—the WVU "state outline"—on their helmets through the 1979 season and have reintroduced the logo as part of a "throwback" helmet since 2013.[117]  West Virginia used a white helmet with the state outline logo from 1970 to 1972, a gold helmet with the same logo from 1973 to 1978, and reverted to the white helmet and state outline logo in 1979 and again in 2013.

The "Flying WV" is the trademark logo for West Virginia Mountaineer football, adorning the team's helmet and uniform. It debuted in 1980 along with the current gold and blue color scheme as a part of a football uniform redesign by head coach Don Nehlen, and has since become one of the most widely recognized logos in collegiate athletics.[118]  In adopting the Flying WV logo on the team's helmets, Nehlen wanted to create a distinct image for the football program that could be easily identified. When Nehlen began his tenure as head coach in 1980, he initially had difficulty in distinguishing between WVU and its opponents while watching game film.[118]  The logo itself was created by sports artist John Martin, brother of then-Athletic Director Dick Martin. John Martin's primary inspiration for the logo was the depiction of mountains created with the combination of the state initials 'W' and 'V'.[118][119]  The surge in the logo's popularity led to its adoption as the official logo of West Virginia University in 1985.[118]

Since 1980, West Virginia's standard uniform has consisted of a dark blue jersey (home) or a white jersey (away) with gold pants and a dark blue helmet adorned by the gold "Flying WV" logo on both sides.[108] West Virginia's uniform scheme has also included a gold helmet, white helmet, gold jersey, dark blue pants, and white pants at various stages throughout its history.  WVU also added a gray uniform and helmet combination to its rotation for the 2012 season.[120]

The Mountaineers wore a Nike Pro Combat uniform or the 2010 season edition of the Backyard Brawl. The uniform was specifically designed to pay tribute to West Virginia's coal mining industry. The jersey and pants consisted of a shade of white accented by a layer of coal dust, along with accents of university gold that referenced canaries utilized in coal mining. The helmet also implemented the coal dust accent, along with a yellow line down the center designed to embody the beam of light emitted by a miner's headlamp.[121]  West Virginia also donned the Pro Combat uniforms later that season for the Champs Sports Bowl.

WVU introduced new uniforms for the 2013 season. The helmets, jerseys, and pants featured blue, gold, and white primary color sets, creating 27 different possible uniform combinations. The reintroduction of the gold and white helmets to the uniform scheme marked the first time each have been used since the late 1970s.[122]
All of the helmets featured a matte, non-glossy paint finish and the "Flying WV" logo adorned on each side.  WVU introduced a white "throwback" helmet during the 2013 season, utilizing the 1970s "state outline" logo.[123]  The West Virginia state motto, Montani Semper Liberi, (“Mountaineers are Always Free”), was stitched inside the back collar of all three jerseys. A canary image was stitched inside the front collar, representative of West Virginia's coal mining heritage for their use in testing toxicity levels in the mines. The jerseys had a unique number style exclusive to WVU, featuring sharp points and edges inspired by a miner's pickaxe.[122]

As of the 2019 season and as part of a larger re-branding of all WVU sports teams, the Mountaineers incorporated new Nike Vapor Untouchable football uniforms. The all-gray alternate uniform also returned to the rotation.[124] West Virginia debuted "Country Roads" uniforms on September 1, 2022, against longtime rival Pitt. The uniforms are inspired by West Virginia's roadways, with interstate maps on the shoulders and yellow striping that resembles center line markings on the helmet.[125]

The teams met 20 times between 1921 and 2011, every year from 2005 to 2011, as conference foes and members of the Big East Conference. West Virginia leads Cincinnati in the series 13–1–3 since 2011.[126]

West Virginia played in-state opponent Marshall in the annual Friends of Coal Bowl until 2012. Marshall and WVU first played in 1911, but it wasn't until 2006 before the two schools from the "Mountain State" faced off annually for the Governor's Cup. Some[who?] believe the rivalry began due to political pressure from the state government. The two last played in 2012, and there are no immediate plans to renew the rivalry. West Virginia holds a 12–0 lead in the series as of 2012.[127] West Virginia has won all 12 meetings against Marshall.

The Mountaineers and Maryland Terrapins have met on a semi-annual basis since 1919, recently rekindling a cross-border rivalry that was once the longest continuous non-conference series for these geographical neighbors.[128] The Mountaineers lead the series 28–23–2 through the 2021 season.[129]

The series between West Virginia and Penn State has been historically one-sided. The teams met annually from 1947 to 1992. From 1959 to 1983, Penn State had won 25 consecutive meetings between the two. The series ended in 1992 with Penn State commencing Big Ten Conference play in 1993. On September 19, 2013, the schools announced that they would renew the series with two games in 2023 and 2024.[130] Penn State leads the series 48–9–2 through the 2017 season.[131]

In terms of competitiveness, intensity and longevity, the Backyard Brawl with the Pittsburgh Panthers is West Virginia's most fierce and storied rivalry. Separated by only 70 miles, the two universities have competed on a mostly annual basis since 1895 (beginning in 1920 and resuming again in 1943 after World War II). Although Pitt holds a 63–41–3 series lead,[132] more than half of its victories in the Backyard Brawl came prior to 1952 when the Panthers dominated the series 34–9–1. The Mountaineers hold a 26–22–2 edge over the Panthers since 1962 when the series began to interchange annually between Morgantown and Pittsburgh. West Virginia has also won seven of the last ten meetings. After being put on hiatus following the 2011 season due to conference realignment, the series was renewed with four games running from 2022 to 2025 and 2029–2032.

In the first game of the restored Brawl, the Mountaineers lost to the Pittsburgh Panthers by a final score of 38 to 31.[133] The most recent version of the Brawl was held on Saturday, September 24, 2024 and was won by Pittsburgh with a final score of 38–34.[134]

West Virginia also enjoyed a long-standing rivalry with the Syracuse Orange. The schools competed annually from 1955 to 2012, with the 1993 addition of the Ben Schwartzwalder Trophy being awarded to the victor. The significance of the trophy resides in the fact that Ben Schwartzwalder was a West Virginia native, former WVU player, and head coach at Syracuse. While Syracuse holds a 34–27 lead in the series, WVU won eight of the last ten games between the schools. Much like the status of the Backyard Brawl, Syracuse's departure from the Big East for the Atlantic Coast Conference and WVU's joining of the Big 12 Conference casts doubt over the future of the series.[135]

The Mountaineers also enjoy a fierce rivalry with their Appalachia counterparts, the Virginia Tech Hokies. The schools once competed on an annual basis from 1973 to 2005, doing so as Big East Conference rivals starting in 1991. Beginning in 1997, West Virginia and Virginia Tech competed for the Black Diamond Trophy, symbolizing the Appalachian region's rich coal heritage.[136] While West Virginia held a 28–22–1 advantage in the series, Virginia Tech won nine of the last 12 meetings between the schools. Since the Hokies departed the Big East for the ACC in 2004 and ended the series in 2006, the rivalry has been dormant.  The rivalry was renewed with a neutral-site game in 2017, then was played in Morgantown in 2021 and was played in Blacksburg in the 2022 season. West Virginia leads the series with Virginia Tech 30–23–1 through the 2022 season.[137]

West Virginia has produced seven Heisman Trophy candidates. Major Harris is the only Mountaineer to be considered as a finalist for the award, garnering consideration in the 1988 and 1989 seasons.[138][139]



A total of 20 Mountaineer players and coaches have been finalists for numerous college football awards. Don Nehlen and Calvin Magee have won awards as coaches, while Dan Mozes, Pat White, and Tavon Austin have earned awards as players.

Forty-Seven Mountaineers have been recognized as First-Team All-Americans by various media selectors. Among those selections, 14 have achieved Consensus All-American status.  Of those consensus All-Americans, four were unanimous selections.[37]

[140]

During WVU's 18-season tenure in the Southern Conference, a total of seven Mountaineers were recognized with superlative conference honors. Art Lewis received Coach of the Year distinction on consecutive occasions (1953 & 1954) while Bruce Bosley was named the SoCon Player of the Year and Jacobs Blocking Award winner in 1955.[141] During WVU's 21 seasons in the Big East, a total of 12 Mountaineers were recognized with superlative conference honors. Don Nehlen (1993) and Rich Rodriguez (2003) were unanimous selections for Big East Coach of the Year, while Todd Sauerbrun was the unanimous selection for Big East Special Teams Player of the Year in 1994 and Amos Zereoué was the unanimous selection for Big East Rookie of the Year in 1996. Tavon Austin was WVU's first Big 12 Conference award recipient, garnering Co-Special Teams Player of the Year honors in 2012.

From 1950 to 1967, West Virginia competed in the Southern Conference. During their 18 seasons in the SoCon, a total of 35 Mountaineers were recognized as First-Team All-Southern Conference selections.[141]



The Mountaineers competed in the Big East Conference from 1991 to 2011. During their 21 seasons in the Big East, a total of 61 Mountaineers were recognized as First-Team All-Big East selections. Among those players, Tavon Austin (as a Return Specialist, 2011), Noel Devine, Pat White (2007), Eric Wicks (2006), Adam "Pac-Man" Jones (as a Cornerback, 2004), Grant Wiley (2003), Barrett Green, Canute Curtis, Aaron Beasley (1995), Todd Sauerbrun (1994) and Adrian Murrell (1992) were unanimous selections by the conference.



Since joining the Big 12 Conference in 2012, 24 Mountaineers have been recognized as First-Team All-Big 12 selections.





West Virginia has retired six jersey numbers. It is the highest possible accolade to achieve within the Mountaineer football program.[142]

Introduced in 2016, the West Virginia University Mountaineer Legends Society is the second level of recognition for the WVU football program. The Legends Society program replaced the retirement of a jersey number, which had previously served as the second form of recognition. Jersey number retirement remains the highest attainable honor for the WVU football program.

Honors include addition into the team's Legends Park outside of Mountaineer Field and in graphics on column wraps around the stadium's concourse in a "ring of honor" configuration.

To be eligible, a coach, player or administrator must meet the following criteria:

The following individuals have been inducted into the Mountaineer Legends Society for their contributions to the Mountaineer football program: 

The West Virginia University Sports Hall of Fame is the first level of recognition for past members of the Mountaineer football program.  Former athletes, coaches and administrators are eligible for selection 10 years following their association with WVU athletics. It is the first step of recognition for former players, coaches and administrators.[147] The following individuals have been inducted into the WVU Sports Hall of Fame for their contributions to the Mountaineer football program: 

The National Football Foundation, overseer of the College Football Hall of Fame, recognizes ten individuals as WVU inductees.[148] Conversely, the Mountaineer football program recognizes 13 individuals as inductees.[149] [Note 2]

† – Recognition by the National Football Foundation and WVU

‡ – Recognition by WVU

Three Mountaineers hold the distinguished title of Pro Football Hall of Fame inductees.

Joe Stydahar, an offensive tackle, was inducted in 1967. Despite Stydahar's impressive collegiate career, Chicago Bears owner/coach George Halas took a chance in selecting the little-known tackle with the Bears' first ever draft selection in the 1936 NFL draft.[150] Halas's gamble paid off as "Jumbo Joe" produced an illustrious playing career with the Bears, earning four NFL All-Star selections, six All-Pro selections, three NFL championships, and an induction into the NFL's All-Decade Team for the 1930s.  Stydahar also served as head coach of the Los Angeles Rams and the Chicago Cardinals, winning the 1951 NFL Championship with the Rams.  During his Hall of Fame enshrinement speech, Stydahar thanked his family and friends from his "dear state, West Virginia."[151]

Sam Huff, a linebacker, was inducted in 1982. Originally a third round selection by the New York Giants in the 1956 NFL draft, Huff played for the Giants from 1956 to 1963 and later for the Washington Redskins from 1964 to 1969. Huff's football career, let alone his future in the NFL, almost never came to pass, however. When Huff was a junior in high school, WVU head coach Art Lewis came to his town to look at another prospect and recruited Huff instead.[152] At the end of Huff's collegiate career Giants scout Al DeRogatis came to Morgantown to look at All-American guard Bruce Bosley. DeRogatis instead discovered Huff, proclaiming that "there's another guard here who will be even greater. His name is Sam Huff."[152] Huff became a five-time Pro Bowl selection, a four-time First-Team All-Pro selection, an inductee in the NFL 1950s All-Decade Team, and was named as one of the 70 Greatest Redskins of all time. Huff was also recognized as the NFL's Top Linebacker in 1959.[152]

Chuck Howley, a linebacker, was inducted in 2023. Selected by the Chicago Bears seventh overall in the 1958 NFL draft, he played for the Bears from 1958 to 1959, before briefly retiring from football, then returning to play for the Dallas Cowboys and played the remainder of his career for them. An original member of the Doomsday Defense, Howley received six Pro Bowl and five first-team All-Pro selections with the Cowboys, while appearing in two consecutive Super Bowls and winning Super Bowl VI. Howley was also named the MVP of Super Bowl V and is the only player on a losing team to receive the award.


WVU has produced a total of 198 NFL Draft selections.[153] 
 Of those players selected in the draft, 12 Mountaineers have been selected in the first round.





Among the numerous Mountaineers that have participated in the NFL, a total of 32 have received all-star or Pro Bowl recognition.







A complete WVU football roster is available here.

Announced schedules as of November 6, 2024.[174]A wildfire, forest fire, or a bushfire is an unplanned and uncontrolled fire in an area of combustible vegetation.[1][2] Depending on the type of vegetation present, a wildfire may be more specifically identified as a bushfire (in Australia), desert fire, grass fire, hill fire, peat fire, prairie fire, vegetation fire, or veld fire.[3] Some natural forest ecosystems depend on wildfire.[4] Modern forest management often engages in prescribed burns to mitigate fire risk and promote natural forest cycles. However, controlled burns can turn into wildfires by mistake.

Wildfires can be classified by cause of ignition, physical properties, combustible material present, and the effect of weather on the fire.[5] Wildfire severity results from a combination of factors such as available fuels, physical setting, and weather.[6][7][8][9] Climatic cycles with wet periods that create substantial fuels, followed by drought and heat, often precede severe wildfires.[10] These cycles have been intensified by climate change,[11]: 247  and can be exacerbated by curtailment of mitigation measures (such as budget or equipment funding), or sheer enormity of the event.

Wildfires are a common type of disaster in some regions, including Siberia (Russia), California, Washington, Oregon, Texas, Florida, (United States), British Columbia (Canada), and Australia.[12][13][14][15][16] Areas with Mediterranean climates or in the taiga biome are particularly susceptible. Wildfires can severely impact humans and their settlements. Effects include for example the direct health impacts of smoke and fire, as well as destruction of property (especially in wildland–urban interfaces), and economic losses. There is also the potential for contamination of water and soil.

At a global level, human practices have made the impacts of wildfire worse, with a doubling in land area burned by wildfires compared to natural levels.[11]: 247  Humans have impacted wildfire through climate change (e.g. more intense heat waves and droughts), land-use change, and wildfire suppression.[11]: 247  The carbon released from wildfires can add to carbon dioxide concentrations in the atmosphere and thus contribute to the greenhouse effect. This creates a climate change feedback.[17]: 20 

Naturally occurring wildfires can have beneficial effects on those ecosystems that have evolved with fire.[18][19][20] In fact, many plant species depend on the effects of fire for growth and reproduction.[21]

The ignition of a fire takes place through either natural causes or human activity (deliberate or not).
Natural occurrences that can ignite wildfires without the involvement of humans include lightning, volcanic eruptions, sparks from rock falls, and spontaneous combustions.[22][23]

Sources of human-caused fire may include arson, accidental ignition, or the uncontrolled use of fire in land-clearing and agriculture such as the slash-and-burn farming.[24] In the tropics, farmers often practice the slash-and-burn method of clearing fields during the dry season.

In middle latitudes, the most common human causes of wildfires are equipment generating sparks (chainsaws, grinders, mowers, etc.), overhead power lines, and arson.[25][26][27][28][29]

Arson may account for over 20% of human caused fires.[30] However, in the 2019–20 Australian bushfire season "an independent study found online bots and trolls exaggerating the role of arson in the fires."[31] In the 2023 Canadian wildfires false claims of arson gained traction on social media; however, arson is generally not a main cause of wildfires in Canada.[32][33] In California, generally 6–10% of wildfires annually are arson.[34]

Coal seam fires burn in the thousands around the world, such as those in Burning Mountain, New South Wales; Centralia, Pennsylvania; and several coal-sustained fires in China. They can also flare up unexpectedly and ignite nearby flammable material.[35]

The spread of wildfires varies based on the flammable material present, its vertical arrangement and moisture content, and weather conditions.[36] Fuel arrangement and density is governed in part by topography, as land shape determines factors such as available sunlight and water for plant growth. Overall, fire types can be generally characterized by their fuels as follows:

Wildfires occur when all the necessary elements of a fire triangle come together in a susceptible area: an ignition source is brought into contact with a combustible material such as vegetation that is subjected to enough heat and has an adequate supply of oxygen from the ambient air. A high moisture content usually prevents ignition and slows propagation, because higher temperatures are needed to evaporate any water in the material and heat the material to its fire point.[8][45]

Dense forests usually provide more shade, resulting in lower ambient temperatures and greater humidity, and are therefore less susceptible to wildfires.[46] Less dense material such as grasses and leaves are easier to ignite because they contain less water than denser material such as branches and trunks.[47] Plants continuously lose water by evapotranspiration, but water loss is usually balanced by water absorbed from the soil, humidity, or rain.[48] When this balance is not maintained, often as a consequence of droughts, plants dry out and are therefore more flammable.[49][50]

A wildfire front is the portion sustaining continuous flaming combustion, where unburned material meets active flames, or the smoldering transition between unburned and burned material.[51] As the front approaches, the fire heats both the surrounding air and woody material through convection and thermal radiation. First, wood is dried as water is vaporized at a temperature of 100 °C (212 °F). Next, the pyrolysis of wood at 230 °C (450 °F) releases flammable gases. Finally, wood can smolder at 380 °C (720 °F) or, when heated sufficiently, ignite at 590 °C (1,000 °F).[52][53] Even before the flames of a wildfire arrive at a particular location, heat transfer from the wildfire front warms the air to 800 °C (1,500 °F), which pre-heats and dries flammable materials, causing materials to ignite faster and allowing the fire to spread faster.[47][54] High-temperature and long-duration surface wildfires may encourage flashover or torching: the drying of tree canopies and their subsequent ignition from below.[55]

 Wildfires have a rapid forward rate of spread (FROS) when burning through dense uninterrupted fuels.[56] They can move as fast as 10.8 kilometres per hour (6.7 mph) in forests and 22 kilometres per hour (14 mph) in grasslands.[57] Wildfires can advance tangential to the main front to form a flanking front, or burn in the opposite direction of the main front by backing.[58] They may also spread by jumping or spotting as winds and vertical convection columns carry firebrands (hot wood embers) and other burning materials through the air over roads, rivers, and other barriers that may otherwise act as firebreaks.[59][60] Torching and fires in tree canopies encourage spotting, and dry ground fuels around a wildfire are especially vulnerable to ignition from firebrands.[61] Spotting can create spot fires as hot embers and firebrands ignite fuels downwind from the fire. In Australian bushfires, spot fires are known to occur as far as 20 kilometres (12 mi) from the fire front.[62]

Especially large wildfires may affect air currents in their immediate vicinities by the stack effect: air rises as it is heated, and large wildfires create powerful updrafts that will draw in new, cooler air from surrounding areas in thermal columns.[63] Great vertical differences in temperature and humidity encourage pyrocumulus clouds, strong winds, and fire whirls with the force of tornadoes at speeds of more than 80 kilometres per hour (50 mph).[64][65][66] Rapid rates of spread, prolific crowning or spotting, the presence of fire whirls, and strong convection columns signify extreme conditions.[67]

Intensity also increases during daytime hours. Burn rates of smoldering logs are up to five times greater during the day due to lower humidity, increased temperatures, and increased wind speeds.[68] Sunlight warms the ground during the day which creates air currents that travel uphill. At night the land cools, creating air currents that travel downhill. Wildfires are fanned by these winds and often follow the air currents over hills and through valleys.[69] Fires in Europe occur frequently during the hours of 12:00 p.m. and 2:00 p.m.[70] Wildfire suppression operations in the United States revolve around a 24-hour fire day that begins at 10:00 a.m. due to the predictable increase in intensity resulting from the daytime warmth.[71]

Climate change promotes the type of weather that makes wildfires more likely. In some areas, an increase of wildfires has been attributed directly to climate change.[11]: 247  Evidence from Earth's past also shows more fire in warmer periods.[74] Climate change increases evapotranspiration. This can cause vegetation and soils to dry out. When a fire starts in an area with very dry vegetation, it can spread rapidly. Higher temperatures can also lengthen the fire season. This is the time of year in which severe wildfires are most likely, particularly in regions where snow is disappearing.[75]

Weather conditions are raising the risks of wildfires. But the total area burnt by wildfires has decreased. This is mostly because savanna has been converted to cropland, so there are fewer trees to burn.[75]

Climate variability including heat waves, droughts, and El Niño, and regional weather patterns, such as high-pressure ridges, can increase the risk and alter the behavior of wildfires dramatically.[76][77][78] Years of high precipitation can produce rapid vegetation growth, which when followed by warmer periods can encourage more widespread fires and longer fire seasons.[79] High temperatures dry out the fuel loads and make them more flammable, increasing tree mortality and posing significant risks to global forest health.[80][81][82] Since the mid-1980s, in the Western US, earlier snowmelt and associated warming has also been associated with an increase in length and severity of the wildfire season, or the most fire-prone time of the year.[83] A 2019 study indicates that the increase in fire risk in California may be partially attributable to human-induced climate change.[84]

In the summer of 1974–1975 (southern hemisphere), Australia suffered its worst recorded wildfire, when 15% of Australia's land mass suffered "extensive fire damage".[85] Fires that summer burned up an estimated 117 million hectares (290 million acres; 1,170,000 square kilometres; 450,000 square miles).[86][87] In Australia, the annual number of hot days (above 35 °C or 95 °F) and very hot days (above 40 °C or 104 °F) has increased significantly in many areas of the country since 1950. The country has always had bushfires but in 2019, the extent and ferocity of these fires increased dramatically.[88] For the first time catastrophic bushfire conditions were declared for Greater Sydney. New South Wales and Queensland declared a state of emergency but fires were also burning in South Australia and Western Australia.[89]

In 2019, extreme heat and dryness caused massive wildfires in Siberia, Alaska, Canary Islands, Australia, and in the Amazon rainforest. The fires in the latter were caused mainly by illegal logging. The smoke from the fires expanded on huge territory including major cities, dramatically reducing air quality.[90]


As of August 2020, the wildfires in that year were 13% worse than in 2019 due primarily to climate change, deforestation and agricultural burning. The Amazon rainforest's existence is threatened by fires.[91][92][93][94] Record-breaking wildfires in 2021 occurred in Turkey, Greece and Russia, thought to be linked to climate change.[95]
The carbon released from wildfires can add to greenhouse gas concentrations. Climate models do not yet fully reflect this feedback.[17]: 20 


Wildfires release large amounts of carbon dioxide, black and brown carbon particles, and ozone precursors such as volatile organic compounds and nitrogen oxides (NOx) into the atmosphere.[96][97] These emissions affect radiation, clouds, and climate on regional and even global scales.[16] Wildfires also emit substantial amounts of semi-volatile organic species that can partition from the gas phase to form secondary organic aerosol (SOA) over hours to days after emission. In addition, the formation of the other pollutants as the air is transported can lead to harmful exposures for populations in regions far away from the wildfires.[98][16] While direct emissions of harmful pollutants can affect first responders and residents, wildfire smoke can also be transported over long distances and impact air quality across local, regional, and global scales.[99]
The health effects of wildfire smoke, such as worsening cardiovascular and respiratory conditions, extend beyond immediate exposure, contributing to nearly 16,000 annual deaths, a number expected to rise to 30,000 by 2050. The economic impact is also significant, with projected costs reaching $240 billion annually by 2050, surpassing other climate-related damages.[100]

Over the past century, wildfires have accounted for 20–25% of global carbon emissions, the remainder from human activities.[101] Global carbon emissions from wildfires through August 2020 equaled the average annual emissions of the European Union.[102] In 2020, the carbon released by California's wildfires was significantly larger than the state's other carbon emissions.[103]

Forest fires in Indonesia in 1997 were estimated to have released between 0.81 and 2.57 gigatonnes (0.89 and 2.83 billion short tons) of CO2 into the atmosphere, which is between 13–40% of the annual global carbon dioxide emissions from burning fossil fuels.[104][105]

In June and July 2019, fires in the Arctic emitted more than 140 megatons of carbon dioxide, according to an analysis by CAMS. To put that into perspective this amounts to the same amount of carbon emitted by 36 million cars in a year. The recent wildfires and their massive CO2 emissions mean that it will be important to take them into consideration when implementing measures for reaching greenhouse gas reduction targets accorded with the Paris climate agreement.[106] Due to the complex oxidative chemistry occurring during the transport of wildfire smoke in the atmosphere,[107] the toxicity of emissions was indicated to increase over time.[108][109]

Atmospheric models suggest that these concentrations of sooty particles could increase absorption of incoming solar radiation during winter months by as much as 15%.[110] The Amazon is estimated to hold around 90 billion tons of carbon. As of 2019, the earth's atmosphere has 415 parts per million of carbon, and the destruction of the Amazon would add about 38 parts per million.[111]

Some research has shown wildfire smoke can have a cooling effect.[112][113][114]

Research in 2007 stated that black carbon in snow changed temperature three times more than atmospheric carbon dioxide. As much as 94 percent of Arctic warming may be caused by dark carbon on snow that initiates melting. The dark carbon comes from fossil fuels burning, wood and other biofuels, and forest fires. Melting can occur even at low concentrations of dark carbon (below five parts per billion).[115]

Wildfire prevention refers to the preemptive methods aimed at reducing the risk of fires as well as lessening its severity and spread.[116] Prevention techniques aim to manage air quality, maintain ecological balances, protect resources,[117] and to affect future fires.[118] Prevention policies must consider the role that humans play in wildfires, since, for example, 95% of forest fires in Europe are related to human involvement.[119]

Wildfire prevention programs around the world may employ techniques such as wildland fire use (WFU) and prescribed or controlled burns.[120][121] Wildland fire use refers to any fire of natural causes that is monitored but allowed to burn. Controlled burns are fires ignited by government agencies under less dangerous weather conditions.[122] Other objectives can include maintenance of healthy forests, rangelands, and wetlands, and support of ecosystem diversity.[123]

Strategies for wildfire prevention, detection, control and suppression have varied over the years.[124] One common and inexpensive technique to reduce the risk of uncontrolled wildfires is controlled burning: intentionally igniting smaller less-intense fires to minimize the amount of flammable material available for a potential wildfire.[125][126] Vegetation may be burned periodically to limit the accumulation of plants and other debris that may serve as fuel, while also maintaining high species diversity.[127][128] While other people claim that controlled burns and a policy of allowing some wildfires to burn is the cheapest method and an ecologically appropriate policy for many forests, they tend not to take into account the economic value of resources that are consumed by the fire, especially merchantable timber.[129] Some studies conclude that while fuels may also be removed by logging, such thinning treatments may not be effective at reducing fire severity under extreme weather conditions.[130]

Building codes in fire-prone areas typically require that structures be built of flame-resistant materials and a defensible space be maintained by clearing flammable materials within a prescribed distance from the structure.[131][132] Communities in the Philippines also maintain fire lines 5 to 10 meters (16 to 33 ft) wide between the forest and their village, and patrol these lines during summer months or seasons of dry weather.[133] Continued residential development in fire-prone areas and rebuilding structures destroyed by fires has been met with criticism.[134] The ecological benefits of fire are often overridden by the economic and safety benefits of protecting structures and human life.[135]

The demand for timely, high-quality fire information has increased in recent years. Fast and effective detection is a key factor in wildfire fighting.[136] Early detection efforts were focused on early response, accurate results in both daytime and nighttime, and the ability to prioritize fire danger.[137] Fire lookout towers were used in the United States in the early 20th century and fires were reported using telephones, carrier pigeons, and heliographs.[138] Aerial and land photography using instant cameras were used in the 1950s until infrared scanning was developed for fire detection in the 1960s. However, information analysis and delivery was often delayed by limitations in communication technology. Early satellite-derived fire analyses were hand-drawn on maps at a remote site and sent via overnight mail to the fire manager. During the Yellowstone fires of 1988, a data station was established in West Yellowstone, permitting the delivery of satellite-based fire information in approximately four hours.[137]

Public hotlines, fire lookouts in towers, and ground and aerial patrols can be used as a means of early detection of forest fires. However, accurate human observation may be limited by operator fatigue, time of day, time of year, and geographic location. Electronic systems have gained popularity in recent years as a possible resolution to human operator error. These systems may be semi- or fully automated and employ systems based on the risk area and degree of human presence, as suggested by GIS data analyses. An integrated approach of multiple systems can be used to merge satellite data, aerial imagery, and personnel position via Global Positioning System (GPS) into a collective whole for near-realtime use by wireless Incident Command Centers.[139]

A small, high risk area that features thick vegetation, a strong human presence, or is close to a critical urban area can be monitored using a local sensor network. Detection systems may include wireless sensor networks that act as automated weather systems: detecting temperature, humidity, and smoke.[140][141][142][143] These may be battery-powered, solar-powered, or tree-rechargeable: able to recharge their battery systems using the small electrical currents in plant material.[144] Larger, medium-risk areas can be monitored by scanning towers that incorporate fixed cameras and sensors to detect smoke or additional factors such as the infrared signature of carbon dioxide produced by fires. Additional capabilities such as night vision, brightness detection, and color change detection may also be incorporated into sensor arrays.[145][146][147]

The Department of Natural Resources signed a contract with PanoAI for the installation of 360 degree 'rapid detection' cameras around the Pacific northwest, which are mounted on cell towers and are capable of continuous monitoring of a 24-kilometre (15 mi) radius.[148] Additionally, Sensaio Tech, based in Brazil and Toronto, has released a sensor device that continuously monitors 14 different variables common in forests, ranging from soil temperature to salinity. This information is connected live back to clients through dashboard visualizations, while mobile notifications are provided regarding dangerous levels.[149]

Satellite and aerial monitoring through the use of planes, helicopter, or UAVs can provide a wider view and may be sufficient to monitor very large, low risk areas. These more sophisticated systems employ GPS and aircraft-mounted infrared or high-resolution visible cameras to identify and target wildfires.[150][151] Satellite-mounted sensors such as Envisat's Advanced Along Track Scanning Radiometer and European Remote-Sensing Satellite's Along-Track Scanning Radiometer can measure infrared radiation emitted by fires, identifying hot spots greater than 39 °C (102 °F).[152][153] The National Oceanic and Atmospheric Administration's Hazard Mapping System combines remote-sensing data from satellite sources such as Geostationary Operational Environmental Satellite (GOES), Moderate-Resolution Imaging Spectroradiometer (MODIS), and Advanced Very High Resolution Radiometer (AVHRR) for detection of fire and smoke plume locations.[154][155] However, satellite detection is prone to offset errors, anywhere from 2 to 3 kilometers (1 to 2 mi) for MODIS and AVHRR data and up to 12 kilometers (7.5 mi) for GOES data.[156] Satellites in geostationary orbits may become disabled, and satellites in polar orbits are often limited by their short window of observation time. Cloud cover and image resolution may also limit the effectiveness of satellite imagery.[157] Global Forest Watch[158] provides detailed daily updates on fire alerts.[159]

In 2015 a new fire detection tool is in operation at the U.S. Department of Agriculture (USDA) Forest Service (USFS) which uses data from the Suomi National Polar-orbiting Partnership (NPP) satellite to detect smaller fires in more detail than previous space-based products. The high-resolution data is used with a computer model to predict how a fire will change direction based on weather and land conditions.[160]

In 2014, an international campaign was organized in South Africa's Kruger National Park to validate fire detection products including the new VIIRS active fire data. In advance of that campaign, the Meraka Institute of the Council for Scientific and Industrial Research in Pretoria, South Africa, an early adopter of the VIIRS 375 m fire product, put it to use during several large wildfires in Kruger.[161]

Since 2021 NASA has provided active fire locations in near real-time via the Fire Information for Resource Management System (FIRMS).

The increased prevalence of wildfires has led to proposals deploy technologies based on artificial intelligence for early detection, prevention, and prediction of wildfires.[162][163][164]

Wildfire suppression depends on the technologies available in the area in which the wildfire occurs. In less developed nations the techniques used can be as simple as throwing sand or beating the fire with sticks or palm fronds.[165] In more advanced nations, the suppression methods vary due to increased technological capacity. Silver iodide can be used to encourage snow fall,[166] while fire retardants and water can be dropped onto fires by unmanned aerial vehicles, planes, and helicopters.[167][168] Complete fire suppression is no longer an expectation, but the majority of wildfires are often extinguished before they grow out of control. While more than 99% of the 10,000 new wildfires each year are contained, escaped wildfires under extreme weather conditions are difficult to suppress without a change in the weather. Wildfires in Canada and the US burn an average of 54,500 square kilometers (13,000,000 acres) per year.[169][170]

Above all, fighting wildfires can become deadly. A wildfire's burning front may also change direction unexpectedly and jump across fire breaks. Intense heat and smoke can lead to disorientation and loss of appreciation of the direction of the fire, which can make fires particularly dangerous. For example, during the 1949 Mann Gulch fire in Montana, United States, thirteen smokejumpers died when they lost their communication links, became disoriented, and were overtaken by the fire.[171] In the Australian February 2009 Victorian bushfires, at least 173 people died and over 2,029 homes and 3,500 structures were lost when they became engulfed by wildfire.[172]

The suppression of wild fires takes up a large amount of a country's gross domestic product which directly affects the country's economy.[173] While costs vary wildly from year to year, depending on the severity of each fire season, in the United States, local, state, federal and tribal agencies collectively spend tens of billions of dollars annually to suppress wildfires. In the United States, it was reported that approximately $6 billion was spent between 2004–2008 to suppress wildfires in the country.[173] In California, the U.S. Forest Service spends about $200 million per year to suppress 98% of wildfires and up to $1 billion to suppress the other 2% of fires that escape initial attack and become large.[174]

Wildland fire fighters face several life-threatening hazards including heat stress, fatigue, smoke and dust, as well as the risk of other injuries such as burns, cuts and scrapes, animal bites, and even rhabdomyolysis.[175][176] Between 2000 and 2016, more than 350 wildland firefighters died on-duty.[177]

Especially in hot weather conditions, fires present the risk of heat stress, which can entail feeling heat, fatigue, weakness, vertigo, headache, or nausea. Heat stress can progress into heat strain, which entails physiological changes such as increased heart rate and core body temperature. This can lead to heat-related illnesses, such as heat rash, cramps, exhaustion or heat stroke. Various factors can contribute to the risks posed by heat stress, including strenuous work, personal risk factors such as age and fitness, dehydration, sleep deprivation, and burdensome personal protective equipment. Rest, cool water, and occasional breaks are crucial to mitigating the effects of heat stress.[175]

Smoke, ash, and debris can also pose serious respiratory hazards for wildland firefighters. The smoke and dust from wildfires can contain gases such as carbon monoxide, sulfur dioxide and formaldehyde, as well as particulates such as ash and silica. To reduce smoke exposure, wildfire fighting crews should, whenever possible, rotate firefighters through areas of heavy smoke, avoid downwind firefighting, use equipment rather than people in holding areas, and minimize mop-up. Camps and command posts should also be located upwind of wildfires. Protective clothing and equipment can also help minimize exposure to smoke and ash.[175]

Firefighters are also at risk of cardiac events including strokes and heart attacks. Firefighters should maintain good physical fitness. Fitness programs, medical screening and examination programs which include stress tests can minimize the risks of firefighting cardiac problems.[175] Other injury hazards wildland firefighters face include slips, trips, falls, burns, scrapes, and cuts from tools and equipment, being struck by trees, vehicles, or other objects, plant hazards such as thorns and poison ivy, snake and animal bites, vehicle crashes, electrocution from power lines or lightning storms, and unstable building structures.[175]

Fire retardants are used to slow wildfires by inhibiting combustion. They are aqueous solutions of ammonium phosphates and ammonium sulfates, as well as thickening agents.[178] The decision to apply retardant depends on the magnitude, location and intensity of the wildfire. In certain instances, fire retardant may also be applied as a precautionary fire defense measure.[179]

Typical fire retardants contain the same agents as fertilizers. Fire retardants may also affect water quality through leaching, eutrophication, or misapplication. Fire retardant's effects on drinking water remain inconclusive.[180] Dilution factors, including water body size, rainfall, and water flow rates lessen the concentration and potency of fire retardant.[179] Wildfire debris (ash and sediment) clog rivers and reservoirs increasing the risk for floods and erosion that ultimately slow and/or damage water treatment systems.[180][181] There is continued concern of fire retardant effects on land, water, wildlife habitats, and watershed quality, additional research is needed. However, on the positive side, fire retardant (specifically its nitrogen and phosphorus components) has been shown to have a fertilizing effect on nutrient-deprived soils and thus creates a temporary increase in vegetation.[179]

Wildfire modeling is concerned with numerical simulation of wildfires to comprehend and predict fire behavior.[182][183] Wildfire modeling aims to aid wildfire suppression, increase the safety of firefighters and the public, and minimize damage. Wildfire modeling can also aid in protecting ecosystems, watersheds, and air quality.

Most of Earth's weather and air pollution resides in the troposphere, the part of the atmosphere that extends from the surface of the planet to a height of about 10 kilometers (6 mi). The vertical lift of a severe thunderstorm or pyrocumulonimbus can be enhanced in the area of a large wildfire, which can propel smoke, soot (black carbon), and other particulate matter as high as the lower stratosphere.[187] Previously, prevailing scientific theory held that most particles in the stratosphere came from volcanoes, but smoke and other wildfire emissions have been detected from the lower stratosphere.[188] Pyrocumulus clouds can reach 6,100 meters (20,000 ft) over wildfires.[189] Satellite observation of smoke plumes from wildfires revealed that the plumes could be traced intact for distances exceeding 1,600 kilometers (1,000 mi).[190] Computer-aided models such as CALPUFF may help predict the size and direction of wildfire-generated smoke plumes by using atmospheric dispersion modeling.[191]

Wildfires can affect local atmospheric pollution,[192] and release carbon in the form of carbon dioxide.[193] Wildfire emissions contain fine particulate matter which can cause cardiovascular and respiratory problems.[194] Increased fire byproducts in the troposphere can increase ozone concentrations beyond safe levels.[195]

Wildfires are common in climates that are sufficiently moist to allow the growth of vegetation but feature extended dry, hot periods.[21] Such places include the vegetated areas of Australia and Southeast Asia, the veld in southern Africa, the fynbos in the Western Cape of South Africa, the forested areas of the United States and Canada, and the Mediterranean Basin.

High-severity wildfire creates complex early seral forest habitat (also called "snag forest habitat"), which often has higher species richness and diversity than unburned old forest.[196] Plant and animal species in most types of North American forests evolved with fire, and many of these species depend on wildfires, and particularly high-severity fires, to reproduce and grow. Fire helps to return nutrients from plant matter back to the soil. The heat from fire is necessary to the germination of certain types of seeds, and the snags (dead trees) and early successional forests created by high-severity fire create habitat conditions that are beneficial to wildlife.[196] Early successional forests created by high-severity fire support some of the highest levels of native biodiversity found in temperate conifer forests.[197][198] Post-fire logging has no ecological benefits and many negative impacts; the same is often true for post-fire seeding.[129] The exclusion of wildfires can contribute to vegetation regime shifts, such as woody plant encroachment.[199][200]

Although some ecosystems rely on naturally occurring fires to regulate growth, some ecosystems suffer from too much fire, such as the chaparral in southern California and lower-elevation deserts in the American Southwest. The increased fire frequency in these ordinarily fire-dependent areas has upset natural cycles, damaged native plant communities, and encouraged the growth of non-native weeds.[201][202][203][204] Invasive species, such as Lygodium microphyllum and Bromus tectorum, can grow rapidly in areas that were damaged by fires. Because they are highly flammable, they can increase the future risk of fire, creating a positive feedback loop that increases fire frequency and further alters native vegetation communities.[42][117]

In the Amazon rainforest, drought, logging, cattle ranching practices, and slash-and-burn agriculture damage fire-resistant forests and promote the growth of flammable brush, creating a cycle that encourages more burning.[205] Fires in the rainforest threaten its collection of diverse species and produce large amounts of CO2.[206] Also, fires in the rainforest, along with drought and human involvement, could damage or destroy more than half of the Amazon rainforest by 2030.[207] Wildfires generate ash, reduce the availability of organic nutrients, and cause an increase in water runoff, eroding other nutrients and creating flash flood conditions.[36][208] A 2003 wildfire in the North Yorkshire Moors burned off 2.5 square kilometers (600 acres) of heather and the underlying peat layers. Afterwards, wind erosion stripped the ash and the exposed soil, revealing archaeological remains dating to 10,000 BC.[209] Wildfires can also have an effect on climate change, increasing the amount of carbon released into the atmosphere and inhibiting vegetation growth, which affects overall carbon uptake by plants.[210]

Debris and chemical runoff into waterways after wildfires can make drinking water sources unsafe.[211] Though it is challenging to quantify the impacts of wildfires on surface water quality, research suggests that the concentration of many pollutants increases post-fire. The impacts occur during active burning and up to years later.[212] Increases in nutrients and total suspended sediments can happen within a year while heavy metal concentrations may peak 1–2 years after a wildfire.[213]

Benzene is one of many chemicals that have been found in drinking water systems after wildfires. Benzene can permeate certain plastic pipes and thus require long times to be removed from the water distribution infrastructure. Researchers estimated that, in worst case scenarios, more than 286 days of constant flushing of a contaminated HDPE service line were needed to reduce benzene below safe drinking water limits.[214][215] Temperature increases caused by fires, including wildfires, can cause plastic water pipes to generate toxic chemicals[216] such as benzene.[217]

Fire adaptations are traits of plants and animals that help them survive wildfire or to use resources created by wildfire. These traits can help plants and animals increase their survival rates during a fire and/or reproduce offspring after a fire. Both plants and animals have multiple strategies for surviving and reproducing after fire. Plants in wildfire-prone ecosystems often survive through adaptations to their local fire regime. Such adaptations include physical protection against heat, increased growth after a fire event, and flammable materials that encourage fire and may eliminate competition.

Wildfire risk is the chance that a wildfire will start in or reach a particular area and the potential loss of human values if it does. Risk is dependent on variable factors such as human activities, weather patterns, availability of wildfire fuels, and the availability or lack of resources to suppress a fire.[223][224] Wildfires have continually been a threat to human populations. However, human-induced geographic and climatic changes are exposing populations more frequently to wildfires and increasing wildfire risk. It is speculated that the increase in wildfires arises from a century of wildfire suppression coupled with the rapid expansion of human developments into fire-prone wildlands.[225] Wildfires are naturally occurring events that aid in promoting forest health. Global warming and climate changes are causing an increase in temperatures and more droughts nationwide which contributes to an increase in wildfire risk.[226][227]

The most noticeable adverse effect of wildfires is the destruction of property. However, hazardous chemicals released also significantly impact human health.[228]

Wildfire smoke is composed primarily of carbon dioxide and water vapor. Other common components present in lower concentrations are carbon monoxide, formaldehyde, acrolein, polyaromatic hydrocarbons, and benzene.[229] Small airborne particulates (in solid form or liquid droplets) are also present in smoke and ash debris. 80–90% of wildfire smoke, by mass, is within the fine particle size class of 2.5 micrometers in diameter or smaller.[230]

Carbon dioxide in smoke poses a low health risk due to its low toxicity. Rather, carbon monoxide and fine particulate matter, particularly 2.5 μm in diameter and smaller, have been identified as the major health threats.[229] High levels of heavy metals, including lead, arsenic, cadmium, and copper were found in the ash debris following the 2007 Californian wildfires. A national clean-up campaign was organised in fear of the health effects from exposure.[231] In the devastating California Camp Fire (2018) that killed 85 people, lead levels increased by around 50 times in the hours following the fire at a site nearby (Chico). Zinc concentration also increased significantly in Modesto, 240 kilometres (150 mi) away. Heavy metals such as manganese and calcium were found in numerous California fires as well.[232] Other chemicals are considered to be significant hazards but are found in concentrations that are too low to cause detectable health effects.[citation needed]

The degree of wildfire smoke exposure to an individual is dependent on the length, severity, duration, and proximity of the fire. People are exposed directly to smoke via the respiratory tract through inhalation of air pollutants. Indirectly, communities are exposed to wildfire debris that can contaminate soil and water supplies.

The U.S. Environmental Protection Agency (EPA) developed the air quality index (AQI), a public resource that provides national air quality standard concentrations for common air pollutants. The public can use it to determine their exposure to hazardous air pollutants based on visibility range.[233]

Wildfire smoke contains particulates that may have adverse effects upon the human respiratory system. Evidence of the health effects should be relayed to the public so that exposure may be limited. The evidence can also be used to influence policy to promote positive health outcomes.[234]

Inhalation of smoke from a wildfire can be a health hazard.[235] Wildfire smoke is composed of combustion products i.e. carbon dioxide, carbon monoxide, water vapor, particulate matter, organic chemicals, nitrogen oxides and other compounds. The principal health concern is the inhalation of particulate matter and carbon monoxide.[236]

Particulate matter (PM) is a type of air pollution made up of particles of dust and liquid droplets. They are characterized into three categories based on particle diameter: coarse PM, fine PM, and ultrafine PM. Coarse particles are between 2.5 micrometers and 10 micrometers, fine particles measure 0.1 to 2.5 micrometers, and ultrafine particle are less than 0.1 micrometer. lmpact on the body upon inhalation varies by size. Coarse PM is filtered by the upper airways and can accumulate and cause pulmonary inflammation. This can result in eye and sinus irritation as well as sore throat and coughing.[237][238] Coarse PM is often composed of heavier and more toxic materials that lead to short-term effects with stronger impact.[238]

Smaller PM moves further into the respiratory system creating issues deep into the lungs and the bloodstream.[237][238] In asthma patients, PM2.5 causes inflammation but also increases oxidative stress in the epithelial cells. These particulates also cause apoptosis and autophagy in lung epithelial cells. Both processes damage the cells and impact cell function. This damage impacts those with respiratory conditions such as asthma where the lung tissues and function are already compromised.[238]  Particulates less than 0.1 micrometer are called ultrafine particle (UFP). It is a major component of wildfire smoke.[239] UFP can enter the bloodstream like PM2.5–0.1 however studies show that it works into the blood much quicker. The inflammation and epithelial damage done by UFP has also shown to be much more severe.[238] PM2.5 is of the largest concern in regards to wildfire.[234] This is particularly hazardous to the very young, elderly and those with chronic conditions such as asthma, chronic obstructive pulmonary disease (COPD), cystic fibrosis and cardiovascular conditions. The illnesses most commonly associated with exposure to fine PM from wildfire smoke are bronchitis, exacerbation of asthma or COPD, and pneumonia. Symptoms of these complications include wheezing and shortness of breath and cardiovascular symptoms include chest pain, rapid heart rate and fatigue.[237]

Several epidemiological studies have demonstrated a close association between air pollution and respiratory allergic diseases such as bronchial asthma.[234]

An observational study of smoke exposure related to the 2007 San Diego wildfires revealed an increase both in healthcare utilization and respiratory diagnoses, especially asthma among the group sampled.[240] Projected climate scenarios of wildfire occurrences predict significant increases in respiratory conditions among young children.[240] PM triggers a series of biological processes including inflammatory immune response, oxidative stress, which are associated with harmful changes in allergic respiratory diseases.[241]

Although some studies demonstrated no significant acute changes in lung function among people with asthma related to PM from wildfires, a possible explanation for these counterintuitive findings is the increased use of quick-relief medications, such as inhalers, in response to elevated levels of smoke among those already diagnosed with asthma.[242]

There is consistent evidence between wildfire smoke and the exacerbation of asthma.[242]

Asthma is one of the most common chronic disease among children in the United States, affecting an estimated 6.2 million children.[243] Research on asthma risk focuses specifically on the risk of air pollution during the gestational period. Several pathophysiology processes are involved in this. Considerable airway development occurs during the 2nd and 3rd trimesters and continues until 3 years of age.[244] It is hypothesized that exposure to these toxins during this period could have consequential effects, as the epithelium of the lungs during this time could have increased permeability to toxins. Exposure to air pollution during parental and pre-natal stage could induce epigenetic changes which are responsible for the development of asthma.[245] Studies have found significant association between PM2.5, NO2 and development of asthma during childhood despite heterogeneity among studies.[246] Furthermore, maternal exposure to chronic stressors is most likely present in distressed communities, and as this can be correlated with childhood asthma, it may further explain links between early childhood exposure to air pollution, neighborhood poverty, and childhood risk.[247]

Carbon monoxide (CO) is a colorless, odorless gas that can be found at the highest concentration at close proximity to a smoldering fire. Thus, it is a serious threat to the health of wildfire firefighters. CO in smoke can be inhaled into the lungs where it is absorbed into the bloodstream and reduces oxygen delivery to the body's vital organs. At high concentrations, it can cause headaches, weakness, dizziness, confusion, nausea, disorientation, visual impairment, coma, and even death. Even at lower concentrations, such as those found at wildfires, individuals with cardiovascular disease may experience chest pain and cardiac arrhythmia.[229] A recent study tracking the number and cause of wildfire firefighter deaths from 1990 to 2006 found that 21.9% of the deaths occurred from heart attacks.[248]

Another important and somewhat less obvious health effect of wildfires is psychiatric diseases and disorders. Both adults and children from various countries who were directly and indirectly affected by wildfires were found to demonstrate different mental conditions linked to their experience with the wildfires. These include post-traumatic stress disorder (PTSD), depression, anxiety, and phobias.[249][250][251][252][253]

The Western US has seen an increase in both the frequency and intensity of wildfires over the last several decades. This has been attributed to the arid climate of there and the effects of global warming. An estimated 46 million people were exposed to wildfire smoke from 2004 to 2009 in the Western US. Evidence has demonstrated that wildfire smoke can increase levels of airborne particulate.[234]

The EPA has defined acceptable concentrations of PM in the air, through the National Ambient Air Quality Standards and monitoring of ambient air quality has been mandated.[254] Due to these monitoring programs and the incidence of several large wildfires near populated areas, epidemiological studies have been conducted and demonstrate an association between human health effects and an increase in fine particulate matter due to wildfire smoke.

An increase in PM smoke emitted from the Hayman fire in Colorado in June 2002, was associated with an increase in respiratory symptoms in patients with COPD.[255] Looking at the wildfires in Southern California in 2003, investigators have shown an increase in hospital admissions due to asthma symptoms while being exposed to peak concentrations of PM in smoke.[256] Another epidemiological study found a 7.2% (95% confidence interval: 0.25%, 15%) increase in risk of respiratory related hospital admissions during smoke wave days with high wildfire-specific particulate matter 2.5 compared to matched non-smoke-wave days.[234]

Children participating in the Children's Health Study were also found to have an increase in eye and respiratory symptoms, medication use and physician visits.[257] Mothers who were pregnant during the fires gave birth to babies with a slightly reduced average birth weight compared to those who were not exposed. Suggesting that pregnant women may also be at greater risk to adverse effects from wildfire.[258] Worldwide, it is estimated that 339,000 people die due to the effects of wildfire smoke each year.[259]


Besides the size of PM, their chemical composition should also be considered. Antecedent studies have demonstrated that the chemical composition of PM2.5 from wildfire smoke can yield different estimates of human health outcomes as compared to other sources of smoke such as solid fuels.[234]
After a wildfire, hazards remain. Residents returning to their homes may be at risk from falling fire-weakened trees. Humans and pets may also be harmed by falling into ash pits. The Intergovernmental Panel on Climate Change (IPCC) also reports that wildfires cause significant damage to electric systems, especially in dry regions.[260]

Chemically contaminated drinking water, at levels of hazardous waste concern, is a growing problem. In particular, hazardous waste scale chemical contamination of buried water systems was first discovered in the U.S. in 2017,[261] and has since been increasingly documented in Hawaii, Colorado, and Oregon after wildfires.[262] In 2021, Canadian authorities adapted their post-fire public safety investigation approaches in British Columbia to screen for this risk, but have not found it as of 2023. Another challenge is that private drinking wells and the plumbing within a building can also become chemically contaminated and unsafe.[263] Households experience a wide-variety of significant economic and health impacts related to this contaminated water.[264] Evidence-based guidance on how to inspect and test wildfire impacted wells [265] and building water systems was developed for the first time in 2020.[266] In Paradise, California, for example,[267] the 2018 Camp Fire caused more than $150 million dollars worth of damage. This required almost a year of time to decontaminate and repair the municipal drinking water system from wildfire damage.

The source of this contamination was first proposed after the 2018 Camp Fire in California as originating from thermally degraded plastics in water systems, smoke and vapors entering depressurized plumbing, and contaminated water in buildings being sucked into the municipal water system. In 2020, it was first shown that thermal degradation of plastic drinking water materials was one potential contamination source.[268] In 2023, the second theory was confirmed where contamination could be sucked into pipes that lost water pressure.[269]

Other post-fire risks, can increase if other extreme weather follows. For example, wildfires make soil less able to absorb precipitation, so heavy rainfall can result in more severe flooding and damages like mud slides.[270][271]

Firefighters are at greatest risk for acute and chronic health effects resulting from wildfire smoke exposure. Some of the most common health conditions that firefighters acquire from prolonged smoke inhalation include cardiovascular and respiratory diseases.[272] For example, wildland firefighters can get hypoxia, which is a condition in which the body does not receive enough oxygen.[273] Due to firefighters' occupational duties, they are frequently exposed to hazardous chemicals at close proximity for longer periods of time. A case study on the exposure of wildfire smoke among wildland firefighters shows that firefighters are exposed to significant levels of carbon monoxide and respiratory irritants above OSHA-permissible exposure limits (PEL) and ACGIH threshold limit values (TLV). 5–10% are overexposed.[274]

Between 2001 and 2012, over 200 fatalities occurred among wildland firefighters. In addition to heat and chemical hazards, firefighters are also at risk for electrocution from power lines; injuries from equipment; slips, trips, and falls; injuries from vehicle rollovers; heat-related illness; insect bites and stings; stress; and rhabdomyolysis.[275] Wildfires that reach urban environments create additional toxic fumes and carcinogenic particles from burning metals, plastics, electronics, paints, and other common materials.[276]

Residents in communities surrounding wildfires are exposed to lower concentrations of chemicals, but they are at a greater risk for indirect exposure through water or soil contamination. Exposure to residents is greatly dependent on individual susceptibility. Vulnerable persons such as children (ages 0–4), the elderly (ages 65 and older), smokers, and pregnant women are at an increased risk due to their already compromised body systems, even when the exposures are present at low chemical concentrations and for relatively short exposure periods.[229] They are also at risk for future wildfires and may move away to areas they consider less risky.[277]

Wildfires affect large numbers of people in Western Canada and the United States. In California alone, more than 350,000 people live in towns and cities in "very high fire hazard severity zones".[278]

Direct risks to building residents in fire-prone areas can be moderated through design choices such as choosing fire-resistant vegetation, maintaining landscaping to avoid debris accumulation and to create firebreaks, and by selecting fire-retardant roofing materials. Potential compounding issues with poor air quality and heat during warmer months may be addressed with MERV 11 or higher outdoor air filtration in building ventilation systems, mechanical cooling, and a provision of a refuge area with additional air cleaning and cooling, if needed.[279]

The first evidence of wildfires is fossils of the giant fungi Prototaxites preserved as charcoal, discovered in South Wales and Poland, dating to the Silurian period (about 430 million years ago).[280] Smoldering surface fires started to occur sometime before the Early Devonian period 405 million years ago. Low atmospheric oxygen during the Middle and Late Devonian was accompanied by a decrease in charcoal abundance.[281][282] Additional charcoal evidence suggests that fires continued through the Carboniferous period. Later, the overall increase of atmospheric oxygen from 13% in the Late Devonian to 30–31% by the Late Permian was accompanied by a more widespread distribution of wildfires.[283] Later, a decrease in wildfire-related charcoal deposits from the late Permian to the Triassic periods is explained by a decrease in oxygen levels.[284]

Wildfires during the Paleozoic and Mesozoic periods followed patterns similar to fires that occur in modern times. Surface fires driven by dry seasons[clarification needed] are evident in Devonian and Carboniferous progymnosperm forests. Lepidodendron forests dating to the Carboniferous period have charred peaks, evidence of crown fires. In Jurassic gymnosperm forests, there is evidence of high frequency, light surface fires.[284] The increase of fire activity in the late Tertiary[285] is possibly due to the increase of C4-type grasses. As these grasses shifted to more mesic habitats, their high flammability increased fire frequency, promoting grasslands over woodlands.[286] However, fire-prone habitats may have contributed to the prominence of trees such as those of the genera Eucalyptus, Pinus and Sequoia, which have thick bark to withstand fires and employ pyriscence.[287][288]

The human use of fire for agricultural and hunting purposes during the Paleolithic and Mesolithic ages altered pre-existing landscapes and fire regimes. Woodlands were gradually replaced by smaller vegetation that facilitated travel, hunting, seed-gathering and planting.[289] In recorded human history, minor allusions to wildfires were mentioned in the Bible and by classical writers such as Homer. However, while ancient Hebrew, Greek, and Roman writers were aware of fires, they were not very interested in the uncultivated lands where wildfires occurred.[290][291] Wildfires were used in battles throughout human history as early thermal weapons. From the Middle Ages, accounts were written of occupational burning as well as customs and laws that governed the use of fire. In Germany, regular burning was documented in 1290 in the Odenwald and in 1344 in the Black Forest.[292] In the 14th century Sardinia, firebreaks were used for wildfire protection. In Spain during the 1550s, sheep husbandry was discouraged in certain provinces by Philip II due to the harmful effects of fires used in transhumance.[290][291] As early as the 17th century, Native Americans were observed using fire for many purposes including cultivation, signaling, and warfare. Scottish botanist David Douglas noted the native use of fire for tobacco cultivation, to encourage deer into smaller areas for hunting purposes, and to improve foraging for honey and grasshoppers. Charcoal found in sedimentary deposits off the Pacific coast of Central America suggests that more burning occurred in the 50 years before the Spanish colonization of the Americas than after the colonization.[293] In the post-World War II Baltic region, socio-economic changes led more stringent air quality standards and bans on fires that eliminated traditional burning practices.[292] In the mid-19th century, explorers from HMS Beagle observed Australian Aborigines using fire for ground clearing, hunting, and regeneration of plant food in a method later named fire-stick farming.[294] Such careful use of fire has been employed for centuries in lands protected by Kakadu National Park to encourage biodiversity.[295]

Wildfires typically occur during periods of increased temperature and drought. An increase in fire-related debris flow in alluvial fans of northeastern Yellowstone National Park was linked to the period between AD 1050 and 1200, coinciding with the Medieval Warm Period.[296] However, human influence caused an increase in fire frequency. Dendrochronological fire scar data and charcoal layer data in Finland suggests that, while many fires occurred during severe drought conditions, an increase in the number of fires during 850 BC and 1660 AD can be attributed to human influence.[297] Charcoal evidence from the Americas suggested a general decrease in wildfires between 1 AD and 1750 compared to previous years. However, a period of increased fire frequency between 1750 and 1870 was suggested by charcoal data from North America and Asia, attributed to human population growth and influences such as land clearing practices. This period was followed by an overall decrease in burning in the 20th century, linked to the expansion of agriculture, increased livestock grazing, and fire prevention efforts.[298] A meta-analysis found that 17 times more land burned annually in California before 1800 compared to recent decades (1,800,000 hectares/year compared to 102,000 hectares/year).[299]

According to a paper published in the journal Science, the number of natural and human-caused fires decreased by 24.3% between 1998 and 2015. Researchers explain this as a transition from nomadism to settled lifestyle and intensification of agriculture that lead to a drop in the use of fire for land clearing.[300][301]

Increases of certain tree species (i.e. conifers) over others (i.e. deciduous trees) can increase wildfire risk, especially if these trees are also planted in monocultures.[302][303]
Some invasive species, moved in by humans (i.e., for the pulp and paper industry) have in some cases also increased the intensity of wildfires. Examples include species such as Eucalyptus in California[304][305] and gamba grass in Australia.

Wildfires have a place in many cultures. "To spread like wildfire" is a common idiom in English, meaning something that "quickly affects or becomes known by more and more people".[306]

Wildfire activity has been attributed as a major factor in the development of Ancient Greece. In modern Greece, as in many other regions, it is the most common disaster caused by a natural hazard and figures prominently in the social and economic lives of its people.[307]

In 1937, U.S. President Franklin D. Roosevelt initiated a nationwide fire prevention campaign, highlighting the role of human carelessness in forest fires. Later posters of the program featured Uncle Sam, characters from the Disney movie Bambi, and the official mascot of the U.S. Forest Service, Smokey Bear.[308] The Smokey Bear fire prevention campaign has yielded one of the most popular characters in the United States; for many years there was a living Smokey Bear mascot, and it has been commemorated on postage stamps.[309]

There are also significant indirect or second-order societal impacts from wildfire, such as demands on utilities to prevent power transmission equipment from becoming ignition sources, and the cancelation or nonrenewal of homeowners insurance for residents living in wildfire-prone areas.[310]

{{Columns-list|colwidth=45em|

AttributionWind power is the use of wind energy to generate useful work. Historically, wind power was used by sails, windmills and windpumps, but today it is mostly used to generate electricity. This article deals only with wind power for electricity generation.
Today, wind power is generated almost completely using wind turbines, generally grouped into wind farms and connected to the electrical grid.

In 2024, wind supplied over 2,494 TWh of electricity, which was 8.1% of world electricity.[1]
With about 100 GW added during 2021, mostly in China and the United States, global installed wind power capacity exceeded 800 GW.[2][3][4] 30 countries generated more than a tenth of their electricity from wind power in 2024 and wind generation has nearly tripled since 2015.[1] To help meet the Paris Agreement goals to limit climate change, analysts say it should expand much faster – by over 1% of electricity generation per year.[5]

Wind power is considered a sustainable, renewable energy source, and has a much smaller impact on the environment compared to burning fossil fuels. Wind power is variable, so it needs energy storage or other dispatchable generation energy sources to attain a reliable supply of electricity. Land-based (onshore) wind farms have a greater visual impact on the landscape than most other power stations per energy produced.[6][7] Wind farms sited offshore have less visual impact and have higher capacity factors, although they are generally more expensive.[2] Offshore wind power currently has a share of about 10% of new installations.[8]

Wind power is one of the lowest-cost electricity sources per unit of energy produced. 
In many locations, new onshore wind farms are cheaper than new coal or gas plants.[9]

Regions in the higher northern and southern latitudes have the highest potential for wind power.[10] In most regions, wind power generation is higher in nighttime, and in winter when solar power output is low. For this reason, combinations of wind and solar power are suitable in many countries.[11]

Wind is air movement in the Earth's atmosphere. In a unit of time, say 1 second, the volume of air that had passed an area 



A


{\displaystyle A}

 is 



A
v


{\displaystyle Av}

. If the air density is 



ρ


{\displaystyle \rho }

, the flow rate of this volume of air is 






M

Δ
t




=
ρ
A
v


{\displaystyle {\tfrac {M}{\Delta t}}=\rho Av}

, and the power transfer, or energy transfer per second is 



P
=



1
2






M

Δ
t





v

2


=



1
2



ρ
A

v

3




{\displaystyle P={\tfrac {1}{2}}{\tfrac {M}{\Delta t}}v^{2}={\tfrac {1}{2}}\rho Av^{3}}

. Wind power is thus proportional to the third power of the wind speed; the available power increases eightfold when the wind speed doubles. Change of wind speed by a factor of 2.1544 increases the wind power by one order of magnitude (multiply by 10).

The global wind kinetic energy averaged approximately 1.50 MJ/m2 over the period from 1979 to 2010, 1.31 MJ/m2 in the Northern Hemisphere with 1.70 MJ/m2 in the Southern Hemisphere. The atmosphere acts as a thermal engine, absorbing heat at higher temperatures, releasing heat at lower temperatures. The process is responsible for the production of wind kinetic energy at a rate of 2.46 W/m2 thus sustaining the circulation of the atmosphere against friction.[14]

Through wind resource assessment, it is possible to estimate wind power potential globally, by country or region, or for a specific site. The Global Wind Atlas provided by the Technical University of Denmark in partnership with the World Bank provides a global assessment of wind power potential.[12][15][16]
Unlike 'static' wind resource atlases which average estimates of wind speed and power density across multiple years, tools such as Renewables.ninja provide time-varying simulations of wind speed and power output from different wind turbine models at an hourly resolution.[17] More detailed, site-specific assessments of wind resource potential can be obtained from specialist commercial providers, and many of the larger wind developers have in-house modeling capabilities.

The total amount of economically extractable power available from the wind is considerably more than present human power use from all sources.[18] The strength of wind varies, and an average value for a given location does not alone indicate the amount of energy a wind turbine could produce there.

To assess prospective wind power sites, a probability distribution function is often fit to the observed wind speed data.[19] Different locations will have different wind speed distributions. The Weibull model closely mirrors the actual distribution of hourly/ten-minute wind speeds at many locations. The Weibull factor is often close to 2 and therefore a Rayleigh distribution can be used as a less accurate, but simpler model.[20]

A wind farm is a group of wind turbines in the same location. A large wind farm may consist of several hundred individual wind turbines distributed over an extended area. The land between the turbines may be used for agricultural or other purposes. A wind farm may also be located offshore. Almost all large wind turbines have the same design — a horizontal axis wind turbine having an upwind rotor with 3 blades, attached to a nacelle on top of a tall tubular tower.

In a wind farm, individual turbines are interconnected with a medium voltage (often 34.5 kV) power collection system[25] and communications network. In general, a distance of 7D (7 times the rotor diameter of the wind turbine) is set between each turbine in a fully developed wind farm.[26] At a substation, this medium-voltage electric current is increased in voltage with a transformer for connection to the high voltage electric power transmission system.[27]

Most modern turbines use variable speed generators combined with either a partial or full-scale power converter between the turbine generator and the collector system, which generally have more desirable properties for grid interconnection and have low voltage ride through-capabilities.[28] Modern turbines use either doubly fed electric machines with partial-scale converters or squirrel-cage induction generators or synchronous generators (both permanently and electrically excited) with full-scale converters.[29] Black start is possible[30] and is being further developed for places (such as Iowa) which generate most of their electricity from wind.[31]

Transmission system operators will supply a wind farm developer with a grid code to specify the requirements for interconnection to the transmission grid. This will include the power factor, the constancy of frequency, and the dynamic behaviour of the wind farm turbines during a system fault.[32][33]

Offshore wind power is wind farms in large bodies of water, usually the sea. These installations can use the more frequent and powerful winds that are available in these locations and have less visual impact on the landscape than land-based projects. However, the construction and maintenance costs are considerably higher.[35][36]

As of November 2021, the Hornsea Wind Farm in the United Kingdom is the largest offshore wind farm in the world at 1,218 MW.[37]

Near offshore wind farms may be connected by AC and far offshore by HVDC.[38]

Wind power resources are not always located near areas with a high population density. As transmission lines become longer, the losses associated with power transmission increase, as modes of losses at lower lengths are exacerbated and new modes of losses are no longer negligible as the length is increased; making it harder to transport large loads over large distances.[39]

When the transmission capacity does not meet the generation capacity, wind farms are forced to produce below their full potential or stop running altogether, in a process known as curtailment. While this leads to potential renewable generation left untapped, it prevents possible grid overload or risk to reliable service.[40]

One of the biggest current challenges to wind power grid integration in some countries is the necessity of developing new transmission lines to carry power from wind farms, usually in remote lowly populated areas due to availability of wind, to high load locations, usually on the coasts where population density is higher.[41] Any existing transmission lines in remote locations may not have been designed for the transport of large amounts of energy.[42] In particular geographic regions, peak wind speeds may not coincide with peak demand for electrical power, whether offshore or onshore. A possible future option may be to interconnect widely dispersed geographic areas with an HVDC super grid.[43]

In 2020, wind supplied almost 1600 TWh of electricity, which was over 5% of worldwide electrical generation and about 2% of energy consumption.[47][3] With over 100 GW added during 2020, mostly in China, global installed wind power capacity reached more than 730 GW.[2][3] But to help meet the Paris Agreement's goals to limit climate change, analysts say it should expand much faster – by over 1% of electricity generation per year.[5] Expansion of wind power is being hindered by fossil fuel subsidies.[48][49][50]

The actual amount of electric power that wind can generate is calculated by multiplying the nameplate capacity by the capacity factor, which varies according to equipment and location. Estimates of the capacity factors for wind installations are in the range of 35% to 44%.[51]

Since wind speed is not constant, a wind farm's annual energy production is never as much as the sum of the generator nameplate ratings multiplied by the total hours in a year. The ratio of actual productivity in a year to this theoretical maximum is called the capacity factor. Online data is available for some locations, and the capacity factor can be calculated from the yearly output.[52][53]

Wind energy penetration is the fraction of energy produced by wind compared with the total generation. Wind power's share of worldwide electricity usage in 2021 was almost 7%,[55] up from 3.5% in 2015.[56][57]

There is no generally accepted maximum level of wind penetration. The limit for a particular grid will depend on the existing generating plants, pricing mechanisms, capacity for energy storage, demand management, and other factors. An interconnected electric power grid will already include reserve generating and transmission capacity to allow for equipment failures. This reserve capacity can also serve to compensate for the varying power generation produced by wind stations. Studies have indicated that 20% of the total annual electrical energy consumption may be incorporated with minimal difficulty.[58] These studies have been for locations with geographically dispersed wind farms, some degree of dispatchable energy or hydropower with storage capacity, demand management, and interconnected to a large grid area enabling the export of electric power when needed. Electrical utilities continue to study the effects of large-scale penetration of wind generation on system stability.[59]

A wind energy penetration figure can be specified for different duration of time but is often quoted annually. To generate almost all electricity from wind annually requires substantial interconnection to other systems, for example some wind power in Scotland is sent to the rest of the British grid.[60] On a monthly, weekly, daily, or hourly basis—or less—wind might supply as much as or more than 100% of current use, with the rest stored, exported or curtailed. The seasonal industry might then take advantage of high wind and low usage times such as at night when wind output can exceed normal demand. Such industry might include the production of silicon, aluminum,[61] steel, or natural gas, and hydrogen, and using future long-term storage to facilitate 100% energy from variable renewable energy.[62][63][better source needed] Homes and businesses can also be programmed to vary electricity demand,[64][65] for example by remotely turning up water heater thermostats.[66]

Wind power is variable, and during low wind periods, it may need to be replaced by other power sources. Transmission networks presently cope with outages of other generation plants and daily changes in electrical demand, but the variability of intermittent power sources such as wind power is more frequent than those of conventional power generation plants which, when scheduled to be operating, may be able to deliver their nameplate capacity around 95% of the time.

Electric power generated from wind power can be highly variable at several different timescales: hourly, daily, or seasonally. Annual variation also exists but is not as significant.[citation needed] Because instantaneous electrical generation and consumption must remain in balance to maintain grid stability, this variability can present substantial challenges to incorporating large amounts of wind power into a grid system. Intermittency and the non-dispatchable nature of wind energy production can raise costs for regulation, incremental operating reserve, and (at high penetration levels) could require an increase in the already existing energy demand management, load shedding, storage solutions, or system interconnection with HVDC cables.

Fluctuations in load and allowance for the failure of large fossil-fuel generating units require operating reserve capacity, which can be increased to compensate for the variability of wind generation.

Utility-scale batteries are often used to balance hourly and shorter timescale variation,[67][68] but car batteries may gain ground from the mid-2020s.[69] Wind power advocates argue that periods of low wind can be dealt with by simply restarting existing power stations that have been held in readiness, or interlinking with HVDC.[70]

The combination of diversifying variable renewables by type and location, forecasting their variation, and integrating them with dispatchable renewables, flexible fueled generators, and demand response can create a power system that has the potential to meet power supply needs reliably. Integrating ever-higher levels of renewables is being successfully demonstrated in the real world.[71]

Solar power tends to be complementary to wind.[73][74] On daily to weekly timescales, high-pressure areas tend to bring clear skies and low surface winds, whereas low-pressure areas tend to be windier and cloudier. On seasonal timescales, solar energy peaks in summer, whereas in many areas wind energy is lower in summer and higher in winter.[A][75] Thus the seasonal variation of wind and solar power tend to cancel each other somewhat.[72] Wind hybrid power systems are becoming more popular.[76]

For any particular generator, there is an 80% chance that wind output will change less than 10% in an hour and a 40% chance that it will change 10% or more in 5 hours.[77]

In summer 2021, wind power in the United Kingdom fell due to the lowest winds in seventy years,[78] In the future, smoothing peaks by producing green hydrogen may help when wind has a larger share of generation.[79]

While the output from a single turbine can vary greatly and rapidly as local wind speeds vary, as more turbines are connected over larger and larger areas the average power output becomes less variable and more predictable.[28][80] Weather forecasting permits the electric-power network to be readied for the predictable variations in production that occur.[81]

It is thought that the most reliable low-carbon electricity systems will include a large share of wind power.[82]

Typically, conventional hydroelectricity complements wind power very well. When the wind is blowing strongly, nearby hydroelectric stations can temporarily hold back their water. When the wind drops they can, provided they have the generation capacity, rapidly increase production to compensate. This gives a very even overall power supply and virtually no loss of energy and uses no more water.

Alternatively, where a suitable head of water is not available, pumped-storage hydroelectricity or other forms of grid energy storage such as compressed air energy storage and thermal energy storage can store energy developed by high-wind periods and release it when needed. The type of storage needed depends on the wind penetration level – low penetration requires daily storage, and high penetration requires both short- and long-term storage – as long as a month or more.[citation needed] Stored energy increases the economic value of wind energy since it can be shifted to displace higher-cost generation during peak demand periods. The potential revenue from this arbitrage can offset the cost and losses of storage. Although pumped-storage power systems are only about 75% efficient and have high installation costs, their low running costs and ability to reduce the required electrical base-load can save both fuel and total electrical generation costs.[83][84]

The energy needed to build a wind farm divided into the total output over its life, Energy Return on Energy Invested, of wind power varies, but averages about 20–25.[85][86] Thus, the energy payback time is typically around a year.

Onshore wind is an inexpensive source of electric power, cheaper than coal plants and new gas plants.[9] According to BusinessGreen, wind turbines reached grid parity (the point at which the cost of wind power matches traditional sources) in some areas of Europe in the mid-2000s, and in the US around the same time. Falling prices continue to drive the Levelized cost down and it has been suggested that it has reached general grid parity in Europe in 2010, and will reach the same point in the US around 2016 due to an expected reduction in capital costs of about 12%.[88][needs update] In 2021, the CEO of Siemens Gamesa warned that increased demand for low-cost wind turbines combined with high input costs and high costs of steel result in increased pressure on the manufacturers and decreasing profit margins.[89]

Northern Eurasia, Canada, some parts of the United States, and Patagonia in Argentina are the best areas for onshore wind: whereas in other parts of the world solar power, or a combination of wind and solar, tend to be cheaper.[90]: 8 

Wind power is capital intensive but has no fuel costs.[91] The price of wind power is therefore much more stable than the volatile prices of fossil fuel sources.[92] However, the estimated average cost per unit of electric power must incorporate the cost of construction of the turbine and transmission facilities, borrowed funds, return to investors (including the cost of risk), estimated annual production, and other components, averaged over the projected useful life of the equipment, which may be more than 20 years. Energy cost estimates are highly dependent on these assumptions so published cost figures can differ substantially.

The presence of wind energy, even when subsidized, can reduce costs for consumers (€5 billion/yr in Germany) by reducing the marginal price and by minimizing the use of expensive peaking power plants.[93]

The cost has decreased as wind turbine technology has improved. There are now longer and lighter wind turbine blades, improvements in turbine performance, and increased power generation efficiency. Also, wind project capital expenditure costs and maintenance costs have continued to decline.[94]

In 2021, a Lazard study of unsubsidized electricity said that wind power levelized cost of electricity continues to fall but more slowly than before. The study estimated new wind-generated electricity cost from $26 to $50/MWh, compared to new gas power from $45 to $74/MWh. The median cost of fully deprecated existing coal power was $42/MWh, nuclear $29/MWh and gas $24/MWh. The study estimated offshore wind at around $83/MWh. Compound annual growth rate was 4% per year from 2016 to 2021, compared to 10% per year from 2009 to 2021.[9]

While the levelised costs of wind power may have reached that of traditional combustion based power technologies, the market value of the generated power is also lower due to the merit order effect, which implies that electricity market prices are lower in hours with substantial generation of variable renewable energy due to the low marginal costs of this technology.[95] The effect has been identified in several European markets.[96] For wind power plants exposed to electricity market pricing in markets with high penetration of variable renewable energy sources, profitability can be challenged.

Turbine prices have fallen significantly in recent years due to tougher competitive conditions such as the increased use of energy auctions, and the elimination of subsidies in many markets.[97] As of 2021, subsidies are still often given to offshore wind. However, they are generally no longer necessary for onshore wind in countries with even a very low carbon price such as China, provided there are no competing fossil fuel subsidies.[98]

Secondary market forces provide incentives for businesses to use wind-generated power, even if there is a premium price for the electricity. For example, socially responsible manufacturers pay utility companies a premium that goes to subsidize and build new wind power infrastructure. Companies use wind-generated power, and in return, they can claim that they are undertaking strong "green" efforts.[99] Wind projects provide local taxes, or payments in place of taxes and strengthen the economy of rural communities by providing income to farmers with wind turbines on their land.[100][101]

The wind energy sector can also produce jobs during the construction and operating phase.[102] Jobs include the manufacturing of wind turbines and the construction process, which includes transporting, installing, and then maintaining the turbines. An estimated 1.25 million people were employed in wind power in 2020.[103]

Small-scale wind power is the name given to wind generation systems with the capacity to produce up to 50 kW of electrical power.[104] Isolated communities, that may otherwise rely on diesel generators, may use wind turbines as an alternative. Individuals may purchase these systems to reduce or eliminate their dependence on grid electric power for economic reasons, or to reduce their carbon footprint. Wind turbines have been used for household electric power generation in conjunction with battery storage over many decades in remote areas.[105]

Examples of small-scale wind power projects in an urban setting can be found in New York City, where, since 2009, several building projects have capped their roofs with Gorlov-type helical wind turbines. Although the energy they generate is small compared to the buildings' overall consumption, they help to reinforce the building's 'green' credentials in ways that "showing people your high-tech boiler" cannot, with some of the projects also receiving the direct support of the New York State Energy Research and Development Authority.[106]

Grid-connected domestic wind turbines may use grid energy storage, thus replacing purchased electric power with locally produced power when available. The surplus power produced by domestic microgenerators can, in some jurisdictions, be fed into the network and sold to the utility company, producing a retail credit for the microgenerators' owners to offset their energy costs.[107]

Off-grid system users can either adapt to intermittent power or use batteries, photovoltaic, or diesel systems to supplement the wind turbine.[108] Equipment such as parking meters, traffic warning signs, street lighting, or wireless Internet gateways may be powered by a small wind turbine, possibly combined with a photovoltaic system, that charges a small battery replacing the need for a connection to the power grid.[109]

Airborne wind turbines, such as kites, can be used in places at risk of hurricanes, as they can be taken down in advance.[110]

The environmental impact of electricity generation from wind power is minor when compared to that of fossil fuel power.[112] Wind turbines have some of the lowest life-cycle greenhouse-gas emissions of energy sources: far less greenhouse gas is emitted than for the average unit of electricity, so wind power helps limit climate change.[113] Use of engineered wood may allow carbon negative wind power.[114] Wind power consumes no fuel, and emits no local air pollution, unlike fossil fuel power sources. Large scale wind energy systems may contribute to land degradation. 

Onshore wind farms can have a significant visual impact.[115] Due to a very low surface power density and spacing requirements, wind farms typically need to be spread over more land than other power stations.[6][116] Their network of turbines, access roads, transmission lines, and substations can result in "energy sprawl";[7] although land between the turbines and roads can still be used for agriculture.[117][118] Some wind farms are opposed for potentially spoiling protected scenic areas, archaeological landscapes and heritage sites.[119][120][121] A report by the Mountaineering Council of Scotland concluded that wind farms harmed tourism in areas known for natural landscapes and panoramic views.[122]

Habitat loss and fragmentation are the greatest potential impacts on wildlife of onshore wind farms,[7] but the worldwide ecological impact is minimal.[112] Thousands of birds and bats, including rare species, have been killed by wind turbine blades,[123] though wind turbines are responsible for far fewer bird deaths than fossil-fueled power stations when climate change effects are included.[124] Not including these effects, modern wind turbines kill about 0.273 birds per GWh in comparison with 0.200 by coal power plants.[124] The effects of wind turbines on birds can be mitigated with proper wildlife monitoring.[125]

Many wind turbine blades are made of fiberglass, and have a lifetime of 20 years.[126] Blades are hollow: some blades are crushed to reduce their volume and then landfilled.[127] However, as they can take a lot of weight they can be made into long lasting small bridges for walkers or cyclists.[128] Blade end-of-life is complicated,[129] and blades manufactured in the 2020s are more likely to be designed to be completely recyclable.[130]

Wind turbines also generate noise. At a distance of 300 metres (980 ft), this may be around 45 dB, which is slightly louder than a refrigerator. At 1.5 km (1 mi), they become inaudible.[131][132] There are anecdotal reports of negative health effects on people who live very close to wind turbines.[133] Peer-reviewed research has generally not supported these claims.[134][135][136]

Although wind turbines with fixed bases are a mature technology and new installations are generally no longer subsidized,[137][138] floating wind turbines are a relatively new technology so some governments subsidize them, for example to use deeper waters.[139]

Fossil fuel subsidies by some governments are slowing the growth of renewables.[140]

Permitting of wind farms can take years and some governments are trying to speed up – the wind industry says this will help limit climate change and increase energy security[141] – sometimes groups such as fishers resist this[142] but governments say that rules protecting biodiversity will still be followed.[143]

Surveys of public attitudes across Europe and in many other countries show strong public support for wind power.[145][146][147] Bakker et al. (2012) found in their study that residents who did not want turbines built near them suffered significantly more stress than those who "benefited economically from wind turbines".[148]

Although wind power is a popular form of energy generation, onshore or near offshore wind farms are sometimes opposed for their impact on the landscape (especially scenic areas, heritage areas and archaeological landscapes), as well as noise, and impact on tourism.[149][150]

In other cases, there is direct community ownership of wind farms. The hundreds of thousands of people who have become involved in Germany's small and medium-sized wind farms demonstrate such support there.[151]

A 2010 Harris Poll found strong support for wind power in Germany, other European countries, and the United States.[145][146][152]

Public support in the United States has decreased from 75% in 2020 to 62% in 2021, with the Democratic Party supporting the use of wind energy twice as much as the Republican Party.[153] President Biden has signed an executive order to begin building large scale wind farms.[154]

In China, Shen et al. (2019) found that Chinese city-dwellers may be resistant to building wind turbines in urban areas, with a surprisingly high proportion of people citing an unfounded fear of radiation as driving their concerns.[155] Also, the study finds that like their counterparts in OECD countries, urban Chinese respondents are sensitive to direct costs and wildlife externalities. Distributing relevant information about turbines to the public may alleviate resistance.

Many wind power companies work with local communities to reduce environmental and other concerns associated with particular wind farms.[158][159][160]
In other cases there is direct community ownership of wind farm projects. Appropriate government consultation, planning and approval procedures also help to minimize environmental risks.[145][161][162]
Some may still object to wind farms[163] but many say their concerns should be weighed against the need to address the threats posed by air pollution,[164][113] climate change[165] and the opinions of the broader community.[166]

In the US, wind power projects are reported to boost local tax bases, helping to pay for schools, roads, and hospitals, and to revitalize the economies of rural communities by providing steady income to farmers and other landowners.[100]

In the UK, both the National Trust and the Campaign to Protect Rural England have expressed concerns about the effects on the rural landscape caused by inappropriately sited wind turbines and wind farms.[167][168]

Some wind farms have become tourist attractions. The Whitelee Wind Farm Visitor Centre has an exhibition room, a learning hub, a café with a viewing deck and also a shop. It is run by the Glasgow Science Centre.[169]

In Denmark, a loss-of-value scheme gives people the right to claim compensation for loss of value of their property if it is caused by proximity to a wind turbine. The loss must be at least 1% of the property's value.[170]

Despite this general support for the concept of wind power in the public at large, local opposition often exists and has delayed or aborted a number of projects.[171][172][173]
As well as concerns about the landscape, there are concerns that some installations can produce excessive sound and vibration levels leading to a decrease in property values.[174] A study of 50,000 home sales near wind turbines found no statistical evidence that prices were affected.[175]

While aesthetic issues are subjective and some find wind farms pleasant and optimistic, or symbols of energy independence and local prosperity, protest groups are often formed to attempt to block some wind power stations for various reasons.[163][176][177]

Some opposition to wind farms is dismissed as NIMBYism,[178] but research carried out in 2009 found that there is little evidence to support the belief that residents only object to wind farms because of a "Not in my Back Yard" attitude.[179]

Wind cannot be cut off unlike oil and gas so can contribute to energy security.[180]

Wind turbines are devices that convert the wind's kinetic energy into electrical power. The result of over a millennium of windmill development and modern engineering, today's wind turbines are manufactured in a wide range of horizontal axis and vertical axis types. The smallest turbines are used for applications such as battery charging for auxiliary power. Slightly larger turbines can be used for making small contributions to a domestic power supply while selling unused power back to the utility supplier via the electrical grid. Arrays of large turbines, known as wind farms, have become an increasingly important source of renewable energy and are used in many countries as part of a strategy to reduce their reliance on fossil fuels.

Wind turbine design is the process of defining the form and specifications of a wind turbine to extract energy from the wind.[181]
A wind turbine installation consists of the necessary systems needed to capture the wind's energy, point the turbine into the wind, convert mechanical rotation into electrical power, and other systems to start, stop, and control the turbine.

In 1919, the German physicist Albert Betz showed that for a hypothetical ideal wind-energy extraction machine, the fundamental laws of conservation of mass and energy allowed no more than 16/27 (59%) of the kinetic energy of the wind to be captured. This Betz limit can be approached in modern turbine designs, which may reach 70 to 80% of the theoretical Betz limit.[182][183]

The aerodynamics of a wind turbine are not straightforward. The airflow at the blades is not the same as the airflow far away from the turbine. The very nature of how energy is extracted from the air also causes air to be deflected by the turbine. This affects the objects or other turbines downstream, which is known as "wake effect". Also, the aerodynamics of a wind turbine at the rotor surface exhibit phenomena that are rarely seen in other aerodynamic fields. The shape and dimensions of the blades of the wind turbine are determined by the aerodynamic performance required to efficiently extract energy from the wind, and by the strength required to resist the forces on the blade.[184]

In addition to the aerodynamic design of the blades, the design of a complete wind power system must also address the design of the installation's rotor hub, nacelle, tower structure, generator, controls, and foundation.[185]

Wind power has been used as long as humans have put sails into the wind. Wind-powered machines used to grind grain and pump water, the windmill and wind pump, were developed in what is now Iran, Afghanistan, and Pakistan by the 9th century.[186][187] Wind power was widely available and not confined to the banks of fast-flowing streams, or later, requiring sources of fuel. Wind-powered pumps drained the polders of the Netherlands, and in arid regions such as the American mid-west or the Australian outback, wind pumps provided water for livestock and steam engines.

The first wind turbine used for the production of electric power was built in Scotland in July 1887 by Prof James Blyth of Anderson's College, Glasgow (the precursor of Strathclyde University).[188] Blyth's 10 metres (33 ft) high cloth-sailed wind turbine was installed in the garden of his holiday cottage at Marykirk in Kincardineshire, and was used to charge accumulators developed by the Frenchman Camille Alphonse Faure, to power the lighting in the cottage,[188] thus making it the first house in the world to have its electric power supplied by wind power.[189] Blyth offered the surplus electric power to the people of Marykirk for lighting the main street, however, they turned down the offer as they thought electric power was "the work of the devil."[188] Although he later built a wind turbine to supply emergency power to the local Lunatic Asylum, Infirmary, and Dispensary of Montrose, the invention never really caught on as the technology was not considered to be economically viable.[188]

Across the Atlantic, in Cleveland, Ohio, a larger and heavily engineered machine was designed and constructed in the winter of 1887–1888 by Charles F. Brush.[190] This was built by his engineering company at his home and operated from 1886 until 1900.[191] The Brush wind turbine had a rotor 17 metres (56 ft) in diameter and was mounted on an 18-metre (59ft) tall tower. Although large by today's standards, the machine was only rated at 12 kW. The connected dynamo was used either to charge a bank of batteries or to operate up to 100 incandescent light bulbs, three arc lamps, and various motors in Brush's laboratory.[192]
With the development of electric power, wind power found new applications in lighting buildings remote from centrally generated power. Throughout the 20th century parallel paths developed small wind stations suitable for farms or residences. 
From 1932 many isolated properties in Australia ran their lighting and electric fans from batteries, charged by a "Freelite" wind-driven generator, producing 100 watts of electrical power from as little wind speed as 10 miles per hour (16 km/h).[193]

The 1973 oil crisis triggered the investigation in Denmark and the United States that led to larger utility-scale wind generators that could be connected to electric power grids for remote use of power. By 2008, the U.S. installed capacity had reached 25.4 gigawatts, and by 2012 the installed capacity was 60 gigawatts.[194] Today, wind-powered generators operate in every size range, from tiny stations for battery charging at isolated residences, up to gigawatt-sized offshore wind farms that provide electric power to national electrical networks. The European Union is working to augment these prospects.[195]

In 2023, the global wind power sector experienced significant growth, with 116.6 gigawatts (GW) of new capacity added to the power grid, representing a 50% increase over the amount added in 2022. This surge in capacity brought the total installed wind power capacity worldwide to 1,021 GW by the end of the year, marking a growth of 13% compared to the previous year.[196]: 138The Winter Olympic Games (French: Jeux olympiques d'hiver)[a], also known as the Winter Olympics, is a major international multi-sport event held once every four years for sports practiced on snow and ice. The first Winter Olympic Games, the 1924 Winter Olympics, were held in Chamonix, France. The modern Olympic Games were inspired by the ancient Olympic Games, which were held in Olympia, Greece, from 776 BCE to 394 CE. The Baron Pierre de Coubertin of France founded the International Olympic Committee (IOC) 1,500 years later in 1894, leading to the first modern Summer Olympic Games in Athens, Greece in 1896. The IOC is the governing body of the Olympic Movement, with the Olympic Charter defining its structure and authority.
The original five Winter Olympic Sports (consisting of nine disciplines) were bobsleigh, curling, ice hockey, Nordic skiing (consisting of the disciplines military patrol,[b] cross-country skiing, Nordic combined, and ski jumping), and skating (consisting of the disciplines figure skating and speed skating).[c] The Games were held every four years from 1924 to 1936, interrupted in 1940 and 1944 by World War II, and resumed in 1948. Until 1992, the Summer Olympic Games and the Winter Olympic Games were held in the same year. A decision to change this was made in 1986, when during the 91st International Olympic Committee session, IOC members decided to alternate the Summer Olympic Games and the Winter Olympic Games on separate four-year cycles in even-numbered years. Also, at that same congress it was decided that 1992 Winter Olympics would be the last to be held in the same year as the Summer Games and that to change the rotation, the games that would be held in 1996 would be brought forward by two years, being scheduled to 1994. After those games, the next were to be held in 1998 when the four-year Olympic Cycle resumed.[2][3]

The Winter Olympic Games have evolved since their inception. Sports and disciplines have been added and some of them, such as alpine skiing, luge, short track speed skating, freestyle skiing, skeleton, and snowboarding, have earned a permanent spot on the Olympic program. Some others, including curling and bobsleigh, have been discontinued and later reintroduced; others have been permanently discontinued, such as military patrol, though the modern Winter Olympic sport of biathlon is descended from it.[b] Still others, such as speed skiing, bandy and skijoring, were demonstration sports but never incorporated as Olympic sports. The rise of television as a global medium for communication enhanced the profile of the Games. It generated income via the sale of broadcast rights and advertising, which has become lucrative for the IOC. This allowed outside interests, such as television companies and corporate sponsors, to exert influence. The IOC has had to address numerous criticisms over the decades like internal scandals, the use of performance-enhancing drugs by Winter Olympians, as well as a political boycott of the Winter Olympic Games. Countries have used the Winter Olympic Games as well as the Summer Olympic Games to proclaim the superiority of their political systems.

The Winter Olympic Games have been hosted on three continents by thirteen countries, all of whom are located in the Northern Hemisphere. They have been held four times in the United States (1932, 1960, 1980, and 2002), three times in France (1924, 1968, and 1992) and twice each in Switzerland (1928 and 1948), Austria (1964 and 1976), Norway (1952 and 1994), Japan (1972 and 1998), Italy (1956 and 2006) and Canada (1988 and 2010). Also, the Winter Olympic Games have been held just once each in Germany (1936), Yugoslavia (1984), Russia (2014), South Korea (2018), and China (2022). The IOC has selected the Italian cities of Milan and Cortina d'Ampezzo to host the 2026 Winter Olympics.[8][9]  The Winter Olympics are usually held in February, during the winter season of the Northern Hemisphere. As of 2024[update], no city in the Southern Hemisphere has applied to host the Winter Olympic Games in the month of August (during the winter months of the Southern Hemisphere).

As of 2022[update], twelve countries have participated in every Winter Olympic Games – Austria, Canada, Finland, France, Great Britain, Hungary, Italy, Norway, Poland, Sweden, Switzerland, and the United States. Also, Czechoslovakia participated in all Winter Olympic Games before its dissolution and its successors, Czech Republic and Slovakia have participated in all Winter Games thereafter. Six of these countries have won medals at every Winter Olympic Games – Austria, Canada, Finland, Norway, Sweden, and the United States. The only country to have won a gold medal at every Winter Olympic Games is the United States. Norway leads the all-time medal record for the Winter Olympic Games. When including defunct states, Germany (comprising the former countries of West Germany and East Germany) leads, followed by Norway, Russia (including the former Soviet Union), and the United States.

 A predecessor, the Nordic Games, were organised by General Viktor Gustaf Balck in Stockholm, Sweden, in 1901 and were held again in 1903 and 1905 and then every fourth year thereafter until 1926.[10] Balck was a charter member of the IOC and a close friend of Olympic Games founder Pierre de Coubertin. He attempted to have winter sports, specifically figure skating, added to the Olympic programme but was unsuccessful until the 1908 Summer Olympics in London.[10] Four figure skating events were contested, at which Ulrich Salchow (10-time world champion) and Madge Syers won the individual titles.[11][12]

Three years later, Italian count Eugenio Brunetta d'Usseaux proposed that the IOC stage a week of winter sports included as part of the 1912 Summer Olympics in Stockholm, Sweden. The organisers opposed this idea because they desired to protect the integrity of the Nordic Games and were concerned about a lack of facilities for winter sports.[13][14][15]

The idea was resurrected for the 1916 Games, which were to be held in Berlin, Germany. A winter sports week with speed skating, figure skating, ice hockey and Nordic skiing were planned, but the 1916 Olympics was cancelled after the outbreak of World War I.[14]

The first Olympics after the war, the 1920 Summer Olympics, were held in Antwerp, Belgium,[16] and featured figure skating[17] and an ice hockey tournament. Germany, Austria, Hungary, Bulgaria and Turkey were banned from competing in the games. At the IOC Congress held the following year it was decided that the host nation of the 1924 Summer Olympics, France, would host a separate "International Winter Sports Week" under the patronage of the IOC. Chamonix was chosen to host this week (actually 11 days) of events.

The 1924 games in Chamonix proved to be a success when more than 250 athletes from 16 nations competed in 16 events.[18] Athletes from Finland and Norway won 28 medals, more than the rest of the participating nations combined.[19] The first gold medal awarded was won by Charles Jewtraw of the United States in the 500-meter speed skate. Sonja Henie of Norway, at just 11 years old, competed in the ladies' figure skating and, although finishing last, became popular with fans. Gillis Grafström of Sweden defended his 1920 gold medal[17] in men's figure skating, becoming the first Olympian to win gold medals in both Summer and Winter Olympics.[20] 
Germany remained banned until 1925, and instead hosted a series of games called Deutsche Kampfspiele, starting with the winter edition of 1922 (which predated the first Winter Olympics). In 1925 the IOC decided to create a separate winter event and the 1924 games in Chamonix were retroactively designated as the first Winter Olympics.[14][18]

St. Moritz, Switzerland, was appointed by the IOC to host the second Winter Games in 1928.[21] Fluctuating weather conditions challenged the hosts. The opening ceremony was held in a blizzard while warm weather conditions plagued sporting events throughout the rest of the games.[22] Because of the weather the 10,000 metre speed-skating event had to be abandoned and officially cancelled.[23] The weather was not the only noteworthy aspect of the 1928 games: Sonja Henie of Norway returned to the Winter Olympics to make history when she won the ladies' figure skating at the age of 15. She became the youngest Olympic champion in history, a distinction she held for 70 years, and went on to defend her title at the next two Winter Olympics.[24] Gillis Grafström won his third consecutive figure skating gold[25] and went on to win silver in 1932,[26] becoming the most decorated men's figure skater to date.

The next Winter Olympics, held in Lake Placid, New York, United States was the first to be hosted outside of Europe. Seventeen nations and 252 athletes participated.[27] This was less than in 1928, as the journey to Lake Placid was too long and expensive for some European nations that encountered financial problems in the midst of the Great Depression. The athletes competed in fourteen events in four sports.[27] Virtually no snow fell for two months before the Games, and there was not enough snow to hold all the events until mid-January.[28] Sonja Henie defended her Olympic title,[26] and Eddie Eagan of the United States, who had been an Olympic champion in boxing in 1920,[29] won the gold medal in the men's bobsleigh event[30] to join Gillis Grafström as the only athletes to have won gold medals in both the Summer and Winter Olympics.[27] Eagan has the distinction as the only Olympian as of 2020 to accomplish this feat in different sports.[31]

The German towns of Garmisch and Partenkirchen joined to organise the 1936 Winter Games, held from 6–16 February.[32] This was the last time the Summer and Winter Olympics were held in the same country in the same year. Alpine skiing made its Olympic debut, but skiing teachers were barred from entering because they were considered to be professionals.[33] Because of this decision the Swiss and Austrian skiers refused to compete at the games.[33]

World War II interrupted the Winter Olympics. The 1940 games had been awarded to Sapporo, Japan, but the decision was rescinded in 1938 because of the Japanese invasion of China. The games were then to be held at Garmisch-Partenkirchen, Germany, but the 1940 games were cancelled following the German invasion of Poland in 1939.[34] Due to the ongoing war, the 1944 games, originally scheduled for Cortina D'Ampezzo, Italy, were cancelled.[35]

St. Moritz was selected to host the first post-war games, in 1948. Switzerland's neutrality had protected the town during World War II, and most venues from the 1928 games remained in place, which made St. Moritz a logical choice. It became the first city to host a Winter Olympics twice.[36] Twenty-eight countries competed in Switzerland, but athletes from Germany and Japan were not invited.[37] Controversy erupted when two hockey teams from the United States arrived, both claiming to be the legitimate U.S. Olympic hockey representative. The Olympic flag presented at the 1920 Summer Olympics in Antwerp was stolen, as was its replacement. There was unprecedented parity at these games, during which 10 countries won gold medals—more than any games to that point.[38]

The Olympic Flame tradition was introduced at the 1952 games in Oslo, when the flame was lit in the fireplace by Norwegian skiing pioneer Sondre Nordheim and the first Winter torch relay was conducted by 94 torchbearers entirely on their skis.[39][40] Bandy, a popular sport in the Nordic countries, was featured as a demonstration sport, though only Norway, Sweden, and Finland fielded teams. Norwegian athletes won 17 medals, which outpaced all the other nations.[41] They were led by Hjalmar Andersen who won three gold medals in four events in the speed skating competition.[42]

After not being able to host the games in 1944, Cortina d'Ampezzo was selected to organise the 1956 Winter Olympics. At the opening ceremonies the final torchbearer, Guido Caroli, entered the Olympic Stadium on ice skates. As he skated around the stadium his skate caught on a cable and he fell and burned his arm, nearly extinguishing the flame. He was able to recover and light the cauldron.[43] These were the first Winter Games to be televised, and the first Olympics ever broadcast to an international audience, though no television rights were sold until the 1960 Summer Olympics in Rome.[44] The Cortina games were used to test the feasibility of televising large sporting events.[44]

The Soviet Union made its Olympic debut and had an immediate impact, winning more medals than any other nation.[45] The Soviets' immediate success might be explained by the advent of the state-sponsored "full-time amateur athlete". The USSR entered teams of athletes who were all nominally students, soldiers, or working in a profession, but many of whom were in reality paid by the state to train full-time.[46][47] Chiharu Igaya won the first Winter Olympics medal for Japan and the continent of Asia when he placed second in the slalom.[48]

The IOC awarded the 1960 Olympics to Squaw Valley, United States.[49] 
the announcement of its selection came as a shock as the resort was undeveloped and unknown outside the United States.About US$80,000,000 was spent over four years to build the completely non-existent infrastructure.[50][51] The opening and closing ceremonies were the firsts produced by Walt Disney Company.[52] The Squaw Valley Olympics was the first Winter Games to have a dedicated athletes' village,[53][54] the first to use a computer (courtesy of IBM) to tabulate results, and the first to feature female speed skating events. This edition is the only one to date to not have bobsleigh competitions, as the number of countries registered in the event was insufficient and the costs of building the track were too high for the Organizing Committee.To replace the event, an extra edition from the FIBT World Championship was held.[52]

The Austrian city of Innsbruck was the host in 1964. For the first time, the Olympic torch of the Winter Olympic Games was lit at the traditional ritual held in the temple of Olympia. Although Innsbruck was a traditional winter sports resort, unprojected warm weather caused a lack of snow and organisers were unable to save enough snow to be used during the Games, and the Austrian army was enlisted to transport snow and ice from other places to the sports venues.[52] Soviet speed-skater Lidia Skoblikova made history by winning all four-speed skating events. Her career total of six gold medals set a record for Winter Olympics athletes.[52] Also, for the first time Luge was added to the Olympic program, but the sport received bad publicity when a competitor was killed in a pre-Olympic training run.[55][56]

Held in the French town of Grenoble, the 1968 Winter Olympics were the first Olympic Games to be broadcast in colour. There were 1,158 athletes from 37 nations competing in 35 events.[57] French alpine ski racer Jean-Claude Killy became only the second person to win all the men's alpine skiing events. Due the high interest around the world, the organising committee sold television rights for US$2 million, which was more than twice the cost of the broadcast rights for the Innsbruck Games.[58] For the first time, the organizers chose to decentralize the Games to save costs and the events were spread across three long distances clusters, which led to the need to build three Olympic Villages. Along the high costs, the organisers claimed that this was necessary to accommodate technological advances, however, critics disputed this, alleging that the layout would incorporate the best possible venues for television broadcasts at the athletes' expense.[58]

The 1972 Winter Games, held in Sapporo, Japan,[59] were the first to be hosted on a continent other than North America or Europe. The issue of professionalism was disputed during these Games when a number of alpine skiers were found to have participated in a ski camp at Mammoth Mountain in the United States; three days before the opening ceremony, IOC president Avery Brundage threatened to bar the skiers from competing in the Games as he insisted that they were no longer amateurs having benefited financially from their status as athletes.[60] Eventually only Austrian Karl Schranz, who earned more than the other skiers, was excluded from the competition.[61] Canada boycotted the 1972 and the 1976 ice hockey tournaments in protest at not being able to use players from professional leagues.[62] Canadian authorities also accused the Soviet Union of using state-sponsored athletes, who were de facto professionals.[63] Francisco Fernández Ochoa became the first and, as of 2022, the only Spaniard to win a Winter Olympic gold medal when he triumphed in the slalom.[64]

The 1976 Winter Olympics had initially been awarded in 1970 to Denver, Colorado in the United States. These Games would have coincided with the year of Colorado's centennial and the United States Bicentennial. However, the increasing costs of the event and the oil crisis led to a local plebiscite held in November 1972, that resulted in the city withdrawing from hosting the Games, as the people of Colorado voted against public funding of the Games by a 3:2 margin.[65][66] The IOC responded by offering the Games to Vancouver-Garibaldi, British Columbia in Canada, which had previously been a finalist bid for the 1976 Games. However, a change in the provincial government resulted in an administration that did not support the Olympic bid, so the IOC's offer was rejected.[67]

Salt Lake City, previously a candidate for the 1972 Winter Olympics, then put itself forward, but a tense political situation led IOC to invite Innsbruck to host the 1976 Games, as all the infrastructure used during the 1964 Games had been maintained. Despite only having half the usual time to prepare for the Games, Innsbruck accepted the invitation to replace Denver in February 1973.[67] During the opening ceremonies, two cauldrons were lit because it was the second time that the Austrian town had hosted the Winter Games.[67] The 1976 Games featured the first combination bobsleigh-and-luge track, in neighbouring Igls.[64] The Soviet Union won its fourth consecutive ice hockey gold medal.[67]

In 1980 the Winter Olympics returned to Lake Placid, which had hosted the 1932 Games.Unlike previous editions, Lake Placid had no competitors in this bid process. Cyprus made their Olympic debut at the games. People's Republic of China and the first tropical nation Costa Rica competed for the first at the Winter Games. The Republic of China boycoted the Games, in protest of the IOC's recognition of the People's Republic of China as the only "China", and its request for the Republic of China to compete as "Chinese Taipei". The PRC, on the other hand, returned to the Olympics for the first time since 1952 and made its Winter Olympic debut.[68][69]

American speed-skater Eric Heiden set either an Olympic or World record in every one of the five events in which he competed, winning a total of five individual gold medals and breaking the record for most individual golds in a single Olympics (both Summer and Winter).[70] Hanni Wenzel won both the slalom and giant slalom and her country, Liechtenstein, became the smallest nation to produce an Olympic gold medallist.[71] In the "Miracle on Ice", the American hockey team composed of college players beat the favoured seasoned professionals from the Soviet Union, and progressed to eventually win the gold medal.[72][d]

Sapporo, Japan, and Gothenburg, Sweden, were front-runners to host the 1984 Winter Olympics. It was therefore a surprise when Sarajevo, Yugoslavia, was selected as host.[74] The Games were well-organised and not affected by the run-up to the war that engulfed the country eight years later.[75] A total of 49 nations and 1,272 athletes participated in 39 events. Host nation Yugoslavia won its first Olympic medal when alpine skier Jure Franko won silver in the giant slalom. Another sporting highlight was the free dance performance of British ice dancers Jayne Torvill and Christopher Dean; their Boléro routine received unanimous perfect scores for artistic impression, earning them the gold medal.[75]

In 1988, the Canadian city of Calgary hosted the first Winter Olympics to span three weekends, lasting for a total of 16 days.[76] New events were added in ski-jumping and speed skating, while future Olympic sports curling, short track speed skating and freestyle skiing made their debut appearance as demonstration sports. The speed skating events were held indoors for the first time, on the Olympic Oval. Dutch skater Yvonne van Gennip won three gold medals and set two world records, beating skaters from the favoured East German team in every race.[77]

Her medal total was equalled by Finnish ski jumper Matti Nykänen, who won all three events in his sport. Alberto Tomba, an Italian skier, made his Olympic debut by winning both the giant slalom and slalom. East German Christa Rothenburger won the women's 1,000 metre speed skating event. Seven months later she would earn a silver in track cycling at the Summer Games in Seoul, to become the only athlete to win medals in both a Summer and Winter Olympics in the same year.[76] The 1988 games are well remembered in popular culture from two films based on its events: Cool Runnings about the Jamaican bobsled team; and Eddie the Eagle about British ski jumper Michael Edwards,  who finished last but set a British record of 73.5 metres.

The 1992 Winter Games were the last to be held in the same year as the Summer Games.[78] They were hosted in the French Savoie region, with 18 events held in the city of Albertville and the remaining events spread out over the Savoie.[78] Political changes of the time were reflected in the composition of the Olympic teams competing in France: this was the first Games to be held after the fall of Communism and the fall of the Berlin Wall, and Germany competed as a single nation for the first time since the 1964 Games.[79]

Former Yugoslavian republics Croatia and Slovenia made their debuts as independent nations; most of the former Soviet republics still competed as a single team known as the Unified Team, but the Baltic States made independent appearances for the first time since before World War II.[79] At 16 years old, Finnish ski jumper Toni Nieminen made history by becoming the youngest male Winter Olympic champion.[80] New Zealand skier Annelise Coberger became the first Winter Olympic medallist from the southern hemisphere when she won a silver medal in the women's slalom.

The 1994 Winter Olympics, held in Lillehammer, Norway, were the first Winter Games to be held in a different year from the Summer Games. This change resulted from the decision reached in the 91st IOC Session (1986) to separate the Summer and Winter Games and place them in alternating even-numbered years.[81] Lillehammer is the northernmost city to ever host the Winter Games. It was the second time the Games were held in Norway, after the 1952 Winter Olympics in Oslo, and the first time the Olympic Truce was observed. As a result, after the dissolution of Czechoslovakia in 1993, the Czech Republic and Slovakia made their Olympic debuts.[82]

The women's figure skating competition drew media attention when American skater Nancy Kerrigan was injured on 6 January 1994, in an assault planned by the ex-husband of opponent Tonya Harding.[83] Both skaters competed in the Games, but the gold medal was narrowly won by Oksana Baiul who became Ukraine's first Olympic champion, while Kerrigan won the silver medal.[84][85] Johann Olav Koss of Norway won three gold medals, coming first in all of the distance speed skating events.[86]

13-year-old Kim Yoon-Mi became the youngest-ever Olympic gold medallist when South Korea won the women's 3,000-metre speed skating relay. Bjørn Dæhli of Norway won a medal in four out of five cross-country events, becoming the most decorated Winter Olympian until then. Russia won the most events, with eleven gold medals, while Norway achieved 26 podium finishes, collecting the most medals overall on home ground. Juan Antonio Samaranch described Lillehammer as "the best Olympic Winter Games ever" in his closing ceremony speech.[87]

The 1998 Winter Olympics were held in the Japanese city of Nagano and were the first Games to host more than 2,000 athletes.[88] The National Hockey League allowed its players to participate in the men's ice hockey tournament for the first time, and the Czech Republic won the tournament. Women's ice hockey made its debut, and the United States won the gold medal.[89] Bjørn Dæhlie of Norway won three gold medals in Nordic skiing, becoming the most decorated Winter Olympic athlete, with eight gold medals and twelve medals overall.[88][90] Austrian Hermann Maier survived a crash during the downhill competition and returned to win gold in the super-G and the giant slalom.[88] Tara Lipinski of the United States, aged just 15, became the youngest ever female gold medallist in an individual event when she won the Ladies' Singles, a record that had stood since Sonja Henie of Norway won the same event, also aged 15, in St. Moritz in 1928. New world records were set in speed skating largely due to the introduction of the clap skate.[91]

After a tumultuous host city process, the 2002 Winter Olympics were held in Salt Lake City, United States. 2,399 athletes from 77 National Olympic Committees participated in 78 events in 7 sports.[92] These Games were the first to take place since the September 11 attacks of 2001, which meant a higher degree of security to avoid a terrorist attack. The opening ceremony saw signs of the aftermath of the events of that day, including the flag that flew at Ground Zero, and honour guards of NYPD and FDNY members.[93]

German Georg Hackl won a silver in the singles luge, becoming the first athlete in Olympic history to win medals in the same individual event in five consecutive Olympics.[92] Canada achieved an unprecedented double by winning both the men's and women's ice hockey gold medals.[92] Canada became embroiled with Russia in a controversy that involved the judging of the pairs figure skating competition. The Russian pair of Yelena Berezhnaya and Anton Sikharulidze competed against the Canadian pair of Jamie Salé and David Pelletier for the gold medal.[94]

The Canadians appeared to have skated well enough to win the competition, yet the Russians were awarded the gold. The French judge, Marie-Reine Le Gougne, awarded the gold to the Russians. An investigation revealed that she had been pressured to give the gold to the Russian pair regardless of how they skated; in return, the Russian judge would look favourably on the French entrants in the ice dancing competition.[94]

The IOC decided to award both pairs the gold medal in a second medal ceremony held later in the Games.[95] Australian Steven Bradbury became the first gold medallist from the southern hemisphere when he won the 1,000 metre short-track speed skating event.[96]

The Italian city of Turin hosted the 2006 Winter Olympics. It was the second time that Italy had hosted the Winter Olympic Games. South Korean athletes won 10 medals, including 6 gold in the short-track speed skating events. Sun-Yu Jin won three gold medals while her teammate Hyun-Soo Ahn won three gold medals and a bronze.[97] In the women's Cross-Country team pursuit Canadian Sara Renner broke one of her poles and, when he saw her dilemma, Norwegian coach Bjørnar Håkensmoen decided to lend her a pole. In so doing she was able to help her team win a silver medal in the event at the expense of the Norwegian team, who finished fourth.[97][98]

On winning the Super-G, Kjetil-Andre Aamodt of Norway became the most decorated ski racer of all time with 4 gold and 8 overall medals. He is also the only ski racer to have won the same event at three Olympics, winning the Super-G in 1992, 2002, and 2006. Claudia Pechstein of Germany became the first speed skater to earn nine career medals.[97]

In February 2009, Pechstein tested positive for "blood manipulation" and received a two-year suspension, which she appealed. The Court of Arbitration for Sport upheld her suspension but a Swiss court ruled that she could compete for a spot on the 2010 German Olympic team.[99] This ruling was brought to the Swiss Federal Tribunal, which overturned the lower court's ruling and precluded her from competing in Vancouver.[100]

In 2003, the IOC awarded the 2010 Winter Olympics to Vancouver, thus allowing Canada to host its second Winter Olympics. With a population of more than 2.5 million people Vancouver is the largest metropolitan area to ever host a Winter Olympic Games.[101] Over 2,500 athletes from 82 countries participated in 86 events.[102] The death of Georgian luger Nodar Kumaritashvili in a training run on the day of the opening ceremonies resulted in the Whistler Sliding Centre changing the track layout on safety grounds.[103]

Norwegian cross-country skier Marit Bjørgen won five medals in the six cross-country events on the women's programme. She finished the Olympics with three golds, a silver and a bronze.[104] For the first time, Canada won a gold medal at an Olympic Games it hosted, having failed to do so at both the 1976 Summer Olympics in Montreal and the 1988 Winter Olympics in Calgary. In contrast to the lack of gold medals at these previous Olympics, the Canadian team finished first overall in gold medal wins,[105] and became the first host nation—since Norway in 1952—to lead the gold medal count, with 14 medals. In doing so, it also broke the record for the most gold medals won by a NOC at a single Winter Olympics (the previous was 13, set by the Soviet Union in 1976 and matched by Norway in 2002).[106]

The Vancouver Games were notable for the poor performance of the Russian athletes. From their first Winter Olympics in 1956 to the 2006 Games, a Soviet or Russian delegation had never been outside the top five medal-winning nations, but in 2010 they finished sixth in total medals and eleventh in gold medals. President Dmitry Medvedev called for the resignation of top sports officials immediately after the Games.[107] Russia's disappointing performance at Vancouver is cited as the reason behind the enhancement of an already existing doping scheme alleged to have been in operation at major events such as the 2014 Games at Sochi.[108]

The success of Asian countries stood in stark contrast to the under-performing Russian team, with Vancouver marking a high point for medals won by Asian countries. At the Albertville Games in 1992 the Asian countries had won fifteen medals, three of which were gold. In Vancouver, the total number of medals won by athletes from Asia had increased to thirty-one, with eleven of them being gold. The rise of Asian nations in Winter Olympics sports is due in part to the growth of winter sports programmes and the interest in winter sports in nations such as Kazakhstan, South Korea, Japan and China. These results increased the chances of an Asian city hosting the 2018 Winter Olympics that would be held the following year.[109][110]

Sochi, Russia, was selected as the host city for the 2014 Winter Olympics over Salzburg, Austria, and Pyeongchang, South Korea. This was the first time that Russia had hosted a Winter Olympics.[111] The Games took place from 7 to 23 February 2014.[112] A record 2,800 athletes from 88 countries competed in 98 events. The Olympic Village and Olympic Stadium were located on the Black Sea coast. All of the mountain venues were 50 kilometres (31 mi) away in the alpine region known as Krasnaya Polyana.[111] The Games were the most expensive until the date, with a cost of £30 billion (US$51 billion).

On the snow, Norwegian biathlete Ole Einar Bjørndalen took two golds to bring his total tally of Olympic medals to 13, overtaking his compatriot Bjørn Dæhlie to become the most decorated Winter Olympian of all time. Another Norwegian, cross-country skier Marit Bjørgen took three golds; her total of ten Olympic medals tied her as the female Winter Olympian with most medals, alongside Raisa Smetanina and Stefania Belmondo. Snowboarder Ayumu Hirano became the youngest medallist on snow at the Winter Games when he took a silver in the halfpipe competition at the age of fifteen.[111]

On the ice, the Netherlands team dominated the speed skating events, taking 23 medals, four clean sweeps of the podium places and at least one medal in each of the twelve medal events. Ireen Wüst was their most successful competitor, taking two golds and three silvers. In figure skating, Yuzuru Hanyu became the first skater to break the 100-point barrier in the short programme on the way to winning the gold medal. Among the sledding disciplines, luger Armin Zöggeler took a bronze, becoming the first Winter Olympian to secure a medal in six consecutive Games.[111]

Following their disappointing performance at the 2010 Games, and an investment of £600 million in elite sport, Russia initially topped the medal table, taking 13 gold and 33 total medals.[113] However, Grigory Rodchenkov, the former head of the Russian national anti-doping laboratory, subsequently claimed that he had been involved in doping dozens of Russian competitors for the Games, and that he had been assisted by the Russian Federal Security Service in opening and re-sealing bottles containing urine samples so that samples with banned substances could be replaced with "clean" urine.[114]

A subsequent investigation commissioned by the World Anti-Doping Agency led by Richard McLaren concluded that a state-sponsored doping programme had operated in Russia from "at least late 2011 to 2015" across the "vast majority" of Summer and Winter Olympic sports.[115] On 5 December 2017, the IOC announced that Russia would compete as the Olympic Athletes from Russia at the 2018 Winter Olympics[116] and by the end of 2017 the IOC Disciplinary Commission had disqualified 43 Russian athletes, stripping thirteen medals and knocking Russia from the top of the medal table, thus putting Norway in the lead.[117][118][119] However, nine medals were later returned, meaning that Russia reclaimed first place in the overall medal table, and joint first place with Norway in terms of gold medals.

On 6 July 2011, Pyeongchang, South Korea, was selected to host the 2018 Winter Olympics over Munich, Germany, and Annecy, France.[120] This was the first time that South Korea had been selected to host a Winter Olympics and it was the second time the Olympics were held in the country overall, after the 1988 Summer Olympics in Seoul. The Games took place from 9 to 25 February 2018.[121] More than 2,900 athletes from 92 countries participated in 102 events. The main venue cluster was the Alpensia Resort in Daegwallyeong-myeon, while the ice events are held at Gangneung Olympic Park in Pyeongchang's neighbouring sea-city of Gangneung.

The lead-up to the 2018 Winter Olympics was affected by the tensions between North and South Korea and the ongoing Russian doping scandal. Despite tense relations, North Korea agreed to participate in the Games, enter with South Korea during the opening ceremony as a unified Korea, and field a unified team in women's ice hockey. Russian athletes, who complied with the IOC's doping regulations, were given the option to compete in Pyeongchang as "Olympic Athletes from Russia" (OAR).[116]

The Games saw the addition of big air snowboarding, mass start speed skating, mixed doubles curling, and mixed team alpine skiing to the programme. Like four years early, the Netherlands again dominated speed skating, winning gold medals in seven of the ten individual events. Dutch speed skater Sven Kramer won gold in the men's 5000m event, becoming the only male speed skater to win the same Olympic event three times. On the snow, Norway led the medal tally in cross-country skiing, with Marit Bjørgen winning bronze in the women's team sprint and gold in the 30-kilometre classical event, bringing her total Olympic medal haul to fifteen, the most won by any athlete (male or female) in Winter Olympics history.

Johannes Høsflot Klæbo of Norway became the youngest ever male to win an Olympic gold in cross-country skiing when he won the men's sprint at age 21. Noriaki Kasai of Japan became the first athlete in history to participate in eight Winter Olympics when he took part in the ski jumping qualification the day before the opening of the Games. Ester Ledecká of the Czech Republic won gold in the skiing super-G event and another gold in the snowboarding parallel giant slalom, making her the first female athlete to win Olympic gold medals in two sports at a single Winter Games.

Norway led the total medal standings with 39, the highest number of medals by a nation in any Winter Olympics, followed by Germany's 31 and Canada's 29. Host nation South Korea won seventeen medals, five of them gold, its highest medal haul at a Winter Olympics.

Beijing, the capital of the People's Republic of China, was elected as the host city for the 2022 Winter Olympics on 31 July 2015 at the 128th IOC Session. Beijing became the first city ever to have hosted both the Summer and Winter Olympics. Like the Summer Olympics held six months earlier in Tokyo, the COVID-19 pandemic resulted in the implementation of strict health and safety protocols, including restrictions on public attendance at the Games. The Games included a record 109 events over 15 disciplines in seven sports with seven new medal events, including mixed team competitions in freestyle skiing aerials, ski jumping, and snowboard cross. The Games were held between 4 and 20 February 2022 at venues in Beijing and Zhangjiakou which for the first time were run entirely on renewable energy. Several of the events were impacted by temperatures as low as minus 20 Celsius and strong wind.

The first gold medal of the Games was won by Therese Johaug of Norway in the women's skiathlon. Johaug had been excluded from the 2018 Winter Olympics in a controversial decision after having used a banned cream for sunburned lips. She went on to also win the women's 10 km and 30 km cross-country distances. In the women's snowboard cross, Lindsey Jacobellis of the United States won the gold, having lost the gold 16 years earlier at the 2006 Winter Olympics in Torino due to a brutal fall. On the ice, the Netherlands dominated with a total of six gold medals and Irene Schouten winning the women's mass start, 3,000m and 5,000m distances. Nils van der Poel of Sweden won the men's 5,000m and 10,000m distances, setting new Olympic records in both distances. Kamila Valieva of Russia was allowed to compete in the women's figure skating despite a failed doping test in December 2021. She failed, however, to win an individual medal after falling in her final routine. Russia's team gold medal remained in limbo for two years, pending investigation into Valieva's positive drug test, before the Court of Arbitration for Sport (CAS) disqualified Valieva for four years retroactive to 25 December 2021, leading the ISU to re-allocate the medals, upgrading the United States to gold and Japan to silver while downgrading the ROC to bronze.[122] Finland claimed its first ice hockey gold, having beaten the Russian Olympic Committee in the men's final on the last day of the Games.

Norway was first in the overall medal standings, claiming 37 medals in total and 16 gold medals, the highest number of gold medals of any country in a single Winter Olympics. This was the ninth time Norway claimed the highest number of gold medals at the Winter Games.

The 2026 Winter Olympics will be held in Milan-Cortina d'Ampezzo, Italy, and take place between 6 and 22 February 2026.[124]  At the 142nd IOC Session in July 2024, it was confirmed that the 2030 Winter Olympics will be hosted by France in the French Alps.[125] The 2034 Winter Olympics was also awarded simultaneously to be hosted by the United States in Salt Lake City, Utah, where the Olympics were held previously in 2002.[126]

The process for awarding host city honours came under intense scrutiny after Salt Lake City had been awarded the right to host the 2002 Games.[127] Soon after the host city had been announced it was discovered that the organisers had engaged in an elaborate bribery scheme to curry favour with IOC officials.[127] Gifts and other financial considerations were given to those who would evaluate and vote on Salt Lake City's bid. These gifts included medical treatment for relatives, a college scholarship for one member's son and a land deal in Utah. Even IOC president Juan Antonio Samaranch received two rifles valued at $2,000. Samaranch defended the gift as inconsequential since, as president, he was a non-voting member.[128]

The subsequent investigation uncovered inconsistencies in the bids for every Olympics (both Summer and Winter) since 1988.[129] For example, the gifts received by IOC members from the Japanese Organising Committee for Nagano's bid for the 1998 Winter Olympics were described by the investigation committee as "astronomical".[130] Although nothing strictly illegal had been done, the IOC feared that corporate sponsors would lose faith in the integrity of the process and that the Olympic brand would be tarnished to such an extent that advertisers would begin to pull their support.[131]

The investigation resulted in the expulsion of 10 IOC members and the sanctioning of another 10. New terms and age limits were established for IOC membership, and 15 former Olympic athletes were added to the committee. Stricter rules for future bids were imposed, with ceilings imposed on the value of gifts IOC members could accept from bid cities.[132][133][134]

About eight years before the Winter Olympics, the IOC invites National Olympic Committees to submit bids to host the games.[135] According to the IOC, the host city for the Winter Olympics is responsible for "...establishing functions and services for all aspects of the Games, such as sports planning, venues, finance, technology, accommodation, catering, media services, etc., as well as operations during the Games".[136] Due to the cost of hosting the Games, most host cities never realise a profit on their investment.[137] For example, the 2006 Winter Olympics in Turin, Italy, cost $3.6 billion to host. By comparison, the 1998 Winter Olympics in Nagano, Japan, cost $12.5 billion.[138] The organisers of the Nagano Games claimed that the cost of extending the bullet train service from Tokyo to Nagano was responsible for the large price tag.[138]

The organising committee had hoped that the exposure gained from hosting the Winter Olympics, and the improved access to Nagano from Tokyo, would benefit the local economy for years afterwards. In fact, Nagano's economy did experience a post-Olympic boom for a year or two, but the long-term effects have not materialised as anticipated.[138] The likelihood of heavy debt is a deterrent to prospective host cities, as well as the prospect of unused sports venues and infrastructure saddling the local community with upkeep costs with no appreciable post-Olympic value.[139]

The Winter Olympics has the added problem of the alpine events requiring a mountain location; the men's downhill needs an 800-metre altitude difference along a suitable course. As this is a focal event that is central to the Games, the IOC has previously not agreed to it taking place a long way from the main host city,[140] in contrast to the Summer Games, where sailing and horse sports have taken place more than 1,000 kilometres (620 mi) away. The requirement for a mountain location also means that venues such as hockey arenas often have to be built in sparsely populated areas with little future need for a large arena and for the hotels and infrastructure needed for all Olympic visitors. Due to cost issues, fewer and fewer cities are willing to host. Both the Torino 2006 and Vancouver 2010 Games, which were hosted in countries where large cities are located close to suitable mountain regions, had lower costs since more venues, hotels and transport infrastructure already existed. In contrast, the Sochi 2014 games had large costs as most installations had to be built.

The IOC has tried to mitigate these concerns. Firstly, it has agreed to fund part of the host city's budget.[141] Secondly, the qualifying host countries are limited to those that have the resources and infrastructure to successfully host an Olympic Games without negatively impacting their region or nation; this rules out a large portion of the developing world.[142] Finally, any prospective host city is required to add a "legacy plan" to their proposal, with a view to the long-term economic and environmental impact that hosting the Olympics will have.[143]

Beginning with the 2022 Winter Games, the IOC is allowing a longer distance between the alpine events and other events. The Oslo bid had 220 kilometres (140 mi) to the Kvitfjell downhill arena, while eventual host Beijing had venues 220 km away from the city as well. For the 2026 Winter Games, Stockholm's unsuccessful bid proposed to hold the alpine event in Åre, 620 kilometres (390 mi) away by road.

In 1967 the IOC began enacting drug testing protocols. They started by randomly testing athletes at the 1968 Winter Olympics.[144] The first Winter Games athlete to test positive for a banned substance was Alois Schloder, a West German hockey player,[145] but his team was still allowed to compete.[146] During the 1970s testing outside of competition was escalated because it was found to deter athletes from using performance-enhancing drugs.[147] The problem with testing during this time was a lack of standardisation of the test procedures, which undermined the credibility of the tests. It was not until the late 1980s that international sporting federations began to coordinate efforts to standardise the drug-testing protocols.[148] The IOC took the lead in the fight against steroids when it established the independent World Anti-Doping Agency (WADA) in November 1999.[149][150]

The 2006 Winter Olympics in Turin became notable for a scandal involving the emerging trend of blood doping, the use of blood transfusions or synthetic hormones such as Erythropoietin (EPO) to improve oxygen flow and thus reduce fatigue.[151] The Italian police conducted a raid on the Austrian cross-country ski team's residence during the Games where they seized blood-doping specimens and equipment.[152] This event followed the pre-Olympics suspension of 12 cross-country skiers who tested positive for unusually high levels of haemoglobin, which is evidence of blood doping.[151]

The 2014 Winter Olympics in Sochi's Russian Doping Scandal has resulted in the International Olympic Committee to begin disciplinary proceedings against 28 (later increased to 46) Russian athletes who competed at the 2014 Winter Games in Sochi, Russia, acting on evidence that their urine samples were tampered with.[153][154][155][156][157]

The Winter Olympics were an ideological front in the Cold War since the Soviet Union first participated at the 1956 Winter Games. It did not take long for the Cold War combatants to discover what a powerful propaganda tool the Olympic Games could be. The advent of the state-sponsored "full-time amateur athlete" of the Eastern Bloc countries further eroded the ideology of the pure amateur, as it put the self-financed amateurs of the Western countries at a disadvantage. The Soviet Union entered teams of athletes who were all nominally students, soldiers, or working in a profession, but many of whom were in reality paid by the state to train on a full-time basis.[46] Nevertheless, the IOC held to the traditional rules regarding amateurism until the '90s.[47]

The Cold War created tensions amongst countries allied to the two superpowers. The strained relationship between East and West Germany created a difficult political situation for the IOC. Because of its role in World War II, Germany was not allowed to compete at the 1948 Winter Olympics.[37] In 1950, the IOC recognised the West German Olympic Committee, and invited East and West Germany to compete as a unified team at the 1952 Winter Games.[158] East Germany declined and instead sought international legitimacy separate from West Germany.[159]

In 1955, the Soviet Union recognised East Germany as a sovereign state, thereby giving more credibility to East Germany's campaign to become an independent participant. The IOC agreed provisionally to accept the East German National Olympic Committee on condition that East and West Germans compete as one team.[160] The situation became tense when the Berlin Wall was constructed by East Germany in 1961 to stop migration of its citizens and Western European nations began refusing visas to East German athletes.[161] The uneasy compromise of a unified team held until the 1968 Grenoble Games when the IOC split the teams and threatened to reject host-city bids from any country that refused entry visas to East German athletes.[162]

The Winter Games have had only one national team boycott when Taiwan decided not to participate in the 1980 Winter Olympics held in Lake Placid. Prior to the Games, the IOC agreed to allow China to compete in the Olympics for the first time since 1952. China was given permission to compete as the "People's Republic of China" (PRC) and to use the PRC flag and anthem. Until 1980 the island of Taiwan had been competing under the name "Republic of China" (ROC) and had been using the ROC flag and anthem.[69] The IOC attempted to have the countries compete together but when this proved to be unacceptable the IOC demanded that Taiwan cease to call itself the "Republic of China".[163][164]

The IOC renamed the island "Chinese Taipei" and demanded that it adopt a different flag and national anthem, stipulations to which Taiwan would not agree. Despite numerous appeals and court hearings, the IOC's decision stood. When the Taiwanese athletes arrived at the Olympic village with their Republic of China identification cards they were not admitted. They subsequently left the Olympics in protest, just before the opening ceremonies.[69] Taiwan returned to Olympic competition at the 1984 Winter Games in Sarajevo as Chinese Taipei. The country agreed to compete under a flag bearing the emblem of their National Olympic Committee and to play the anthem of their National Olympic Committee should one of their athletes win a gold medal. The agreement remains in place to this day.[165]

The Olympic Charter limits winter sports to "those sports which are practised on snow or ice".[166] Since 1992 a number of new sports have been added to the Olympic programme, which include short-track speed skating, snowboarding, freestyle and moguls skiing. The addition of these events has broadened the appeal of the Winter Olympics beyond Europe and North America. While European powers such as Norway and Germany still dominate the traditional Winter Olympic sports, countries such as South Korea, Australia and Canada are finding success in these new sports. The results are more parity in the national medal tables, more interest in the Winter Olympics, and higher global television ratings.[167]

Demonstration sports have historically provided a venue for host countries to attract publicity to locally popular sports by having a competition without granting medals. Demonstration sports were discontinued after 1992.[184] Military patrol, a precursor to the biathlon, was a medal sport in 1924 and was demonstrated in 1928, 1936 and 1948, becoming an official sport in 1960.[185] The special figures figure skating event was only contested at the 1908 Summer Olympics.[186] Bandy (Russian hockey) is a sport popular in the Nordic countries and Russia. In the latter it is considered a national sport.[187] It was demonstrated at the Oslo Games.[188]

Ice stock sport, a German variant of curling, was demonstrated in 1936 in Germany and 1964 in Austria.[33] The ski ballet event, later known as ski-acro, was demonstrated in 1988 and 1992.[189] Skijöring, skiing behind dogs, was a demonstration sport in St. Moritz in 1928.[188] A sled-dog race was held at Lake Placid in 1932.[188] Speed skiing was demonstrated in Albertville at the 1992 Winter Olympics.[190] Winter pentathlon, a variant of the modern pentathlon, was included as a demonstration event at the 1948 Games in Switzerland. It included cross-country skiing, shooting, downhill skiing, fencing, and horse riding.[169]

The table below uses official data provided by the IOC.

Number of occurrencesX-ray crystallography is the experimental science of determining the atomic and molecular structure of a crystal, in which the crystalline structure causes a beam of incident X-rays to diffract in specific directions. By measuring the angles and intensities of the X-ray diffraction, a crystallographer can produce a three-dimensional picture of the density of electrons within the crystal and the positions of the atoms, as well as their chemical bonds, crystallographic disorder, and other information.

X-ray crystallography has been fundamental in the development of many scientific fields. In its first decades of use, this method determined the size of atoms, the lengths and types of chemical bonds, and the atomic-scale differences between various materials, especially minerals and alloys. The method has also revealed the structure and function of many biological molecules, including vitamins, drugs, proteins and nucleic acids such as DNA. X-ray crystallography is still the primary method for characterizing the atomic structure of materials and in differentiating materials that appear similar in other experiments. X-ray crystal structures can also help explain unusual electronic or elastic properties of a material, shed light on chemical interactions and processes, or serve as the basis for designing pharmaceuticals against diseases.

Modern work involves a number of steps all of which are important. The preliminary steps include preparing good quality samples, careful recording of the diffracted intensities, and processing of the data to remove artifacts. A variety of different methods are then used to obtain an estimate of the atomic structure, generically called direct methods. With an initial estimate further computational techniques such as those involving difference maps are used to complete the structure. The final step is a numerical refinement of the atomic positions against the experimental data, sometimes assisted by ab-initio calculations. In almost all cases new structures are deposited in databases available to the international community.

Crystals, though long admired for their regularity and symmetry, were not investigated scientifically until the 17th century. Johannes Kepler hypothesized in his work Strena seu de Nive Sexangula (A New Year's Gift of Hexagonal Snow) (1611) that the hexagonal symmetry of snowflake crystals was due to a regular packing of spherical water particles.[1] The Danish scientist Nicolas Steno (1669) pioneered experimental investigations of crystal symmetry. Steno showed that the angles between the faces are the same in every exemplar of a particular type of crystal (law of constancy of interfacial angles).[2] René Just Haüy (1784) discovered that every face of a crystal can be described by simple stacking patterns of blocks of the same shape and size (law of decrements). Hence, William Hallowes Miller in 1839 was able to give each face a unique label of three small integers, the Miller indices which remain in use for identifying crystal faces. Haüy's study led to the idea that crystals are a regular three-dimensional array (a Bravais lattice) of atoms and molecules; a single unit cell is repeated indefinitely along three principal directions. In the 19th century, a complete catalog of the possible symmetries of a crystal was worked out by Johan Hessel,[3] Auguste Bravais,[4] Evgraf Fedorov,[5] Arthur Schönflies[6] and (belatedly) William Barlow (1894). Barlow proposed several crystal structures in the 1880s that were validated later by X-ray crystallography;[7] however, the available data were too scarce in the 1880s to accept his models as conclusive.

Wilhelm Röntgen discovered X-rays in 1895.[8] Physicists were uncertain of the nature of X-rays, but suspected that they were waves of electromagnetic radiation. The Maxwell theory of electromagnetic radiation was well accepted, and experiments by Charles Glover Barkla showed that X-rays exhibited phenomena associated with electromagnetic waves, including transverse polarization and spectral lines akin to those observed in the visible wavelengths. Barkla created the x-ray notation for sharp spectral lines, noting in 1909 two separate energies, at first naming them "A" and "B" and then supposing that there may be lines prior to "A", he started an alphabet numbering beginning with "K."[9][10] Single-slit experiments in the laboratory of Arnold Sommerfeld suggested that X-rays had a wavelength of about 1 angstrom.[11] X-rays are not only waves but also have particle properties causing Sommerfeld to coin the name Bremsstrahlung for the continuous spectra when they were formed when electrons bombarded a material.[10] Albert Einstein introduced the photon concept in 1905,[12] but it was not broadly accepted until 1922,[13][14] when Arthur Compton confirmed it by the scattering of X-rays from electrons.[15] The particle-like properties of X-rays, such as their ionization of gases, had prompted William Henry Bragg to argue in 1907 that X-rays were not electromagnetic radiation.[16][17][18][19]  Bragg's view proved unpopular and the observation of X-ray diffraction by Max von Laue in 1912[20] confirmed that X-rays are a form of electromagnetic radiation. 
The idea that crystals could be used as a diffraction grating for X-rays arose in 1912 in a conversation between Paul Peter Ewald and Max von Laue in the English Garden in Munich. Ewald had proposed a resonator model of crystals for his thesis, but this model could not be validated using visible light, since the wavelength was much larger than the spacing between the resonators. Von Laue realized that electromagnetic radiation of a shorter wavelength was needed, and suggested that X-rays might have a wavelength comparable to the unit-cell spacing in crystals. Von Laue worked with two technicians, Walter Friedrich and his assistant Paul Knipping, to shine a beam of X-rays through a copper sulfate crystal and record its diffraction on a photographic plate. After being developed, the plate showed a large number of well-defined spots arranged in a pattern of intersecting circles around the spot produced by the central beam. The results were presented to the Bavarian Academy of Sciences and Humanities in June 1912 as "Interferenz-Erscheinungen bei Röntgenstrahlen" (Interference phenomena in X-rays).[20][21] Von Laue developed a law that connects the scattering angles and the size and orientation of the unit-cell spacings in the crystal, for which he was awarded the Nobel Prize in Physics in 1914.[22]

After Von Laue's pioneering research, the field developed rapidly, most notably by physicists William Lawrence Bragg and his father William Henry Bragg. In 1912–1913, the younger Bragg developed Bragg's law, which connects the scattering with evenly spaced planes within a crystal.[8][23][24][25] The Braggs, father and son, shared the 1915 Nobel Prize in Physics for their work in crystallography. The earliest structures were generally simple; as computational and experimental methods improved over the next decades, it became feasible to deduce reliable atomic positions for more complicated arrangements of atoms.

The earliest structures were simple inorganic crystals and minerals, but even these revealed fundamental laws of physics and chemistry. The first atomic-resolution structure to be "solved" (i.e., determined) in 1914 was that of table salt.[26][27][28] The distribution of electrons in the table-salt structure showed that crystals are not necessarily composed of covalently bonded molecules, and proved the existence of ionic compounds.[29] The structure of diamond was solved in the same year,[30][31] proving the tetrahedral arrangement of its chemical bonds and showing that the length of C–C single bond was about 1.52 angstroms. Other early structures included copper,[32] calcium fluoride (CaF2, also known as fluorite), calcite (CaCO3) and pyrite (FeS2)[33] in 1914; spinel (MgAl2O4) in 1915;[34][35] the rutile and anatase forms of titanium dioxide (TiO2) in 1916;[36] pyrochroite (Mn(OH)2) and, by extension, brucite (Mg(OH)2) in 1919.[37][38] Also in 1919, sodium nitrate (NaNO3) and caesium dichloroiodide (CsICl2) were determined by Ralph Walter Graystone Wyckoff, and the wurtzite (hexagonal ZnS) structure was determined in 1920.[39]

The structure of graphite was solved in 1916[40] by the related method of powder diffraction,[41] which was developed by Peter Debye and Paul Scherrer and, independently, by Albert Hull in 1917.[42] The structure of graphite was determined from single-crystal diffraction in 1924 by two groups independently.[43][44] Hull also used the powder method to determine the structures of various metals, such as iron[45] and magnesium.[46]

X-ray crystallography has led to a better understanding of chemical bonds and non-covalent interactions. The initial studies revealed the typical radii of atoms, and confirmed many theoretical models of chemical bonding, such as the tetrahedral bonding of carbon in the diamond structure,[30] the octahedral bonding of metals observed in ammonium hexachloroplatinate (IV),[47] and the resonance observed in the planar carbonate group[33] and in aromatic molecules.[48] Kathleen Lonsdale's 1928 structure of hexamethylbenzene[49] established the hexagonal symmetry of benzene and showed a clear difference in bond length between the aliphatic C–C bonds and aromatic C–C bonds; this finding led to the idea of resonance between chemical bonds, which had profound consequences for the development of chemistry.[50] Her conclusions were anticipated by William Henry Bragg, who published models of naphthalene and anthracene in 1921 based on other molecules, an early form of molecular replacement.[48][51]

The first structure of an organic compound, hexamethylenetetramine, was solved in 1923.[52] This was rapidly followed by several studies of different long-chain fatty acids, which are an important component of biological membranes.[53][54][55][56][57][58][59][60][61] In the 1930s, the structures of much larger molecules with two-dimensional complexity began to be solved. A significant advance was the structure of phthalocyanine,[62] a large planar molecule that is closely related to porphyrin molecules important in biology, such as heme, corrin and chlorophyll.

In the 1920s, Victor Moritz Goldschmidt and later Linus Pauling developed rules for eliminating chemically unlikely structures and for determining the relative sizes of atoms. These rules led to the structure of brookite (1928) and an understanding of the relative stability of the rutile, brookite and anatase forms of titanium dioxide.

The distance between two bonded atoms is a sensitive measure of the bond strength and its bond order; thus, X-ray crystallographic studies have led to the discovery of even more exotic types of bonding in inorganic chemistry, such as metal-metal double bonds,[63][64][65] metal-metal quadruple bonds,[66][67][68] and three-center, two-electron bonds.[69] X-ray crystallography—or, strictly speaking, an inelastic Compton scattering experiment—has also provided evidence for the partly covalent character of hydrogen bonds.[70] In the field of organometallic chemistry, the X-ray structure of ferrocene initiated scientific studies of sandwich compounds,[71][72] while that of Zeise's salt stimulated research into "back bonding" and metal-pi complexes.[73][74][75][76] Finally, X-ray crystallography had a pioneering role in the development of supramolecular chemistry, particularly in clarifying the structures of the crown ethers and the principles of host–guest chemistry.[citation needed]

The application of X-ray crystallography to mineralogy began with the structure of garnet, which was determined in 1924 by Menzer. A systematic X-ray crystallographic study of the silicates was undertaken in the 1920s. This study showed that, as the Si/O ratio is altered, the silicate crystals exhibit significant changes in their atomic arrangements. Machatschki extended these insights to minerals in which aluminium substitutes for the silicon atoms of the silicates. The first application of X-ray crystallography to metallurgy also occurred in the mid-1920s.[78][79][80][81][82][83] Most notably, Linus Pauling's structure of the alloy Mg2Sn[84] led to his theory of the stability and structure of complex ionic crystals.[85] Many complicated inorganic and organometallic systems have been analyzed using single-crystal methods, such as fullerenes, metalloporphyrins, and other complicated compounds. Single-crystal diffraction is also used in the pharmaceutical industry. The Cambridge Structural Database contains over 1,000,000 structures as of June 2019; most of these structures were determined by X-ray crystallography.[86]

On October 17, 2012, the Curiosity rover on the planet Mars at "Rocknest" performed the first X-ray diffraction analysis of Martian soil. The results from the rover's CheMin analyzer revealed the presence of several minerals, including feldspar, pyroxenes and olivine, and suggested that the Martian soil in the sample was similar to the "weathered basaltic soils" of Hawaiian volcanoes.[77]


X-ray crystallography of biological molecules took off with Dorothy Crowfoot Hodgkin, who solved the structures of cholesterol (1937), penicillin (1946) and vitamin B12 (1956), for which she was awarded the Nobel Prize in Chemistry in 1964. In 1969, she succeeded in solving the structure of insulin, on which she worked for over thirty years.[87]
Crystal structures of proteins (which are irregular and hundreds of times larger than cholesterol) began to be solved in the late 1950s, beginning with the structure of sperm whale myoglobin by Sir John Cowdery Kendrew,[88] for which he shared the Nobel Prize in Chemistry with Max Perutz in 1962.[89] Since that success, 190,000 X-ray crystal structures of proteins, nucleic acids and other biological molecules have been determined.[90] The nearest competing method in number of structures analyzed is nuclear magnetic resonance (NMR) spectroscopy, which has resolved less than one tenth as many.[91] Crystallography can solve structures of arbitrarily large molecules, whereas solution-state NMR is restricted to relatively small ones (less than 70 kDa). X-ray crystallography is used routinely to determine how a pharmaceutical drug interacts with its protein target and what changes might improve it.[92] However, intrinsic membrane proteins remain challenging to crystallize because they require detergents or other denaturants to solubilize them in isolation, and such detergents often interfere with crystallization. Membrane proteins are a large component of the genome, and include many proteins of great physiological importance, such as ion channels and receptors.[93][94] Helium cryogenics are used to prevent radiation damage in protein crystals.[95]

Two limiting cases of X-ray crystallography—"small-molecule" (which includes continuous inorganic solids) and "macromolecular" crystallography—are often used. Small-molecule crystallography typically involves crystals with fewer than 100 atoms in their asymmetric unit; such crystal structures are usually so well resolved that the atoms can be discerned as isolated "blobs" of electron density. In contrast, macromolecular crystallography often involves tens of thousands of atoms in the unit cell. Such crystal structures are generally less well-resolved; the atoms and chemical bonds appear as tubes of electron density, rather than as isolated atoms. In general, small molecules are also easier to crystallize than macromolecules; however, X-ray crystallography has proven possible even for viruses and proteins with hundreds of thousands of atoms, through improved crystallographic imaging and technology.[96]

The technique of single-crystal X-ray crystallography has three basic steps. The first—and often most difficult—step is to obtain an adequate crystal of the material under study. The crystal should be sufficiently large (typically larger than 0.1 mm in all dimensions), pure in composition and regular in structure, with no significant internal imperfections such as cracks or twinning.[97]

In the second step, the crystal is placed in an intense beam of X-rays, usually of a single wavelength (monochromatic X-rays), producing the regular pattern of reflections. The angles and intensities of diffracted X-rays are measured, with each compound having a unique diffraction pattern.[98] As the crystal is gradually rotated, previous reflections disappear and new ones appear; the intensity of every spot is recorded at every orientation of the crystal. Multiple data sets may have to be collected, with each set covering slightly more than half a full rotation of the crystal and typically containing tens of thousands of reflections.[99]

In the third step, these data are combined computationally with complementary chemical information to produce and refine a model of the arrangement of atoms within the crystal. The final, refined model of the atomic arrangement—now called a crystal structure—is usually stored in a public database.[100]

Although crystallography can be used to characterize the disorder in an impure or irregular crystal, crystallography generally requires a pure crystal of high regularity to solve the structure of a complicated arrangement of atoms. Pure, regular crystals can sometimes be obtained from natural or synthetic materials, such as samples of metals, minerals or other macroscopic materials. The regularity of such crystals can sometimes be improved with macromolecular crystal annealing[101][102][103] and other methods. However, in many cases, obtaining a diffraction-quality crystal is the chief barrier to solving its atomic-resolution structure.[104]

Small-molecule and macromolecular crystallography differ in the range of possible techniques used to produce diffraction-quality crystals. Small molecules generally have few degrees of conformational freedom, and may be crystallized by a wide range of methods, such as chemical vapor deposition and recrystallization. By contrast, macromolecules generally have many degrees of freedom and their crystallization must be carried out so as to maintain a stable structure. For example, proteins and larger RNA molecules cannot be crystallized if their tertiary structure has been unfolded; therefore, the range of crystallization conditions is restricted to solution conditions in which such molecules remain folded.[citation needed]

 Protein crystals are almost always grown in solution. The most common approach is to lower the solubility of its component molecules very gradually; if this is done too quickly, the molecules will precipitate from solution, forming a useless dust or amorphous gel on the bottom of the container. Crystal growth in solution is characterized by two steps: nucleation of a microscopic crystallite (possibly having only 100 molecules), followed by growth of that crystallite, ideally to a diffraction-quality crystal.[105][106] The solution conditions that favor the first step (nucleation) are not always the same conditions that favor the second step (subsequent growth). The solution conditions should disfavor the first step (nucleation) but favor the second (growth), so that only one large crystal forms per droplet. If nucleation is favored too much, a shower of small crystallites will form in the droplet, rather than one large crystal; if favored too little, no crystal will form whatsoever. Other approaches involve crystallizing proteins under oil, where aqueous protein solutions are dispensed under liquid oil, and water evaporates through the layer of oil. Different oils have different evaporation permeabilities, therefore yielding changes in concentration rates from different percipient/protein mixture.[107]

It is difficult to predict good conditions for nucleation or growth of well-ordered crystals.[108] In practice, favorable conditions are identified by screening; a very large batch of the molecules is prepared, and a wide variety of crystallization solutions are tested.[109] Hundreds, even thousands, of solution conditions are generally tried before finding the successful one. The various conditions can use one or more physical mechanisms to lower the solubility of the molecule; for example, some may change the pH, some contain salts of the Hofmeister series or chemicals that lower the dielectric constant of the solution, and still others contain large polymers such as polyethylene glycol that drive the molecule out of solution by entropic effects. It is also common to try several temperatures for encouraging crystallization, or to gradually lower the temperature so that the solution becomes supersaturated. These methods require large amounts of the target molecule, as they use high concentration of the molecule(s) to be crystallized. Due to the difficulty in obtaining such large quantities (milligrams) of crystallization-grade protein, robots have been developed that are capable of accurately dispensing crystallization trial drops that are in the order of 100 nanoliters in volume. This means that 10-fold less protein is used per experiment when compared to crystallization trials set up by hand (in the order of 1 microliter).[110]

Several factors are known to inhibit crystallization. The growing crystals are generally held at a constant temperature and protected from shocks or vibrations that might disturb their crystallization. Impurities in the molecules or in the crystallization solutions are often inimical to crystallization. Conformational flexibility in the molecule also tends to make crystallization less likely, due to entropy. Molecules that tend to self-assemble into regular helices are often unwilling to assemble into crystals.[citation needed] Crystals can be marred by twinning, which can occur when a unit cell can pack equally favorably in multiple orientations; although recent advances in computational methods may allow solving the structure of some twinned crystals. Having failed to crystallize a target molecule, a crystallographer may try again with a slightly modified version of the molecule; even small changes in molecular properties can lead to large differences in crystallization behavior.[citation needed]

The crystal is mounted for measurements so that it may be held in the X-ray beam and rotated. There are several methods of mounting. In the past, crystals were loaded into glass capillaries with the crystallization solution (the mother liquor).  Crystals of small molecules are typically attached with oil or glue to a glass fiber or a loop, which is made of nylon or plastic and attached to a solid rod.  Protein crystals are scooped up by a loop, then flash-frozen with liquid nitrogen.[111] This freezing reduces the radiation damage of the X-rays, as well as thermal motion (the Debye-Waller effect). However, untreated protein crystals often crack if flash-frozen; therefore, they are generally pre-soaked in a cryoprotectant solution before freezing.[112] This pre-soak may itself cause the crystal to crack, ruining it for crystallography. Generally, successful cryo-conditions are identified by trial and error.[citation needed]

The capillary or loop is mounted on a goniometer, which allows it to be positioned accurately within the X-ray beam and rotated. Since both the crystal and the beam are often very small, the crystal must be centered within the beam to within ~25 micrometers accuracy, which is aided by a camera focused on the crystal. The most common type of goniometer is the "kappa goniometer", which offers three angles of rotation: the ω angle, which rotates about an axis perpendicular to the beam; the κ angle, about an axis at ~50° to the ω axis; and, finally, the φ angle about the loop/capillary axis. When the κ angle is zero, the ω and φ axes are aligned. The κ rotation allows for convenient mounting of the crystal, since the arm in which the crystal is mounted may be swung out towards the crystallographer. The oscillations carried out during data collection (mentioned below) involve the ω axis only. An older type of goniometer is the four-circle goniometer, and its relatives such as the six-circle goniometer.[citation needed]

The relative intensities of the reflections provides information to determine the arrangement of molecules within the crystal in atomic detail. The intensities of these reflections may be recorded with photographic film, an area detector (such as a pixel detector) or with a charge-coupled device (CCD) image sensor. The peaks at small angles correspond to low-resolution data, whereas those at high angles represent high-resolution data; thus, an upper limit on the eventual resolution of the structure can be determined from the first few images. Some measures of diffraction quality can be determined at this point, such as the mosaicity of the crystal and its overall disorder, as observed in the peak widths. Some pathologies of the crystal that would render it unfit for solving the structure can also be diagnosed quickly at this point.[citation needed]

One set of spots is insufficient to reconstruct the whole crystal; it represents only a small slice of the full three dimensional set. To collect all the necessary information, the crystal must be rotated step-by-step through 180°, with an image recorded at every step; actually, slightly more than 180° is required to cover reciprocal space, due to the curvature of the Ewald sphere. However, if the crystal has a higher symmetry, a smaller angular range such as 90° or 45° may be recorded. The rotation axis should be changed at least once, to avoid developing a "blind spot" in reciprocal space close to the rotation axis. It is customary to rock the crystal slightly (by 0.5–2°) to catch a broader region of reciprocal space.[citation needed]

Multiple data sets may be necessary for certain phasing methods. For example, multi-wavelength anomalous dispersion phasing requires that the scattering be recorded at least three (and usually four, for redundancy) wavelengths of the incoming X-ray radiation. A single crystal may degrade too much during the collection of one data set, owing to radiation damage; in such cases, data sets on multiple crystals must be taken.[113]

The recorded series of two-dimensional diffraction patterns, each corresponding to a different crystal orientation, is converted into a three-dimensional set. Data processing begins with indexing the reflections. This means identifying the dimensions of the unit cell and which image peak corresponds to which position in reciprocal space. A byproduct of indexing is to determine the symmetry of the crystal, i.e., its space group. Some space groups can be eliminated from the beginning. For example, reflection symmetries cannot be observed in chiral molecules; thus, only 65 space groups of 230 possible are allowed for protein molecules which are almost always chiral. Indexing is generally accomplished using an autoindexing routine.[114] Having assigned symmetry, the data is then integrated. This converts the hundreds of images containing the thousands of reflections into a single file, consisting of (at the very least) records of the Miller index of each reflection, and an intensity for each reflection (at this state the file often also includes error estimates and measures of partiality (what part of a given reflection was recorded on that image)).

A full data set may consist of hundreds of separate images taken at different orientations of the crystal. These have to be merged and scaled using peaks that appear in two or more images (merging) and scaling so there is a consistent intensity scale. Optimizing the intensity scale is critical because the relative intensity of the peaks is the key information from which the structure is determined. The repetitive technique of crystallographic data collection and the often high symmetry of crystalline materials cause the diffractometer to record many symmetry-equivalent reflections multiple times. This allows calculating the symmetry-related R-factor, a reliability index based upon how similar are the measured intensities of symmetry-equivalent reflections,[clarification needed] thus assessing the quality of the data.

The intensity of each diffraction 'spot' is proportional to the modulus squared of the structure factor. The structure factor  is a complex number containing information relating to both the amplitude and phase of a wave. In order to obtain an interpretable electron density map, both amplitude and phase must be known (an electron density map allows a crystallographer to build a starting model of the molecule). The phase cannot be directly recorded during a diffraction experiment: this is known as the phase problem. Initial phase estimates can be obtained in a variety of ways:

Having obtained initial phases, an initial model can be built. The atomic positions in the model and their respective Debye-Waller factors (or B-factors, accounting for the thermal motion of the atom) can be refined to fit the observed diffraction data, ideally yielding a better set of phases. A new model can then be fit to the new electron density map and successive rounds of refinement are carried out. This iterative process continues until the correlation between the diffraction data and the model is maximized. The agreement is measured by an R-factor defined as

where F is the structure factor. A similar quality criterion is Rfree, which is calculated from a subset (~10%) of reflections that were not included in the structure refinement. Both R factors depend on the resolution of the data. As a rule of thumb, Rfree should be approximately the resolution in angstroms divided by 10; thus, a data-set with 2 Å resolution should yield a final Rfree ~ 0.2. Chemical bonding features such as stereochemistry, hydrogen bonding and distribution of bond lengths and angles are complementary measures of the model quality. In iterative model building, it is common to encounter phase bias or model bias: because phase estimations come from the model, each round of calculated map tends to show density wherever the model has density, regardless of whether there truly is a density. This problem can be mitigated by maximum-likelihood weighting and checking using omit maps.[121]

It may not be possible to observe every atom in the asymmetric unit. In many cases, crystallographic disorder smears the electron density map. Weakly scattering atoms such as hydrogen are routinely invisible. It is also possible for a single atom to appear multiple times in an electron density map, e.g., if a protein sidechain has multiple (<4) allowed conformations. In still other cases, the crystallographer may detect that the covalent structure deduced for the molecule was incorrect, or changed. For example, proteins may be cleaved or undergo post-translational modifications that were not detected prior to the crystallization.

A common challenge in refinement of crystal structures results from crystallographic disorder.  Disorder can take many forms but in general involves the coexistence of two or more species or conformations.  Failure to recognize disorder results in flawed interpretation.  Pitfalls from improper modeling of disorder are illustrated by the discounted hypothesis of bond stretch isomerism.[122]  Disorder is modelled with respect to the relative population of the components, often only two, and their identity.  In structures of large molecules and ions, solvent and counterions are often disordered.

The use of computational methods for the powder X-ray diffraction data analysis is now generalized. It typically compares the experimental data to the simulated diffractogram of a model structure, taking into account the instrumental parameters, and refines the structural or microstructural parameters of the model using least squares based minimization algorithm. Most available tools allowing phase identification and structural refinement are based on the Rietveld method,[123][124] some of them being open and free software such as FullProf Suite,[125][126] Jana2006,[127] MAUD,[128][129][130] Rietan,[131] GSAS,[132] etc. while others are available under commercial licenses such as Diffrac.Suite TOPAS,[133] Match!,[134] etc. Most of these tools also allow Le Bail refinement (also referred to as profile matching), that is, refinement of the cell parameters based on the Bragg peaks positions and peak profiles, without taking into account the crystallographic structure by itself. More recent tools allow the refinement of both structural and microstructural data, such as the FAULTS program included in the FullProf Suite,[135] which allows the refinement of structures with planar defects (e.g. stacking faults, twinnings, intergrowths).

Once the model of a molecule's structure has been finalized, it is often deposited in a crystallographic database such as the Cambridge Structural Database (for small molecules), the Inorganic Crystal Structure Database (ICSD) (for inorganic compounds) or the Protein Data Bank (for protein and sometimes nucleic acids). Many structures obtained in private commercial ventures to crystallize medicinally relevant proteins are not deposited in public crystallographic databases.

A number of women were pioneers in X-ray crystallography at a time when they were excluded from most other branches of physical science.[136]

Kathleen Lonsdale was a research student of William Henry Bragg, who had 11 women research students out of a total of 18. She is known for both her experimental and theoretical work. Lonsdale joined his crystallography research team at the Royal Institution in London in 1923, and after getting married and having children, went back to work with Bragg as a researcher. She confirmed the structure of the benzene ring, carried out studies of diamond, was one of the first two women to be elected to the Royal Society in 1945, and in 1949 was appointed the first female tenured professor of chemistry and head of the Department of crystallography at University College London.[137] Lonsdale always advocated greater participation of women in science and  said in 1970: "Any country that wants to make full use of all its potential scientists and technologists could do so, but it must not expect to get the women quite so simply as it gets the men. ... It is utopian, then, to suggest that any country that really wants married women to return to a scientific career, when her children no longer need her physical presence, should make special arrangements to encourage her to do so?".[138] During this period, Lonsdale began a collaboration with William T. Astbury on a set of 230 space group tables which was published in 1924 and became an essential tool for crystallographers.

In 1932 Dorothy Hodgkin joined the laboratory of the physicist John Desmond Bernal, who was a former student of Bragg, in Cambridge, UK. She and Bernal took the first X-ray photographs of crystalline proteins. Hodgkin also played a role in the foundation of the International Union of Crystallography. She was awarded the Nobel Prize in Chemistry in 1964 for her work using X-ray techniques to study the structures of penicillin, insulin and vitamin B12. Her work on penicillin began in 1942 during the war and on vitamin B12 in 1948. While her group slowly grew, their predominant focus was on the X-ray analysis of natural products. She is the only British woman ever to have won a Nobel Prize in a science subject.

Rosalind Franklin took the X-ray photograph of a DNA fibre that proved key to James Watson and Francis Crick's  discovery of the double helix, for which they both won the Nobel Prize for Physiology or Medicine in 1962. Watson revealed in his autobiographic account of the discovery of the structure of DNA, The Double Helix,[139] that he had used Franklin's X-ray photograph without her permission. Franklin died of cancer in her 30s, before Watson received the Nobel Prize. Franklin also carried out important structural studies of carbon in coal and graphite, and of plant and animal viruses.

Isabella Karle of the United States Naval Research Laboratory developed an experimental approach to the mathematical theory of crystallography. Her work improved the speed and accuracy of chemical and biomedical analysis. Yet only her husband Jerome shared the 1985 Nobel Prize in Chemistry with Herbert Hauptman, "for outstanding achievements in the development of direct methods for the determination of crystal structures". Other prize-giving bodies have showered Isabella with awards in her own right.

Women have written many textbooks and research papers in the field of X-ray crystallography. For many years Lonsdale edited the International Tables for Crystallography, which provide information on crystal lattices, symmetry, and space groups, as well as mathematical, physical and chemical data on structures. Olga Kennard of the University of Cambridge, founded and ran the Cambridge Crystallographic Data Centre, an internationally recognized source of structural data on small molecules, from 1965 until 1997. Jenny Pickworth Glusker, a British scientist, co-authored Crystal Structure Analysis: A Primer,[140] first published in 1971 and as of 2010 in its third edition. Eleanor Dodson, an Australian-born biologist, who began as Dorothy Hodgkin's technician, was the main instigator behind CCP4, the collaborative computing project that currently shares more than 250 software tools with protein crystallographers worldwide.Yoko Ono (Japanese: 小野 洋子, romanized: Ono Yōko, usually spelled in katakana as オノ・ヨーコ; born February 18, 1933) is a Japanese multimedia artist, singer, songwriter, and peace activist. Her work also encompasses performance art and filmmaking.[1]

Ono grew up in Tokyo and moved to New York City in 1952 to join her family. She became involved with New York City's downtown artists scene in the early 1960s, which included the Fluxus group, and became well known in 1969 when she married English musician John Lennon of the Beatles, with whom she would subsequently record as a duo in the Plastic Ono Band. The couple used their honeymoon as a stage for public protests against the Vietnam War with what they called a bed-in. She and Lennon remained married until he was murdered in front of the couple's apartment building, the Dakota, on December 8, 1980. Together, they had one son, Sean, who later also became a musician.

Ono began a career in popular music in 1969, forming the Plastic Ono Band with Lennon and producing a number of avant-garde music albums in the 1970s. She achieved commercial and critical success in 1980 with the chart-topping album Double Fantasy, a collaboration with Lennon that was released three weeks before his murder, winning the Grammy Award for Album of the Year. To date, she has had twelve number one singles on the US Dance charts, and in 2016 was named the 11th most successful dance club artist of all time by Billboard magazine.[2] Many musicians have paid tribute to Ono as an artist in her own right and as a muse and icon, including Elvis Costello who recorded his version of "Walking on Thin Ice" with The Attractions for the Every Man Has a Woman tribute album to Yoko Ono, the B-52's,[3] Sonic Youth[4] and Meredith Monk.[5]

As Lennon's widow, Ono works to preserve his legacy. She funded the Strawberry Fields memorial in Manhattan's Central Park,[6] the Imagine Peace Tower in Iceland,[7] and the John Lennon Museum in Saitama, Japan (which closed in 2010).[8] She has made significant philanthropic contributions to the arts, peace and disaster relief in Japan and the Philippines,[9][10] and other such causes. In 2002, she inaugurated a biennial $50,000 LennonOno Grant for Peace.[11] In 2012, she received the Dr. Rainer Hildebrandt Human Rights Award[12] and co-founded the group Artists Against Fracking.[13]

Ono was born in Tokyo City on February 18, 1933, to mother Isoko Ono (小野 磯子, Ono Isoko) (1911–1999)[14] and father Eisuke Ono (小野 英輔, Ono Eisuke), a wealthy banker and former classical pianist.[15] Isoko's adoptive maternal grandfather Zenjiro Yasuda (安田 善次郎, Yasuda Zenjirō) was an affiliate of the Yasuda clan and zaibatsu. Eisuke came from a long line of samurai warrior-scholars.[16] The kanji translation of Yōko (洋子) means "ocean child".[15][17] Two weeks before Ono's birth, Eisuke was transferred to San Francisco, California, by his employer, the Yokohama Specie Bank.[18] The rest of the family followed soon after, with Ono first meeting her father when she was two years old.[3] Her younger brother Keisuke was born in December 1936.[citation needed]

In 1937, the family was transferred back to Japan, and Ono enrolled at Tokyo's elite Gakushūin (also known as the Peers School), one of the most exclusive schools in Japan.[18] Ono was enrolled in piano lessons from the age of 4, until the age of 12 or 13.[19] She attended kabuki performances with her mother, who was trained in shamisen, koto, otsuzumi, kotsuzumi, nagauta, and could read Japanese musical scores.[citation needed]

The family moved to New York City in 1940. The next year, Eisuke was transferred from New York City to Hanoi in French Indochina, and the family returned to Japan. Ono was enrolled in Keimei Gakuen, an exclusive Christian primary school run by the Mitsui family. She remained in Tokyo throughout World War II and the fire-bombing of March 9, 1945, during which she was sheltered with other family members in a special bunker in Tokyo's Azabu district, away from the heavy bombing. Ono later went to the Karuizawa mountain resort with members of her family.[18]

Starvation was rampant in the destruction that followed the Tokyo bombings; the Ono family was forced to beg for food while pulling their belongings in a wheelbarrow. Ono said it was during this period in her life that she developed her "aggressive" attitude and understanding of "outsider" status. Other stories tell of her mother bringing a large number of goods to the countryside, where they were bartered for food. In one anecdote, her mother traded a German-made sewing machine for 60 kilograms (130 lb) of rice to feed the family.[18] During this time, Ono's father, who had been in Hanoi, was believed to be in a prisoner of war camp in China. Ono told Amy Goodman of Democracy Now! on October 16, 2007, that "He was in French Indochina, which is Vietnam actually ... in Saigon. He was in a concentration camp."[20]

After the war ended in 1945, Ono remained in Japan when her family moved to the United States and settled in Scarsdale, New York, an affluent town 25 miles (40 km) north of midtown Manhattan. By April 1946, Gakushūin was reopened and Ono re-enrolled. The school, located near the Tokyo Imperial Palace, had not been damaged by the war, and Ono found herself a classmate of Prince Akihito, the future emperor of Japan.[15][16] At 14 years old, she took up vocal training in lieder-singing.[citation needed]

Ono graduated from Gakushūin in 1951, and was accepted into the philosophy program of Gakushuin University as the first woman to enter the department. However, she left the school after two semesters.[18]

Ono joined her family in New York in September 1952,[21] enrolling at nearby Sarah Lawrence College. Ono's parents approved of her college choice, but disapproved of her lifestyle and chastised her for befriending people they felt were beneath her. In 1956, Ono left college to elope with Japanese composer Toshi Ichiyanagi,[16][22] a star in Tokyo's experimental community, then studying at Juilliard.[23]

At Sarah Lawrence, Ono studied poetry with Alastair Reid, English literature with Kathryn Mansell, and music composition with the Viennese-trained André Singer.[19] Ono has said that her heroes at this time were the twelve-tone composers Arnold Schoenberg and Alban Berg. She said, "I was just fascinated with what they could do. I wrote some twelve-tone songs, then my music went into [an] area that my teacher felt was really a bit off track, and... he said, 'Well, look, there are people who are doing things like what you do, and they're called avant-garde.'" Singer introduced her to the work of Edgar Varèse, John Cage, and Henry Cowell. Ono left college and moved to New York in 1957, supporting herself through secretarial work and lessons in the traditional Japanese arts at the Japan Society.[24]

Ono has often been associated with the Fluxus group, a loose association of Dada-inspired avant-garde artists which was founded in the early 1960s by Lithuanian-American artist George Maciunas. Maciunas promoted her work, giving Ono her first solo exhibition at his AG Gallery in New York in 1961. He formally invited Ono to join Fluxus, but she declined because she wanted to remain independent.[25] However, she did collaborate with Maciunas,[26] Charlotte Moorman, George Brecht, and the poet Jackson Mac Low, among others associated with the group.[27]

Ono first met John Cage through his student Ichiyanagi Toshi, in Cage's experimental composition class at the New School for Social Research.[28] She was introduced to more of Cage's unconventional neo-Dadaism first hand, and via his New York City protégés Allan Kaprow, Brecht, Mac Low, Al Hansen and the poet Dick Higgins.[27]

After Cage finished teaching at the New School in the summer of 1960, Ono was determined to rent a place to present her works along with the work of other avant-garde artists in the city. She eventually found an inexpensive loft in downtown Manhattan at 112 Chambers Street and used the apartment as a studio and living space, also allowing composer La Monte Young to organize concerts in the loft.[27] They both held a series of events there from December 1960 through June 1961;[24] the events were attended by people such as Marcel Duchamp and Peggy Guggenheim.[29] Ono and Young both claimed to have been the primary curator of these events,[30] with Ono claiming to have been eventually pushed into a subsidiary role by Young.[28] Ono presented work only once during the series.[24]

In 1961, Ono had her first major public performance in a concert at the 258-seat Carnegie Recital Hall (smaller than the "Main Hall"). This concert featured radical experimental music and performances.[31]

The Chambers Street series hosted some of Ono's earliest conceptual artwork, including Painting to Be Stepped On, a scrap of canvas on the floor that became a completed artwork as footprints were left on it. With that work, Ono suggested that a work of art no longer needed to be mounted on a wall and inaccessible. She showed this work and other instructional work again at Macunias's AG Gallery in July 1961.[29] After Ono set a painting on fire at one performance, Cage advised her to treat the paper with flame retardant.[16] She is credited for the album cover art for the album Nirvana Symphony by Toshiro Mayuzumi, released by Time Records in 1962.

After living apart for several years, Ono and Ichiyanagi filed for divorce in 1962. Ono returned home to live with her parents, and, suffering from clinical depression, was briefly placed into a Japanese mental institution.[15][32]

On November 28, 1962, Ono married Anthony Cox, an American film producer and art promoter who had been instrumental in securing her release from the mental institution.[16] Ono's second marriage was annulled on March 1, 1963, because she had neglected to finalize her divorce from Ichiyanagi. After finalizing that divorce, Cox and Ono married again on June 6, 1963. She gave birth to their daughter Kyoko Chan Cox two months later, on August 8, 1963.[15]

The marriage quickly fell apart, but the couple continued working together for the sake of their joint careers. They performed at Tokyo's Sogetsu Hall, with Ono lying atop a piano played by John Cage. Soon, the couple returned to New York with Kyoko. In the early years of the marriage, Ono left most of Kyoko's parenting to Cox while she pursued her art full-time, with Cox also managing her publicity.

Ono had a second engagement at the Carnegie Recital Hall in 1965, in which she debuted Cut Piece.[33] In September 1966, Ono visited London to meet artist and political activist Gustav Metzger's Destruction in Art Symposium in September 1966. She was the only woman artist chosen to perform her own events and only one of two invited to speak.[34] She premiered The Fog Machine during her Concert of Music for the Mind at the Bluecoat Society of Arts in Liverpool, England in 1967.[35]

Ono and Cox divorced on February 2, 1969, and she married John Lennon later that same year. During a 1971 custody battle, Cox disappeared with their eight-year-old daughter. He won custody after successfully claiming that Ono was an unfit mother due to her drug use.[32] Ono's ex-husband changed Kyoko's name to "Ruth Holman" and subsequently raised the girl in an organization known as the Church of the Living Word.[36] Ono and Lennon searched for Kyoko for years, but to no avail. She would finally see Kyoko again in 1998.[32]

Ono's first contact with any member of the Beatles occurred when she visited Paul McCartney at his home in London to obtain a Lennon–McCartney song manuscript for a book John Cage was working on, Notations.[37] McCartney declined to give her any of his manuscripts but suggested that Lennon might oblige.[37] Lennon later gave Ono the original handwritten lyrics to "The Word".[38]

Ono and Lennon first met on November 7, 1966, at the Indica Gallery in London, where she was preparing Unfinished Paintings, her conceptual art exhibit about interactive painting and sculpture. They were introduced by gallery owner John Dunbar.[39] One piece, Ceiling Painting/Yes Painting, had a ladder painted white with a magnifying glass at the top. When Lennon climbed the ladder, he looked through the magnifying glass and was able to read the word YES which was written in miniature. He greatly enjoyed this experience as it was a positive message, whereas most concept art he encountered at the time was anti-everything.[40]

Lennon was also intrigued by Ono's Hammer a Nail where viewers were invited to hammer a nail into a wooden board painted white. Although the exhibition had not yet opened, Lennon wanted to hammer a nail into the clean board, but Ono stopped him. Dunbar asked her, "Don't you know who this is? He's a millionaire! He might buy it." Ono feigned not knowing of the Beatles (even as she had gone to see Paul McCartney asking for a Beatle song score), but relented on the condition that Lennon pay her five shillings, to which Lennon replied, "I'll give you an imaginary five shillings and hammer an imaginary nail in."[40][41]

In a 2002 interview, Ono said, "I was very attracted to him. It was a really strange situation."[42] Ono started writing to Lennon, sending him her conceptual artworks, and soon the two began corresponding. In September 1967, Lennon sponsored Ono's solo Half-A-Wind Show, at Lisson Gallery in London.[43] When Lennon's wife Cynthia asked for an explanation of why Ono was telephoning them at home, he told her that Ono was only trying to obtain money for her "avant-garde bullshit".[44]

In early 1968, while the Beatles were making their visit to India, Lennon wrote the song "Julia" and included a reference to Ono: "Ocean child calls me", referring to the translation of Yoko's Japanese spelling.[17] In May 1968, while his wife was on holiday in Greece, Lennon invited Ono to visit. They spent the night recording a selection of avant-garde tape loops,[43] after which, he said, they "made love at dawn".[45] The recordings made by the two during this session ultimately became their first collaborative album, the musique concrete work Unfinished Music No. 1: Two Virgins. When Lennon's wife returned home, she found Ono wearing her bathrobe and drinking tea with Lennon, who simply said, "Oh, hi."[46]

On September 24 and 25, 1968, Lennon wrote and recorded "Happiness Is a Warm Gun",[47] which contains sexual references to Ono. Ono became pregnant, but had a miscarriage of a male child on November 21, 1968, a few weeks after Lennon's divorce from Cynthia was granted.[48][49] On December 12, 1968, Lennon and Ono participated in the BBC documentary about The Rolling Stones, The Rolling Stones Rock And Roll Circus, along with several other high-profile musicians. Lennon performed his Beatles composition "Yer Blues" towards the end, with an improvised vocal performance by Ono rounding out the set.[50] The film would not be released until 1996, due to the death of The Rolling Stones' founding member Brian Jones a few months after it was shot.

During the final two years of the Beatles, Lennon and Ono created and attended public protests against the Vietnam War. They collaborated on a series of avant-garde recordings, beginning in 1968 with Unfinished Music No.1: Two Virgins, which notoriously featured an unretouched image of the two artists nude on the front cover. The same year, the couple contributed an experimental sound collage to The Beatles' self-titled "White Album" called "Revolution 9", with Ono contributing additional vocals to "Birthday",[51] and one lead vocal line on "The Continuing Story of Bungalow Bill", marking the only occasion in a Beatles recording in which a woman sings lead vocals.[52]

On March 20, 1969, Lennon and Ono were married at the registry office in Gibraltar and spent their honeymoon in Amsterdam, campaigning with a week-long bed-in for peace. They planned another bed-in in the US, but were denied entry to the country.[53] They held one instead at the Queen Elizabeth Hotel in Montreal, where they recorded "Give Peace a Chance".[54][55] Lennon later stated his regrets about feeling "guilty enough to give McCartney credit as co-writer on my first independent single instead of giving it to Yoko, who had actually written it with me."[56] The couple often combined advocacy with performance art, such as in "bagism", first introduced during a Vienna press conference, where they satirised prejudice and stereotyping by wearing a bag over their entire bodies. Lennon detailed this period in the Beatles' song "The Ballad of John and Yoko".[57]

During the Amsterdam Bed In press conference, Yoko also earned controversy in the Jewish community for saying during the press conference that, "If I was a Jewish girl in Hitler's day, I would approach him and become his girlfriend. After 10 days in bed, he would come to my way of thinking. This world needs communication. And making love is a great way of communicating."[58]

Lennon changed his name by deed poll on April 22, 1969, switching out Winston for Ono as a middle name. Although he used the name John Ono Lennon after that, official documents referred to him as John Winston Ono Lennon.[59] The couple settled at Tittenhurst Park at Sunninghill, Berkshire, in southeast England.[60] When Ono was injured in a car crash, Lennon arranged for a king-sized bed to be brought to the recording studio as he worked on the Beatles' last recorded album, Abbey Road.[61]

After "The Ballad of John and Yoko", Lennon and Ono decided it would be better to form their own band to release their newer, more personally representative art work, rather than release the sound material as the Beatles.[62] To this end they formed the Plastic Ono Band, a name based on their 1968 Fluxus conceptual art project of the same name.[63] Plastic Ono Band was first conceived of by Ono in 1967 as an idea for an art exhibition in Berlin[64] but The Plastic Ono Band was first physically realized in 1968 as a multi-media machine maquette by John Lennon, also called The Plastic Ono Band.[63] In 1968, Lennon and Ono began a personal and artistic relationship in which they decided to credit their future endeavours as work of The Plastic Ono Band. Under that name Ono and Lennon collaborated on several art exhibitions, concerts, happenings and experimental noise music recording projects, including a sound and light installation in the Apple press office that consisted of four perspex columns, each representing a member of the Beatles, with one holding a tape recorder and amplifier, the second a closed-circuit TV and camera, the third a record player and amplifier, and the fourth a miniature light show and loud speaker. Soon after The Plastic Ono Band name was used in recording and releasing somewhat more standard rock-based albums.

In July 1969, Lennon's first solo single, "Give Peace a Chance" (backed by Ono's "Remember Love") was the first release to be credited to the Plastic Ono Band. It was followed in October by "Cold Turkey" (backed by Ono's "Don't Worry Kyoko (Mummy's Only Looking for her Hand in the Snow)"). The singles were followed in December by the group's first album, Live Peace in Toronto 1969, which had been recorded live at the Toronto Rock and Roll Revival festival in September. This incarnation of the group also consisted of guitarist Eric Clapton, bass player Klaus Voormann, and drummer Alan White. The first half of their performance consisted of rock standards. During the second half, Ono took to the microphone and performed two original feedback-driven compositions, "Don't Worry Kyoko" and "John John (Let's Hope For Peace)",[65][66] constituting the entirety of the second half of the live album.

Ono released her first solo album, Yoko Ono/Plastic Ono Band in 1970, as a companion piece to Lennon's John Lennon/Plastic Ono Band. The two albums also had companion covers: Ono's featured a photo of her leaning on Lennon, and Lennon's a photo of him leaning on Ono. Her album included raw, harsh vocals, which bore a similarity with sounds in nature (especially those made by animals) and free jazz techniques used by wind and brass players. Performers included Ornette Coleman, other renowned free jazz performers, and Ringo Starr. Some songs on the album consisted of wordless vocalizations, in a style that would influence Meredith Monk[67] and other musical artists who have used screams and vocal noise instead of words. The album reached No. 182 on the US charts.[68]

When Lennon was invited to play with Frank Zappa at the Fillmore (then the Filmore West) on June 5, 1971, Ono joined them.[69] Later that year, she released Fly, a double album. In it, she explored slightly more conventional psychedelic rock with tracks including "Midsummer New York" and "Mind Train", in addition to a number of Fluxus experiments. She also received minor airplay with the ballad "Mrs. Lennon". The track "Don't Worry, Kyoko (Mummy's Only Looking for Her Hand in the Snow)" was an ode to Ono's missing daughter,[70] and featured Eric Clapton on guitar. In 1971, while studying with Maharishi Mahesh Yogi in Majorca, Spain, Ono's ex-husband Anthony Cox accused Ono of abducting their daughter Kyoko from the kindergarten. They reached an out of court agreement and the charges were dismissed. Cox eventually moved away with Kyoko.[71] Ono would not see her daughter until 1998.[32] During this time, she wrote "Don't Worry Kyoko", which also appears on Lennon and Ono's album Live Peace in Toronto 1969, in addition to Fly. Kyoko is also referenced in the first line of "Happy Xmas (War Is Over)" when Yoko whispers "Happy Christmas, Kyoko", followed by Lennon whispering, "Happy Christmas, Julian."[72] The song reached No. 4 in the UK, where its release was delayed until 1972, and has periodically reemerged on the UK Singles Chart. Originally a protest song about the Vietnam War, "Happy Xmas (War Is Over)" has since become a Christmas standard.[73][74] That August the couple appeared together at a benefit in Madison Square Garden with Roberta Flack, Stevie Wonder, and Sha Na Na for mentally disabled children organized by WABC-TV's Geraldo Rivera.[75]

In a 2018 issue of Portland Magazine, editor Colin W. Sargent writes of interviewing Yoko while she was visiting Portland, Maine, in 2005. She spoke of driving along the coast with Lennon and dreamed of buying a house in Maine. "We talked excitedly in the car. We were looking for a house on the water… We did examine the place! We kept driving north along the water until I don't really remember the name of the town. We went quite a ways up, actually, because it was so beautiful."[76]

In 1973, Ono recorded a single, "Joseijoi Banzai, Parts 1 and 2" with musicians billed as the Plastic Ono Band and Elephants Memory and released it only in Japan. She cheered feminism by combining lyrics inspired by Japanese war songs with Pop rhythms, signalling a new direction.[77]

After the Beatles disbanded in 1970, Ono and Lennon lived together in London and then moved permanently to Manhattan to escape tabloid racism towards Ono.[78] Their relationship became strained because Lennon was facing deportation due to drug charges that had been filed against him in England, and because of Ono's separation from her daughter. The couple separated in July 1973, with Ono pursuing her career and Lennon living between Los Angeles and New York with personal assistant May Pang; Ono had given her blessing to Lennon and Pang's relationship.[79][80]

By December 1974, Lennon and Pang considered buying a house together, and he refused to accept Ono's phone calls. The next month, Lennon agreed to meet Ono, who claimed to have found a cure for smoking. After the meeting, Lennon failed to return home or call Pang. When she telephoned the next day, Ono told her Lennon was unavailable, because he was exhausted after a hypnotherapy session. Two days later, Lennon reappeared at a joint dental appointment with Pang; he was stupefied and confused to such an extent that Pang believed he had been brainwashed. He told her his separation from Ono was now over, though Ono would allow him to continue seeing her as his mistress, which did not happen.[81]

Ono and Lennon's son, Sean, was born on October 9, 1975, Lennon's 35th birthday. Following the birth of Sean, both Lennon and Ono took a hiatus from the music industry, with Lennon becoming a stay-at-home dad to care for his infant son. Sean has followed in his parents' footsteps with a career in music; he performs solo work, works with Ono and formed bands as, The Ghost of a Saber Tooth Tiger and The Claypool Lennon Delirium.[82]

In early 1980, Lennon heard Lene Lovich and the B-52's' "Rock Lobster" while on vacation in Bermuda. The latter reminded him of Ono's musical sound and he took this as an indication that she had reached the mainstream[83] (the band had in fact been influenced by Ono).[84] Ono and Lennon began trading songs over the phone with each other, quickly accumulating enough material to record. The emerging album was structured as a dialogue, and was to be credited to John Lennon and Yoko Ono as a duo. It would also mark the return of Lennon to the public eye after a five-year absence, as well as a public reconciliation of Ono and Lennon.

Double Fantasy was released on November 17, 1980, and received tepid initial reviews, with much of the criticism centering on the idealization of Lennon and Ono's marriage and supposed domestic bliss. However, the reception and the legacy of the album would be forever linked with what happened just weeks after its release.

On the evening of December 8, 1980, Lennon and Ono were at the Record Plant Studio and working on Ono's song "Walking on Thin Ice". When they returned to their Manhattan home The Dakota, Lennon was shot dead by Mark David Chapman, who had been stalking Lennon for two months. Yoko cradled the dying Lennon in her arms, and for a time afterward, lived in constant fear of her own and her son Sean's assassination.

After John's death, the interior decorator Sam Havadtoy moved in to support her.[85] "Walking on Thin Ice (For John)" was released as a single less than a month later, and became Ono's first chart success as a solo artist, peaking at No. 58 and gaining significant underground airplay. Double Fantasy received an instant critical reappraisal, eventually becoming a landmark album of the 1980s, and winning Ono the 1981 Grammy Award for Album of the Year at the 24th Annual Grammy Awards.

In 1981, she released the album Season of Glass, which featured the striking cover photo of Lennon's bloody spectacles next to a half-filled glass of water, with a window overlooking Central Park in the background. This photograph sold at an auction in London in April 2002 for about $13,000. In the liner notes to Season of Glass, Ono explained that the album was not dedicated to Lennon because "he would have been offended—he was one of us." The album received highly favorable reviews[3] and reflected the public's mood after Lennon's assassination.[86][87]

In 1982, she released It's Alright. The cover featured Ono in her wrap-around sunglasses, looking towards the sun, while on the back the ghost of Lennon looks over her and their son. The album scored minor chart success[88] and airplay with the single "Never Say Goodbye".[89]

In 1984, a tribute album titled Every Man Has a Woman was released, featuring a selection of songs written by Ono performed by artists such as Elvis Costello, Roberta Flack, Eddie Money, Rosanne Cash, and Harry Nilsson.[90]  Later that year, Ono and Lennon's final album, Milk and Honey, was released as a mixture of unfinished Lennon recordings from the Double Fantasy sessions, and new Ono recordings.[91] It peaked at No. 3 in the UK and No. 11 in the U.S.,[92] going gold in both countries as well as in Canada.[93][94][95]

Ono funded the construction and maintenance of the Strawberry Fields memorial in Manhattan's Central Park, directly across from the Dakota, which was the scene of the murder. It was officially dedicated on October 9, 1985, which would have been his 45th birthday.[96]

Ono's final album of the 1980s was Starpeace, a concept album that she intended as an antidote to Ronald Reagan's "Star Wars" missile defense system. On the cover, a warm, smiling Ono holds the Earth in the palm of her hand. Starpeace became Ono's most successful non-Lennon effort. The single "Hell in Paradise" was a hit, reaching No. 16 on the US dance charts and No. 26 on the Billboard Hot 100, and the video, directed by Zbigniew Rybczyński received major airplay on MTV and won "Most Innovative Video" at Billboard Music Video Awards in 1986.[97]

In 1986, Ono set out on a goodwill world tour for Starpeace, primarily visiting Eastern European countries.[43]

In 1990, Ono collaborated with music consultant Jeff Pollack to honor what would have been Lennon's 50th birthday with a worldwide broadcast of "Imagine". Over 1,000 stations in over 50 countries participated in the simultaneous broadcast. Ono felt the timing was perfect, considering the escalating conflicts in the Middle East, Eastern Europe, and Germany.[98]

Ono went on a musical hiatus following the release of Starpeace, until she signed with Rykodisc in 1992 and released the comprehensive six-disc box set Onobox.[43] The box set included remastered highlights from Ono's solo albums and previously unreleased material from the 1974 "lost weekend" sessions.[99] She also released a one-disc sampler of highlights from Onobox, simply titled Walking on Thin Ice.[100] That year, she sat down for an extensive interview with music journalist Mark Kemp for a cover story in the alternative music magazine Option. The story took a revisionist look at Ono's music for a new generation of fans more accepting of her role as a pioneer in the blending of pop and avant-garde music.[101]

In 1994, Ono produced her own off-Broadway musical entitled New York Rock, which featured Broadway renditions of her songs.[102]

In 1995, Ono released Rising, a collaboration with her son Sean and his then-band, Ima. Rising spawned a world tour that traveled through Europe, Japan, and the United States. The following year, she collaborated with various alternative rock musicians for an EP entitled Rising Mixes.[103] Guest remixers of Rising material included Cibo Matto, Ween, Tricky, and Thurston Moore.[104]

In 1997, Rykodisc reissued Ono's catalog of solo recordings on CD, from Yoko Ono/Plastic Ono Band through Starpeace.[43] Ono and her engineer Rob Stevens personally remastered the audio, and various bonus tracks were added, including outtakes, demos, and live cuts.[105][106][107] In the same year, Ono and the BMI Foundation established an annual music competition program for songwriters of contemporary musical genres to honor John Lennon's memory and his large creative legacy.[108] Over $350,000 has been given through BMI Foundation's John Lennon Scholarships to talented young musicians in the United States, making it one of the most respected awards for emerging songwriters.[citation needed]

In 2000, Ono founded the John Lennon Museum in Saitama, Japan, which housed over 130 pieces of Lennon and Beatles memorabilia from Ono's private collection. The museum closed in 2010.[8]

Ono's feminist concept album Blueprint for a Sunrise was released in 2001.[109] A month after the 9/11 attacks, Ono organized the concert "Come Together: A Night for John Lennon's Words and Music" at Radio City Music Hall. Hosted by the actor Kevin Spacey and featuring Lou Reed, Cyndi Lauper and Nelly Furtado, it raised money for September 11 relief efforts[42] and aired on TNT and the WB.[110]

In 2002, Ono joined the B-52's in New York for their 25th anniversary concerts; she came out for the encore and performed "Rock Lobster" with the band.[84] In March 2002, she was present with Cherie Blair at the unveiling of a seven-foot statue of Lennon to mark the renaming of Liverpool airport to Liverpool John Lennon Airport.[42]

Beginning in 2003, some DJs remixed other Ono songs for dance clubs. For the remix project, she dropped her first name and became known simply as "ONO", in response to the "Oh, no!" jokes that dogged her throughout her career. Ono had great success with new versions of "Walking on Thin Ice", remixed by top DJs and dance artists including Pet Shop Boys,[111] Orange Factory,[112] Peter Rauhofer, and Danny Tenaglia.[113] In April 2003, Ono's Walking on Thin Ice (Remixes) was rated number 1 on Billboard's Dance/Club Play chart, gaining Ono her first no. 1 hit. She would have a second no. 1 hit on the same chart in November 2004 with "Everyman... Everywoman...", a reworking of her song "Every Man Has a Woman Who Loves Him".

During the Liverpool Biennial in 2004, Ono flooded the city with two images on banners, bags, stickers, postcards, flyers, posters and badges: one of a woman's naked breast, the other of the same model's vulva. During her stay in Lennon's city of birth, she said she was "astounded" by the city's renaissance.[114] The piece, titled My Mummy Was Beautiful, was dedicated to Lennon's mother, Julia, who had died when he was a teenager.[115] According to Ono, the work was meant to be innocent, not shocking; she was attempting to replicate the experience of a baby looking up at its mother's body, those parts of the mother's body being a child's introduction to humanity.[116]

Ono performed at the opening ceremony for the 2006 Winter Olympic Games in Turin, Italy,[117] Like many of the other performers during the ceremony, she wore white to symbolize the snow of winter. She read a free verse poem calling for world peace[118] as an introduction to Peter Gabriel's performance of "Imagine".[119][120]

On December 13, 2006, one of Ono's bodyguards was arrested after he was allegedly taped trying to extort $2 million from her. The tapes revealed that he threatened to release private conversations and photographs.[121] His bail was revoked, and he pleaded not guilty to two counts of attempted grand larceny.[122] On February 16, 2007, a deal was reached where extortion charges were dropped, and he pleaded guilty to attempted grand larceny in the third degree, a felony, and was sentenced to the 60 days that he had already spent in jail. After reading an unapologetic statement, he was released to immigration officials because he had also been found guilty of overstaying his business visa.[123]

Ono released the album Yes, I'm a Witch in February 2007, a collection of remixes and covers from her back catalog by various artists including The Flaming Lips, Cat Power, Anohni, DJ Spooky, Porcupine Tree, and Peaches, along with a special edition of Yoko Ono/Plastic Ono Band.[124] Yes I'm a Witch was critically well received.[125] A similar compilation of Ono dance remixes entitled Open Your Box was also released in April.[126]

On June 26, 2007, Ono appeared on Larry King Live along with McCartney, Starr and Olivia Harrison.[127] She headlined the Pitchfork Music Festival in Chicago on July 14, 2007, performing a full set that mixed music and performance art. She sang "Mulberry", a song about her time in the countryside after the Japanese collapse in World War II for only the third time ever, with Thurston Moore: She had previously performed the song with John and with Sean. On October 9 of that year, the Imagine Peace Tower on Viðey Island in Iceland, dedicated to peace and to Lennon, was turned on with her, Sean, Ringo, and Olivia in attendance.[128] Each year between October 9 and December 8, it projects a vertical beam of light into the sky.

Ono returned to Liverpool for the 2008 Liverpool Biennial, where she unveiled Sky Ladders in the ruins of Church of St Luke (which was largely destroyed during World War II and now stands roofless as a memorial to those killed in the Liverpool Blitz).[129] Two years later, on March 31, 2009, she went to the inauguration of the exhibition "Imagine: The Peace Ballad of John & Yoko" to mark the 40th anniversary of the Lennon-Ono Bed-In at the Queen Elizabeth Hotel in Montreal, Canada, from May 26 to June 2, 1969. The hotel had been doing steady business with the room they stayed in for over 40 years.[130] That year Ono became a grandmother when Emi was born to her daughter Kyoko.[131]

Ono had further Dance/Club Play chart no. 1 hits with "No No No" in January 2008, and "Give Peace a Chance" the following August. In June 2009, at the age of 76, Ono scored her fifth no. 1 hit on the Dance/Club Play chart with "I'm Not Getting Enough".[3]

In May 2009, she designed a T-shirt for the second Fashion Against AIDS campaign and collection of HIV/AIDS awareness, NGO Designers Against AIDS, and H&M, with the statement "Imagine Peace" depicted in 21 languages.[132] Ono appeared onstage at Microsoft's June 1, 2009, E3 Expo press conference with Olivia Harrison, Paul McCartney, and Ringo Starr to promote the Beatles: Rock Band video game,[133] which was universally praised by critics.[134][135] Ono appeared on the Basement Jaxx album Scars, featuring on the single "Day of the Sunflowers (We March On)".[136] In the same year, she became an honorary patron to Alder Hey Charity,[137] and created an exhibit called "John Lennon: The New York City Years" for the NYC Rock and Roll Hall of Fame Annex. The exhibit used music, photographs, and personal items to depict Lennon's life in New York. A portion of the cost of each ticket was donated to Spirit Foundation, a charitable foundation set up and founded by Lennon and Ono.[138][139][140]

In 2009, Ono recorded Between My Head and the Sky, which was her first album to be released as "Yoko Ono/Plastic Ono Band" since 1973's Feeling the Space. The all-new Plastic Ono Band lineup included Sean Lennon, Cornelius, and Yuka Honda.[141][142] On February 16, 2010, Sean organized a concert at the Brooklyn Academy of Music called "We Are Plastic Ono Band", at which Yoko performed her music with Sean, Clapton, Klaus Voormann and Jim Keltner for the first time since the 1970s. Guests including Bette Midler, Paul Simon and his son Harper, and principal members of Sonic Youth and the Scissor Sisters interpreted her songs in their own styles.[143]

On April 1, 2010, she was named the first "Global Autism Ambassador" by the Autism Speaks organization. She had created an artwork the year before for autism awareness and allowed it to be auctioned off in 67 parts to benefit the organization.[144] In April 2010, RCRD LBL made available free downloads of Junior Boys' mix of "I'm Not Getting Enough", a single originally released 10 years prior on Blueprint for a Sunrise.[145] That song and "Wouldnit (I'm a Star)", released September 14,[146] made it to Billboard's end of the year list of favorite Dance/Club songs at No. 23 and No. 50 respectively.[147][148]

Ono appeared with Starr on July 7 at New York's Radio City Music Hall in celebration of Starr's 70th birthday, performing "With a Little Help from My Friends" and "Give Peace a Chance".[149] On September 16, she and Sean attended the opening of Julian Lennon's photo exhibition at the Morrison Hotel in New York City,[150] appearing for the first time photos with Cynthia and Julian.[151] She also promoted his work on her website.[152] On October 2, Ono and the Plastic Ono Band performed at the Orpheum Theatre in Los Angeles, with special guest Lady Gaga, whom she deeply admires.[153]

On February 18, 2011 (her 78th birthday), Ono took out a full-page advert in the UK free newspaper Metro for "Imagine Peace 2011". It took the form of an open letter, inviting people to think of, and wish for, peace.[154] With son Sean, she held a benefit concert to aid in the relief efforts for earthquake and tsunami-ravaged Japan on March 27 in New York City.[155] The effort raised a total of $33,000.[155] The same year, "Move on Fast" became her sixth consecutive number-one hit on the Billboard Hot Dance Club Songs chart and her eighth number-one hit overall.[156] She also collaborated with The Flaming Lips on an EP entitled The Flaming Lips with Yoko Ono/Plastic Ono Band.

In July 2011, she visited Japan to support earthquake and tsunami victims and tourism to the country. During her visit, Ono gave a lecture and performance entitled "The Road of Hope" at Tokyo's Mori Art Museum, during which she painted a large calligraphy piece entitled "Dream" to help raise funds for construction of the Rainbow House, an institution for the orphans of the Great East Japan earthquake.[157] She also collected the 8th Hiroshima Art Prize for her contributions to art and for peace, that she was awarded the year prior.[158]

In January 2012, a Ralphi Rosario mix of her 1995 song "Talking to the Universe" became her seventh consecutive No. 1 hit on the Billboard Hot Dance Club Songs chart.[159] In March of the same year, she was awarded the 20,000-euro ($26,400) Oskar Kokoschka Prize in Austria.[160] From June 19 to September 9, her work To the Light was exhibited at the Serpentine Gallery in London.[161] It was held in conjunction with the London 2012 Festival, a 12-week UK-wide celebration featuring internationally renowned artists from Midsummer's Day (June 21) to the final day of the Paralympic Games on September 9.[162] The album Yokokimthurston was also released in 2012, featuring a collaboration with Thurston Moore and Kim Gordon of Sonic Youth. AllMusic characterized it as "focused and risk-taking" and "above the best" of the couple's experimental music, with Ono's voice described as "one-of-a-kind".[163]

On June 29, 2012, Ono received a lifetime achievement award at the Dublin Biennial. During this (her second) trip to Ireland (the first was with John before they married), she visited the crypt of Irish leader Daniel O'Connell at Glasnevin Cemetery and Dún Laoghaire, from where Irish departed for England to escape the famine.[164] In February 2013, Ono accepted the Rainer Hildebrandt Medal at Berlin's Checkpoint Charlie Museum, awarded to her and Lennon for their lifetime of work for peace and human rights.[165] The next month, she tweeted an anti-gun message with the Season of Glass image of Lennon's bloodied glasses on what would have been her and Lennon's 44th anniversary, noting that guns have killed more than 1 million people since Lennon's death in 1980.[166] She was also given a Congressional citation from the Philippines for her monetary aid to the victims of typhoon Pablo,[167] as well as her donation to disaster relief efforts after typhoon Ondoy in 2009 and assistance of Filipino schoolchildren.[168]

In 2013, she and the Plastic Ono Band released the LP Take Me to the Land of Hell, which featured numerous guests including Yuka Honda, Cornelius, Hirotaka "Shimmy" Shimizu, mi-gu's Yuko Araki, Wilco's Nels Cline, Tune-Yards, Questlove, Lenny Kravitz, and Ad-Rock and Mike D of the Beastie Boys. In June 2013, she curated the Meltdown festival in London, where she played two concerts, one with the Plastic Ono Band,[169] and the second on backing vocals during Siouxsie Sioux's rendition of "Walking on Thin Ice" at the Double Fantasy show.[170] In July, OR Books published Ono's sequel to 1964's Grapefruit, another book of instruction-based 'action poems' this time entitled, Acorn.

Her online video for "Bad Dancer" released in November 2013, which featured some of these guests, was well-liked by the press.[171][172] By the end of the year she had become one of three artists with two songs in the Top 20 Dance/Club and had two consecutive number 1 hits on Billboard's Hot Dance Club Play Charts. On the strength of the singles "Hold Me" (Featuring Dave Audé) and "Walking on Thin Ice", the then-80-year-old beat Katy Perry, Robin Thicke and her friend Lady Gaga.[111]

In 2014, "Angel" was Ono's twelfth number one on the US Dance chart.[173] Yoko Ono/Plastic Ono Band continued to perform live into 2015.

On February 16, 2016, Manimal Vinyl released Yes, I'm a Witch Too, which features remixes from Moby, Death Cab For Cutie, Sparks, and Miike Snow. Like its predecessor, Yes, I'm a Witch Too received critical acclaim. On February 26, 2016, Ono was hospitalized after suffering what was rumored to be a possible stroke. It was later announced that she was experiencing extreme symptoms of the flu.[174]  On September 6, 2016, Secretly Canadian announced that they would be re-issuing 11 of Ono's albums from 1968 to 1985; Unfinished Music No. 1: Two Virgins through Starpeace.[175][176] In December 2016, Billboard magazine named her the 11th most successful dance club artist of all time.[2]

In October 2018, Ono released Warzone, which included new versions of previously recorded tracks including "Imagine".[177]

In a piece for the New Yorker published in November 2021, it was noted that Ono had "withdrawn from public life", with her son Sean now acting as the public representative for the family's interests in the Beatles' business.[178]

Ono was a pioneer of conceptual art and performance art. A seminal performance work is Cut Piece, first performed in 1964 at the Yamaichi Concert Hall in Kyoto, Japan. The piece consisted of Ono, dressed in her best suit, kneeling on a stage with a pair of scissors in front of her. She invited and then instructed audience members to join her on stage and cut pieces of her clothing off. Confronting issues of gender, class and cultural identity, Ono sat silently until the piece concluded at her discretion.[179] The piece was subsequently performed at the Sogetsu Art Centre in Tokyo that same year, New York's Carnegie Hall in 1965 and London's Africa Center as part of the Destruction in Art Symposium in 1966.[180] Of the piece, John Hendricks wrote in the catalogue to Ono's Japan Society retrospective: "[Cut Piece] unveils the interpersonal alienation that characterizes social relationships between subjects, dismantling the disinterested Kantian aesthetic model ... It demonstrates the reciprocity between artists, objects, and viewers and the responsibility beholders have to the reception and preservation of art."[179]

Other performers of the piece have included Charlotte Moorman and John Hendricks.[179] Ono reprised the piece in Paris in 2003, in the low post-9/11 period between the US and France, saying she hoped to show that this is "a time where we need to trust each other".[16] In 2013, the Canadian singer Peaches reprised it at the multi-day Meltdown festival at the Southbank Centre in London, which Ono curated.[181]

Ono's small book titled Grapefruit is another seminal piece of conceptual art. First published in 1964, the book reads as a set of instructions through which the work of art is completed-either literally or in the imagination of the viewer participant. One example is "Hide and Seek Piece: Hide until everybody goes home. Hide until everybody forgets about you. Hide until everybody dies." Grapefruit has been published several times, most widely distributed by Simon & Schuster in 1971, who reprinted it again in 2000. David Bourdon, art critic for The Village Voice and Vogue, called Grapefruit "one of the monuments of conceptual art of the early 1960s". He noted that her conceptual approach was made more acceptable when white male artists like Joseph Kosuth and Lawrence Weiner came in and "did virtually the same things" she did, and that her take also has a poetic and lyrical side that sets it apart from the work of other conceptual artists.[182]

Ono would enact many of the book's scenarios as performance pieces throughout her career, which formed the basis for her art exhibitions, including the highly publicized retrospective exhibition, This Is Not Here in 1971 at the Everson Museum in Syracuse, New York,[183] that was nearly closed when it was besieged by excited Beatles fans, who broke several of the art pieces and flooded the toilets.[184] It was her last major exhibition until 1989's Yoko Ono: Objects, Films retrospective at the Whitney.[182]

Nearly fifty years later in July 2013, she released a sequel to Grapefruit, another book of instructions, Acorn via OR Books.[185]

a 20-piece collection conjoining short instructional texts by Ono with Maciunas' graphic illustrations. First printed in "3 newspaper events for the price of $1", the No. 7, February 1966 issue of the Fluxus magazine cc V TRE, the compilation underscores the Fluxus idea that anyone can make art. These amusing pieces find meaning in the humorous dialogue that exists between Ono's instructions and Maciunas' skillful treatment of text with relation to pictorial motifs.[186]

Ono was also an experimental filmmaker who made 16 films between 1964 and 1972, gaining particular renown for a 1966 Fluxus film called simply No. 4, often referred to as Bottoms.[187][188] The 80-minute film consists of a series of close-ups of human buttocks walking on a treadmill. The screen is divided into four almost equal sections by the elements of the gluteal cleft and the horizontal gluteal crease. The soundtrack consists of interviews with those who are being filmed, as well as those considering joining the project. In 1996, the watch manufacturing company Swatch produced a limited edition watch that commemorated this film.[189] She also collaborated with Lennon on the film Fly (1970), the soundtrack of which appeared on her 1971 album Fly; and on Up Your Legs Forever, a quasi-sequel to No. 4.[190]

In March 2004, the ICA London, showed most of her films from this period in their exhibition The Rare Films of Yoko Ono.[187] She also acted in an obscure exploitation film in 1965, Satan's Bed.[188]

Another example of Ono's participatory art was her Wish Tree project, in which a tree native to the installation site is installed. Her 1996 Wish Piece had the following instructions:

Her Wish Tree installation in the Sculpture Garden of the Museum of Modern Art, New York, established in July 2010, has attracted contributions from all over the world. Other installation locations include London;[192] St. Louis;[193] Washington, D.C.; San Francisco; Copenhagen;[194] the Stanford University campus in Palo Alto, California;[16] Japan;[195] Venice;[196] Dublin;[164] and, Miami at the Fairchild Tropical Botanic Garden in 2010.[197]


In 2014 Ono's Imagine Peace exhibit opened at the Bob Rauschenburg Gallery at Florida SouthWestern State College in Fort Myers, Florida. Ono installed a billboard on U.S. Route 41 in Fort Myers to promote the show and peace.[198] 
 When the exhibit closed, wishes that had been placed on the installed Wish Trees were sent to the Imagine Peace Tower in Iceland and added to the millions of wishes already there.[199] Imagine Peace was also installed in Houston in 2011 through the Deborah Colton Gallery, returning in 2016.[200]

One of two pieces Ono installed as part of the 2014 Folkestone Triennial, Earth Peace originally consisted of many parts and appeared in many locations and media around Folkestone, including posters, stickers, billboards and badges.[201] Three of the pieces remain in Folkestone, on loan to the town and part of the Creative Folkestone Artworks collection. These include an inscribed stone, a flag – which is flown on an annual basis on International Peace Day and a beacon of light installed on the dome roof of The Grand in Folkestone Leas. Ono's beacon flashes a morse code message, "Earth Peace", across the English Channel.[202]

The second of Ono's 2014 Folkestone Triennial pieces and now also on loan to the town as part of the Folkestone Artworks collection, Skyladder is displayed in two locations – on a high wall of the Quarterhouse bar and in the staircase of the Folkestone public library. Skyladder takes the form of an artistic 'instruction' or invitation to the people of Folkestone and beyond. The instruction reads: "Audience should bring a ladder they like. Colour it. Word it. Take pictures of it. Keep adding things to it. And send it as a postcard to a friend"[201].

In 2015, Ono created the piece Arising in Venice. As part of the exhibition Personal Structures, organised by Global Art Affairs, the installation was on view from June 1 through November 24, 2013, at the European Cultural Centre's Palazzo Bembo.[203] In this feminist work of art, female silicon bodies were burnt in the Venetian lagoon, evoking the imagery of mythical phoenixes. When asked for the resemblance between the naming of her record Rising and this piece, Ono responded: "Rising was telling all people that it is time for us to rise and fight for our rights. But in the process of fighting together, women are still being treated separately in an inhuman way. It weakens the power of men and women all together. I hope Arising will wake up Women Power, and make us, men and women, heal together."[204]

In October 2016, Ono unveiled her first permanent art installation in the United States; the collection is located in Jackson Park, Chicago and promotes peace.[205] Ono was inspired during a visit to the Garden of the Phoenix in 2013 and feels a connection to the city of Chicago.[206]

Participating in Lower Manhattan's River to River Festival in 2019, Ono presented her participatory installation Add Color (Refugee Boat) (1960/2019). The work comprises a white room with a white rowing boat in it, which were both covered by messages and drawings from members of the audience throughout the festival. Through the participatory nature of the work, the artist emphasised the need for solidarity and the history of immigrants and refugees in the United States. Refugee Boat belongs to Ono's Add Color Painting series, first enacted in 1960, which invites the audience to make marks over the designated objects, often white.[207]

John Lennon once described his wife as "the world's most famous unknown artist: everybody knows her name, but nobody knows what she does".[208] Her circle of friends in the New York art world has included Kate Millett, Nam June Paik,[209] Dan Richter, Jonas Mekas,[210] Merce Cunningham,[211] Judith Malina,[212] Erica Abeel, Fred DeAsis, Peggy Guggenheim,[213] Betty Rollin, Shusaku Arakawa, Adrian Morris, Stefan Wolpe,[211] Keith Haring, and Andy Warhol[212] (she was one of the speakers at Warhol's 1987 funeral), as well as George Maciunas and La Monte Young. In addition to Mekas, Maciunas, Young, and Warhol, she has also collaborated with DeAsis, Yvonne Rainer[214] and Zbigniew Rybczyński.[215]

In 1989, the Whitney Museum held a retrospective of her work, Yoko Ono: Objects, Films, marking Ono's reentry into the New York art world after a hiatus. At the suggestion of Ono's live-in companion at the time, interior decorator Sam Havadtoy, she recast her old pieces in bronze after some initial reluctance. "I realized that for something to move me so much that I would cry, there's something there. There seemed like a shimmering air in the 60s when I made these pieces, and now the air is bronzified. Now it's the 80s, and bronze is very 80s in a way – solidity, commodity, all of that. For someone who went through the 60s revolution, there has of course been an incredible change. . . . I call the pieces petrified bronze. That freedom, all the hope and wishes are in some ways petrified."[182]

Over a decade later, in 2001, Y E S YOKO ONO, a 40-year retrospective of Ono's work, received the International Association of Art Critics USA Award for Best Museum Show Originating in New York City, considered one of the highest accolades in the museum profession. YES refers to the title of a 1966 sculptural work by Yoko Ono, shown at Indica Gallery, London: viewers climb a ladder to read the word "yes", printed on a small canvas suspended from the ceiling.[216] The exhibition's curator Alexandra Munroe wrote that "John Lennon got it, on his first meeting with Yoko: when he climbed the ladder to peer at the framed paper on the ceiling, he encountered the tiny word YES. 'So it was positive. I felt relieved.'"[217] The exhibition traveled to 13 museums in the U.S., Canada, Japan, and Korea from 2000 through 2003.[218] In 2001, she received an honorary Doctorate of Laws from Liverpool University and, in 2002, was presented with the honorary degree of Doctor of Fine Arts from Bard College[219]
and the Skowhegan Medal for work in assorted media.[220] The next year, she was awarded the fifth MOCA Award to Distinguished Women in the Arts from the Museum of Contemporary Art Los Angeles.[221] In 2005, she received a lifetime achievement award from the Japan Society of New York, which had hosted Yes Yoko Ono[222] and where she had worked in the late 1950s and early 1960s.

In 2008, she showed a large retrospective exhibition, Between The Sky and My Head, at the Kunsthalle Bielefeld, Bielefeld, Germany, and the Baltic Centre for Contemporary Art in Gateshead, England. The following year, she showed a selection of new and old work as part of her show "Anton's Memory" in Venice, Italy.[223] She also received a Golden Lion Award for lifetime achievement from the Venice Biennale in 2009.[224] In 2012, Ono held a major exhibition of her work To The Light at the Serpentine Galleries, London.[225] She was also the winner of the 2012 Oskar Kokoschka Prize, Austria's highest award for applied contemporary art.[226] In February 2013, to coincide with her 80th birthday, the largest retrospective of her work, Half-a-Wind Show, opened at the Schirn Kunsthalle Frankfurt[1][227]
and travelled to Denmark's Louisiana Museum of Modern Art,[193] Austria's Kunsthalle Krems, and Spain's Guggenheim Museum Bilbao.[227][228]
In 2014 she contributed several artworks to the triennial Folkestone art festival. In 2015 the Museum of Modern Art in New York City held a retrospective exhibition of her early work, "Yoko Ono: One Woman Show, 1960– 1971".[229] In 2015, Yoko Ono received the European Cultural Centre Art Award for her continuing efforts to promote "Imagine Peace".[230]

Ono has been an activist for peace and human rights since the 1960s. After she and Lennon married in Gibraltar, they held a March 1969 "Bed-in for Peace" in their honeymoon suite at the Amsterdam Hilton Hotel.[43] The newlyweds were eager to talk about and promote world peace; they wore pajamas and invited visitors and members of the press. Two months later, Ono and Lennon held another Bed-in at the Queen Elizabeth Fairmont in Montreal, where they recorded their first single, "Give Peace A Chance".[54] The song became a top-20 hit for the newly christened Plastic Ono Band.[231] Other performance/demonstrations with John included "bagism", iterations with John of the Bag Pieces she introduced in the early 1960s,[232] which encouraged a disregard for physical appearance in judging others.[15] In December 1969, the two continued to spread their message of peace with billboards in 12 major world cities reading "WAR IS OVER! If You Want It – Happy Christmas from John & Yoko".[233]

In the 1970s, Ono and Lennon became close to many radical, counterculture leaders, including Bobby Seale,[234] Abbie Hoffman, Jerry Rubin,[235] Michael X,[236] John Sinclair (for whose rally in Michigan they flew to sing Lennon's song "Free John Sinclair" that effectively released the poet from prison),[237] Angela Davis, and street musician David Peel.[238] Friend and Sexual Politics author Kate Millett has said Ono inspired her activism.[239] Ono and Lennon appeared on The Mike Douglas Show, taking over hosting duties for a week.[240] Ono spoke at length about the evils of racism and sexism. She remained outspoken in her support of feminism, and openly bitter about the racism she had experienced from rock fans, especially in the UK.[78] Her reception within the US media was not much better. For example, an Esquire article of the period was titled "John Rennon's Excrusive Gloupie"[43] and featured an unflattering David Levine cartoon.[241]

After the Columbine High School massacre in 1999, Ono paid for billboards to be put up in New York City and Los Angeles that bore the image of Lennon's blood-splashed spectacles.[42] Early in 2002[242] she paid about £150,000 ($213,375)[243] for a billboard in Piccadilly Circus with a line from Lennon's "Imagine": "Imagine all the people living life in peace."[42] Later the same year, she inaugurated a peace award, the LennonOno Grant for Peace, by giving $50,000 (£31,900) in prize money originally to artists living "in regions of conflict". The award is given out every two years in conjunction with the lighting of the Imagine Peace Tower, and was first given to Israeli and Palestinian artists. Its program has since expanded to include writers, such as Michael Pollan and Alice Walker, activists such as Vandana Shiva and Pussy Riot, organizations such as New York's Center for Constitutional Rights, even an entire country (Iceland).[244]

On Valentine's Day 2003, which was the eve of the Iraqi invasion by the US and UK, Ono heard about a couple, Andrew and Christine Gale, who were holding a love-in protest in their tiny bedroom in Addingham, West Yorkshire. She phoned them and said, "It's good to speak to you. We're supporting you. We're all sisters together."[245] The couple said that songs like "Give Peace a Chance" and "Imagine" inspired their protest. In 2004, Ono remade her song "Everyman..... Everywoman....." to support same-sex marriage, releasing remixes that included "Every Man Has a Man Who Loves Him" and "Every Woman Has a Woman Who Loves Her".[246]

In August 2011, she made the documentary film about the Bed-ins Bed Peace available for free on YouTube,[247] and as part of her website "Imagine Peace".[248]
In January 2013, the 79-year-old Ono, along with Sean Lennon and Susan Sarandon, took to rural Pennsylvania in a bus under the banner of the Artists Against Fracking group she and Sean created with Mark Ruffalo in August 2012 to protest against hydraulic fracturing.[249] Other group members include Lady Gaga and Alec Baldwin.[250]

Ono promotes her art and shares inspirational messages and images[251] through a robust and active Twitter, Instagram, and Facebook presence. In April 2014 her Twitter followers reached 4.69 million,[252][non-primary source needed] while her Instagram followers exceeded 99,000. Her tweets are short instructional poems,[253] comments on media and politics,[254] and notes about performances.[255]

In 1987, Ono travelled to Moscow to participate in the "International Forum for a Nuclear-free World and for the Survival of Mankind". She also visited Leningrad, where she met with members of the local John Lennon memorial club. Among these members was Kolya Vasin, who was considered the biggest Beatles fan in the Soviet Union.[256][257][258]

Public appreciation of Ono's work has shifted over time and was helped by a retrospective at a Whitney Museum branch in 1989[259] and the 1992 release of the six-disc box set Onobox. Retrospectives of her artwork have also been presented at the Japan Society in New York City in 2001,[260] in Bielefeld, Germany, and the UK in 2008, Frankfurt, and Bilbao, Spain, in 2013 and The Museum of Modern Art in New York City in 2015. She received a Golden Lion Award for lifetime achievement from the Venice Biennale in 2009 and the 2012 Oskar Kokoschka Prize, Austria's highest award for applied contemporary art.

In January 2021, Ono was one of the founders of The Coda Collection, a service that launched in the U.S. via Amazon Prime Video Channels on February 18, 2021, the day Ono turned 88. The Coda Collection will feature a slew of music documentaries and concert films. Jim Spinello will run The Coda Channel. Yoko Ono added, "John Lennon was always on the cutting edge of music and culture. The Coda Collection will be a new way for fans to connect on a deeper level."[261][262]

For many years, Ono was frequently criticized by both the press and the public. She was blamed for the breakup of the Beatles[263][164] and repeatedly criticized for her influence over Lennon and his music.[15] Her experimental art was also not popularly accepted.[3] The British press was particularly negative and prompted the couple's move to the US.[78] As late as December 1999, NME was calling her a "no-talent charlatan".[4]

Lennon and Ono were injured in a car crash in June 1969, partway through recording Abbey Road. According to journalist Barry Miles, a bed with a microphone was then installed in the studio so that Ono could make artistic comments about the album.[264] Miles thought Ono's continual presence in the studio during the latter part of the Beatles' career put strain on Lennon's relationship with the other band members. George Harrison got into a shouting match with Lennon after Ono took one of his chocolate digestive biscuits without asking.[265]

The English press dubbed Ono "the woman who broke up the Beatles",[263] which had been foreseen by Paul McCartney in 1969 during the group's rehearsals for their film and album Let It Be, when he said "It's going to be such an incredible sort of comical thing, like, in fifty years' time, you know: 'They broke up 'cause Yoko sat on an amp.'"[178] In an interview with Dick Cavett, Lennon explicitly denied that Ono broke up the Beatles,[266] and Harrison said during an interview with Cavett that the problems within the group began long before Ono came onto the scene.[267] Ono herself has said that the Beatles broke up without any direct involvement from her, adding "I don't think I could have tried even to break them up."[268]

While the Beatles were together, every song written by Lennon or McCartney was credited as Lennon–McCartney regardless of whether the song was a collaboration or written solely by one of the two (except for those appearing on their first album, Please Please Me, which originally credited the songs to McCartney–Lennon). In 1976, McCartney released a live album called Wings over America, which credited the five Beatles tracks as P. McCartney–J. Lennon compositions, but neither Lennon nor Ono objected. After Lennon's death, however, McCartney again attempted to change the order to McCartney–Lennon for songs that were solely or predominantly written by him, such as "Yesterday",[269][clarification needed] but Ono would not allow it, saying she felt this broke an agreement that the two had made while Lennon was still alive, and the surviving former Beatle argued that such an agreement never existed. A spokesman for Ono said McCartney was making "an attempt to rewrite history".[270]

In a Rolling Stone interview in 1987, Ono pointed out McCartney's place in the disintegration of the band.[271] On the 1998 John Lennon anthology, Lennon Legend, the composer credit of "Give Peace a Chance" was changed to "John Lennon" from its original composing credit of "Lennon–McCartney". Although Lennon wrote the song during his tenure with the Beatles, it was both written and recorded without the help of the band, and released as Lennon's first independent single under the "Plastic Ono Band" moniker. Lennon subsequently expressed regret that he had not given co-writing credit to Ono instead, who actually helped him write the song.[54] In 2002, McCartney released another live album, Back in the U.S. Live 2002, and the 19 Beatles songs included are described as "composed by Paul McCartney and John Lennon", which reignited the debate over credits with Ono. Her spokesperson Elliott Mintz called it "an attempt to rewrite history". Nevertheless, Ono did not sue.[270]

In 1995, after the Beatles released Lennon's "Free as a Bird" and "Real Love", with demos provided by Ono, McCartney and his family collaborated with her and Sean to create the song "Hiroshima Sky Is Always Blue", which commemorates the 50th anniversary of the atomic bombing of that Japanese city. Ono publicly compared Lennon to Wolfgang Amadeus Mozart, while McCartney, she said, more closely resembled his less-talented rival Antonio Salieri.[272] This remark infuriated McCartney's wife Linda, who was dying from breast cancer at the time. When Linda died less than a year later, McCartney did not invite Ono to his wife's memorial service in Manhattan.[42]

Accepting an award at the 2005 Q Awards, Ono mentioned that Lennon had once felt insecure about his songwriting. She had responded, "You're a good songwriter. It's not June with spoon that you write. You're a good singer, and most musicians are probably a little bit nervous about covering your songs."[273]

In an October 2010 interview, Ono spoke about Lennon's "lost weekend" and her subsequent reconciliation with him. She credited McCartney with helping save her marriage to John. "I want the world to know that it was a very touching thing that [Paul] did for John."[274] While visiting Ono in March 1974, McCartney, on leaving, asked "[W]hat will make you come back to John?" McCartney subsequently passed her response to Lennon while visiting him in Los Angeles. "John often said he didn't understand why Paul did this for us, but he did." In 2012, McCartney revealed that he did not blame Ono for the breakup of the Beatles and credited Ono with inspiring much of Lennon's post-Beatles work.[275]

Ono had a difficult relationship with her stepson Julian, but the relationship improved over the years. He expressed disappointment at her handling of Lennon's estate, and at the difference between his upbringing and Sean's, adding, "when Dad gave up music for a couple of years to be with Sean, why couldn't he do that with me?"[276] Julian was left out of his father's will, and he battled Ono in court for years, settling in 1996 for an unspecified amount that the media reported was "believed to" be in the area of £20 million, which Julian has denied.[42]

He has said that he is his "mother's boy", which Ono has cited as the reason why she was never able to get close to him: "Julian and I tried to be friends. Of course, if he's too friendly with me, then I think that it hurts his other relatives. He was very loyal to his mother. That was the first thing that was in his mind."[151] Nevertheless, she and Sean attended the opening of Julian's photo exhibition at the Morrison Hotel in New York City in 2010,[150] appearing for the first time for photos with Cynthia and Julian.[151] She also promoted the exhibition on her website.

Julian and his half-brother Sean are close.[152]

Mary Beth Edelson's Some Living American Women Artists/Last Supper (1972) appropriated Leonardo da Vinci's The Last Supper, with the heads of notable women artists collaged over the heads of Christ and his apostles; Ono was among those notable women artists. This image, addressing the role of religious and art historical iconography in the subordination of women, became "one of the most iconic images of the feminist art movement".[277][278]

The post-punk rock band Death of Samantha, founded in 1983, named themselves after a song from Ono's 1972 album Approximately Infinite Universe, also called "Death of Samantha".[279]

Canadian rock band Barenaked Ladies' debut single was "Be My Yoko Ono", first released in 1990 and later appearing on their 1992 album Gordon.[280] The lyrics are "a shy entreaty to a potential girlfriend, caged in terms that self-deflatingly compare himself to one of pop music's foremost geniuses". It also has a "sarcastic imitation of Yoko Ono's unique vocal style in the bridge".[281]

In 2000, American folk singer Dar Williams recorded a song titled "I Won't Be Your Yoko Ono".[282] Bryan Wawzenek of the website Ultimate Classic Rock described the song as "us[ing] John and Yoko as a starting point for exploring love, and particularly, love between artists".[283]

The British band Elbow mentioned Ono in their song "New York Morning" from their 2014 album The Take Off and Landing of Everything ("Oh, my giddy aunt, New York can talk / It's the modern Rome and folk are nice to Yoko"). In response Ono posted an open letter to the band on her website, thanking them and reflecting on her and Lennon's relationship with the city.[284] In Public Enemy's song "Bring the Noise", Chuck D and Flavor Flav rap, "Beat is for Sonny Bono/Beat is for Yoko Ono!"[285][286] Ono's name also appears in the lyrics of the Le Tigre song "Hot Topic", and the Tally Hall song "&".[287]

In The Simpsons' episode 1 of season 5, "Homer's Barbershop Quartet", Barney who is in Homer's band, has creative disputes within the group when he falls in love with a Japanese conceptual artist who resembles Yoko Ono.[288]

Ono was a central theme in English comedian James Acaster's 2013 show Lawnmower, which was nominated for the Edinburgh Comedy Award for Best Show.[289][290]

The anime Detective Conan features a recurring character named Yoko Okino, who is a pop star and actress based on Yoko Ono.